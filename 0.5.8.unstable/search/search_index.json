{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the ZenML Api Docs Core The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. Pipelines A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. Materializers Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. Steps A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. Artifact Stores An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . Constants Config The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. Post Execution After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. Logger Utils The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. Exceptions ZenML specific exception definitions Orchestrators An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. Container Registries Io The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . Visualizers The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. Artifacts Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. Integrations The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. Metadata Stores The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. Enums Stacks A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it.","title":"ZenML"},{"location":"#welcome-to-the-zenml-api-docs","text":"","title":"Welcome to the ZenML Api Docs"},{"location":"#core","text":"The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command.","title":"Core"},{"location":"#pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator.","title":"Pipelines"},{"location":"#materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class.","title":"Materializers"},{"location":"#steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator.","title":"Steps"},{"location":"#artifact-stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores .","title":"Artifact Stores"},{"location":"#constants","text":"","title":"Constants"},{"location":"#config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions.","title":"Config"},{"location":"#post-execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object.","title":"Post Execution"},{"location":"#logger","text":"","title":"Logger"},{"location":"#utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions.","title":"Utils"},{"location":"#exceptions","text":"ZenML specific exception definitions","title":"Exceptions"},{"location":"#orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline.","title":"Orchestrators"},{"location":"#container-registries","text":"","title":"Container Registries"},{"location":"#io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"Io"},{"location":"#visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves.","title":"Visualizers"},{"location":"#artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer.","title":"Artifacts"},{"location":"#integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch.","title":"Integrations"},{"location":"#metadata-stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store.","title":"Metadata Stores"},{"location":"#enums","text":"","title":"Enums"},{"location":"#stacks","text":"A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it.","title":"Stacks"},{"location":"cli/","text":"Cli zenml.cli special ZenML CLI The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process. How to use the CLI Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that). Beginning a Project In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version. Loading and using pre-built examples If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart Using integrations Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME Customizing your Metadata Store The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME Customizing your Artifact Store The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME Customizing your Orchestrator An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME Customizing your Container Registry The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete Administering the Stack The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"CLI docs"},{"location":"cli/#cli","text":"","title":"Cli"},{"location":"cli/#zenml.cli","text":"","title":"cli"},{"location":"cli/#zenml.cli--zenml-cli","text":"The ZenML CLI tool is usually downloaded and installed via PyPI and a pip install zenml command. Please see the Installation & Setup section above for more information about that process.","title":"ZenML CLI"},{"location":"cli/#zenml.cli--how-to-use-the-cli","text":"Our CLI behaves similarly to many other CLIs for basic features. In order to find out which version of ZenML you are running, type: .. code:: bash zenml version If you ever need more information on exactly what a certain command will do, use the --help flag attached to the end of your command string. For example, to get a sense of all of the commands available to you while using the zenml command, type: .. code:: bash zenml --help If you were instead looking to know more about a specific command, you can type something like this: .. code:: bash zenml metadata-store register --help This will give you information about how to register a metadata store. (See below for more on that).","title":"How to use the CLI"},{"location":"cli/#zenml.cli--beginning-a-project","text":"In order to start working on your project, initialize a ZenML repository within your current directory with ZenML\u2019s own config and resource management tools: .. code:: bash zenml init This is all you need to begin using all the MLOps goodness that ZenML provides! By default, zenml init will install its own hidden .zen folder inside the current directory from which you are running the command. You can also pass in a directory path manually using the --repo_path option: .. code:: bash zenml init --repo_path /path/to/dir If you wish to specify that you do not want analytics to be transmitted back to ZenML about your usage of the tool, pass in False to the --analytics_opt_in option: .. code:: bash zenml init --analytics_opt_in false If you wish to delete all data relating to your project from the directory, use the zenml clean command. This will: delete all pipelines delete all artifacts delete all metadata Note that the clean command is not implemented for the current version.","title":"Beginning a Project"},{"location":"cli/#zenml.cli--loading-and-using-pre-built-examples","text":"If you don\u2019t have a project of your own that you\u2019re currently working on, or if you just want to play around a bit and see some functional code, we\u2019ve got your back! You can use the ZenML CLI tool to download some pre-built examples. We know that working examples are a great way to get to know a tool, so we\u2019ve made some examples for you to use to get started. (This is something that will grow as we add more). To list all the examples available to you, type: .. code:: bash zenml example list If you want more detailed information about a specific example, use the info subcommand in combination with the name of the example, like this: .. code:: bash zenml example info quickstart If you want to pull all the examples into your current working directory (wherever you are executing the zenml command from in your terminal), the CLI will create a zenml_examples folder for you if it doesn\u2019t already exist whenever you use the pull subcommand. The default is to copy all the examples, like this: .. code:: bash zenml example pull If you\u2019d only like to pull a single example, add the name of that example (for example, quickstart ) as an argument to the same command, as follows: .. code:: bash zenml example pull quickstart If you would like to force-redownload the examples, use the --force or -f flag as in this example: .. code:: bash zenml example pull --force This will redownload all the examples afresh, using the same version of ZenML as you currently have installed. If for some reason you want to download examples corresponding to a previous release of ZenML, use the --version or -v flag to specify, as in the following example: .. code:: bash zenml example pull --force --version 0.3.8 If you wish to run the example, allowing the ZenML CLI to do the work of setting up whatever dependencies are required, use the run subcommand: .. code:: bash zenml example run quickstart","title":"Loading and using pre-built examples"},{"location":"cli/#zenml.cli--using-integrations","text":"Integrations are the different pieces of a project stack that enable custom functionality. This ranges from bigger libraries like kubeflow for orchestration down to smaller visualization tools like facets . Our CLI is an easy way to get started with these integrations. To list all the integrations available to you, type: zenml integration list To see the requirements for a specific integration, use the requirements command: zenml integration requirements INTEGRATION_NAME If you wish to install the integration, using the requirements listed in the previous command, install allows you to do this for your local environment: zenml integration install INTEGRATION_NAME Note that if you don't specify a specific integration to be installed, the ZenML CLI will install all available integrations. Uninstalling a specific integration is as simple as typing: zenml integration uninstall INTEGRATION_NAME","title":"Using integrations"},{"location":"cli/#zenml.cli--customizing-your-metadata-store","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. By default ZenML initializes your repository with a metadata store kept on your local machine. If you wish to register a new metadata store, do so with the register command: .. code:: bash zenml metadata-store register METADATA_STORE_NAME --type METADATA_STORE_TYPE [--OPTIONS] If you wish to list the metadata stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml metadata-store list If you wish to delete a particular metadata store, pass the name of the metadata store into the CLI with the following command: .. code:: bash zenml metadata-store delete METADATA_STORE_NAME","title":"Customizing your Metadata Store"},{"location":"cli/#zenml.cli--customizing-your-artifact-store","text":"The artifact store is where all the inputs and outputs of your pipeline steps are stored. By default, ZenML initializes your repository with an artifact store with everything kept on your local machine. If you wish to register a new artifact store, do so with the register command: .. code:: bash zenml artifact-store register ARTIFACT_STORE_NAME --type ARTIFACT_STORE_TYPE [--OPTIONS] If you wish to list the artifact stores that have already been registered within your ZenML project / repository, type: .. code:: bash zenml artifact-store list If you wish to delete a particular artifact store, pass the name of the artifact store into the CLI with the following command: .. code:: bash zenml artifact-store delete ARTIFACT_STORE_NAME","title":"Customizing your Artifact Store"},{"location":"cli/#zenml.cli--customizing-your-orchestrator","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. By default, ZenML initializes your repository with an orchestrator that runs everything on your local machine. If you wish to register a new orchestrator, do so with the register command: .. code:: bash zenml orchestrator register ORCHESTRATOR_NAME --type ORCHESTRATOR_TYPE [--ORCHESTRATOR_OPTIONS] If you wish to list the orchestrators that have already been registered within your ZenML project / repository, type: .. code:: bash zenml orchestrator list If you wish to delete a particular orchestrator, pass the name of the orchestrator into the CLI with the following command: .. code:: bash zenml orchestrator delete ORCHESTRATOR_NAME","title":"Customizing your Orchestrator"},{"location":"cli/#zenml.cli--customizing-your-container-registry","text":"The container registry is where all the images that are used by a container-based orchestrator are stored. By default, a default ZenML local stack will not register a container registry. If you wish to register a new container registry, do so with the register command: zenml container-registry register REGISTRY_NAME --type REGISTRY_TYPE [--REGISTRY_OPTIONS] If you want the name of the current container registry, use the get command: zenml container-registry get To list all container registries available and registered for use, use the list command: zenml container-registry list For details about a particular container registry, use the describe command. By default (without a specific registry name passed in) it will describe the active or currently used container registry: zenml container-registry describe [REGISTRY_NAME] To delete a container registry (and all of its contents), use the delete command: zenml container-registry delete","title":"Customizing your Container Registry"},{"location":"cli/#zenml.cli--administering-the-stack","text":"The stack is a grouping of your artifact store, your metadata store and your orchestrator. With the ZenML tool, switching from a local stack to a distributed cloud environment can be accomplished with just a few CLI commands. To register a new stack, you must already have registered the individual components of the stack using the commands listed above. Use the zenml stack register command to register your stack. It takes four arguments as in the following example: .. code:: bash zenml stack register STACK_NAME -m METADATA_STORE_NAME -a ARTIFACT_STORE_NAME -o ORCHESTRATOR_NAME Each corresponding argument should be the name you passed in as an identifier for the artifact store, metadata store or orchestrator when you originally registered it. To list the stacks that you have registered within your current ZenML project, type: .. code:: bash zenml stack list To delete a stack that you have previously registered, type: .. code:: bash zenml stack delete STACK_NAME By default, ZenML uses a local stack whereby all pipelines run on your local computer. If you wish to set a different stack as the current active stack to be used when running your pipeline, type: .. code:: bash zenml stack set STACK_NAME This changes a configuration property within your local environment. To see which stack is currently set as the default active stack, type: .. code:: bash zenml stack get","title":"Administering the Stack"},{"location":"api_docs/artifact_stores/","text":"Artifact Stores zenml.artifact_stores special An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores . base_artifact_store Definition of an Artifact Store BaseArtifactStore ( BaseComponent ) pydantic-model Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. Source code in zenml/artifact_stores/base_artifact_store.py class BaseArtifactStore ( BaseComponent ): \"\"\"Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. \"\"\" path : str _ARTIFACT_STORE_DIR_NAME : str = \"artifact_stores\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseArtifactStore instance. Args: repo_path: Path to the repository of this artifact store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ARTIFACT_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @staticmethod def get_component_name_from_uri ( artifact_uri : str ) -> str : \"\"\"Gets component name from artifact URI. Args: artifact_uri: URI to artifact. Returns: Name of the component. \"\"\" return fileio . get_parent ( artifact_uri ) def resolve_uri_locally ( self , artifact_uri : str , path : Optional [ str ] = None ) -> str : \"\"\"Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri: uri to artifact. path: optional path to download to. If None, is inferred. Returns: Locally resolved uri. \"\"\" if not fileio . is_remote ( artifact_uri ): # It's already local return artifact_uri if path is None : # Create a unique path in local machine path = os . path . join ( GlobalConfig () . get_serialization_dir (), str ( self . uuid ), BaseArtifactStore . get_component_name_from_uri ( artifact_uri ), Path ( artifact_uri ) . stem , # unique ID from MLMD ) # Create if not exists and download fileio . create_dir_recursive_if_not_exists ( path ) fileio . copy_dir ( artifact_uri , path , overwrite = True ) return path class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_artifact_store_\" Config Configuration of settings. Source code in zenml/artifact_stores/base_artifact_store.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_artifact_store_\" __init__ ( self , repo_path , ** kwargs ) special Initializes a BaseArtifactStore instance. Parameters: Name Type Description Default repo_path str Path to the repository of this artifact store. required Source code in zenml/artifact_stores/base_artifact_store.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseArtifactStore instance. Args: repo_path: Path to the repository of this artifact store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ARTIFACT_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) get_component_name_from_uri ( artifact_uri ) staticmethod Gets component name from artifact URI. Parameters: Name Type Description Default artifact_uri str URI to artifact. required Returns: Type Description str Name of the component. Source code in zenml/artifact_stores/base_artifact_store.py @staticmethod def get_component_name_from_uri ( artifact_uri : str ) -> str : \"\"\"Gets component name from artifact URI. Args: artifact_uri: URI to artifact. Returns: Name of the component. \"\"\" return fileio . get_parent ( artifact_uri ) resolve_uri_locally ( self , artifact_uri , path = None ) Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Parameters: Name Type Description Default artifact_uri str uri to artifact. required path Optional[str] optional path to download to. If None, is inferred. None Returns: Type Description str Locally resolved uri. Source code in zenml/artifact_stores/base_artifact_store.py def resolve_uri_locally ( self , artifact_uri : str , path : Optional [ str ] = None ) -> str : \"\"\"Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri: uri to artifact. path: optional path to download to. If None, is inferred. Returns: Locally resolved uri. \"\"\" if not fileio . is_remote ( artifact_uri ): # It's already local return artifact_uri if path is None : # Create a unique path in local machine path = os . path . join ( GlobalConfig () . get_serialization_dir (), str ( self . uuid ), BaseArtifactStore . get_component_name_from_uri ( artifact_uri ), Path ( artifact_uri ) . stem , # unique ID from MLMD ) # Create if not exists and download fileio . create_dir_recursive_if_not_exists ( path ) fileio . copy_dir ( artifact_uri , path , overwrite = True ) return path local_artifact_store LocalArtifactStore ( BaseArtifactStore ) pydantic-model Artifact Store for local artifacts. Source code in zenml/artifact_stores/local_artifact_store.py class LocalArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for local artifacts.\"\"\" @validator ( \"path\" ) def must_be_local_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a local path.\"\"\" if any ([ v . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ]): raise ValueError ( \"Must be a local path.\" ) return v must_be_local_path ( v ) classmethod Validates that the path is a local path. Source code in zenml/artifact_stores/local_artifact_store.py @validator ( \"path\" ) def must_be_local_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a local path.\"\"\" if any ([ v . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ]): raise ValueError ( \"Must be a local path.\" ) return v","title":"Artifact Stores"},{"location":"api_docs/artifact_stores/#artifact-stores","text":"","title":"Artifact Stores"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores","text":"An artifact store is a place where artifacts are stored. These artifacts may have been produced by the pipeline steps, or they may be the data first ingested into a pipeline via an ingestion step. Definitions of the BaseArtifactStore class and the LocalArtifactStore that builds on it are in this module. Other artifact stores corresponding to specific integrations are to be found in the integrations module. For example, the GCPArtifactStore , used when running ZenML on Google Cloud Platform, is defined in integrations.gcp.artifact_stores .","title":"artifact_stores"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store","text":"Definition of an Artifact Store","title":"base_artifact_store"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore","text":"Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. Source code in zenml/artifact_stores/base_artifact_store.py class BaseArtifactStore ( BaseComponent ): \"\"\"Base class for all ZenML Artifact Store. Every ZenML Artifact Store should override this class. \"\"\" path : str _ARTIFACT_STORE_DIR_NAME : str = \"artifact_stores\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseArtifactStore instance. Args: repo_path: Path to the repository of this artifact store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ARTIFACT_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @staticmethod def get_component_name_from_uri ( artifact_uri : str ) -> str : \"\"\"Gets component name from artifact URI. Args: artifact_uri: URI to artifact. Returns: Name of the component. \"\"\" return fileio . get_parent ( artifact_uri ) def resolve_uri_locally ( self , artifact_uri : str , path : Optional [ str ] = None ) -> str : \"\"\"Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri: uri to artifact. path: optional path to download to. If None, is inferred. Returns: Locally resolved uri. \"\"\" if not fileio . is_remote ( artifact_uri ): # It's already local return artifact_uri if path is None : # Create a unique path in local machine path = os . path . join ( GlobalConfig () . get_serialization_dir (), str ( self . uuid ), BaseArtifactStore . get_component_name_from_uri ( artifact_uri ), Path ( artifact_uri ) . stem , # unique ID from MLMD ) # Create if not exists and download fileio . create_dir_recursive_if_not_exists ( path ) fileio . copy_dir ( artifact_uri , path , overwrite = True ) return path class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_artifact_store_\"","title":"BaseArtifactStore"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.Config","text":"Configuration of settings. Source code in zenml/artifact_stores/base_artifact_store.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_artifact_store_\"","title":"Config"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.__init__","text":"Initializes a BaseArtifactStore instance. Parameters: Name Type Description Default repo_path str Path to the repository of this artifact store. required Source code in zenml/artifact_stores/base_artifact_store.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseArtifactStore instance. Args: repo_path: Path to the repository of this artifact store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ARTIFACT_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs )","title":"__init__()"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.get_component_name_from_uri","text":"Gets component name from artifact URI. Parameters: Name Type Description Default artifact_uri str URI to artifact. required Returns: Type Description str Name of the component. Source code in zenml/artifact_stores/base_artifact_store.py @staticmethod def get_component_name_from_uri ( artifact_uri : str ) -> str : \"\"\"Gets component name from artifact URI. Args: artifact_uri: URI to artifact. Returns: Name of the component. \"\"\" return fileio . get_parent ( artifact_uri )","title":"get_component_name_from_uri()"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.base_artifact_store.BaseArtifactStore.resolve_uri_locally","text":"Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Parameters: Name Type Description Default artifact_uri str uri to artifact. required path Optional[str] optional path to download to. If None, is inferred. None Returns: Type Description str Locally resolved uri. Source code in zenml/artifact_stores/base_artifact_store.py def resolve_uri_locally ( self , artifact_uri : str , path : Optional [ str ] = None ) -> str : \"\"\"Takes a URI that points within the artifact store, downloads the URI locally, then returns local URI. Args: artifact_uri: uri to artifact. path: optional path to download to. If None, is inferred. Returns: Locally resolved uri. \"\"\" if not fileio . is_remote ( artifact_uri ): # It's already local return artifact_uri if path is None : # Create a unique path in local machine path = os . path . join ( GlobalConfig () . get_serialization_dir (), str ( self . uuid ), BaseArtifactStore . get_component_name_from_uri ( artifact_uri ), Path ( artifact_uri ) . stem , # unique ID from MLMD ) # Create if not exists and download fileio . create_dir_recursive_if_not_exists ( path ) fileio . copy_dir ( artifact_uri , path , overwrite = True ) return path","title":"resolve_uri_locally()"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store","text":"","title":"local_artifact_store"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store.LocalArtifactStore","text":"Artifact Store for local artifacts. Source code in zenml/artifact_stores/local_artifact_store.py class LocalArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for local artifacts.\"\"\" @validator ( \"path\" ) def must_be_local_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a local path.\"\"\" if any ([ v . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ]): raise ValueError ( \"Must be a local path.\" ) return v","title":"LocalArtifactStore"},{"location":"api_docs/artifact_stores/#zenml.artifact_stores.local_artifact_store.LocalArtifactStore.must_be_local_path","text":"Validates that the path is a local path. Source code in zenml/artifact_stores/local_artifact_store.py @validator ( \"path\" ) def must_be_local_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a local path.\"\"\" if any ([ v . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ]): raise ValueError ( \"Must be a local path.\" ) return v","title":"must_be_local_path()"},{"location":"api_docs/artifacts/","text":"Artifacts zenml.artifacts special Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer. base_artifact The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation BaseArtifact ( Artifact ) Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. Source code in zenml/artifacts/base_artifact.py class BaseArtifact ( Artifact ): \"\"\"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: - Upon creation, each artifact class needs to be given a unique TYPE_NAME. - Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. \"\"\" # TODO [ENG-172]: Write about the materializers TYPE_NAME : str = \"BaseArtifact\" # type: ignore[assignment] PROPERTIES : Dict [ str , Property ] = { # type: ignore[assignment] MATERIALIZER_PROPERTY_KEY : MATERIALIZER_PROPERTY , DATATYPE_PROPERTY_KEY : DATATYPE_PROPERTY , } _MLMD_ARTIFACT_TYPE : Any = None def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type __init__ ( self , * args , ** kwargs ) special Init method for BaseArtifact Source code in zenml/artifacts/base_artifact.py def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) set_zenml_artifact_type () classmethod Set the type of the artifact. Source code in zenml/artifacts/base_artifact.py @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type data_analysis_artifact DataAnalysisArtifact ( BaseArtifact ) Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. Source code in zenml/artifacts/data_analysis_artifact.py class DataAnalysisArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. \"\"\" TYPE_NAME = \"DataAnalysisArtifact\" data_artifact DataArtifact ( BaseArtifact ) Class for all ZenML data artifacts. Source code in zenml/artifacts/data_artifact.py class DataArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data artifacts.\"\"\" TYPE_NAME = \"DataArtifact\" model_artifact ModelArtifact ( BaseArtifact ) Class for all ZenML model artifacts. Source code in zenml/artifacts/model_artifact.py class ModelArtifact ( BaseArtifact ): \"\"\"Class for all ZenML model artifacts.\"\"\" TYPE_NAME = \"ModelArtifact\" schema_artifact SchemaArtifact ( BaseArtifact ) Class for all ZenML schema artifacts. Source code in zenml/artifacts/schema_artifact.py class SchemaArtifact ( BaseArtifact ): \"\"\"Class for all ZenML schema artifacts.\"\"\" TYPE_NAME = \"SchemaArtifact\" statistics_artifact StatisticsArtifact ( BaseArtifact ) Class for all ZenML statistics artifacts. Source code in zenml/artifacts/statistics_artifact.py class StatisticsArtifact ( BaseArtifact ): \"\"\"Class for all ZenML statistics artifacts.\"\"\" TYPE_NAME = \"StatisticsArtifact\" type_registry ArtifactTypeRegistry A registry to keep track of which datatypes map to which artifact types Source code in zenml/artifacts/type_registry.py class ArtifactTypeRegistry ( object ): \"\"\"A registry to keep track of which datatypes map to which artifact types\"\"\" def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_ def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ] __init__ ( self ) special Initialization with an empty registry Source code in zenml/artifacts/type_registry.py def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} get_artifact_type ( self , key ) Method to extract the list of artifact types given the data type Source code in zenml/artifacts/type_registry.py def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ] register_integration ( self , key , type_ ) Method to register an integration within the registry Parameters: Name Type Description Default key Type[Any] any datatype required type_ List[Type[BaseArtifact]] the list of artifact type that the given datatypes is associated with required Source code in zenml/artifacts/type_registry.py def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_","title":"Artifacts"},{"location":"api_docs/artifacts/#artifacts","text":"","title":"Artifacts"},{"location":"api_docs/artifacts/#zenml.artifacts","text":"Artifacts are the data that power your experimentation and model training. It is actually steps that produce artifacts, which are then stored in the artifact store. Artifacts are written in the signature of a step like so: def my_step(first_artifact: int, second_artifact: torch.nn.Module -> int: # first_artifact is an integer # second_artifact is a torch.nn.Module return 1 Artifacts can be serialized and deserialized (i.e. written and read from the Artifact Store) in various ways like TFRecords or saved model pickles, depending on what the step produces.The serialization and deserialization logic of artifacts is defined by the appropriate Materializer.","title":"artifacts"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact","text":"The below code is copied from the TFX source repo with minor changes. All credits go to the TFX team for the core implementation","title":"base_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact","text":"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: Upon creation, each artifact class needs to be given a unique TYPE_NAME. Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. Source code in zenml/artifacts/base_artifact.py class BaseArtifact ( Artifact ): \"\"\"Base class for all ZenML artifacts. Every implementation of an artifact needs to inherit this class. While inheriting from this class there are a few things to consider: - Upon creation, each artifact class needs to be given a unique TYPE_NAME. - Your artifact can feature different properties under the parameter PROPERTIES which will be tracked throughout your pipeline runs. \"\"\" # TODO [ENG-172]: Write about the materializers TYPE_NAME : str = \"BaseArtifact\" # type: ignore[assignment] PROPERTIES : Dict [ str , Property ] = { # type: ignore[assignment] MATERIALIZER_PROPERTY_KEY : MATERIALIZER_PROPERTY , DATATYPE_PROPERTY_KEY : DATATYPE_PROPERTY , } _MLMD_ARTIFACT_TYPE : Any = None def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs ) @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type","title":"BaseArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact.__init__","text":"Init method for BaseArtifact Source code in zenml/artifacts/base_artifact.py def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Init method for BaseArtifact\"\"\" self . set_zenml_artifact_type () super ( BaseArtifact , self ) . __init__ ( * args , ** kwargs )","title":"__init__()"},{"location":"api_docs/artifacts/#zenml.artifacts.base_artifact.BaseArtifact.set_zenml_artifact_type","text":"Set the type of the artifact. Source code in zenml/artifacts/base_artifact.py @classmethod def set_zenml_artifact_type ( cls ) -> None : \"\"\"Set the type of the artifact.\"\"\" type_name = cls . TYPE_NAME if not ( type_name and isinstance ( type_name , str )): raise ValueError ( ( \"The Artifact subclass %s must override the TYPE_NAME attribute \" \"with a string type name identifier (got %r instead).\" ) % ( cls , type_name ) ) artifact_type = metadata_store_pb2 . ArtifactType () artifact_type . name = type_name if cls . PROPERTIES : # Perform validation on PROPERTIES dictionary. if not isinstance ( cls . PROPERTIES , dict ): raise ValueError ( \"Artifact subclass %s .PROPERTIES is not a dictionary.\" % cls ) for key , value in cls . PROPERTIES . items (): if not ( isinstance ( key , ( str , bytes )) and isinstance ( value , Property ) ): raise ValueError ( ( \"Artifact subclass %s .PROPERTIES dictionary must have keys of \" \"type string and values of type artifact.Property.\" ) % cls ) # Populate ML Metadata artifact properties dictionary. for key , value in cls . PROPERTIES . items (): artifact_type . properties [ key ] = value . mlmd_type () # type: ignore[no-untyped-call] cls . _MLMD_ARTIFACT_TYPE = artifact_type","title":"set_zenml_artifact_type()"},{"location":"api_docs/artifacts/#zenml.artifacts.data_analysis_artifact","text":"","title":"data_analysis_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_analysis_artifact.DataAnalysisArtifact","text":"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. Source code in zenml/artifacts/data_analysis_artifact.py class DataAnalysisArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data analysis artifacts. This should act as a base class for all artifacts generated from processes such as data profiling, data drift analyses, model drift detection etc. \"\"\" TYPE_NAME = \"DataAnalysisArtifact\"","title":"DataAnalysisArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_artifact","text":"","title":"data_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.data_artifact.DataArtifact","text":"Class for all ZenML data artifacts. Source code in zenml/artifacts/data_artifact.py class DataArtifact ( BaseArtifact ): \"\"\"Class for all ZenML data artifacts.\"\"\" TYPE_NAME = \"DataArtifact\"","title":"DataArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.model_artifact","text":"","title":"model_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.model_artifact.ModelArtifact","text":"Class for all ZenML model artifacts. Source code in zenml/artifacts/model_artifact.py class ModelArtifact ( BaseArtifact ): \"\"\"Class for all ZenML model artifacts.\"\"\" TYPE_NAME = \"ModelArtifact\"","title":"ModelArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.schema_artifact","text":"","title":"schema_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.schema_artifact.SchemaArtifact","text":"Class for all ZenML schema artifacts. Source code in zenml/artifacts/schema_artifact.py class SchemaArtifact ( BaseArtifact ): \"\"\"Class for all ZenML schema artifacts.\"\"\" TYPE_NAME = \"SchemaArtifact\"","title":"SchemaArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.statistics_artifact","text":"","title":"statistics_artifact"},{"location":"api_docs/artifacts/#zenml.artifacts.statistics_artifact.StatisticsArtifact","text":"Class for all ZenML statistics artifacts. Source code in zenml/artifacts/statistics_artifact.py class StatisticsArtifact ( BaseArtifact ): \"\"\"Class for all ZenML statistics artifacts.\"\"\" TYPE_NAME = \"StatisticsArtifact\"","title":"StatisticsArtifact"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry","text":"","title":"type_registry"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry","text":"A registry to keep track of which datatypes map to which artifact types Source code in zenml/artifacts/type_registry.py class ArtifactTypeRegistry ( object ): \"\"\"A registry to keep track of which datatypes map to which artifact types\"\"\" def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {} def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_ def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ]","title":"ArtifactTypeRegistry"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.__init__","text":"Initialization with an empty registry Source code in zenml/artifacts/type_registry.py def __init__ ( self ) -> None : \"\"\"Initialization with an empty registry\"\"\" self . _artifact_types : Dict [ Type [ Any ], List [ Type [ \"BaseArtifact\" ]]] = {}","title":"__init__()"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.get_artifact_type","text":"Method to extract the list of artifact types given the data type Source code in zenml/artifacts/type_registry.py def get_artifact_type ( self , key : Type [ Any ]) -> List [ Type [ \"BaseArtifact\" ]]: \"\"\"Method to extract the list of artifact types given the data type\"\"\" return self . _artifact_types [ key ]","title":"get_artifact_type()"},{"location":"api_docs/artifacts/#zenml.artifacts.type_registry.ArtifactTypeRegistry.register_integration","text":"Method to register an integration within the registry Parameters: Name Type Description Default key Type[Any] any datatype required type_ List[Type[BaseArtifact]] the list of artifact type that the given datatypes is associated with required Source code in zenml/artifacts/type_registry.py def register_integration ( self , key : Type [ Any ], type_ : List [ Type [ \"BaseArtifact\" ]] ) -> None : \"\"\"Method to register an integration within the registry Args: key: any datatype type_: the list of artifact type that the given datatypes is associated with \"\"\" self . _artifact_types [ key ] = type_","title":"register_integration()"},{"location":"api_docs/config/","text":"Config zenml.config special The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions. config_keys ConfigKeys Class to validate dictionary configurations. Source code in zenml/config/config_keys.py class ConfigKeys : \"\"\"Class to validate dictionary configurations.\"\"\" @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , ) get_keys () classmethod Gets all the required and optional config keys for this class. Returns: Type Description Tuple[List[str], List[str]] A tuple (required, optional) which are lists of the required/optional keys for this class. Source code in zenml/config/config_keys.py @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional key_check ( config ) classmethod Checks whether a configuration dict contains all required keys and no unknown keys. Parameters: Name Type Description Default config Dict[str, Any] The configuration dict to verify. required Exceptions: Type Description AssertionError If the dictionary contains unknown keys or is missing any required key. Source code in zenml/config/config_keys.py @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , ) PipelineConfigurationKeys ( ConfigKeys ) Keys for a pipeline configuration dict. Source code in zenml/config/config_keys.py class PipelineConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a pipeline configuration dict.\"\"\" NAME = \"name\" STEPS = \"steps\" StepConfigurationKeys ( ConfigKeys ) Keys for a step configuration dict. Source code in zenml/config/config_keys.py class StepConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a step configuration dict.\"\"\" SOURCE_ = \"source\" PARAMETERS_ = \"parameters\" MATERIALIZERS_ = \"materializers\" global_config Global config for the ZenML installation. GlobalConfig ( BaseComponent ) pydantic-model Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. Source code in zenml/config/global_config.py class GlobalConfig ( BaseComponent ): \"\"\"Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. \"\"\" user_id : UUID = Field ( default_factory = uuid4 ) analytics_opt_in : bool = True def __init__ ( self , ** data : Any ): \"\"\"We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.\"\"\" super () . __init__ ( serialization_dir = get_global_config_directory (), ** data ) # At this point, if the serialization file does not exist we should # create it and dump our data. f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): self . _dump () def get_serialization_file_name ( self ) -> str : \"\"\"Gets the global config dir for installed package.\"\"\" return GLOBAL_CONFIG_NAME __init__ ( self , ** data ) special We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time. Source code in zenml/config/global_config.py def __init__ ( self , ** data : Any ): \"\"\"We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.\"\"\" super () . __init__ ( serialization_dir = get_global_config_directory (), ** data ) # At this point, if the serialization file does not exist we should # create it and dump our data. f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): self . _dump () get_serialization_file_name ( self ) Gets the global config dir for installed package. Source code in zenml/config/global_config.py def get_serialization_file_name ( self ) -> str : \"\"\"Gets the global config dir for installed package.\"\"\" return GLOBAL_CONFIG_NAME","title":"Config"},{"location":"api_docs/config/#config","text":"","title":"Config"},{"location":"api_docs/config/#zenml.config","text":"The config module contains classes and functions that manage user-specific configuration. ZenML's configuration is stored in a file called .zenglobal.json , located on the user's directory for configuration files. (The exact location differs from operating system to operating system.) The GlobalConfig class is the main class in this module. It provides a Pydantic configuration object that is used to store and retrieve configuration. This GlobalConfig object handles the serialization and deserialization of the configuration options that are stored in the file in order to persist the configuration across sessions.","title":"config"},{"location":"api_docs/config/#zenml.config.config_keys","text":"","title":"config_keys"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys","text":"Class to validate dictionary configurations. Source code in zenml/config/config_keys.py class ConfigKeys : \"\"\"Class to validate dictionary configurations.\"\"\" @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , )","title":"ConfigKeys"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys.get_keys","text":"Gets all the required and optional config keys for this class. Returns: Type Description Tuple[List[str], List[str]] A tuple (required, optional) which are lists of the required/optional keys for this class. Source code in zenml/config/config_keys.py @classmethod def get_keys ( cls ) -> Tuple [ List [ str ], List [ str ]]: \"\"\"Gets all the required and optional config keys for this class. Returns: A tuple (required, optional) which are lists of the required/optional keys for this class. \"\"\" keys = { key : value for key , value in cls . __dict__ . items () if not isinstance ( value , classmethod ) and not isinstance ( value , staticmethod ) and not callable ( value ) and not key . startswith ( \"__\" ) } required = [ v for k , v in keys . items () if not k . endswith ( \"_\" )] optional = [ v for k , v in keys . items () if k . endswith ( \"_\" )] return required , optional","title":"get_keys()"},{"location":"api_docs/config/#zenml.config.config_keys.ConfigKeys.key_check","text":"Checks whether a configuration dict contains all required keys and no unknown keys. Parameters: Name Type Description Default config Dict[str, Any] The configuration dict to verify. required Exceptions: Type Description AssertionError If the dictionary contains unknown keys or is missing any required key. Source code in zenml/config/config_keys.py @classmethod def key_check ( cls , config : Dict [ str , Any ]) -> None : \"\"\"Checks whether a configuration dict contains all required keys and no unknown keys. Args: config: The configuration dict to verify. Raises: AssertionError: If the dictionary contains unknown keys or is missing any required key. \"\"\" assert isinstance ( config , dict ), \"Please specify a dict for {} \" . format ( cls . __name__ ) # Required and optional keys for the config dict required , optional = cls . get_keys () # Check for missing keys missing_keys = [ k for k in required if k not in config . keys ()] assert len ( missing_keys ) == 0 , \"Missing key(s) {} in {} \" . format ( missing_keys , cls . __name__ ) # Check for unknown keys unknown_keys = [ k for k in config . keys () if k not in required and k not in optional ] assert ( len ( unknown_keys ) == 0 ), \"Unknown key(s) {} in {} . Required keys : {} \" \"Optional Keys: {} \" . format ( unknown_keys , cls . __name__ , required , optional , )","title":"key_check()"},{"location":"api_docs/config/#zenml.config.config_keys.PipelineConfigurationKeys","text":"Keys for a pipeline configuration dict. Source code in zenml/config/config_keys.py class PipelineConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a pipeline configuration dict.\"\"\" NAME = \"name\" STEPS = \"steps\"","title":"PipelineConfigurationKeys"},{"location":"api_docs/config/#zenml.config.config_keys.StepConfigurationKeys","text":"Keys for a step configuration dict. Source code in zenml/config/config_keys.py class StepConfigurationKeys ( ConfigKeys ): \"\"\"Keys for a step configuration dict.\"\"\" SOURCE_ = \"source\" PARAMETERS_ = \"parameters\" MATERIALIZERS_ = \"materializers\"","title":"StepConfigurationKeys"},{"location":"api_docs/config/#zenml.config.global_config","text":"Global config for the ZenML installation.","title":"global_config"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig","text":"Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. Source code in zenml/config/global_config.py class GlobalConfig ( BaseComponent ): \"\"\"Class definition for the global config. Defines global data such as unique user ID and whether they opted in for analytics. \"\"\" user_id : UUID = Field ( default_factory = uuid4 ) analytics_opt_in : bool = True def __init__ ( self , ** data : Any ): \"\"\"We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.\"\"\" super () . __init__ ( serialization_dir = get_global_config_directory (), ** data ) # At this point, if the serialization file does not exist we should # create it and dump our data. f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): self . _dump () def get_serialization_file_name ( self ) -> str : \"\"\"Gets the global config dir for installed package.\"\"\" return GLOBAL_CONFIG_NAME","title":"GlobalConfig"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.__init__","text":"We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time. Source code in zenml/config/global_config.py def __init__ ( self , ** data : Any ): \"\"\"We persist the attributes in the config file. For the global config, we want to persist the data as soon as it is initialized for the first time.\"\"\" super () . __init__ ( serialization_dir = get_global_config_directory (), ** data ) # At this point, if the serialization file does not exist we should # create it and dump our data. f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): self . _dump ()","title":"__init__()"},{"location":"api_docs/config/#zenml.config.global_config.GlobalConfig.get_serialization_file_name","text":"Gets the global config dir for installed package. Source code in zenml/config/global_config.py def get_serialization_file_name ( self ) -> str : \"\"\"Gets the global config dir for installed package.\"\"\" return GLOBAL_CONFIG_NAME","title":"get_serialization_file_name()"},{"location":"api_docs/constants/","text":"Constants zenml.constants handle_bool_env_var ( var , default = False ) Converts normal env var to boolean Source code in zenml/constants.py def handle_bool_env_var ( var : str , default : bool = False ) -> bool : \"\"\"Converts normal env var to boolean\"\"\" value = os . getenv ( var ) if value in [ \"1\" , \"y\" , \"yes\" , \"True\" , \"true\" ]: return True return default handle_int_env_var ( var , default = 0 ) Converts normal env var to int Source code in zenml/constants.py def handle_int_env_var ( var : str , default : int = 0 ) -> int : \"\"\"Converts normal env var to int\"\"\" value = os . getenv ( var , \"\" ) try : return int ( value ) except ( ValueError , TypeError ): return default","title":"Constants"},{"location":"api_docs/constants/#constants","text":"","title":"Constants"},{"location":"api_docs/constants/#zenml.constants","text":"","title":"constants"},{"location":"api_docs/constants/#zenml.constants.handle_bool_env_var","text":"Converts normal env var to boolean Source code in zenml/constants.py def handle_bool_env_var ( var : str , default : bool = False ) -> bool : \"\"\"Converts normal env var to boolean\"\"\" value = os . getenv ( var ) if value in [ \"1\" , \"y\" , \"yes\" , \"True\" , \"true\" ]: return True return default","title":"handle_bool_env_var()"},{"location":"api_docs/constants/#zenml.constants.handle_int_env_var","text":"Converts normal env var to int Source code in zenml/constants.py def handle_int_env_var ( var : str , default : int = 0 ) -> int : \"\"\"Converts normal env var to int\"\"\" value = os . getenv ( var , \"\" ) try : return int ( value ) except ( ValueError , TypeError ): return default","title":"handle_int_env_var()"},{"location":"api_docs/container_registries/","text":"Container Registries zenml.container_registries special base_container_registry Base class for all container registries. BaseContainerRegistry ( BaseComponent ) pydantic-model Base class for all ZenML container registries. Source code in zenml/container_registries/base_container_registry.py class BaseContainerRegistry ( BaseComponent ): \"\"\"Base class for all ZenML container registries.\"\"\" uri : str _CONTAINER_REGISTRY_DIR_NAME : str = \"container_registries\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseContainerRegistry instance. Args: repo_path: Path to the repository of this container registry. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _CONTAINER_REGISTRY_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_container_registry_\" Config Configuration of settings. Source code in zenml/container_registries/base_container_registry.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_container_registry_\" __init__ ( self , repo_path , ** kwargs ) special Initializes a BaseContainerRegistry instance. Parameters: Name Type Description Default repo_path str Path to the repository of this container registry. required Source code in zenml/container_registries/base_container_registry.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseContainerRegistry instance. Args: repo_path: Path to the repository of this container registry. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _CONTAINER_REGISTRY_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs )","title":"Container Registries"},{"location":"api_docs/container_registries/#container-registries","text":"","title":"Container Registries"},{"location":"api_docs/container_registries/#zenml.container_registries","text":"","title":"container_registries"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry","text":"Base class for all container registries.","title":"base_container_registry"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry","text":"Base class for all ZenML container registries. Source code in zenml/container_registries/base_container_registry.py class BaseContainerRegistry ( BaseComponent ): \"\"\"Base class for all ZenML container registries.\"\"\" uri : str _CONTAINER_REGISTRY_DIR_NAME : str = \"container_registries\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseContainerRegistry instance. Args: repo_path: Path to the repository of this container registry. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _CONTAINER_REGISTRY_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_container_registry_\"","title":"BaseContainerRegistry"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry.Config","text":"Configuration of settings. Source code in zenml/container_registries/base_container_registry.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_container_registry_\"","title":"Config"},{"location":"api_docs/container_registries/#zenml.container_registries.base_container_registry.BaseContainerRegistry.__init__","text":"Initializes a BaseContainerRegistry instance. Parameters: Name Type Description Default repo_path str Path to the repository of this container registry. required Source code in zenml/container_registries/base_container_registry.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseContainerRegistry instance. Args: repo_path: Path to the repository of this container registry. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _CONTAINER_REGISTRY_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs )","title":"__init__()"},{"location":"api_docs/core/","text":"Core zenml.core special The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command. base_component BaseComponent ( BaseSettings ) pydantic-model Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. Source code in zenml/core/base_component.py class BaseComponent ( BaseSettings ): \"\"\"Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: * If a `uuid` is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. \"\"\" uuid : Optional [ UUID ] = Field ( default_factory = uuid4 ) _file_suffix = \".json\" _superfluous_options : Dict [ str , Any ] = {} _serialization_dir : str def __init__ ( self , serialization_dir : str , ** values : Any ): # Here, we insert monkey patch the `customise_sources` function # because we want to dynamically generate the serialization # file path and name. if hasattr ( self , \"uuid\" ): self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) elif \"uuid\" in values : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , f \" { str ( values [ 'uuid' ]) }{ self . _file_suffix } \" , ) else : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) # Initialize values from the above sources. super () . __init__ ( ** values ) self . _serialization_dir = serialization_dir self . _save_backup_file_if_required () def _save_backup_file_if_required ( self ) -> None : \"\"\"Saves a backup of the config file if the schema changed.\"\"\" if self . _superfluous_options : logger . warning ( \"Found superfluous configuration values for class ` %s `: %s \" , self . __class__ . __name__ , set ( self . _superfluous_options ), ) config_path = self . get_serialization_full_path () if fileio . file_exists ( config_path ): backup_path = config_path + \".backup\" fileio . copy ( config_path , backup_path , overwrite = True ) logger . warning ( \"Saving backup configuration to ' %s '.\" , backup_path ) # save the updated file without the extra options self . update () def _dump ( self ) -> None : \"\"\"Dumps all current values to the serialization file.\"\"\" self . _create_serialization_file_if_not_exists () file_path = self . get_serialization_full_path () file_content = self . json ( indent = 2 , sort_keys = True , exclude = { SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME }, ) zenml . io . utils . write_file_contents_as_string ( file_path , file_content ) def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) } def _create_serialization_file_if_not_exists ( self ) -> None : \"\"\"Creates the serialization file if it does not exist.\"\"\" f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): fileio . create_file_if_not_exists ( str ( f )) def get_serialization_dir ( self ) -> str : \"\"\"Return the dir where object is serialized.\"\"\" return self . _serialization_dir def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.\"\"\" if hasattr ( self , \"uuid\" ): return f \" { str ( self . uuid ) }{ self . _file_suffix } \" else : return f \"DEFAULT { self . _file_suffix } \" def get_serialization_full_path ( self ) -> str : \"\"\"Returns the full path of the serialization file.\"\"\" return os . path . join ( self . _serialization_dir , self . get_serialization_file_name () ) def update ( self ) -> None : \"\"\"Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. \"\"\" self . _dump () def delete ( self ) -> None : \"\"\"Deletes the persisted state of this object.\"\"\" fileio . remove ( self . get_serialization_full_path ()) @root_validator ( pre = True ) def check_superfluous_options ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes `_superfluous_options` attribute.\"\"\" field_names = { field . alias for field in cls . __fields__ . values ()} superfluous_options : Dict [ str , Any ] = {} for key in set ( values ): if key not in field_names : superfluous_options [ key ] = values . pop ( key ) values [ SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME ] = superfluous_options return values class Config : \"\"\"Configuration of settings.\"\"\" arbitrary_types_allowed = True env_prefix = \"zenml_\" # allow extra options so we can detect legacy configuration files extra = \"allow\" Config Configuration of settings. Source code in zenml/core/base_component.py class Config : \"\"\"Configuration of settings.\"\"\" arbitrary_types_allowed = True env_prefix = \"zenml_\" # allow extra options so we can detect legacy configuration files extra = \"allow\" __init__ ( self , serialization_dir , ** values ) special Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. Source code in zenml/core/base_component.py def __init__ ( self , serialization_dir : str , ** values : Any ): # Here, we insert monkey patch the `customise_sources` function # because we want to dynamically generate the serialization # file path and name. if hasattr ( self , \"uuid\" ): self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) elif \"uuid\" in values : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , f \" { str ( values [ 'uuid' ]) }{ self . _file_suffix } \" , ) else : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) # Initialize values from the above sources. super () . __init__ ( ** values ) self . _serialization_dir = serialization_dir self . _save_backup_file_if_required () check_superfluous_options ( values ) classmethod Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute. Source code in zenml/core/base_component.py @root_validator ( pre = True ) def check_superfluous_options ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes `_superfluous_options` attribute.\"\"\" field_names = { field . alias for field in cls . __fields__ . values ()} superfluous_options : Dict [ str , Any ] = {} for key in set ( values ): if key not in field_names : superfluous_options [ key ] = values . pop ( key ) values [ SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME ] = superfluous_options return values delete ( self ) Deletes the persisted state of this object. Source code in zenml/core/base_component.py def delete ( self ) -> None : \"\"\"Deletes the persisted state of this object.\"\"\" fileio . remove ( self . get_serialization_full_path ()) dict ( self , ** kwargs ) Removes private attributes from pydantic dict so they don't get stored in our config files. Source code in zenml/core/base_component.py def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) } get_serialization_dir ( self ) Return the dir where object is serialized. Source code in zenml/core/base_component.py def get_serialization_dir ( self ) -> str : \"\"\"Return the dir where object is serialized.\"\"\" return self . _serialization_dir get_serialization_file_name ( self ) Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default. Source code in zenml/core/base_component.py def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.\"\"\" if hasattr ( self , \"uuid\" ): return f \" { str ( self . uuid ) }{ self . _file_suffix } \" else : return f \"DEFAULT { self . _file_suffix } \" get_serialization_full_path ( self ) Returns the full path of the serialization file. Source code in zenml/core/base_component.py def get_serialization_full_path ( self ) -> str : \"\"\"Returns the full path of the serialization file.\"\"\" return os . path . join ( self . _serialization_dir , self . get_serialization_file_name () ) update ( self ) Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. Source code in zenml/core/base_component.py def update ( self ) -> None : \"\"\"Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. \"\"\" self . _dump () component_factory Factory to register all components. ComponentFactory Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. Source code in zenml/core/component_factory.py class ComponentFactory : \"\"\"Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. \"\"\" def __init__ ( self , name : str ): \"\"\"Constructor for the factory. Args: name: Unique name for the factory. \"\"\" self . name = name self . components : Dict [ str , BaseComponentType ] = {} def get_components ( self ) -> Dict [ str , BaseComponentType ]: \"\"\"Return all components\"\"\" return self . components def get_single_component ( self , key : str ) -> BaseComponentType : \"\"\"Get a registered component from a key.\"\"\" if key in self . components : return self . components [ key ] raise KeyError ( f \"Type ' { key } ' does not exist! Available options: \" f \" { [ str ( k ) for k in self . components . keys ()] } \" ) def get_component_key ( self , component : BaseComponentType ) -> str : \"\"\"Gets the key of a registered component.\"\"\" for k , v in self . components . items (): if v == component : return k raise KeyError ( f \"Type ' { component } ' does not exist! Available options: \" f \" { [ str ( v ) for v in self . components . values ()] } \" ) def register_component ( self , key : str , component : BaseComponentType ) -> None : \"\"\"Registers a single component class for a given key.\"\"\" self . components [ str ( key )] = component def register ( self , name : str ) -> Callable [[ BaseComponentType ], BaseComponentType ]: \"\"\"Class decorator to register component classes to the internal registry. Args: name: The name of the component. Returns: A function which registers the class at this ComponentFactory. \"\"\" def inner_wrapper ( wrapped_class : BaseComponentType , ) -> BaseComponentType : \"\"\"Inner wrapper for decorator.\"\"\" if name in self . components : logger . debug ( \"Executor %s already exists for factory %s , replacing it..\" , name , self . name , ) self . register_component ( name , wrapped_class ) return wrapped_class return inner_wrapper __init__ ( self , name ) special Constructor for the factory. Parameters: Name Type Description Default name str Unique name for the factory. required Source code in zenml/core/component_factory.py def __init__ ( self , name : str ): \"\"\"Constructor for the factory. Args: name: Unique name for the factory. \"\"\" self . name = name self . components : Dict [ str , BaseComponentType ] = {} get_component_key ( self , component ) Gets the key of a registered component. Source code in zenml/core/component_factory.py def get_component_key ( self , component : BaseComponentType ) -> str : \"\"\"Gets the key of a registered component.\"\"\" for k , v in self . components . items (): if v == component : return k raise KeyError ( f \"Type ' { component } ' does not exist! Available options: \" f \" { [ str ( v ) for v in self . components . values ()] } \" ) get_components ( self ) Return all components Source code in zenml/core/component_factory.py def get_components ( self ) -> Dict [ str , BaseComponentType ]: \"\"\"Return all components\"\"\" return self . components get_single_component ( self , key ) Get a registered component from a key. Source code in zenml/core/component_factory.py def get_single_component ( self , key : str ) -> BaseComponentType : \"\"\"Get a registered component from a key.\"\"\" if key in self . components : return self . components [ key ] raise KeyError ( f \"Type ' { key } ' does not exist! Available options: \" f \" { [ str ( k ) for k in self . components . keys ()] } \" ) register ( self , name ) Class decorator to register component classes to the internal registry. Parameters: Name Type Description Default name str The name of the component. required Returns: Type Description Callable[[Type[zenml.core.base_component.BaseComponent]], Type[zenml.core.base_component.BaseComponent]] A function which registers the class at this ComponentFactory. Source code in zenml/core/component_factory.py def register ( self , name : str ) -> Callable [[ BaseComponentType ], BaseComponentType ]: \"\"\"Class decorator to register component classes to the internal registry. Args: name: The name of the component. Returns: A function which registers the class at this ComponentFactory. \"\"\" def inner_wrapper ( wrapped_class : BaseComponentType , ) -> BaseComponentType : \"\"\"Inner wrapper for decorator.\"\"\" if name in self . components : logger . debug ( \"Executor %s already exists for factory %s , replacing it..\" , name , self . name , ) self . register_component ( name , wrapped_class ) return wrapped_class return inner_wrapper register_component ( self , key , component ) Registers a single component class for a given key. Source code in zenml/core/component_factory.py def register_component ( self , key : str , component : BaseComponentType ) -> None : \"\"\"Registers a single component class for a given key.\"\"\" self . components [ str ( key )] = component git_wrapper Wrapper class to handle Git integration GitWrapper Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. Source code in zenml/core/git_wrapper.py class GitWrapper : \"\"\"Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. \"\"\" def __init__ ( self , repo_path : str ): \"\"\" Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError: If repository is not a git repository. NoSuchPathError: If the repo_path does not exist. \"\"\" # TODO [ENG-163]: Raise ZenML exceptions here instead. self . repo_path : str = repo_path self . git_root_path : str = os . path . join ( repo_path , GIT_FOLDER_NAME ) self . git_repo = GitRepo ( self . repo_path ) def check_file_committed ( self , file_path : str ) -> bool : \"\"\" Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. \"\"\" uncommitted_files = [ i . a_path for i in self . git_repo . index . diff ( None )] try : staged_files = [ i . a_path for i in self . git_repo . index . diff ( \"HEAD\" )] except BadName : # for Ref 'HEAD' did not resolve to an object logger . debug ( \"No committed files in the repo. No staged files.\" ) staged_files = [] # source: https://stackoverflow.com/questions/3801321/ untracked_files = self . git_repo . git . ls_files ( others = True , exclude_standard = True ) . split ( \" \\n \" ) for item in uncommitted_files + staged_files + untracked_files : # These are all changed files if file_path == item : return False return True def get_current_sha ( self ) -> str : \"\"\" Finds the git sha that each file within the module is currently on. \"\"\" return cast ( str , self . git_repo . head . object . hexsha ) def check_module_clean ( self , source : str ) -> bool : \"\"\"Returns `True` if all files within source's module are committed. Args: source: relative module path pointing to a Class. \"\"\" # Get the module path module_path = source_utils . get_module_source_from_source ( source ) # Get relative path of module because check_file_committed needs that module_dir = source_utils . get_relative_path_from_module_source ( module_path ) # Get absolute path of module because fileio.list_dir needs that mod_abs_dir = source_utils . get_absolute_path_from_module_source ( module_path ) module_file_names = fileio . list_dir ( mod_abs_dir , only_file_names = True ) # Go through each file in module and see if there are uncommitted ones for file_path in module_file_names : path = os . path . join ( module_dir , file_path ) # if its .gitignored then continue and don't do anything if len ( self . git_repo . ignored ( path )) > 0 : continue if fileio . is_dir ( os . path . join ( mod_abs_dir , file_path )): logger . warning ( f \"The step { source } is contained inside a module \" f \"that \" f \"has sub-directories (the sub-directory { file_path } at \" f \" { mod_abs_dir } ). For now, ZenML supports only a flat \" f \"directory structure in which to place Steps. Please make\" f \" sure that the Step does not utilize the sub-directory.\" ) if not self . check_file_committed ( path ): return False return True def stash ( self ) -> None : \"\"\"Wrapper for git stash\"\"\" git = self . git_repo . git git . stash () def stash_pop ( self ) -> None : \"\"\"Wrapper for git stash pop. Only pops if there's something to pop.\"\"\" git = self . git_repo . git if git . stash ( \"list\" ) != \"\" : git . stash ( \"pop\" ) def checkout ( self , sha_or_branch : Optional [ str ] = None , directory : Optional [ str ] = None , ) -> None : \"\"\"Wrapper for git checkout Args: sha_or_branch: hex string of len 40 representing git sha OR name of branch directory: relative path to directory to scope checkout \"\"\" # TODO [ENG-164]: Implement exception handling git = self . git_repo . git if sha_or_branch is None : # Checks out directory at sha_or_branch assert directory is not None git . checkout ( \"--\" , directory ) elif directory is not None : assert sha_or_branch is not None # Basically discards all changes in directory git . checkout ( sha_or_branch , \"--\" , directory ) else : # The case where sha_or_branch is not None and directory is None # In this case, the whole repo is checked out at sha_or_branch git . checkout ( sha_or_branch ) def reset ( self , directory : Optional [ str ] = None ) -> None : \"\"\"Wrapper for `git reset HEAD <directory>`. Args: directory: Relative path to directory to scope checkout \"\"\" git = self . git_repo . git git . reset ( \"HEAD\" , directory ) def resolve_class_source ( self , class_source : str ) -> str : \"\"\"Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within `module` are all committed. If even one file is not committed, then returns `source` unchanged. Args: class_source (str): class_source e.g. this.module.Class \"\"\" if \"@\" in class_source : # already pinned return class_source if is_standard_source ( class_source ): # that means use standard version return resolve_standard_source ( class_source ) # otherwise use Git resolution if not self . check_module_clean ( class_source ): # Return the source path if not clean logger . warning ( \"Found uncommitted file. Pipelines run with this \" \"configuration may not be reproducible. Please commit \" \"all files in this module and then run the pipeline to \" \"ensure reproducibility.\" ) return class_source return class_source + \"@\" + self . get_current_sha () def is_valid_source ( self , source : str ) -> bool : \"\"\" Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. \"\"\" try : self . load_source_path_class ( source ) except GitException : return False return True def load_source_path_class ( self , source : str ) -> Type [ Any ]: \"\"\" Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" source = source . split ( \"@\" )[ 0 ] pin = source . split ( \"@\" )[ - 1 ] is_standard = is_standard_pin ( pin ) if \"@\" in source and not is_standard : logger . debug ( \"Pinned step found with git sha. \" \"Loading class from git history.\" ) module_source = get_module_source_from_source ( source ) relative_module_path = get_relative_path_from_module_source ( module_source ) logger . warning ( \"Found source with a pinned sha. Will now checkout \" f \"module: { module_source } \" ) # critical step if not self . check_module_clean ( source ): raise GitException ( f \"One of the files at { relative_module_path } \" f \"is not committed and we \" f \"are trying to load that directory from git \" f \"history due to a pinned step in the pipeline. \" f \"Please commit the file and then run the \" f \"pipeline.\" ) # Check out the directory at that sha self . checkout ( sha_or_branch = pin , directory = relative_module_path ) # After this point, all exceptions will first undo the above try : class_ = source_utils . import_class_by_path ( source ) self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) except Exception as e : self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) raise GitException ( f \"A git exception occurred when checking out repository \" f \"from git history. Resetting repository to original \" f \"state. Original exception: { e } \" ) elif \"@\" in source and is_standard : logger . debug ( f \"Default { APP_NAME } class used. Loading directly.\" ) # TODO [ENG-165]: Check if ZenML version is installed before loading. class_ = source_utils . import_class_by_path ( source ) else : logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = source_utils . import_class_by_path ( source ) return class_ def resolve_class ( self , class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. \"\"\" class_source = source_utils . resolve_class ( class_ ) return self . resolve_class_source ( class_source ) __init__ ( self , repo_path ) special Initialize GitWrapper. Should be initialized by ZenML Repository. Parameters: Name Type Description Default repo_path str required Exceptions: Type Description InvalidGitRepositoryError If repository is not a git repository. NoSuchPathError If the repo_path does not exist. Source code in zenml/core/git_wrapper.py def __init__ ( self , repo_path : str ): \"\"\" Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError: If repository is not a git repository. NoSuchPathError: If the repo_path does not exist. \"\"\" # TODO [ENG-163]: Raise ZenML exceptions here instead. self . repo_path : str = repo_path self . git_root_path : str = os . path . join ( repo_path , GIT_FOLDER_NAME ) self . git_repo = GitRepo ( self . repo_path ) check_file_committed ( self , file_path ) Checks file is committed. If yes, return True, else False. Parameters: Name Type Description Default file_path str Path to any file within the ZenML repo. required Source code in zenml/core/git_wrapper.py def check_file_committed ( self , file_path : str ) -> bool : \"\"\" Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. \"\"\" uncommitted_files = [ i . a_path for i in self . git_repo . index . diff ( None )] try : staged_files = [ i . a_path for i in self . git_repo . index . diff ( \"HEAD\" )] except BadName : # for Ref 'HEAD' did not resolve to an object logger . debug ( \"No committed files in the repo. No staged files.\" ) staged_files = [] # source: https://stackoverflow.com/questions/3801321/ untracked_files = self . git_repo . git . ls_files ( others = True , exclude_standard = True ) . split ( \" \\n \" ) for item in uncommitted_files + staged_files + untracked_files : # These are all changed files if file_path == item : return False return True check_module_clean ( self , source ) Returns True if all files within source's module are committed. Parameters: Name Type Description Default source str relative module path pointing to a Class. required Source code in zenml/core/git_wrapper.py def check_module_clean ( self , source : str ) -> bool : \"\"\"Returns `True` if all files within source's module are committed. Args: source: relative module path pointing to a Class. \"\"\" # Get the module path module_path = source_utils . get_module_source_from_source ( source ) # Get relative path of module because check_file_committed needs that module_dir = source_utils . get_relative_path_from_module_source ( module_path ) # Get absolute path of module because fileio.list_dir needs that mod_abs_dir = source_utils . get_absolute_path_from_module_source ( module_path ) module_file_names = fileio . list_dir ( mod_abs_dir , only_file_names = True ) # Go through each file in module and see if there are uncommitted ones for file_path in module_file_names : path = os . path . join ( module_dir , file_path ) # if its .gitignored then continue and don't do anything if len ( self . git_repo . ignored ( path )) > 0 : continue if fileio . is_dir ( os . path . join ( mod_abs_dir , file_path )): logger . warning ( f \"The step { source } is contained inside a module \" f \"that \" f \"has sub-directories (the sub-directory { file_path } at \" f \" { mod_abs_dir } ). For now, ZenML supports only a flat \" f \"directory structure in which to place Steps. Please make\" f \" sure that the Step does not utilize the sub-directory.\" ) if not self . check_file_committed ( path ): return False return True checkout ( self , sha_or_branch = None , directory = None ) Wrapper for git checkout Parameters: Name Type Description Default sha_or_branch Optional[str] hex string of len 40 representing git sha OR name of branch None directory Optional[str] relative path to directory to scope checkout None Source code in zenml/core/git_wrapper.py def checkout ( self , sha_or_branch : Optional [ str ] = None , directory : Optional [ str ] = None , ) -> None : \"\"\"Wrapper for git checkout Args: sha_or_branch: hex string of len 40 representing git sha OR name of branch directory: relative path to directory to scope checkout \"\"\" # TODO [ENG-164]: Implement exception handling git = self . git_repo . git if sha_or_branch is None : # Checks out directory at sha_or_branch assert directory is not None git . checkout ( \"--\" , directory ) elif directory is not None : assert sha_or_branch is not None # Basically discards all changes in directory git . checkout ( sha_or_branch , \"--\" , directory ) else : # The case where sha_or_branch is not None and directory is None # In this case, the whole repo is checked out at sha_or_branch git . checkout ( sha_or_branch ) get_current_sha ( self ) Finds the git sha that each file within the module is currently on. Source code in zenml/core/git_wrapper.py def get_current_sha ( self ) -> str : \"\"\" Finds the git sha that each file within the module is currently on. \"\"\" return cast ( str , self . git_repo . head . object . hexsha ) is_valid_source ( self , source ) Checks whether the source_path is valid or not. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/core/git_wrapper.py def is_valid_source ( self , source : str ) -> bool : \"\"\" Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. \"\"\" try : self . load_source_path_class ( source ) except GitException : return False return True load_source_path_class ( self , source ) Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/core/git_wrapper.py def load_source_path_class ( self , source : str ) -> Type [ Any ]: \"\"\" Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" source = source . split ( \"@\" )[ 0 ] pin = source . split ( \"@\" )[ - 1 ] is_standard = is_standard_pin ( pin ) if \"@\" in source and not is_standard : logger . debug ( \"Pinned step found with git sha. \" \"Loading class from git history.\" ) module_source = get_module_source_from_source ( source ) relative_module_path = get_relative_path_from_module_source ( module_source ) logger . warning ( \"Found source with a pinned sha. Will now checkout \" f \"module: { module_source } \" ) # critical step if not self . check_module_clean ( source ): raise GitException ( f \"One of the files at { relative_module_path } \" f \"is not committed and we \" f \"are trying to load that directory from git \" f \"history due to a pinned step in the pipeline. \" f \"Please commit the file and then run the \" f \"pipeline.\" ) # Check out the directory at that sha self . checkout ( sha_or_branch = pin , directory = relative_module_path ) # After this point, all exceptions will first undo the above try : class_ = source_utils . import_class_by_path ( source ) self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) except Exception as e : self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) raise GitException ( f \"A git exception occurred when checking out repository \" f \"from git history. Resetting repository to original \" f \"state. Original exception: { e } \" ) elif \"@\" in source and is_standard : logger . debug ( f \"Default { APP_NAME } class used. Loading directly.\" ) # TODO [ENG-165]: Check if ZenML version is installed before loading. class_ = source_utils . import_class_by_path ( source ) else : logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = source_utils . import_class_by_path ( source ) return class_ reset ( self , directory = None ) Wrapper for git reset HEAD <directory> . Parameters: Name Type Description Default directory Optional[str] Relative path to directory to scope checkout None Source code in zenml/core/git_wrapper.py def reset ( self , directory : Optional [ str ] = None ) -> None : \"\"\"Wrapper for `git reset HEAD <directory>`. Args: directory: Relative path to directory to scope checkout \"\"\" git = self . git_repo . git git . reset ( \"HEAD\" , directory ) resolve_class ( self , class_ ) Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class[@pin]. Source code in zenml/core/git_wrapper.py def resolve_class ( self , class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. \"\"\" class_source = source_utils . resolve_class ( class_ ) return self . resolve_class_source ( class_source ) resolve_class_source ( self , class_source ) Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Parameters: Name Type Description Default class_source str class_source e.g. this.module.Class required Source code in zenml/core/git_wrapper.py def resolve_class_source ( self , class_source : str ) -> str : \"\"\"Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within `module` are all committed. If even one file is not committed, then returns `source` unchanged. Args: class_source (str): class_source e.g. this.module.Class \"\"\" if \"@\" in class_source : # already pinned return class_source if is_standard_source ( class_source ): # that means use standard version return resolve_standard_source ( class_source ) # otherwise use Git resolution if not self . check_module_clean ( class_source ): # Return the source path if not clean logger . warning ( \"Found uncommitted file. Pipelines run with this \" \"configuration may not be reproducible. Please commit \" \"all files in this module and then run the pipeline to \" \"ensure reproducibility.\" ) return class_source return class_source + \"@\" + self . get_current_sha () stash ( self ) Wrapper for git stash Source code in zenml/core/git_wrapper.py def stash ( self ) -> None : \"\"\"Wrapper for git stash\"\"\" git = self . git_repo . git git . stash () stash_pop ( self ) Wrapper for git stash pop. Only pops if there's something to pop. Source code in zenml/core/git_wrapper.py def stash_pop ( self ) -> None : \"\"\"Wrapper for git stash pop. Only pops if there's something to pop.\"\"\" git = self . git_repo . git if git . stash ( \"list\" ) != \"\" : git . stash ( \"pop\" ) local_service LocalService ( BaseComponent ) pydantic-model Definition of a local service that keeps track of all ZenML components. Source code in zenml/core/local_service.py class LocalService ( BaseComponent ): \"\"\"Definition of a local service that keeps track of all ZenML components. \"\"\" stacks : Dict [ str , BaseStack ] = {} active_stack_key : str = \"local_stack\" metadata_store_map : Dict [ str , UUIDSourceTuple ] = {} artifact_store_map : Dict [ str , UUIDSourceTuple ] = {} orchestrator_map : Dict [ str , UUIDSourceTuple ] = {} container_registry_map : Dict [ str , UUIDSourceTuple ] = {} _LOCAL_SERVICE_FILE_NAME = \"zenservice.json\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a LocalService instance. Args: repo_path: Path to the repository of this service. \"\"\" serialization_dir = get_zenml_config_dir ( repo_path ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) self . _repo_path = repo_path for stack in self . stacks . values (): stack . _repo_path = repo_path def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized.\"\"\" return self . _LOCAL_SERVICE_FILE_NAME @property def metadata_stores ( self ) -> Dict [ str , \"BaseMetadataStore\" ]: \"\"\"Returns all registered metadata stores.\"\"\" from zenml.metadata_stores import BaseMetadataStore return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseMetadataStore . _METADATA_STORE_DIR_NAME , self . metadata_store_map , self . _repo_path , ) @property def artifact_stores ( self ) -> Dict [ str , \"BaseArtifactStore\" ]: \"\"\"Returns all registered artifact stores.\"\"\" from zenml.artifact_stores import BaseArtifactStore return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseArtifactStore . _ARTIFACT_STORE_DIR_NAME , self . artifact_store_map , self . _repo_path , ) @property def orchestrators ( self ) -> Dict [ str , \"BaseOrchestrator\" ]: \"\"\"Returns all registered orchestrators.\"\"\" from zenml.orchestrators import BaseOrchestrator return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseOrchestrator . _ORCHESTRATOR_STORE_DIR_NAME , self . orchestrator_map , self . _repo_path , ) @property def container_registries ( self ) -> Dict [ str , \"BaseContainerRegistry\" ]: \"\"\"Returns all registered container registries.\"\"\" from zenml.container_registries import BaseContainerRegistry return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseContainerRegistry . _CONTAINER_REGISTRY_DIR_NAME , self . container_registry_map , self . _repo_path , ) def get_active_stack_key ( self ) -> str : \"\"\"Returns the active stack key.\"\"\" return self . active_stack_key def set_active_stack_key ( self , stack_key : str ) -> None : \"\"\"Sets the active stack key.\"\"\" if stack_key not in self . stacks : raise DoesNotExistException ( f \"Unable to set active stack for key ` { stack_key } ` because no \" f \"stack is registered for this key. Available keys: \" f \" { set ( self . stacks ) } \" ) self . active_stack_key = stack_key self . update () def get_stack ( self , key : str ) -> BaseStack : \"\"\"Return a single stack based on key. Args: key: Unique key of stack. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching stack with key { key } \" ) if key not in self . stacks : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . stacks . keys ()) } \" ) return self . stacks [ key ] @track ( event = REGISTERED_STACK ) def register_stack ( self , key : str , stack : BaseStack ) -> None : \"\"\"Register a stack. Args: key: Unique key for the stack. stack: Stack to be registered. \"\"\" logger . debug ( f \"Registering stack with key { key } , details: \" f \" { stack . dict () } \" ) # Check if the individual components actually exist. # TODO [ENG-190]: Add tests to check cases of registering a stack with a # non-existing individual component. We can also improve the error # logging for the CLI while we're at it. self . get_orchestrator ( stack . orchestrator_name ) self . get_artifact_store ( stack . artifact_store_name ) self . get_metadata_store ( stack . metadata_store_name ) if stack . container_registry_name : self . get_container_registry ( stack . container_registry_name ) if key in self . stacks : raise AlreadyExistsException ( message = f \"Stack ` { key } ` already exists!\" ) # Add the mapping. self . stacks [ key ] = stack self . update () def delete_stack ( self , key : str ) -> None : \"\"\"Delete a stack specified with a key. Args: key: Unique key of stack. \"\"\" _ = self . get_stack ( key ) # check whether it exists del self . stacks [ key ] self . update () logger . debug ( f \"Deleted stack with key: { key } .\" ) logger . info ( \"Deleting a stack currently does not delete the underlying \" \"architecture of the stack. It just deletes the reference to it. \" \"Therefore please make sure to delete these resources on your \" \"own. Also, if this stack was the active stack, please make sure \" \"to set a not active stack via `zenml stack set`.\" ) def get_artifact_store ( self , key : str ) -> \"BaseArtifactStore\" : \"\"\"Return a single artifact store based on key. Args: key: Unique key of artifact store. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching artifact_store with key { key } \" ) if key not in self . artifact_store_map : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . artifact_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . artifact_store_map , self . _repo_path ) def register_artifact_store ( self , key : str , artifact_store : \"BaseArtifactStore\" ) -> None : \"\"\"Register an artifact store. Args: artifact_store: Artifact store to be registered. key: Unique key for the artifact store. \"\"\" logger . debug ( f \"Registering artifact store with key { key } , details: \" f \" { artifact_store . dict () } \" ) if key in self . artifact_store_map : raise AlreadyExistsException ( message = f \"Artifact Store ` { key } ` already exists!\" ) # Add the mapping. artifact_store . update () source = source_utils . resolve_class ( artifact_store . __class__ ) self . artifact_store_map [ key ] = UUIDSourceTuple ( uuid = artifact_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import artifact_store_factory track_event ( REGISTERED_ARTIFACT_STORE , { \"type\" : artifact_store_factory . get_component_key ( artifact_store . __class__ ) }, ) def delete_artifact_store ( self , key : str ) -> None : \"\"\"Delete an artifact_store. Args: key: Unique key of artifact_store. \"\"\" s = self . get_artifact_store ( key ) # check whether it exists s . delete () del self . artifact_store_map [ key ] self . update () logger . debug ( f \"Deleted artifact_store with key: { key } .\" ) def get_metadata_store ( self , key : str ) -> \"BaseMetadataStore\" : \"\"\"Return a single metadata store based on key. Args: key: Unique key of metadata store. Returns: Metadata store specified by key. \"\"\" logger . debug ( f \"Fetching metadata store with key { key } \" ) if key not in self . metadata_store_map : raise DoesNotExistException ( f \"Metadata store of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . metadata_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . metadata_store_map , self . _repo_path ) def register_metadata_store ( self , key : str , metadata_store : \"BaseMetadataStore\" ) -> None : \"\"\"Register a metadata store. Args: metadata_store: Metadata store to be registered. key: Unique key for the metadata store. \"\"\" logger . debug ( f \"Registering metadata store with key { key } , details: \" f \" { metadata_store . dict () } \" ) if key in self . metadata_store_map : raise AlreadyExistsException ( message = f \"Metadata store ` { key } ` already exists!\" ) # Add the mapping. metadata_store . update () source = source_utils . resolve_class ( metadata_store . __class__ ) self . metadata_store_map [ key ] = UUIDSourceTuple ( uuid = metadata_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import metadata_store_factory track_event ( REGISTERED_METADATA_STORE , { \"type\" : metadata_store_factory . get_component_key ( metadata_store . __class__ ) }, ) def delete_metadata_store ( self , key : str ) -> None : \"\"\"Delete a metadata store. Args: key: Unique key of metadata store. \"\"\" s = self . get_metadata_store ( key ) # check whether it exists s . delete () del self . metadata_store_map [ key ] self . update () logger . debug ( f \"Deleted metadata store with key: { key } .\" ) def get_orchestrator ( self , key : str ) -> \"BaseOrchestrator\" : \"\"\"Return a single orchestrator based on key. Args: key: Unique key of orchestrator. Returns: Orchestrator specified by key. \"\"\" logger . debug ( f \"Fetching orchestrator with key { key } \" ) if key not in self . orchestrator_map : raise DoesNotExistException ( f \"Orchestrator of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . orchestrator_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . orchestrator_map , self . _repo_path ) def register_orchestrator ( self , key : str , orchestrator : \"BaseOrchestrator\" ) -> None : \"\"\"Register an orchestrator. Args: orchestrator: Orchestrator to be registered. key: Unique key for the orchestrator. \"\"\" logger . debug ( f \"Registering orchestrator with key { key } , details: \" f \" { orchestrator . dict () } \" ) if key in self . orchestrator_map : raise AlreadyExistsException ( message = f \"Orchestrator ` { key } ` already exists!\" ) # Add the mapping. orchestrator . update () source = source_utils . resolve_class ( orchestrator . __class__ ) self . orchestrator_map [ key ] = UUIDSourceTuple ( uuid = orchestrator . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import orchestrator_store_factory track_event ( REGISTERED_ORCHESTRATOR , { \"type\" : orchestrator_store_factory . get_component_key ( orchestrator . __class__ ) }, ) def delete_orchestrator ( self , key : str ) -> None : \"\"\"Delete a orchestrator. Args: key: Unique key of orchestrator. \"\"\" s = self . get_orchestrator ( key ) # check whether it exists s . delete () del self . orchestrator_map [ key ] self . update () logger . debug ( f \"Deleted orchestrator with key: { key } .\" ) def get_container_registry ( self , key : str ) -> \"BaseContainerRegistry\" : \"\"\"Return a single container registry based on key. Args: key: Unique key of a container registry. Returns: Container registry specified by key. \"\"\" logger . debug ( f \"Fetching container registry with key { key } \" ) if key not in self . container_registry_map : raise DoesNotExistException ( f \"Container registry of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . container_registry_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . container_registry_map , self . _repo_path ) @track ( event = REGISTERED_CONTAINER_REGISTRY ) def register_container_registry ( self , key : str , container_registry : \"BaseContainerRegistry\" ) -> None : \"\"\"Register a container registry. Args: container_registry: Container registry to be registered. key: Unique key for the container registry. \"\"\" logger . debug ( f \"Registering container registry with key { key } , details: \" f \" { container_registry . dict () } \" ) if key in self . container_registry_map : raise AlreadyExistsException ( message = f \"Container registry ` { key } ` already exists!\" ) # Add the mapping. container_registry . update () source = source_utils . resolve_class ( container_registry . __class__ ) self . container_registry_map [ key ] = UUIDSourceTuple ( uuid = container_registry . uuid , source = source ) self . update () def delete_container_registry ( self , key : str ) -> None : \"\"\"Delete a container registry. Args: key: Unique key of the container registry. \"\"\" container_registry = self . get_container_registry ( key ) container_registry . delete () del self . container_registry_map [ key ] self . update () logger . debug ( f \"Deleted container registry with key: { key } .\" ) def delete ( self ) -> None : \"\"\"Deletes the entire service. Dangerous operation\"\"\" for m in self . metadata_stores . values (): m . delete () for a in self . artifact_stores . values (): a . delete () for o in self . orchestrators . values (): o . delete () for c in self . container_registries . values (): c . delete () super () . delete () artifact_stores : Dict [ str , BaseArtifactStore ] property readonly Returns all registered artifact stores. container_registries : Dict [ str , BaseContainerRegistry ] property readonly Returns all registered container registries. metadata_stores : Dict [ str , BaseMetadataStore ] property readonly Returns all registered metadata stores. orchestrators : Dict [ str , BaseOrchestrator ] property readonly Returns all registered orchestrators. __init__ ( self , repo_path , ** kwargs ) special Initializes a LocalService instance. Parameters: Name Type Description Default repo_path str Path to the repository of this service. required Source code in zenml/core/local_service.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a LocalService instance. Args: repo_path: Path to the repository of this service. \"\"\" serialization_dir = get_zenml_config_dir ( repo_path ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) self . _repo_path = repo_path for stack in self . stacks . values (): stack . _repo_path = repo_path delete ( self ) Deletes the entire service. Dangerous operation Source code in zenml/core/local_service.py def delete ( self ) -> None : \"\"\"Deletes the entire service. Dangerous operation\"\"\" for m in self . metadata_stores . values (): m . delete () for a in self . artifact_stores . values (): a . delete () for o in self . orchestrators . values (): o . delete () for c in self . container_registries . values (): c . delete () super () . delete () delete_artifact_store ( self , key ) Delete an artifact_store. Parameters: Name Type Description Default key str Unique key of artifact_store. required Source code in zenml/core/local_service.py def delete_artifact_store ( self , key : str ) -> None : \"\"\"Delete an artifact_store. Args: key: Unique key of artifact_store. \"\"\" s = self . get_artifact_store ( key ) # check whether it exists s . delete () del self . artifact_store_map [ key ] self . update () logger . debug ( f \"Deleted artifact_store with key: { key } .\" ) delete_container_registry ( self , key ) Delete a container registry. Parameters: Name Type Description Default key str Unique key of the container registry. required Source code in zenml/core/local_service.py def delete_container_registry ( self , key : str ) -> None : \"\"\"Delete a container registry. Args: key: Unique key of the container registry. \"\"\" container_registry = self . get_container_registry ( key ) container_registry . delete () del self . container_registry_map [ key ] self . update () logger . debug ( f \"Deleted container registry with key: { key } .\" ) delete_metadata_store ( self , key ) Delete a metadata store. Parameters: Name Type Description Default key str Unique key of metadata store. required Source code in zenml/core/local_service.py def delete_metadata_store ( self , key : str ) -> None : \"\"\"Delete a metadata store. Args: key: Unique key of metadata store. \"\"\" s = self . get_metadata_store ( key ) # check whether it exists s . delete () del self . metadata_store_map [ key ] self . update () logger . debug ( f \"Deleted metadata store with key: { key } .\" ) delete_orchestrator ( self , key ) Delete a orchestrator. Parameters: Name Type Description Default key str Unique key of orchestrator. required Source code in zenml/core/local_service.py def delete_orchestrator ( self , key : str ) -> None : \"\"\"Delete a orchestrator. Args: key: Unique key of orchestrator. \"\"\" s = self . get_orchestrator ( key ) # check whether it exists s . delete () del self . orchestrator_map [ key ] self . update () logger . debug ( f \"Deleted orchestrator with key: { key } .\" ) delete_stack ( self , key ) Delete a stack specified with a key. Parameters: Name Type Description Default key str Unique key of stack. required Source code in zenml/core/local_service.py def delete_stack ( self , key : str ) -> None : \"\"\"Delete a stack specified with a key. Args: key: Unique key of stack. \"\"\" _ = self . get_stack ( key ) # check whether it exists del self . stacks [ key ] self . update () logger . debug ( f \"Deleted stack with key: { key } .\" ) logger . info ( \"Deleting a stack currently does not delete the underlying \" \"architecture of the stack. It just deletes the reference to it. \" \"Therefore please make sure to delete these resources on your \" \"own. Also, if this stack was the active stack, please make sure \" \"to set a not active stack via `zenml stack set`.\" ) get_active_stack_key ( self ) Returns the active stack key. Source code in zenml/core/local_service.py def get_active_stack_key ( self ) -> str : \"\"\"Returns the active stack key.\"\"\" return self . active_stack_key get_artifact_store ( self , key ) Return a single artifact store based on key. Parameters: Name Type Description Default key str Unique key of artifact store. required Returns: Type Description BaseArtifactStore Stack specified by key. Source code in zenml/core/local_service.py def get_artifact_store ( self , key : str ) -> \"BaseArtifactStore\" : \"\"\"Return a single artifact store based on key. Args: key: Unique key of artifact store. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching artifact_store with key { key } \" ) if key not in self . artifact_store_map : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . artifact_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . artifact_store_map , self . _repo_path ) get_container_registry ( self , key ) Return a single container registry based on key. Parameters: Name Type Description Default key str Unique key of a container registry. required Returns: Type Description BaseContainerRegistry Container registry specified by key. Source code in zenml/core/local_service.py def get_container_registry ( self , key : str ) -> \"BaseContainerRegistry\" : \"\"\"Return a single container registry based on key. Args: key: Unique key of a container registry. Returns: Container registry specified by key. \"\"\" logger . debug ( f \"Fetching container registry with key { key } \" ) if key not in self . container_registry_map : raise DoesNotExistException ( f \"Container registry of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . container_registry_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . container_registry_map , self . _repo_path ) get_metadata_store ( self , key ) Return a single metadata store based on key. Parameters: Name Type Description Default key str Unique key of metadata store. required Returns: Type Description BaseMetadataStore Metadata store specified by key. Source code in zenml/core/local_service.py def get_metadata_store ( self , key : str ) -> \"BaseMetadataStore\" : \"\"\"Return a single metadata store based on key. Args: key: Unique key of metadata store. Returns: Metadata store specified by key. \"\"\" logger . debug ( f \"Fetching metadata store with key { key } \" ) if key not in self . metadata_store_map : raise DoesNotExistException ( f \"Metadata store of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . metadata_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . metadata_store_map , self . _repo_path ) get_orchestrator ( self , key ) Return a single orchestrator based on key. Parameters: Name Type Description Default key str Unique key of orchestrator. required Returns: Type Description BaseOrchestrator Orchestrator specified by key. Source code in zenml/core/local_service.py def get_orchestrator ( self , key : str ) -> \"BaseOrchestrator\" : \"\"\"Return a single orchestrator based on key. Args: key: Unique key of orchestrator. Returns: Orchestrator specified by key. \"\"\" logger . debug ( f \"Fetching orchestrator with key { key } \" ) if key not in self . orchestrator_map : raise DoesNotExistException ( f \"Orchestrator of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . orchestrator_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . orchestrator_map , self . _repo_path ) get_serialization_file_name ( self ) Return the name of the file where object is serialized. Source code in zenml/core/local_service.py def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized.\"\"\" return self . _LOCAL_SERVICE_FILE_NAME get_stack ( self , key ) Return a single stack based on key. Parameters: Name Type Description Default key str Unique key of stack. required Returns: Type Description BaseStack Stack specified by key. Source code in zenml/core/local_service.py def get_stack ( self , key : str ) -> BaseStack : \"\"\"Return a single stack based on key. Args: key: Unique key of stack. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching stack with key { key } \" ) if key not in self . stacks : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . stacks . keys ()) } \" ) return self . stacks [ key ] register_artifact_store ( self , key , artifact_store ) Register an artifact store. Parameters: Name Type Description Default artifact_store BaseArtifactStore Artifact store to be registered. required key str Unique key for the artifact store. required Source code in zenml/core/local_service.py def register_artifact_store ( self , key : str , artifact_store : \"BaseArtifactStore\" ) -> None : \"\"\"Register an artifact store. Args: artifact_store: Artifact store to be registered. key: Unique key for the artifact store. \"\"\" logger . debug ( f \"Registering artifact store with key { key } , details: \" f \" { artifact_store . dict () } \" ) if key in self . artifact_store_map : raise AlreadyExistsException ( message = f \"Artifact Store ` { key } ` already exists!\" ) # Add the mapping. artifact_store . update () source = source_utils . resolve_class ( artifact_store . __class__ ) self . artifact_store_map [ key ] = UUIDSourceTuple ( uuid = artifact_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import artifact_store_factory track_event ( REGISTERED_ARTIFACT_STORE , { \"type\" : artifact_store_factory . get_component_key ( artifact_store . __class__ ) }, ) register_container_registry ( * args , ** kwargs ) Inner decorator function. Source code in zenml/core/local_service.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result register_metadata_store ( self , key , metadata_store ) Register a metadata store. Parameters: Name Type Description Default metadata_store BaseMetadataStore Metadata store to be registered. required key str Unique key for the metadata store. required Source code in zenml/core/local_service.py def register_metadata_store ( self , key : str , metadata_store : \"BaseMetadataStore\" ) -> None : \"\"\"Register a metadata store. Args: metadata_store: Metadata store to be registered. key: Unique key for the metadata store. \"\"\" logger . debug ( f \"Registering metadata store with key { key } , details: \" f \" { metadata_store . dict () } \" ) if key in self . metadata_store_map : raise AlreadyExistsException ( message = f \"Metadata store ` { key } ` already exists!\" ) # Add the mapping. metadata_store . update () source = source_utils . resolve_class ( metadata_store . __class__ ) self . metadata_store_map [ key ] = UUIDSourceTuple ( uuid = metadata_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import metadata_store_factory track_event ( REGISTERED_METADATA_STORE , { \"type\" : metadata_store_factory . get_component_key ( metadata_store . __class__ ) }, ) register_orchestrator ( self , key , orchestrator ) Register an orchestrator. Parameters: Name Type Description Default orchestrator BaseOrchestrator Orchestrator to be registered. required key str Unique key for the orchestrator. required Source code in zenml/core/local_service.py def register_orchestrator ( self , key : str , orchestrator : \"BaseOrchestrator\" ) -> None : \"\"\"Register an orchestrator. Args: orchestrator: Orchestrator to be registered. key: Unique key for the orchestrator. \"\"\" logger . debug ( f \"Registering orchestrator with key { key } , details: \" f \" { orchestrator . dict () } \" ) if key in self . orchestrator_map : raise AlreadyExistsException ( message = f \"Orchestrator ` { key } ` already exists!\" ) # Add the mapping. orchestrator . update () source = source_utils . resolve_class ( orchestrator . __class__ ) self . orchestrator_map [ key ] = UUIDSourceTuple ( uuid = orchestrator . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import orchestrator_store_factory track_event ( REGISTERED_ORCHESTRATOR , { \"type\" : orchestrator_store_factory . get_component_key ( orchestrator . __class__ ) }, ) register_stack ( * args , ** kwargs ) Inner decorator function. Source code in zenml/core/local_service.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result set_active_stack_key ( self , stack_key ) Sets the active stack key. Source code in zenml/core/local_service.py def set_active_stack_key ( self , stack_key : str ) -> None : \"\"\"Sets the active stack key.\"\"\" if stack_key not in self . stacks : raise DoesNotExistException ( f \"Unable to set active stack for key ` { stack_key } ` because no \" f \"stack is registered for this key. Available keys: \" f \" { set ( self . stacks ) } \" ) self . active_stack_key = stack_key self . update () mapping_utils UUIDSourceTuple ( BaseModel ) pydantic-model Container used to store UUID and source information of a single BaseComponent subclass. Attributes: Name Type Description uuid UUID Identifier of the BaseComponent source str Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag Source code in zenml/core/mapping_utils.py class UUIDSourceTuple ( BaseModel ): \"\"\"Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid: Identifier of the BaseComponent source: Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag \"\"\" uuid : UUID source : str get_component_from_key ( key , mapping , repo_path ) Given a key and a mapping, return an initialized component. Parameters: Name Type Description Default key str Unique key. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict of type str -> UUIDSourceTuple. required repo_path str Path to the repo from which to load the component. required Returns: Type Description BaseComponent An object which is a subclass of type BaseComponent. Source code in zenml/core/mapping_utils.py def get_component_from_key ( key : str , mapping : Dict [ str , UUIDSourceTuple ], repo_path : str ) -> BaseComponent : \"\"\"Given a key and a mapping, return an initialized component. Args: key: Unique key. mapping: Dict of type str -> UUIDSourceTuple. repo_path: Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent. \"\"\" tuple_ = mapping [ key ] class_ = source_utils . load_source_path_class ( tuple_ . source ) if not issubclass ( class_ , BaseComponent ): raise TypeError ( \"\" ) return class_ ( uuid = tuple_ . uuid , repo_path = repo_path ) # type: ignore[call-arg] # noqa get_components_from_store ( store_name , mapping , repo_path ) Returns a list of components from a store. Parameters: Name Type Description Default store_name str Name of the store. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict of type str -> UUIDSourceTuple. required repo_path str Path to the repo from which to load the components. required Returns: Type Description Dict[str, zenml.core.base_component.BaseComponent] A dict of objects which are a subclass of type BaseComponent. Source code in zenml/core/mapping_utils.py def get_components_from_store ( store_name : str , mapping : Dict [ str , UUIDSourceTuple ], repo_path : str ) -> Dict [ str , BaseComponent ]: \"\"\"Returns a list of components from a store. Args: store_name: Name of the store. mapping: Dict of type str -> UUIDSourceTuple. repo_path: Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent. \"\"\" store_dir = os . path . join ( zenml . io . utils . get_zenml_config_dir ( repo_path ), store_name , ) comps = {} for fnames in fileio . list_dir ( store_dir , only_file_names = True ): uuid = Path ( fnames ) . stem key = get_key_from_uuid ( UUID ( uuid ), mapping ) comps [ key ] = get_component_from_key ( key , mapping , repo_path ) return comps get_key_from_uuid ( uuid , mapping ) Return the key that points to a certain uuid in a mapping. Parameters: Name Type Description Default uuid UUID uuid to query. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict mapping keys to UUIDs and source information. required Returns: Type Description str Returns the key from the mapping. Source code in zenml/core/mapping_utils.py def get_key_from_uuid ( uuid : UUID , mapping : Dict [ str , UUIDSourceTuple ]) -> str : \"\"\"Return the key that points to a certain uuid in a mapping. Args: uuid: uuid to query. mapping: Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping. \"\"\" inverted_map = { v . uuid : k for k , v in mapping . items ()} return inverted_map [ uuid ] repo Base ZenML repository Repository ZenML repository definition. Every ZenML project exists inside a ZenML repository. Source code in zenml/core/repo.py class Repository : \"\"\"ZenML repository definition. Every ZenML project exists inside a ZenML repository. \"\"\" def __init__ ( self , path : Optional [ str ] = None ): \"\"\" Construct reference to a ZenML repository. Args: path (str): Path to root of repository \"\"\" self . path = zenml . io . utils . get_zenml_dir ( path ) self . service = LocalService ( repo_path = self . path ) try : self . git_wrapper = GitWrapper ( self . path ) except InvalidGitRepositoryError : self . git_wrapper = None # type: ignore[assignment] @staticmethod def init_repo ( path : str = os . getcwd ()) -> None : \"\"\"Initializes a ZenML repository. Args: path: Path where the ZenML repository should be created. Raises: InitializationException: If a ZenML repository already exists at the given path. \"\"\" if zenml . io . utils . is_zenml_dir ( path ): raise InitializationException ( f \"A ZenML repository already exists at path ' { path } '.\" ) # Create the base dir zen_dir = os . path . join ( path , ZENML_DIR_NAME ) fileio . create_dir_recursive_if_not_exists ( zen_dir ) from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator service = LocalService ( repo_path = path ) artifact_store_path = os . path . join ( zenml . io . utils . get_global_config_directory (), \"local_stores\" , str ( service . uuid ), ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) service . register_artifact_store ( \"local_artifact_store\" , LocalArtifactStore ( path = artifact_store_path , repo_path = path ), ) service . register_metadata_store ( \"local_metadata_store\" , SQLiteMetadataStore ( uri = metadata_store_path , repo_path = path ), ) service . register_orchestrator ( \"local_orchestrator\" , LocalOrchestrator ( repo_path = path ) ) service . register_stack ( \"local_stack\" , BaseStack ( metadata_store_name = \"local_metadata_store\" , artifact_store_name = \"local_artifact_store\" , orchestrator_name = \"local_orchestrator\" , ), ) service . set_active_stack_key ( \"local_stack\" ) def get_git_wrapper ( self ) -> GitWrapper : \"\"\"Returns the git wrapper for the repo.\"\"\" return self . git_wrapper def get_service ( self ) -> LocalService : \"\"\"Returns the active service. For now, always local.\"\"\" return self . service @track ( event = SET_STACK ) def set_active_stack ( self , stack_key : str ) -> None : \"\"\"Set the active stack for the repo. This change is local for the machine. Args: stack_key: Key of the stack to set active. \"\"\" self . service . set_active_stack_key ( stack_key ) def get_active_stack_key ( self ) -> str : \"\"\"Get the active stack key from global config. Returns: Currently active stacks key. \"\"\" return self . service . get_active_stack_key () def get_active_stack ( self ) -> BaseStack : \"\"\"Get the active stack from global config. Returns: Currently active stack. \"\"\" return self . service . get_stack ( self . get_active_stack_key ()) @track ( event = GET_PIPELINES ) def get_pipelines ( self , stack_key : Optional [ str ] = None ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines. Args: stack_key: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. \"\"\" stack_key = stack_key or self . get_active_stack_key () metadata_store = self . service . get_stack ( stack_key ) . metadata_store return metadata_store . get_pipelines () @track ( event = GET_PIPELINE ) def get_pipeline ( self , pipeline_name : str , stack_key : Optional [ str ] = None ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name or `None` if it doesn't exist. Args: pipeline_name: Name of the pipeline. stack_key: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. \"\"\" stack_key = stack_key or self . get_active_stack_key () metadata_store = self . service . get_stack ( stack_key ) . metadata_store return metadata_store . get_pipeline ( pipeline_name ) def clean ( self ) -> None : \"\"\"Deletes associated metadata store, pipelines dir and artifacts\"\"\" raise NotImplementedError __init__ ( self , path = None ) special Construct reference to a ZenML repository. Parameters: Name Type Description Default path str Path to root of repository None Source code in zenml/core/repo.py def __init__ ( self , path : Optional [ str ] = None ): \"\"\" Construct reference to a ZenML repository. Args: path (str): Path to root of repository \"\"\" self . path = zenml . io . utils . get_zenml_dir ( path ) self . service = LocalService ( repo_path = self . path ) try : self . git_wrapper = GitWrapper ( self . path ) except InvalidGitRepositoryError : self . git_wrapper = None # type: ignore[assignment] clean ( self ) Deletes associated metadata store, pipelines dir and artifacts Source code in zenml/core/repo.py def clean ( self ) -> None : \"\"\"Deletes associated metadata store, pipelines dir and artifacts\"\"\" raise NotImplementedError get_active_stack ( self ) Get the active stack from global config. Returns: Type Description BaseStack Currently active stack. Source code in zenml/core/repo.py def get_active_stack ( self ) -> BaseStack : \"\"\"Get the active stack from global config. Returns: Currently active stack. \"\"\" return self . service . get_stack ( self . get_active_stack_key ()) get_active_stack_key ( self ) Get the active stack key from global config. Returns: Type Description str Currently active stacks key. Source code in zenml/core/repo.py def get_active_stack_key ( self ) -> str : \"\"\"Get the active stack key from global config. Returns: Currently active stacks key. \"\"\" return self . service . get_active_stack_key () get_git_wrapper ( self ) Returns the git wrapper for the repo. Source code in zenml/core/repo.py def get_git_wrapper ( self ) -> GitWrapper : \"\"\"Returns the git wrapper for the repo.\"\"\" return self . git_wrapper get_pipeline ( * args , ** kwargs ) Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result get_pipelines ( * args , ** kwargs ) Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result get_service ( self ) Returns the active service. For now, always local. Source code in zenml/core/repo.py def get_service ( self ) -> LocalService : \"\"\"Returns the active service. For now, always local.\"\"\" return self . service init_repo ( path = '/home/apenner/PycharmProjects/zenml/docs' ) staticmethod Initializes a ZenML repository. Parameters: Name Type Description Default path str Path where the ZenML repository should be created. '/home/apenner/PycharmProjects/zenml/docs' Exceptions: Type Description InitializationException If a ZenML repository already exists at the given path. Source code in zenml/core/repo.py @staticmethod def init_repo ( path : str = os . getcwd ()) -> None : \"\"\"Initializes a ZenML repository. Args: path: Path where the ZenML repository should be created. Raises: InitializationException: If a ZenML repository already exists at the given path. \"\"\" if zenml . io . utils . is_zenml_dir ( path ): raise InitializationException ( f \"A ZenML repository already exists at path ' { path } '.\" ) # Create the base dir zen_dir = os . path . join ( path , ZENML_DIR_NAME ) fileio . create_dir_recursive_if_not_exists ( zen_dir ) from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator service = LocalService ( repo_path = path ) artifact_store_path = os . path . join ( zenml . io . utils . get_global_config_directory (), \"local_stores\" , str ( service . uuid ), ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) service . register_artifact_store ( \"local_artifact_store\" , LocalArtifactStore ( path = artifact_store_path , repo_path = path ), ) service . register_metadata_store ( \"local_metadata_store\" , SQLiteMetadataStore ( uri = metadata_store_path , repo_path = path ), ) service . register_orchestrator ( \"local_orchestrator\" , LocalOrchestrator ( repo_path = path ) ) service . register_stack ( \"local_stack\" , BaseStack ( metadata_store_name = \"local_metadata_store\" , artifact_store_name = \"local_artifact_store\" , orchestrator_name = \"local_orchestrator\" , ), ) service . set_active_stack_key ( \"local_stack\" ) set_active_stack ( * args , ** kwargs ) Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result utils define_json_config_settings_source ( config_dir , config_name ) Define a function to essentially deserialize a model from a serialized json config. Parameters: Name Type Description Default config_dir str A path to a dir where we want the config file to exist. required config_name str Full name of config file. required Returns: Type Description Callable[[BaseSettings], Dict[str, Any]] A json_config_settings_source callable reading from the passed path. Source code in zenml/core/utils.py def define_json_config_settings_source ( config_dir : str , config_name : str ) -> SettingsSourceCallable : \"\"\" Define a function to essentially deserialize a model from a serialized json config. Args: config_dir: A path to a dir where we want the config file to exist. config_name: Full name of config file. Returns: A `json_config_settings_source` callable reading from the passed path. \"\"\" def json_config_settings_source ( settings : BaseSettings ) -> Dict [ str , Any ]: \"\"\" A simple settings source that loads variables from a YAML file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading the config json file. Args: settings (BaseSettings): BaseSettings from pydantic. Returns: A dict with all configuration, empty dict if config not found. \"\"\" full_path = Path ( config_dir ) / config_name logger . debug ( f \"Parsing file: { full_path } \" ) if fileio . file_exists ( str ( full_path )): return cast ( Dict [ str , Any ], yaml_utils . read_json ( str ( full_path ))) return {} return json_config_settings_source generate_customise_sources ( file_dir , file_name ) Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Parameters: Name Type Description Default file_dir str Dir where file is stored. required file_name str Name of the file to persist. required Returns: Type Description Callable[[Type[pydantic.env_settings.BaseSettings.Config], Callable[[BaseSettings], Dict[str, Any]], Callable[[BaseSettings], Dict[str, Any]], Callable[[BaseSettings], Dict[str, Any]]], Tuple[Callable[[BaseSettings], Dict[str, Any]], ...]] A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. Source code in zenml/core/utils.py def generate_customise_sources ( file_dir : str , file_name : str ) -> Callable [ [ Type [ BaseSettings . Config ], SettingsSourceCallable , SettingsSourceCallable , SettingsSourceCallable , ], Tuple [ SettingsSourceCallable , ... ], ]: \"\"\"Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the `define_json_config_settings_source` is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir: Dir where file is stored. file_name: Name of the file to persist. Returns: A `customise_sources` class method to be defined the a Pydantic BaseSettings inner Config class. \"\"\" def customise_sources ( cls : Type [ BaseSettings . Config ], init_settings : SettingsSourceCallable , env_settings : SettingsSourceCallable , file_secret_settings : SettingsSourceCallable , ) -> Tuple [ SettingsSourceCallable , ... ]: \"\"\"Defines precedence of sources to read/write settings from.\"\"\" return ( init_settings , env_settings , define_json_config_settings_source ( file_dir , file_name , ), file_secret_settings , ) return classmethod ( customise_sources ) # type: ignore[return-value]","title":"Core"},{"location":"api_docs/core/#core","text":"","title":"Core"},{"location":"api_docs/core/#zenml.core","text":"The core module is where all the base ZenML functionality is defined, including a Pydantic base class for components, a git wrapper and a class for ZenML's own repository methods. This module is also where the local service functionality (which keeps track of all the ZenML components) is defined. Every ZenML project has its own ZenML repository, and the repo module is where associated methods are defined. The repo.init_repo method is where all our functionality is kickstarted when you first initialize everything through the `zenml init CLI command.","title":"core"},{"location":"api_docs/core/#zenml.core.base_component","text":"","title":"base_component"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent","text":"Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: If a uuid is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. Source code in zenml/core/base_component.py class BaseComponent ( BaseSettings ): \"\"\"Class definition for the base config. The base component class defines the basic serialization / deserialization of various components used in ZenML. The logic of the serialization / deserialization is as follows: * If a `uuid` is passed in, then the object is read from a file, so theconstructor becomes a query for an object that is assumed to already been serialized. * If a 'uuid` is NOT passed, then a new object is created with the default args (and any other args that are passed), and therefore a fresh serialization takes place. \"\"\" uuid : Optional [ UUID ] = Field ( default_factory = uuid4 ) _file_suffix = \".json\" _superfluous_options : Dict [ str , Any ] = {} _serialization_dir : str def __init__ ( self , serialization_dir : str , ** values : Any ): # Here, we insert monkey patch the `customise_sources` function # because we want to dynamically generate the serialization # file path and name. if hasattr ( self , \"uuid\" ): self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) elif \"uuid\" in values : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , f \" { str ( values [ 'uuid' ]) }{ self . _file_suffix } \" , ) else : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) # Initialize values from the above sources. super () . __init__ ( ** values ) self . _serialization_dir = serialization_dir self . _save_backup_file_if_required () def _save_backup_file_if_required ( self ) -> None : \"\"\"Saves a backup of the config file if the schema changed.\"\"\" if self . _superfluous_options : logger . warning ( \"Found superfluous configuration values for class ` %s `: %s \" , self . __class__ . __name__ , set ( self . _superfluous_options ), ) config_path = self . get_serialization_full_path () if fileio . file_exists ( config_path ): backup_path = config_path + \".backup\" fileio . copy ( config_path , backup_path , overwrite = True ) logger . warning ( \"Saving backup configuration to ' %s '.\" , backup_path ) # save the updated file without the extra options self . update () def _dump ( self ) -> None : \"\"\"Dumps all current values to the serialization file.\"\"\" self . _create_serialization_file_if_not_exists () file_path = self . get_serialization_full_path () file_content = self . json ( indent = 2 , sort_keys = True , exclude = { SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME }, ) zenml . io . utils . write_file_contents_as_string ( file_path , file_content ) def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) } def _create_serialization_file_if_not_exists ( self ) -> None : \"\"\"Creates the serialization file if it does not exist.\"\"\" f = self . get_serialization_full_path () if not fileio . file_exists ( str ( f )): fileio . create_file_if_not_exists ( str ( f )) def get_serialization_dir ( self ) -> str : \"\"\"Return the dir where object is serialized.\"\"\" return self . _serialization_dir def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.\"\"\" if hasattr ( self , \"uuid\" ): return f \" { str ( self . uuid ) }{ self . _file_suffix } \" else : return f \"DEFAULT { self . _file_suffix } \" def get_serialization_full_path ( self ) -> str : \"\"\"Returns the full path of the serialization file.\"\"\" return os . path . join ( self . _serialization_dir , self . get_serialization_file_name () ) def update ( self ) -> None : \"\"\"Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. \"\"\" self . _dump () def delete ( self ) -> None : \"\"\"Deletes the persisted state of this object.\"\"\" fileio . remove ( self . get_serialization_full_path ()) @root_validator ( pre = True ) def check_superfluous_options ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes `_superfluous_options` attribute.\"\"\" field_names = { field . alias for field in cls . __fields__ . values ()} superfluous_options : Dict [ str , Any ] = {} for key in set ( values ): if key not in field_names : superfluous_options [ key ] = values . pop ( key ) values [ SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME ] = superfluous_options return values class Config : \"\"\"Configuration of settings.\"\"\" arbitrary_types_allowed = True env_prefix = \"zenml_\" # allow extra options so we can detect legacy configuration files extra = \"allow\"","title":"BaseComponent"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.Config","text":"Configuration of settings. Source code in zenml/core/base_component.py class Config : \"\"\"Configuration of settings.\"\"\" arbitrary_types_allowed = True env_prefix = \"zenml_\" # allow extra options so we can detect legacy configuration files extra = \"allow\"","title":"Config"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.__init__","text":"Create a new model by parsing and validating input data from keyword arguments. Raises ValidationError if the input data cannot be parsed to form a valid model. Source code in zenml/core/base_component.py def __init__ ( self , serialization_dir : str , ** values : Any ): # Here, we insert monkey patch the `customise_sources` function # because we want to dynamically generate the serialization # file path and name. if hasattr ( self , \"uuid\" ): self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) elif \"uuid\" in values : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , f \" { str ( values [ 'uuid' ]) }{ self . _file_suffix } \" , ) else : self . __config__ . customise_sources = generate_customise_sources ( # type: ignore[assignment] # noqa serialization_dir , self . get_serialization_file_name (), ) # Initialize values from the above sources. super () . __init__ ( ** values ) self . _serialization_dir = serialization_dir self . _save_backup_file_if_required ()","title":"__init__()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.check_superfluous_options","text":"Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes _superfluous_options attribute. Source code in zenml/core/base_component.py @root_validator ( pre = True ) def check_superfluous_options ( cls , values : Dict [ str , Any ] ) -> Dict [ str , Any ]: \"\"\"Detects superfluous config values (usually read from an existing config file after the schema changed) and saves them in the classes `_superfluous_options` attribute.\"\"\" field_names = { field . alias for field in cls . __fields__ . values ()} superfluous_options : Dict [ str , Any ] = {} for key in set ( values ): if key not in field_names : superfluous_options [ key ] = values . pop ( key ) values [ SUPERFLUOUS_OPTIONS_ATTRIBUTE_NAME ] = superfluous_options return values","title":"check_superfluous_options()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.delete","text":"Deletes the persisted state of this object. Source code in zenml/core/base_component.py def delete ( self ) -> None : \"\"\"Deletes the persisted state of this object.\"\"\" fileio . remove ( self . get_serialization_full_path ())","title":"delete()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.dict","text":"Removes private attributes from pydantic dict so they don't get stored in our config files. Source code in zenml/core/base_component.py def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) }","title":"dict()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.get_serialization_dir","text":"Return the dir where object is serialized. Source code in zenml/core/base_component.py def get_serialization_dir ( self ) -> str : \"\"\"Return the dir where object is serialized.\"\"\" return self . _serialization_dir","title":"get_serialization_dir()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.get_serialization_file_name","text":"Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default. Source code in zenml/core/base_component.py def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized. This has a sane default in cases where uuid is not passed externally, and therefore reading from a serialize file is not an option for the table. However, we still this function to go through without an exception, therefore the sane default.\"\"\" if hasattr ( self , \"uuid\" ): return f \" { str ( self . uuid ) }{ self . _file_suffix } \" else : return f \"DEFAULT { self . _file_suffix } \"","title":"get_serialization_file_name()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.get_serialization_full_path","text":"Returns the full path of the serialization file. Source code in zenml/core/base_component.py def get_serialization_full_path ( self ) -> str : \"\"\"Returns the full path of the serialization file.\"\"\" return os . path . join ( self . _serialization_dir , self . get_serialization_file_name () )","title":"get_serialization_full_path()"},{"location":"api_docs/core/#zenml.core.base_component.BaseComponent.update","text":"Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. Source code in zenml/core/base_component.py def update ( self ) -> None : \"\"\"Persist the current state of the component. Calling this will result in a persistent, stateful change in the system. \"\"\" self . _dump ()","title":"update()"},{"location":"api_docs/core/#zenml.core.component_factory","text":"Factory to register all components.","title":"component_factory"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory","text":"Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. Source code in zenml/core/component_factory.py class ComponentFactory : \"\"\"Definition of ComponentFactory to track all BaseComponent subclasses. All BaseComponents (including custom ones) are to be registered here. \"\"\" def __init__ ( self , name : str ): \"\"\"Constructor for the factory. Args: name: Unique name for the factory. \"\"\" self . name = name self . components : Dict [ str , BaseComponentType ] = {} def get_components ( self ) -> Dict [ str , BaseComponentType ]: \"\"\"Return all components\"\"\" return self . components def get_single_component ( self , key : str ) -> BaseComponentType : \"\"\"Get a registered component from a key.\"\"\" if key in self . components : return self . components [ key ] raise KeyError ( f \"Type ' { key } ' does not exist! Available options: \" f \" { [ str ( k ) for k in self . components . keys ()] } \" ) def get_component_key ( self , component : BaseComponentType ) -> str : \"\"\"Gets the key of a registered component.\"\"\" for k , v in self . components . items (): if v == component : return k raise KeyError ( f \"Type ' { component } ' does not exist! Available options: \" f \" { [ str ( v ) for v in self . components . values ()] } \" ) def register_component ( self , key : str , component : BaseComponentType ) -> None : \"\"\"Registers a single component class for a given key.\"\"\" self . components [ str ( key )] = component def register ( self , name : str ) -> Callable [[ BaseComponentType ], BaseComponentType ]: \"\"\"Class decorator to register component classes to the internal registry. Args: name: The name of the component. Returns: A function which registers the class at this ComponentFactory. \"\"\" def inner_wrapper ( wrapped_class : BaseComponentType , ) -> BaseComponentType : \"\"\"Inner wrapper for decorator.\"\"\" if name in self . components : logger . debug ( \"Executor %s already exists for factory %s , replacing it..\" , name , self . name , ) self . register_component ( name , wrapped_class ) return wrapped_class return inner_wrapper","title":"ComponentFactory"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.__init__","text":"Constructor for the factory. Parameters: Name Type Description Default name str Unique name for the factory. required Source code in zenml/core/component_factory.py def __init__ ( self , name : str ): \"\"\"Constructor for the factory. Args: name: Unique name for the factory. \"\"\" self . name = name self . components : Dict [ str , BaseComponentType ] = {}","title":"__init__()"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.get_component_key","text":"Gets the key of a registered component. Source code in zenml/core/component_factory.py def get_component_key ( self , component : BaseComponentType ) -> str : \"\"\"Gets the key of a registered component.\"\"\" for k , v in self . components . items (): if v == component : return k raise KeyError ( f \"Type ' { component } ' does not exist! Available options: \" f \" { [ str ( v ) for v in self . components . values ()] } \" )","title":"get_component_key()"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.get_components","text":"Return all components Source code in zenml/core/component_factory.py def get_components ( self ) -> Dict [ str , BaseComponentType ]: \"\"\"Return all components\"\"\" return self . components","title":"get_components()"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.get_single_component","text":"Get a registered component from a key. Source code in zenml/core/component_factory.py def get_single_component ( self , key : str ) -> BaseComponentType : \"\"\"Get a registered component from a key.\"\"\" if key in self . components : return self . components [ key ] raise KeyError ( f \"Type ' { key } ' does not exist! Available options: \" f \" { [ str ( k ) for k in self . components . keys ()] } \" )","title":"get_single_component()"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.register","text":"Class decorator to register component classes to the internal registry. Parameters: Name Type Description Default name str The name of the component. required Returns: Type Description Callable[[Type[zenml.core.base_component.BaseComponent]], Type[zenml.core.base_component.BaseComponent]] A function which registers the class at this ComponentFactory. Source code in zenml/core/component_factory.py def register ( self , name : str ) -> Callable [[ BaseComponentType ], BaseComponentType ]: \"\"\"Class decorator to register component classes to the internal registry. Args: name: The name of the component. Returns: A function which registers the class at this ComponentFactory. \"\"\" def inner_wrapper ( wrapped_class : BaseComponentType , ) -> BaseComponentType : \"\"\"Inner wrapper for decorator.\"\"\" if name in self . components : logger . debug ( \"Executor %s already exists for factory %s , replacing it..\" , name , self . name , ) self . register_component ( name , wrapped_class ) return wrapped_class return inner_wrapper","title":"register()"},{"location":"api_docs/core/#zenml.core.component_factory.ComponentFactory.register_component","text":"Registers a single component class for a given key. Source code in zenml/core/component_factory.py def register_component ( self , key : str , component : BaseComponentType ) -> None : \"\"\"Registers a single component class for a given key.\"\"\" self . components [ str ( key )] = component","title":"register_component()"},{"location":"api_docs/core/#zenml.core.git_wrapper","text":"Wrapper class to handle Git integration","title":"git_wrapper"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper","text":"Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. Source code in zenml/core/git_wrapper.py class GitWrapper : \"\"\"Wrapper class for Git. This class is responsible for handling git interactions, primarily handling versioning of different steps in pipelines. \"\"\" def __init__ ( self , repo_path : str ): \"\"\" Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError: If repository is not a git repository. NoSuchPathError: If the repo_path does not exist. \"\"\" # TODO [ENG-163]: Raise ZenML exceptions here instead. self . repo_path : str = repo_path self . git_root_path : str = os . path . join ( repo_path , GIT_FOLDER_NAME ) self . git_repo = GitRepo ( self . repo_path ) def check_file_committed ( self , file_path : str ) -> bool : \"\"\" Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. \"\"\" uncommitted_files = [ i . a_path for i in self . git_repo . index . diff ( None )] try : staged_files = [ i . a_path for i in self . git_repo . index . diff ( \"HEAD\" )] except BadName : # for Ref 'HEAD' did not resolve to an object logger . debug ( \"No committed files in the repo. No staged files.\" ) staged_files = [] # source: https://stackoverflow.com/questions/3801321/ untracked_files = self . git_repo . git . ls_files ( others = True , exclude_standard = True ) . split ( \" \\n \" ) for item in uncommitted_files + staged_files + untracked_files : # These are all changed files if file_path == item : return False return True def get_current_sha ( self ) -> str : \"\"\" Finds the git sha that each file within the module is currently on. \"\"\" return cast ( str , self . git_repo . head . object . hexsha ) def check_module_clean ( self , source : str ) -> bool : \"\"\"Returns `True` if all files within source's module are committed. Args: source: relative module path pointing to a Class. \"\"\" # Get the module path module_path = source_utils . get_module_source_from_source ( source ) # Get relative path of module because check_file_committed needs that module_dir = source_utils . get_relative_path_from_module_source ( module_path ) # Get absolute path of module because fileio.list_dir needs that mod_abs_dir = source_utils . get_absolute_path_from_module_source ( module_path ) module_file_names = fileio . list_dir ( mod_abs_dir , only_file_names = True ) # Go through each file in module and see if there are uncommitted ones for file_path in module_file_names : path = os . path . join ( module_dir , file_path ) # if its .gitignored then continue and don't do anything if len ( self . git_repo . ignored ( path )) > 0 : continue if fileio . is_dir ( os . path . join ( mod_abs_dir , file_path )): logger . warning ( f \"The step { source } is contained inside a module \" f \"that \" f \"has sub-directories (the sub-directory { file_path } at \" f \" { mod_abs_dir } ). For now, ZenML supports only a flat \" f \"directory structure in which to place Steps. Please make\" f \" sure that the Step does not utilize the sub-directory.\" ) if not self . check_file_committed ( path ): return False return True def stash ( self ) -> None : \"\"\"Wrapper for git stash\"\"\" git = self . git_repo . git git . stash () def stash_pop ( self ) -> None : \"\"\"Wrapper for git stash pop. Only pops if there's something to pop.\"\"\" git = self . git_repo . git if git . stash ( \"list\" ) != \"\" : git . stash ( \"pop\" ) def checkout ( self , sha_or_branch : Optional [ str ] = None , directory : Optional [ str ] = None , ) -> None : \"\"\"Wrapper for git checkout Args: sha_or_branch: hex string of len 40 representing git sha OR name of branch directory: relative path to directory to scope checkout \"\"\" # TODO [ENG-164]: Implement exception handling git = self . git_repo . git if sha_or_branch is None : # Checks out directory at sha_or_branch assert directory is not None git . checkout ( \"--\" , directory ) elif directory is not None : assert sha_or_branch is not None # Basically discards all changes in directory git . checkout ( sha_or_branch , \"--\" , directory ) else : # The case where sha_or_branch is not None and directory is None # In this case, the whole repo is checked out at sha_or_branch git . checkout ( sha_or_branch ) def reset ( self , directory : Optional [ str ] = None ) -> None : \"\"\"Wrapper for `git reset HEAD <directory>`. Args: directory: Relative path to directory to scope checkout \"\"\" git = self . git_repo . git git . reset ( \"HEAD\" , directory ) def resolve_class_source ( self , class_source : str ) -> str : \"\"\"Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within `module` are all committed. If even one file is not committed, then returns `source` unchanged. Args: class_source (str): class_source e.g. this.module.Class \"\"\" if \"@\" in class_source : # already pinned return class_source if is_standard_source ( class_source ): # that means use standard version return resolve_standard_source ( class_source ) # otherwise use Git resolution if not self . check_module_clean ( class_source ): # Return the source path if not clean logger . warning ( \"Found uncommitted file. Pipelines run with this \" \"configuration may not be reproducible. Please commit \" \"all files in this module and then run the pipeline to \" \"ensure reproducibility.\" ) return class_source return class_source + \"@\" + self . get_current_sha () def is_valid_source ( self , source : str ) -> bool : \"\"\" Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. \"\"\" try : self . load_source_path_class ( source ) except GitException : return False return True def load_source_path_class ( self , source : str ) -> Type [ Any ]: \"\"\" Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" source = source . split ( \"@\" )[ 0 ] pin = source . split ( \"@\" )[ - 1 ] is_standard = is_standard_pin ( pin ) if \"@\" in source and not is_standard : logger . debug ( \"Pinned step found with git sha. \" \"Loading class from git history.\" ) module_source = get_module_source_from_source ( source ) relative_module_path = get_relative_path_from_module_source ( module_source ) logger . warning ( \"Found source with a pinned sha. Will now checkout \" f \"module: { module_source } \" ) # critical step if not self . check_module_clean ( source ): raise GitException ( f \"One of the files at { relative_module_path } \" f \"is not committed and we \" f \"are trying to load that directory from git \" f \"history due to a pinned step in the pipeline. \" f \"Please commit the file and then run the \" f \"pipeline.\" ) # Check out the directory at that sha self . checkout ( sha_or_branch = pin , directory = relative_module_path ) # After this point, all exceptions will first undo the above try : class_ = source_utils . import_class_by_path ( source ) self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) except Exception as e : self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) raise GitException ( f \"A git exception occurred when checking out repository \" f \"from git history. Resetting repository to original \" f \"state. Original exception: { e } \" ) elif \"@\" in source and is_standard : logger . debug ( f \"Default { APP_NAME } class used. Loading directly.\" ) # TODO [ENG-165]: Check if ZenML version is installed before loading. class_ = source_utils . import_class_by_path ( source ) else : logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = source_utils . import_class_by_path ( source ) return class_ def resolve_class ( self , class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. \"\"\" class_source = source_utils . resolve_class ( class_ ) return self . resolve_class_source ( class_source )","title":"GitWrapper"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.__init__","text":"Initialize GitWrapper. Should be initialized by ZenML Repository. Parameters: Name Type Description Default repo_path str required Exceptions: Type Description InvalidGitRepositoryError If repository is not a git repository. NoSuchPathError If the repo_path does not exist. Source code in zenml/core/git_wrapper.py def __init__ ( self , repo_path : str ): \"\"\" Initialize GitWrapper. Should be initialized by ZenML Repository. Args: repo_path: Raises: InvalidGitRepositoryError: If repository is not a git repository. NoSuchPathError: If the repo_path does not exist. \"\"\" # TODO [ENG-163]: Raise ZenML exceptions here instead. self . repo_path : str = repo_path self . git_root_path : str = os . path . join ( repo_path , GIT_FOLDER_NAME ) self . git_repo = GitRepo ( self . repo_path )","title":"__init__()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.check_file_committed","text":"Checks file is committed. If yes, return True, else False. Parameters: Name Type Description Default file_path str Path to any file within the ZenML repo. required Source code in zenml/core/git_wrapper.py def check_file_committed ( self , file_path : str ) -> bool : \"\"\" Checks file is committed. If yes, return True, else False. Args: file_path (str): Path to any file within the ZenML repo. \"\"\" uncommitted_files = [ i . a_path for i in self . git_repo . index . diff ( None )] try : staged_files = [ i . a_path for i in self . git_repo . index . diff ( \"HEAD\" )] except BadName : # for Ref 'HEAD' did not resolve to an object logger . debug ( \"No committed files in the repo. No staged files.\" ) staged_files = [] # source: https://stackoverflow.com/questions/3801321/ untracked_files = self . git_repo . git . ls_files ( others = True , exclude_standard = True ) . split ( \" \\n \" ) for item in uncommitted_files + staged_files + untracked_files : # These are all changed files if file_path == item : return False return True","title":"check_file_committed()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.check_module_clean","text":"Returns True if all files within source's module are committed. Parameters: Name Type Description Default source str relative module path pointing to a Class. required Source code in zenml/core/git_wrapper.py def check_module_clean ( self , source : str ) -> bool : \"\"\"Returns `True` if all files within source's module are committed. Args: source: relative module path pointing to a Class. \"\"\" # Get the module path module_path = source_utils . get_module_source_from_source ( source ) # Get relative path of module because check_file_committed needs that module_dir = source_utils . get_relative_path_from_module_source ( module_path ) # Get absolute path of module because fileio.list_dir needs that mod_abs_dir = source_utils . get_absolute_path_from_module_source ( module_path ) module_file_names = fileio . list_dir ( mod_abs_dir , only_file_names = True ) # Go through each file in module and see if there are uncommitted ones for file_path in module_file_names : path = os . path . join ( module_dir , file_path ) # if its .gitignored then continue and don't do anything if len ( self . git_repo . ignored ( path )) > 0 : continue if fileio . is_dir ( os . path . join ( mod_abs_dir , file_path )): logger . warning ( f \"The step { source } is contained inside a module \" f \"that \" f \"has sub-directories (the sub-directory { file_path } at \" f \" { mod_abs_dir } ). For now, ZenML supports only a flat \" f \"directory structure in which to place Steps. Please make\" f \" sure that the Step does not utilize the sub-directory.\" ) if not self . check_file_committed ( path ): return False return True","title":"check_module_clean()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.checkout","text":"Wrapper for git checkout Parameters: Name Type Description Default sha_or_branch Optional[str] hex string of len 40 representing git sha OR name of branch None directory Optional[str] relative path to directory to scope checkout None Source code in zenml/core/git_wrapper.py def checkout ( self , sha_or_branch : Optional [ str ] = None , directory : Optional [ str ] = None , ) -> None : \"\"\"Wrapper for git checkout Args: sha_or_branch: hex string of len 40 representing git sha OR name of branch directory: relative path to directory to scope checkout \"\"\" # TODO [ENG-164]: Implement exception handling git = self . git_repo . git if sha_or_branch is None : # Checks out directory at sha_or_branch assert directory is not None git . checkout ( \"--\" , directory ) elif directory is not None : assert sha_or_branch is not None # Basically discards all changes in directory git . checkout ( sha_or_branch , \"--\" , directory ) else : # The case where sha_or_branch is not None and directory is None # In this case, the whole repo is checked out at sha_or_branch git . checkout ( sha_or_branch )","title":"checkout()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.get_current_sha","text":"Finds the git sha that each file within the module is currently on. Source code in zenml/core/git_wrapper.py def get_current_sha ( self ) -> str : \"\"\" Finds the git sha that each file within the module is currently on. \"\"\" return cast ( str , self . git_repo . head . object . hexsha )","title":"get_current_sha()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.is_valid_source","text":"Checks whether the source_path is valid or not. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/core/git_wrapper.py def is_valid_source ( self , source : str ) -> bool : \"\"\" Checks whether the source_path is valid or not. Args: source (str): class_source e.g. this.module.Class[@pin]. \"\"\" try : self . load_source_path_class ( source ) except GitException : return False return True","title":"is_valid_source()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.load_source_path_class","text":"Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/core/git_wrapper.py def load_source_path_class ( self , source : str ) -> Type [ Any ]: \"\"\" Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" source = source . split ( \"@\" )[ 0 ] pin = source . split ( \"@\" )[ - 1 ] is_standard = is_standard_pin ( pin ) if \"@\" in source and not is_standard : logger . debug ( \"Pinned step found with git sha. \" \"Loading class from git history.\" ) module_source = get_module_source_from_source ( source ) relative_module_path = get_relative_path_from_module_source ( module_source ) logger . warning ( \"Found source with a pinned sha. Will now checkout \" f \"module: { module_source } \" ) # critical step if not self . check_module_clean ( source ): raise GitException ( f \"One of the files at { relative_module_path } \" f \"is not committed and we \" f \"are trying to load that directory from git \" f \"history due to a pinned step in the pipeline. \" f \"Please commit the file and then run the \" f \"pipeline.\" ) # Check out the directory at that sha self . checkout ( sha_or_branch = pin , directory = relative_module_path ) # After this point, all exceptions will first undo the above try : class_ = source_utils . import_class_by_path ( source ) self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) except Exception as e : self . reset ( relative_module_path ) self . checkout ( directory = relative_module_path ) raise GitException ( f \"A git exception occurred when checking out repository \" f \"from git history. Resetting repository to original \" f \"state. Original exception: { e } \" ) elif \"@\" in source and is_standard : logger . debug ( f \"Default { APP_NAME } class used. Loading directly.\" ) # TODO [ENG-165]: Check if ZenML version is installed before loading. class_ = source_utils . import_class_by_path ( source ) else : logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = source_utils . import_class_by_path ( source ) return class_","title":"load_source_path_class()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.reset","text":"Wrapper for git reset HEAD <directory> . Parameters: Name Type Description Default directory Optional[str] Relative path to directory to scope checkout None Source code in zenml/core/git_wrapper.py def reset ( self , directory : Optional [ str ] = None ) -> None : \"\"\"Wrapper for `git reset HEAD <directory>`. Args: directory: Relative path to directory to scope checkout \"\"\" git = self . git_repo . git git . reset ( \"HEAD\" , directory )","title":"reset()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.resolve_class","text":"Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class[@pin]. Source code in zenml/core/git_wrapper.py def resolve_class ( self , class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class[@pin]. \"\"\" class_source = source_utils . resolve_class ( class_ ) return self . resolve_class_source ( class_source )","title":"resolve_class()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.resolve_class_source","text":"Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within module are all committed. If even one file is not committed, then returns source unchanged. Parameters: Name Type Description Default class_source str class_source e.g. this.module.Class required Source code in zenml/core/git_wrapper.py def resolve_class_source ( self , class_source : str ) -> str : \"\"\"Resolves class_source with an optional pin. Takes source (e.g. this.module.ClassName), and appends relevant sha to it if the files within `module` are all committed. If even one file is not committed, then returns `source` unchanged. Args: class_source (str): class_source e.g. this.module.Class \"\"\" if \"@\" in class_source : # already pinned return class_source if is_standard_source ( class_source ): # that means use standard version return resolve_standard_source ( class_source ) # otherwise use Git resolution if not self . check_module_clean ( class_source ): # Return the source path if not clean logger . warning ( \"Found uncommitted file. Pipelines run with this \" \"configuration may not be reproducible. Please commit \" \"all files in this module and then run the pipeline to \" \"ensure reproducibility.\" ) return class_source return class_source + \"@\" + self . get_current_sha ()","title":"resolve_class_source()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.stash","text":"Wrapper for git stash Source code in zenml/core/git_wrapper.py def stash ( self ) -> None : \"\"\"Wrapper for git stash\"\"\" git = self . git_repo . git git . stash ()","title":"stash()"},{"location":"api_docs/core/#zenml.core.git_wrapper.GitWrapper.stash_pop","text":"Wrapper for git stash pop. Only pops if there's something to pop. Source code in zenml/core/git_wrapper.py def stash_pop ( self ) -> None : \"\"\"Wrapper for git stash pop. Only pops if there's something to pop.\"\"\" git = self . git_repo . git if git . stash ( \"list\" ) != \"\" : git . stash ( \"pop\" )","title":"stash_pop()"},{"location":"api_docs/core/#zenml.core.local_service","text":"","title":"local_service"},{"location":"api_docs/core/#zenml.core.local_service.LocalService","text":"Definition of a local service that keeps track of all ZenML components. Source code in zenml/core/local_service.py class LocalService ( BaseComponent ): \"\"\"Definition of a local service that keeps track of all ZenML components. \"\"\" stacks : Dict [ str , BaseStack ] = {} active_stack_key : str = \"local_stack\" metadata_store_map : Dict [ str , UUIDSourceTuple ] = {} artifact_store_map : Dict [ str , UUIDSourceTuple ] = {} orchestrator_map : Dict [ str , UUIDSourceTuple ] = {} container_registry_map : Dict [ str , UUIDSourceTuple ] = {} _LOCAL_SERVICE_FILE_NAME = \"zenservice.json\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a LocalService instance. Args: repo_path: Path to the repository of this service. \"\"\" serialization_dir = get_zenml_config_dir ( repo_path ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) self . _repo_path = repo_path for stack in self . stacks . values (): stack . _repo_path = repo_path def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized.\"\"\" return self . _LOCAL_SERVICE_FILE_NAME @property def metadata_stores ( self ) -> Dict [ str , \"BaseMetadataStore\" ]: \"\"\"Returns all registered metadata stores.\"\"\" from zenml.metadata_stores import BaseMetadataStore return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseMetadataStore . _METADATA_STORE_DIR_NAME , self . metadata_store_map , self . _repo_path , ) @property def artifact_stores ( self ) -> Dict [ str , \"BaseArtifactStore\" ]: \"\"\"Returns all registered artifact stores.\"\"\" from zenml.artifact_stores import BaseArtifactStore return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseArtifactStore . _ARTIFACT_STORE_DIR_NAME , self . artifact_store_map , self . _repo_path , ) @property def orchestrators ( self ) -> Dict [ str , \"BaseOrchestrator\" ]: \"\"\"Returns all registered orchestrators.\"\"\" from zenml.orchestrators import BaseOrchestrator return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseOrchestrator . _ORCHESTRATOR_STORE_DIR_NAME , self . orchestrator_map , self . _repo_path , ) @property def container_registries ( self ) -> Dict [ str , \"BaseContainerRegistry\" ]: \"\"\"Returns all registered container registries.\"\"\" from zenml.container_registries import BaseContainerRegistry return mapping_utils . get_components_from_store ( # type: ignore[return-value] # noqa BaseContainerRegistry . _CONTAINER_REGISTRY_DIR_NAME , self . container_registry_map , self . _repo_path , ) def get_active_stack_key ( self ) -> str : \"\"\"Returns the active stack key.\"\"\" return self . active_stack_key def set_active_stack_key ( self , stack_key : str ) -> None : \"\"\"Sets the active stack key.\"\"\" if stack_key not in self . stacks : raise DoesNotExistException ( f \"Unable to set active stack for key ` { stack_key } ` because no \" f \"stack is registered for this key. Available keys: \" f \" { set ( self . stacks ) } \" ) self . active_stack_key = stack_key self . update () def get_stack ( self , key : str ) -> BaseStack : \"\"\"Return a single stack based on key. Args: key: Unique key of stack. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching stack with key { key } \" ) if key not in self . stacks : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . stacks . keys ()) } \" ) return self . stacks [ key ] @track ( event = REGISTERED_STACK ) def register_stack ( self , key : str , stack : BaseStack ) -> None : \"\"\"Register a stack. Args: key: Unique key for the stack. stack: Stack to be registered. \"\"\" logger . debug ( f \"Registering stack with key { key } , details: \" f \" { stack . dict () } \" ) # Check if the individual components actually exist. # TODO [ENG-190]: Add tests to check cases of registering a stack with a # non-existing individual component. We can also improve the error # logging for the CLI while we're at it. self . get_orchestrator ( stack . orchestrator_name ) self . get_artifact_store ( stack . artifact_store_name ) self . get_metadata_store ( stack . metadata_store_name ) if stack . container_registry_name : self . get_container_registry ( stack . container_registry_name ) if key in self . stacks : raise AlreadyExistsException ( message = f \"Stack ` { key } ` already exists!\" ) # Add the mapping. self . stacks [ key ] = stack self . update () def delete_stack ( self , key : str ) -> None : \"\"\"Delete a stack specified with a key. Args: key: Unique key of stack. \"\"\" _ = self . get_stack ( key ) # check whether it exists del self . stacks [ key ] self . update () logger . debug ( f \"Deleted stack with key: { key } .\" ) logger . info ( \"Deleting a stack currently does not delete the underlying \" \"architecture of the stack. It just deletes the reference to it. \" \"Therefore please make sure to delete these resources on your \" \"own. Also, if this stack was the active stack, please make sure \" \"to set a not active stack via `zenml stack set`.\" ) def get_artifact_store ( self , key : str ) -> \"BaseArtifactStore\" : \"\"\"Return a single artifact store based on key. Args: key: Unique key of artifact store. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching artifact_store with key { key } \" ) if key not in self . artifact_store_map : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . artifact_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . artifact_store_map , self . _repo_path ) def register_artifact_store ( self , key : str , artifact_store : \"BaseArtifactStore\" ) -> None : \"\"\"Register an artifact store. Args: artifact_store: Artifact store to be registered. key: Unique key for the artifact store. \"\"\" logger . debug ( f \"Registering artifact store with key { key } , details: \" f \" { artifact_store . dict () } \" ) if key in self . artifact_store_map : raise AlreadyExistsException ( message = f \"Artifact Store ` { key } ` already exists!\" ) # Add the mapping. artifact_store . update () source = source_utils . resolve_class ( artifact_store . __class__ ) self . artifact_store_map [ key ] = UUIDSourceTuple ( uuid = artifact_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import artifact_store_factory track_event ( REGISTERED_ARTIFACT_STORE , { \"type\" : artifact_store_factory . get_component_key ( artifact_store . __class__ ) }, ) def delete_artifact_store ( self , key : str ) -> None : \"\"\"Delete an artifact_store. Args: key: Unique key of artifact_store. \"\"\" s = self . get_artifact_store ( key ) # check whether it exists s . delete () del self . artifact_store_map [ key ] self . update () logger . debug ( f \"Deleted artifact_store with key: { key } .\" ) def get_metadata_store ( self , key : str ) -> \"BaseMetadataStore\" : \"\"\"Return a single metadata store based on key. Args: key: Unique key of metadata store. Returns: Metadata store specified by key. \"\"\" logger . debug ( f \"Fetching metadata store with key { key } \" ) if key not in self . metadata_store_map : raise DoesNotExistException ( f \"Metadata store of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . metadata_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . metadata_store_map , self . _repo_path ) def register_metadata_store ( self , key : str , metadata_store : \"BaseMetadataStore\" ) -> None : \"\"\"Register a metadata store. Args: metadata_store: Metadata store to be registered. key: Unique key for the metadata store. \"\"\" logger . debug ( f \"Registering metadata store with key { key } , details: \" f \" { metadata_store . dict () } \" ) if key in self . metadata_store_map : raise AlreadyExistsException ( message = f \"Metadata store ` { key } ` already exists!\" ) # Add the mapping. metadata_store . update () source = source_utils . resolve_class ( metadata_store . __class__ ) self . metadata_store_map [ key ] = UUIDSourceTuple ( uuid = metadata_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import metadata_store_factory track_event ( REGISTERED_METADATA_STORE , { \"type\" : metadata_store_factory . get_component_key ( metadata_store . __class__ ) }, ) def delete_metadata_store ( self , key : str ) -> None : \"\"\"Delete a metadata store. Args: key: Unique key of metadata store. \"\"\" s = self . get_metadata_store ( key ) # check whether it exists s . delete () del self . metadata_store_map [ key ] self . update () logger . debug ( f \"Deleted metadata store with key: { key } .\" ) def get_orchestrator ( self , key : str ) -> \"BaseOrchestrator\" : \"\"\"Return a single orchestrator based on key. Args: key: Unique key of orchestrator. Returns: Orchestrator specified by key. \"\"\" logger . debug ( f \"Fetching orchestrator with key { key } \" ) if key not in self . orchestrator_map : raise DoesNotExistException ( f \"Orchestrator of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . orchestrator_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . orchestrator_map , self . _repo_path ) def register_orchestrator ( self , key : str , orchestrator : \"BaseOrchestrator\" ) -> None : \"\"\"Register an orchestrator. Args: orchestrator: Orchestrator to be registered. key: Unique key for the orchestrator. \"\"\" logger . debug ( f \"Registering orchestrator with key { key } , details: \" f \" { orchestrator . dict () } \" ) if key in self . orchestrator_map : raise AlreadyExistsException ( message = f \"Orchestrator ` { key } ` already exists!\" ) # Add the mapping. orchestrator . update () source = source_utils . resolve_class ( orchestrator . __class__ ) self . orchestrator_map [ key ] = UUIDSourceTuple ( uuid = orchestrator . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import orchestrator_store_factory track_event ( REGISTERED_ORCHESTRATOR , { \"type\" : orchestrator_store_factory . get_component_key ( orchestrator . __class__ ) }, ) def delete_orchestrator ( self , key : str ) -> None : \"\"\"Delete a orchestrator. Args: key: Unique key of orchestrator. \"\"\" s = self . get_orchestrator ( key ) # check whether it exists s . delete () del self . orchestrator_map [ key ] self . update () logger . debug ( f \"Deleted orchestrator with key: { key } .\" ) def get_container_registry ( self , key : str ) -> \"BaseContainerRegistry\" : \"\"\"Return a single container registry based on key. Args: key: Unique key of a container registry. Returns: Container registry specified by key. \"\"\" logger . debug ( f \"Fetching container registry with key { key } \" ) if key not in self . container_registry_map : raise DoesNotExistException ( f \"Container registry of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . container_registry_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . container_registry_map , self . _repo_path ) @track ( event = REGISTERED_CONTAINER_REGISTRY ) def register_container_registry ( self , key : str , container_registry : \"BaseContainerRegistry\" ) -> None : \"\"\"Register a container registry. Args: container_registry: Container registry to be registered. key: Unique key for the container registry. \"\"\" logger . debug ( f \"Registering container registry with key { key } , details: \" f \" { container_registry . dict () } \" ) if key in self . container_registry_map : raise AlreadyExistsException ( message = f \"Container registry ` { key } ` already exists!\" ) # Add the mapping. container_registry . update () source = source_utils . resolve_class ( container_registry . __class__ ) self . container_registry_map [ key ] = UUIDSourceTuple ( uuid = container_registry . uuid , source = source ) self . update () def delete_container_registry ( self , key : str ) -> None : \"\"\"Delete a container registry. Args: key: Unique key of the container registry. \"\"\" container_registry = self . get_container_registry ( key ) container_registry . delete () del self . container_registry_map [ key ] self . update () logger . debug ( f \"Deleted container registry with key: { key } .\" ) def delete ( self ) -> None : \"\"\"Deletes the entire service. Dangerous operation\"\"\" for m in self . metadata_stores . values (): m . delete () for a in self . artifact_stores . values (): a . delete () for o in self . orchestrators . values (): o . delete () for c in self . container_registries . values (): c . delete () super () . delete ()","title":"LocalService"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.artifact_stores","text":"Returns all registered artifact stores.","title":"artifact_stores"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.container_registries","text":"Returns all registered container registries.","title":"container_registries"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.metadata_stores","text":"Returns all registered metadata stores.","title":"metadata_stores"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.orchestrators","text":"Returns all registered orchestrators.","title":"orchestrators"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.__init__","text":"Initializes a LocalService instance. Parameters: Name Type Description Default repo_path str Path to the repository of this service. required Source code in zenml/core/local_service.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a LocalService instance. Args: repo_path: Path to the repository of this service. \"\"\" serialization_dir = get_zenml_config_dir ( repo_path ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) self . _repo_path = repo_path for stack in self . stacks . values (): stack . _repo_path = repo_path","title":"__init__()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete","text":"Deletes the entire service. Dangerous operation Source code in zenml/core/local_service.py def delete ( self ) -> None : \"\"\"Deletes the entire service. Dangerous operation\"\"\" for m in self . metadata_stores . values (): m . delete () for a in self . artifact_stores . values (): a . delete () for o in self . orchestrators . values (): o . delete () for c in self . container_registries . values (): c . delete () super () . delete ()","title":"delete()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete_artifact_store","text":"Delete an artifact_store. Parameters: Name Type Description Default key str Unique key of artifact_store. required Source code in zenml/core/local_service.py def delete_artifact_store ( self , key : str ) -> None : \"\"\"Delete an artifact_store. Args: key: Unique key of artifact_store. \"\"\" s = self . get_artifact_store ( key ) # check whether it exists s . delete () del self . artifact_store_map [ key ] self . update () logger . debug ( f \"Deleted artifact_store with key: { key } .\" )","title":"delete_artifact_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete_container_registry","text":"Delete a container registry. Parameters: Name Type Description Default key str Unique key of the container registry. required Source code in zenml/core/local_service.py def delete_container_registry ( self , key : str ) -> None : \"\"\"Delete a container registry. Args: key: Unique key of the container registry. \"\"\" container_registry = self . get_container_registry ( key ) container_registry . delete () del self . container_registry_map [ key ] self . update () logger . debug ( f \"Deleted container registry with key: { key } .\" )","title":"delete_container_registry()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete_metadata_store","text":"Delete a metadata store. Parameters: Name Type Description Default key str Unique key of metadata store. required Source code in zenml/core/local_service.py def delete_metadata_store ( self , key : str ) -> None : \"\"\"Delete a metadata store. Args: key: Unique key of metadata store. \"\"\" s = self . get_metadata_store ( key ) # check whether it exists s . delete () del self . metadata_store_map [ key ] self . update () logger . debug ( f \"Deleted metadata store with key: { key } .\" )","title":"delete_metadata_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete_orchestrator","text":"Delete a orchestrator. Parameters: Name Type Description Default key str Unique key of orchestrator. required Source code in zenml/core/local_service.py def delete_orchestrator ( self , key : str ) -> None : \"\"\"Delete a orchestrator. Args: key: Unique key of orchestrator. \"\"\" s = self . get_orchestrator ( key ) # check whether it exists s . delete () del self . orchestrator_map [ key ] self . update () logger . debug ( f \"Deleted orchestrator with key: { key } .\" )","title":"delete_orchestrator()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.delete_stack","text":"Delete a stack specified with a key. Parameters: Name Type Description Default key str Unique key of stack. required Source code in zenml/core/local_service.py def delete_stack ( self , key : str ) -> None : \"\"\"Delete a stack specified with a key. Args: key: Unique key of stack. \"\"\" _ = self . get_stack ( key ) # check whether it exists del self . stacks [ key ] self . update () logger . debug ( f \"Deleted stack with key: { key } .\" ) logger . info ( \"Deleting a stack currently does not delete the underlying \" \"architecture of the stack. It just deletes the reference to it. \" \"Therefore please make sure to delete these resources on your \" \"own. Also, if this stack was the active stack, please make sure \" \"to set a not active stack via `zenml stack set`.\" )","title":"delete_stack()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_active_stack_key","text":"Returns the active stack key. Source code in zenml/core/local_service.py def get_active_stack_key ( self ) -> str : \"\"\"Returns the active stack key.\"\"\" return self . active_stack_key","title":"get_active_stack_key()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_artifact_store","text":"Return a single artifact store based on key. Parameters: Name Type Description Default key str Unique key of artifact store. required Returns: Type Description BaseArtifactStore Stack specified by key. Source code in zenml/core/local_service.py def get_artifact_store ( self , key : str ) -> \"BaseArtifactStore\" : \"\"\"Return a single artifact store based on key. Args: key: Unique key of artifact store. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching artifact_store with key { key } \" ) if key not in self . artifact_store_map : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . artifact_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . artifact_store_map , self . _repo_path )","title":"get_artifact_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_container_registry","text":"Return a single container registry based on key. Parameters: Name Type Description Default key str Unique key of a container registry. required Returns: Type Description BaseContainerRegistry Container registry specified by key. Source code in zenml/core/local_service.py def get_container_registry ( self , key : str ) -> \"BaseContainerRegistry\" : \"\"\"Return a single container registry based on key. Args: key: Unique key of a container registry. Returns: Container registry specified by key. \"\"\" logger . debug ( f \"Fetching container registry with key { key } \" ) if key not in self . container_registry_map : raise DoesNotExistException ( f \"Container registry of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . container_registry_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . container_registry_map , self . _repo_path )","title":"get_container_registry()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_metadata_store","text":"Return a single metadata store based on key. Parameters: Name Type Description Default key str Unique key of metadata store. required Returns: Type Description BaseMetadataStore Metadata store specified by key. Source code in zenml/core/local_service.py def get_metadata_store ( self , key : str ) -> \"BaseMetadataStore\" : \"\"\"Return a single metadata store based on key. Args: key: Unique key of metadata store. Returns: Metadata store specified by key. \"\"\" logger . debug ( f \"Fetching metadata store with key { key } \" ) if key not in self . metadata_store_map : raise DoesNotExistException ( f \"Metadata store of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . metadata_store_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . metadata_store_map , self . _repo_path )","title":"get_metadata_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_orchestrator","text":"Return a single orchestrator based on key. Parameters: Name Type Description Default key str Unique key of orchestrator. required Returns: Type Description BaseOrchestrator Orchestrator specified by key. Source code in zenml/core/local_service.py def get_orchestrator ( self , key : str ) -> \"BaseOrchestrator\" : \"\"\"Return a single orchestrator based on key. Args: key: Unique key of orchestrator. Returns: Orchestrator specified by key. \"\"\" logger . debug ( f \"Fetching orchestrator with key { key } \" ) if key not in self . orchestrator_map : raise DoesNotExistException ( f \"Orchestrator of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . orchestrator_map . keys ()) } \" ) return mapping_utils . get_component_from_key ( # type: ignore[return-value] # noqa key , self . orchestrator_map , self . _repo_path )","title":"get_orchestrator()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_serialization_file_name","text":"Return the name of the file where object is serialized. Source code in zenml/core/local_service.py def get_serialization_file_name ( self ) -> str : \"\"\"Return the name of the file where object is serialized.\"\"\" return self . _LOCAL_SERVICE_FILE_NAME","title":"get_serialization_file_name()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.get_stack","text":"Return a single stack based on key. Parameters: Name Type Description Default key str Unique key of stack. required Returns: Type Description BaseStack Stack specified by key. Source code in zenml/core/local_service.py def get_stack ( self , key : str ) -> BaseStack : \"\"\"Return a single stack based on key. Args: key: Unique key of stack. Returns: Stack specified by key. \"\"\" logger . debug ( f \"Fetching stack with key { key } \" ) if key not in self . stacks : raise DoesNotExistException ( f \"Stack of key ` { key } ` does not exist. \" f \"Available keys: { list ( self . stacks . keys ()) } \" ) return self . stacks [ key ]","title":"get_stack()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.register_artifact_store","text":"Register an artifact store. Parameters: Name Type Description Default artifact_store BaseArtifactStore Artifact store to be registered. required key str Unique key for the artifact store. required Source code in zenml/core/local_service.py def register_artifact_store ( self , key : str , artifact_store : \"BaseArtifactStore\" ) -> None : \"\"\"Register an artifact store. Args: artifact_store: Artifact store to be registered. key: Unique key for the artifact store. \"\"\" logger . debug ( f \"Registering artifact store with key { key } , details: \" f \" { artifact_store . dict () } \" ) if key in self . artifact_store_map : raise AlreadyExistsException ( message = f \"Artifact Store ` { key } ` already exists!\" ) # Add the mapping. artifact_store . update () source = source_utils . resolve_class ( artifact_store . __class__ ) self . artifact_store_map [ key ] = UUIDSourceTuple ( uuid = artifact_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import artifact_store_factory track_event ( REGISTERED_ARTIFACT_STORE , { \"type\" : artifact_store_factory . get_component_key ( artifact_store . __class__ ) }, )","title":"register_artifact_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.register_container_registry","text":"Inner decorator function. Source code in zenml/core/local_service.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"register_container_registry()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.register_metadata_store","text":"Register a metadata store. Parameters: Name Type Description Default metadata_store BaseMetadataStore Metadata store to be registered. required key str Unique key for the metadata store. required Source code in zenml/core/local_service.py def register_metadata_store ( self , key : str , metadata_store : \"BaseMetadataStore\" ) -> None : \"\"\"Register a metadata store. Args: metadata_store: Metadata store to be registered. key: Unique key for the metadata store. \"\"\" logger . debug ( f \"Registering metadata store with key { key } , details: \" f \" { metadata_store . dict () } \" ) if key in self . metadata_store_map : raise AlreadyExistsException ( message = f \"Metadata store ` { key } ` already exists!\" ) # Add the mapping. metadata_store . update () source = source_utils . resolve_class ( metadata_store . __class__ ) self . metadata_store_map [ key ] = UUIDSourceTuple ( uuid = metadata_store . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import metadata_store_factory track_event ( REGISTERED_METADATA_STORE , { \"type\" : metadata_store_factory . get_component_key ( metadata_store . __class__ ) }, )","title":"register_metadata_store()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.register_orchestrator","text":"Register an orchestrator. Parameters: Name Type Description Default orchestrator BaseOrchestrator Orchestrator to be registered. required key str Unique key for the orchestrator. required Source code in zenml/core/local_service.py def register_orchestrator ( self , key : str , orchestrator : \"BaseOrchestrator\" ) -> None : \"\"\"Register an orchestrator. Args: orchestrator: Orchestrator to be registered. key: Unique key for the orchestrator. \"\"\" logger . debug ( f \"Registering orchestrator with key { key } , details: \" f \" { orchestrator . dict () } \" ) if key in self . orchestrator_map : raise AlreadyExistsException ( message = f \"Orchestrator ` { key } ` already exists!\" ) # Add the mapping. orchestrator . update () source = source_utils . resolve_class ( orchestrator . __class__ ) self . orchestrator_map [ key ] = UUIDSourceTuple ( uuid = orchestrator . uuid , source = source ) self . update () # Telemetry from zenml.core.component_factory import orchestrator_store_factory track_event ( REGISTERED_ORCHESTRATOR , { \"type\" : orchestrator_store_factory . get_component_key ( orchestrator . __class__ ) }, )","title":"register_orchestrator()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.register_stack","text":"Inner decorator function. Source code in zenml/core/local_service.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"register_stack()"},{"location":"api_docs/core/#zenml.core.local_service.LocalService.set_active_stack_key","text":"Sets the active stack key. Source code in zenml/core/local_service.py def set_active_stack_key ( self , stack_key : str ) -> None : \"\"\"Sets the active stack key.\"\"\" if stack_key not in self . stacks : raise DoesNotExistException ( f \"Unable to set active stack for key ` { stack_key } ` because no \" f \"stack is registered for this key. Available keys: \" f \" { set ( self . stacks ) } \" ) self . active_stack_key = stack_key self . update ()","title":"set_active_stack_key()"},{"location":"api_docs/core/#zenml.core.mapping_utils","text":"","title":"mapping_utils"},{"location":"api_docs/core/#zenml.core.mapping_utils.UUIDSourceTuple","text":"Container used to store UUID and source information of a single BaseComponent subclass. Attributes: Name Type Description uuid UUID Identifier of the BaseComponent source str Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag Source code in zenml/core/mapping_utils.py class UUIDSourceTuple ( BaseModel ): \"\"\"Container used to store UUID and source information of a single BaseComponent subclass. Attributes: uuid: Identifier of the BaseComponent source: Contains the fully qualified class name and information about a git hash/tag. E.g. foo.bar.BaseComponentSubclass@git_tag \"\"\" uuid : UUID source : str","title":"UUIDSourceTuple"},{"location":"api_docs/core/#zenml.core.mapping_utils.get_component_from_key","text":"Given a key and a mapping, return an initialized component. Parameters: Name Type Description Default key str Unique key. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict of type str -> UUIDSourceTuple. required repo_path str Path to the repo from which to load the component. required Returns: Type Description BaseComponent An object which is a subclass of type BaseComponent. Source code in zenml/core/mapping_utils.py def get_component_from_key ( key : str , mapping : Dict [ str , UUIDSourceTuple ], repo_path : str ) -> BaseComponent : \"\"\"Given a key and a mapping, return an initialized component. Args: key: Unique key. mapping: Dict of type str -> UUIDSourceTuple. repo_path: Path to the repo from which to load the component. Returns: An object which is a subclass of type BaseComponent. \"\"\" tuple_ = mapping [ key ] class_ = source_utils . load_source_path_class ( tuple_ . source ) if not issubclass ( class_ , BaseComponent ): raise TypeError ( \"\" ) return class_ ( uuid = tuple_ . uuid , repo_path = repo_path ) # type: ignore[call-arg] # noqa","title":"get_component_from_key()"},{"location":"api_docs/core/#zenml.core.mapping_utils.get_components_from_store","text":"Returns a list of components from a store. Parameters: Name Type Description Default store_name str Name of the store. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict of type str -> UUIDSourceTuple. required repo_path str Path to the repo from which to load the components. required Returns: Type Description Dict[str, zenml.core.base_component.BaseComponent] A dict of objects which are a subclass of type BaseComponent. Source code in zenml/core/mapping_utils.py def get_components_from_store ( store_name : str , mapping : Dict [ str , UUIDSourceTuple ], repo_path : str ) -> Dict [ str , BaseComponent ]: \"\"\"Returns a list of components from a store. Args: store_name: Name of the store. mapping: Dict of type str -> UUIDSourceTuple. repo_path: Path to the repo from which to load the components. Returns: A dict of objects which are a subclass of type BaseComponent. \"\"\" store_dir = os . path . join ( zenml . io . utils . get_zenml_config_dir ( repo_path ), store_name , ) comps = {} for fnames in fileio . list_dir ( store_dir , only_file_names = True ): uuid = Path ( fnames ) . stem key = get_key_from_uuid ( UUID ( uuid ), mapping ) comps [ key ] = get_component_from_key ( key , mapping , repo_path ) return comps","title":"get_components_from_store()"},{"location":"api_docs/core/#zenml.core.mapping_utils.get_key_from_uuid","text":"Return the key that points to a certain uuid in a mapping. Parameters: Name Type Description Default uuid UUID uuid to query. required mapping Dict[str, zenml.core.mapping_utils.UUIDSourceTuple] Dict mapping keys to UUIDs and source information. required Returns: Type Description str Returns the key from the mapping. Source code in zenml/core/mapping_utils.py def get_key_from_uuid ( uuid : UUID , mapping : Dict [ str , UUIDSourceTuple ]) -> str : \"\"\"Return the key that points to a certain uuid in a mapping. Args: uuid: uuid to query. mapping: Dict mapping keys to UUIDs and source information. Returns: Returns the key from the mapping. \"\"\" inverted_map = { v . uuid : k for k , v in mapping . items ()} return inverted_map [ uuid ]","title":"get_key_from_uuid()"},{"location":"api_docs/core/#zenml.core.repo","text":"Base ZenML repository","title":"repo"},{"location":"api_docs/core/#zenml.core.repo.Repository","text":"ZenML repository definition. Every ZenML project exists inside a ZenML repository. Source code in zenml/core/repo.py class Repository : \"\"\"ZenML repository definition. Every ZenML project exists inside a ZenML repository. \"\"\" def __init__ ( self , path : Optional [ str ] = None ): \"\"\" Construct reference to a ZenML repository. Args: path (str): Path to root of repository \"\"\" self . path = zenml . io . utils . get_zenml_dir ( path ) self . service = LocalService ( repo_path = self . path ) try : self . git_wrapper = GitWrapper ( self . path ) except InvalidGitRepositoryError : self . git_wrapper = None # type: ignore[assignment] @staticmethod def init_repo ( path : str = os . getcwd ()) -> None : \"\"\"Initializes a ZenML repository. Args: path: Path where the ZenML repository should be created. Raises: InitializationException: If a ZenML repository already exists at the given path. \"\"\" if zenml . io . utils . is_zenml_dir ( path ): raise InitializationException ( f \"A ZenML repository already exists at path ' { path } '.\" ) # Create the base dir zen_dir = os . path . join ( path , ZENML_DIR_NAME ) fileio . create_dir_recursive_if_not_exists ( zen_dir ) from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator service = LocalService ( repo_path = path ) artifact_store_path = os . path . join ( zenml . io . utils . get_global_config_directory (), \"local_stores\" , str ( service . uuid ), ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) service . register_artifact_store ( \"local_artifact_store\" , LocalArtifactStore ( path = artifact_store_path , repo_path = path ), ) service . register_metadata_store ( \"local_metadata_store\" , SQLiteMetadataStore ( uri = metadata_store_path , repo_path = path ), ) service . register_orchestrator ( \"local_orchestrator\" , LocalOrchestrator ( repo_path = path ) ) service . register_stack ( \"local_stack\" , BaseStack ( metadata_store_name = \"local_metadata_store\" , artifact_store_name = \"local_artifact_store\" , orchestrator_name = \"local_orchestrator\" , ), ) service . set_active_stack_key ( \"local_stack\" ) def get_git_wrapper ( self ) -> GitWrapper : \"\"\"Returns the git wrapper for the repo.\"\"\" return self . git_wrapper def get_service ( self ) -> LocalService : \"\"\"Returns the active service. For now, always local.\"\"\" return self . service @track ( event = SET_STACK ) def set_active_stack ( self , stack_key : str ) -> None : \"\"\"Set the active stack for the repo. This change is local for the machine. Args: stack_key: Key of the stack to set active. \"\"\" self . service . set_active_stack_key ( stack_key ) def get_active_stack_key ( self ) -> str : \"\"\"Get the active stack key from global config. Returns: Currently active stacks key. \"\"\" return self . service . get_active_stack_key () def get_active_stack ( self ) -> BaseStack : \"\"\"Get the active stack from global config. Returns: Currently active stack. \"\"\" return self . service . get_stack ( self . get_active_stack_key ()) @track ( event = GET_PIPELINES ) def get_pipelines ( self , stack_key : Optional [ str ] = None ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines. Args: stack_key: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. \"\"\" stack_key = stack_key or self . get_active_stack_key () metadata_store = self . service . get_stack ( stack_key ) . metadata_store return metadata_store . get_pipelines () @track ( event = GET_PIPELINE ) def get_pipeline ( self , pipeline_name : str , stack_key : Optional [ str ] = None ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name or `None` if it doesn't exist. Args: pipeline_name: Name of the pipeline. stack_key: If specified, pipelines in the metadata store of the given stack are returned. Otherwise pipelines in the metadata store of the currently active stack are returned. \"\"\" stack_key = stack_key or self . get_active_stack_key () metadata_store = self . service . get_stack ( stack_key ) . metadata_store return metadata_store . get_pipeline ( pipeline_name ) def clean ( self ) -> None : \"\"\"Deletes associated metadata store, pipelines dir and artifacts\"\"\" raise NotImplementedError","title":"Repository"},{"location":"api_docs/core/#zenml.core.repo.Repository.__init__","text":"Construct reference to a ZenML repository. Parameters: Name Type Description Default path str Path to root of repository None Source code in zenml/core/repo.py def __init__ ( self , path : Optional [ str ] = None ): \"\"\" Construct reference to a ZenML repository. Args: path (str): Path to root of repository \"\"\" self . path = zenml . io . utils . get_zenml_dir ( path ) self . service = LocalService ( repo_path = self . path ) try : self . git_wrapper = GitWrapper ( self . path ) except InvalidGitRepositoryError : self . git_wrapper = None # type: ignore[assignment]","title":"__init__()"},{"location":"api_docs/core/#zenml.core.repo.Repository.clean","text":"Deletes associated metadata store, pipelines dir and artifacts Source code in zenml/core/repo.py def clean ( self ) -> None : \"\"\"Deletes associated metadata store, pipelines dir and artifacts\"\"\" raise NotImplementedError","title":"clean()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_active_stack","text":"Get the active stack from global config. Returns: Type Description BaseStack Currently active stack. Source code in zenml/core/repo.py def get_active_stack ( self ) -> BaseStack : \"\"\"Get the active stack from global config. Returns: Currently active stack. \"\"\" return self . service . get_stack ( self . get_active_stack_key ())","title":"get_active_stack()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_active_stack_key","text":"Get the active stack key from global config. Returns: Type Description str Currently active stacks key. Source code in zenml/core/repo.py def get_active_stack_key ( self ) -> str : \"\"\"Get the active stack key from global config. Returns: Currently active stacks key. \"\"\" return self . service . get_active_stack_key ()","title":"get_active_stack_key()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_git_wrapper","text":"Returns the git wrapper for the repo. Source code in zenml/core/repo.py def get_git_wrapper ( self ) -> GitWrapper : \"\"\"Returns the git wrapper for the repo.\"\"\" return self . git_wrapper","title":"get_git_wrapper()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_pipeline","text":"Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"get_pipeline()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_pipelines","text":"Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"get_pipelines()"},{"location":"api_docs/core/#zenml.core.repo.Repository.get_service","text":"Returns the active service. For now, always local. Source code in zenml/core/repo.py def get_service ( self ) -> LocalService : \"\"\"Returns the active service. For now, always local.\"\"\" return self . service","title":"get_service()"},{"location":"api_docs/core/#zenml.core.repo.Repository.init_repo","text":"Initializes a ZenML repository. Parameters: Name Type Description Default path str Path where the ZenML repository should be created. '/home/apenner/PycharmProjects/zenml/docs' Exceptions: Type Description InitializationException If a ZenML repository already exists at the given path. Source code in zenml/core/repo.py @staticmethod def init_repo ( path : str = os . getcwd ()) -> None : \"\"\"Initializes a ZenML repository. Args: path: Path where the ZenML repository should be created. Raises: InitializationException: If a ZenML repository already exists at the given path. \"\"\" if zenml . io . utils . is_zenml_dir ( path ): raise InitializationException ( f \"A ZenML repository already exists at path ' { path } '.\" ) # Create the base dir zen_dir = os . path . join ( path , ZENML_DIR_NAME ) fileio . create_dir_recursive_if_not_exists ( zen_dir ) from zenml.artifact_stores import LocalArtifactStore from zenml.metadata_stores import SQLiteMetadataStore from zenml.orchestrators import LocalOrchestrator service = LocalService ( repo_path = path ) artifact_store_path = os . path . join ( zenml . io . utils . get_global_config_directory (), \"local_stores\" , str ( service . uuid ), ) metadata_store_path = os . path . join ( artifact_store_path , \"metadata.db\" ) service . register_artifact_store ( \"local_artifact_store\" , LocalArtifactStore ( path = artifact_store_path , repo_path = path ), ) service . register_metadata_store ( \"local_metadata_store\" , SQLiteMetadataStore ( uri = metadata_store_path , repo_path = path ), ) service . register_orchestrator ( \"local_orchestrator\" , LocalOrchestrator ( repo_path = path ) ) service . register_stack ( \"local_stack\" , BaseStack ( metadata_store_name = \"local_metadata_store\" , artifact_store_name = \"local_artifact_store\" , orchestrator_name = \"local_orchestrator\" , ), ) service . set_active_stack_key ( \"local_stack\" )","title":"init_repo()"},{"location":"api_docs/core/#zenml.core.repo.Repository.set_active_stack","text":"Inner decorator function. Source code in zenml/core/repo.py def inner_func ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function.\"\"\" track_event ( event_name , metadata = metadata ) result = func ( * args , ** kwargs ) return result","title":"set_active_stack()"},{"location":"api_docs/core/#zenml.core.utils","text":"","title":"utils"},{"location":"api_docs/core/#zenml.core.utils.define_json_config_settings_source","text":"Define a function to essentially deserialize a model from a serialized json config. Parameters: Name Type Description Default config_dir str A path to a dir where we want the config file to exist. required config_name str Full name of config file. required Returns: Type Description Callable[[BaseSettings], Dict[str, Any]] A json_config_settings_source callable reading from the passed path. Source code in zenml/core/utils.py def define_json_config_settings_source ( config_dir : str , config_name : str ) -> SettingsSourceCallable : \"\"\" Define a function to essentially deserialize a model from a serialized json config. Args: config_dir: A path to a dir where we want the config file to exist. config_name: Full name of config file. Returns: A `json_config_settings_source` callable reading from the passed path. \"\"\" def json_config_settings_source ( settings : BaseSettings ) -> Dict [ str , Any ]: \"\"\" A simple settings source that loads variables from a YAML file at the project's root. Here we happen to choose to use the `env_file_encoding` from Config when reading the config json file. Args: settings (BaseSettings): BaseSettings from pydantic. Returns: A dict with all configuration, empty dict if config not found. \"\"\" full_path = Path ( config_dir ) / config_name logger . debug ( f \"Parsing file: { full_path } \" ) if fileio . file_exists ( str ( full_path )): return cast ( Dict [ str , Any ], yaml_utils . read_json ( str ( full_path ))) return {} return json_config_settings_source","title":"define_json_config_settings_source()"},{"location":"api_docs/core/#zenml.core.utils.generate_customise_sources","text":"Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the define_json_config_settings_source is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Parameters: Name Type Description Default file_dir str Dir where file is stored. required file_name str Name of the file to persist. required Returns: Type Description Callable[[Type[pydantic.env_settings.BaseSettings.Config], Callable[[BaseSettings], Dict[str, Any]], Callable[[BaseSettings], Dict[str, Any]], Callable[[BaseSettings], Dict[str, Any]]], Tuple[Callable[[BaseSettings], Dict[str, Any]], ...]] A customise_sources class method to be defined the a Pydantic BaseSettings inner Config class. Source code in zenml/core/utils.py def generate_customise_sources ( file_dir : str , file_name : str ) -> Callable [ [ Type [ BaseSettings . Config ], SettingsSourceCallable , SettingsSourceCallable , SettingsSourceCallable , ], Tuple [ SettingsSourceCallable , ... ], ]: \"\"\"Generate a customise_sources function as defined here: https://pydantic-docs.helpmanual.io/usage/settings/. This function generates a function that configures the priorities of the sources through which the model is loaded. The important thing to note here is that the `define_json_config_settings_source` is dynamically generated with the provided file_dir and file_name. This allows us to dynamically generate a file name for the serialization and deserialization of the model. Args: file_dir: Dir where file is stored. file_name: Name of the file to persist. Returns: A `customise_sources` class method to be defined the a Pydantic BaseSettings inner Config class. \"\"\" def customise_sources ( cls : Type [ BaseSettings . Config ], init_settings : SettingsSourceCallable , env_settings : SettingsSourceCallable , file_secret_settings : SettingsSourceCallable , ) -> Tuple [ SettingsSourceCallable , ... ]: \"\"\"Defines precedence of sources to read/write settings from.\"\"\" return ( init_settings , env_settings , define_json_config_settings_source ( file_dir , file_name , ), file_secret_settings , ) return classmethod ( customise_sources ) # type: ignore[return-value]","title":"generate_customise_sources()"},{"location":"api_docs/enums/","text":"Enums zenml.enums ArtifactStoreTypes ( str , Enum ) All supported Artifact Store types. Source code in zenml/enums.py class ArtifactStoreTypes ( str , Enum ): \"\"\"All supported Artifact Store types.\"\"\" local = \"local\" gcp = \"gcp\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value ExecutionStatus ( Enum ) Enum that represents the current status of a step or pipeline run. Source code in zenml/enums.py class ExecutionStatus ( Enum ): \"\"\"Enum that represents the current status of a step or pipeline run.\"\"\" FAILED = \"failed\" COMPLETED = \"completed\" RUNNING = \"running\" CACHED = \"cached\" LoggingLevels ( Enum ) Enum for logging levels. Source code in zenml/enums.py class LoggingLevels ( Enum ): \"\"\"Enum for logging levels.\"\"\" NOTSET = logging . NOTSET ERROR = logging . ERROR WARN = logging . WARN INFO = logging . INFO DEBUG = logging . DEBUG CRITICAL = logging . CRITICAL MLMetadataTypes ( str , Enum ) All supported ML Metadata types. Source code in zenml/enums.py class MLMetadataTypes ( str , Enum ): \"\"\"All supported ML Metadata types.\"\"\" sqlite = \"sqlite\" mysql = \"mysql\" kubeflow = \"kubeflow\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value OrchestratorTypes ( str , Enum ) All supported Orchestrator types Source code in zenml/enums.py class OrchestratorTypes ( str , Enum ): \"\"\"All supported Orchestrator types\"\"\" local = \"local\" airflow = \"airflow\" kubeflow = \"kubeflow\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value StackTypes ( str , Enum ) All supported Stack types. Source code in zenml/enums.py class StackTypes ( str , Enum ): \"\"\"All supported Stack types.\"\"\" base = \"base\"","title":"Enums"},{"location":"api_docs/enums/#enums","text":"","title":"Enums"},{"location":"api_docs/enums/#zenml.enums","text":"","title":"enums"},{"location":"api_docs/enums/#zenml.enums.ArtifactStoreTypes","text":"All supported Artifact Store types. Source code in zenml/enums.py class ArtifactStoreTypes ( str , Enum ): \"\"\"All supported Artifact Store types.\"\"\" local = \"local\" gcp = \"gcp\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value","title":"ArtifactStoreTypes"},{"location":"api_docs/enums/#zenml.enums.ExecutionStatus","text":"Enum that represents the current status of a step or pipeline run. Source code in zenml/enums.py class ExecutionStatus ( Enum ): \"\"\"Enum that represents the current status of a step or pipeline run.\"\"\" FAILED = \"failed\" COMPLETED = \"completed\" RUNNING = \"running\" CACHED = \"cached\"","title":"ExecutionStatus"},{"location":"api_docs/enums/#zenml.enums.LoggingLevels","text":"Enum for logging levels. Source code in zenml/enums.py class LoggingLevels ( Enum ): \"\"\"Enum for logging levels.\"\"\" NOTSET = logging . NOTSET ERROR = logging . ERROR WARN = logging . WARN INFO = logging . INFO DEBUG = logging . DEBUG CRITICAL = logging . CRITICAL","title":"LoggingLevels"},{"location":"api_docs/enums/#zenml.enums.MLMetadataTypes","text":"All supported ML Metadata types. Source code in zenml/enums.py class MLMetadataTypes ( str , Enum ): \"\"\"All supported ML Metadata types.\"\"\" sqlite = \"sqlite\" mysql = \"mysql\" kubeflow = \"kubeflow\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value","title":"MLMetadataTypes"},{"location":"api_docs/enums/#zenml.enums.OrchestratorTypes","text":"All supported Orchestrator types Source code in zenml/enums.py class OrchestratorTypes ( str , Enum ): \"\"\"All supported Orchestrator types\"\"\" local = \"local\" airflow = \"airflow\" kubeflow = \"kubeflow\" def __str__ ( self ) -> str : \"\"\"Returns the enum string value.\"\"\" return self . value","title":"OrchestratorTypes"},{"location":"api_docs/enums/#zenml.enums.StackTypes","text":"All supported Stack types. Source code in zenml/enums.py class StackTypes ( str , Enum ): \"\"\"All supported Stack types.\"\"\" base = \"base\"","title":"StackTypes"},{"location":"api_docs/exceptions/","text":"Exceptions zenml.exceptions ZenML specific exception definitions AlreadyExistsException ( Exception ) Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. Source code in zenml/exceptions.py class AlreadyExistsException ( Exception ): \"\"\"Raises exception when the `name` already exist in the system but an action is trying to create a resource with the same name.\"\"\" def __init__ ( self , message : Optional [ str ] = None , name : str = \"\" , resource_type : str = \"\" , ): if message is None : message = f \" { resource_type } ` { name } ` already exists!\" super () . __init__ ( message ) ArtifactInterfaceError ( Exception ) Raises exception when interacting with the Artifact interface in an unsupported way. Source code in zenml/exceptions.py class ArtifactInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Artifact interface in an unsupported way.\"\"\" DoesNotExistException ( Exception ) Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. Source code in zenml/exceptions.py class DoesNotExistException ( Exception ): \"\"\"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.\"\"\" def __init__ ( self , message : str ): super () . __init__ ( message ) DuplicateRunNameError ( RuntimeError ) Raises exception when a run with the same name already exists. Source code in zenml/exceptions.py class DuplicateRunNameError ( RuntimeError ): \"\"\"Raises exception when a run with the same name already exists.\"\"\" def __init__ ( self , message : str = \"Unable to run a pipeline with a run name that \" \"already exists.\" , ): super () . __init__ ( message ) EmptyDatasourceException ( Exception ) Raises exception when a datasource data is accessed without running an associated pipeline. Source code in zenml/exceptions.py class EmptyDatasourceException ( Exception ): \"\"\"Raises exception when a datasource data is accessed without running an associated pipeline.\"\"\" def __init__ ( self , message : str = \"This datasource has not been used in \" \"any pipelines, therefore the associated data has no \" \"versions. Please use this datasource in any ZenML \" \"pipeline with `pipeline.add_datasource(\" \"datasource)`\" , ): super () . __init__ ( message ) GitException ( Exception ) Raises exception when a problem occurs in git resolution. Source code in zenml/exceptions.py class GitException ( Exception ): \"\"\"Raises exception when a problem occurs in git resolution.\"\"\" def __init__ ( self , message : str = \"There is a problem with git resolution. \" \"Please make sure that all relevant files \" \"are committed.\" , ): super () . __init__ ( message ) InitializationException ( Exception ) Raises exception when a function is run before zenml initialization. Source code in zenml/exceptions.py class InitializationException ( Exception ): \"\"\"Raises exception when a function is run before zenml initialization.\"\"\" def __init__ ( self , message : str = \"ZenML config is none. Did you do `zenml init`?\" ): super () . __init__ ( message ) IntegrationError ( Exception ) Raises exceptions when a requested integration can not be activated. Source code in zenml/exceptions.py class IntegrationError ( Exception ): \"\"\"Raises exceptions when a requested integration can not be activated.\"\"\" MissingStepParameterError ( Exception ) Raises exceptions when a step parameter is missing when running a pipeline. Source code in zenml/exceptions.py class MissingStepParameterError ( Exception ): \"\"\"Raises exceptions when a step parameter is missing when running a pipeline.\"\"\" def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message ) __init__ ( self , step_name , missing_parameters , config_class ) special Initializes a MissingStepParameterError object. Parameters: Name Type Description Default step_name str Name of the step for which one or more parameters are missing. required missing_parameters List[str] Names of all parameters which are missing. required config_class Type[BaseStepConfig] Class of the configuration object for which the parameters are missing. required Source code in zenml/exceptions.py def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message ) PipelineConfigurationError ( Exception ) Raises exceptions when a pipeline configuration contains invalid values. Source code in zenml/exceptions.py class PipelineConfigurationError ( Exception ): \"\"\"Raises exceptions when a pipeline configuration contains invalid values.\"\"\" PipelineInterfaceError ( Exception ) Raises exception when interacting with the Pipeline interface in an unsupported way. Source code in zenml/exceptions.py class PipelineInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Pipeline interface in an unsupported way.\"\"\" PipelineNotSucceededException ( Exception ) Raises exception when trying to fetch artifacts from a not succeeded pipeline. Source code in zenml/exceptions.py class PipelineNotSucceededException ( Exception ): \"\"\"Raises exception when trying to fetch artifacts from a not succeeded pipeline.\"\"\" def __init__ ( self , name : str = \"\" , message : str = \" {} is not yet completed successfully.\" , ): super () . __init__ ( message . format ( name )) StepContextError ( Exception ) Raises exception when interacting with a StepContext in an unsupported way. Source code in zenml/exceptions.py class StepContextError ( Exception ): \"\"\"Raises exception when interacting with a StepContext in an unsupported way.\"\"\" StepInterfaceError ( Exception ) Raises exception when interacting with the Step interface in an unsupported way. Source code in zenml/exceptions.py class StepInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Step interface in an unsupported way.\"\"\"","title":"Exceptions"},{"location":"api_docs/exceptions/#exceptions","text":"","title":"Exceptions"},{"location":"api_docs/exceptions/#zenml.exceptions","text":"ZenML specific exception definitions","title":"exceptions"},{"location":"api_docs/exceptions/#zenml.exceptions.AlreadyExistsException","text":"Raises exception when the name already exist in the system but an action is trying to create a resource with the same name. Source code in zenml/exceptions.py class AlreadyExistsException ( Exception ): \"\"\"Raises exception when the `name` already exist in the system but an action is trying to create a resource with the same name.\"\"\" def __init__ ( self , message : Optional [ str ] = None , name : str = \"\" , resource_type : str = \"\" , ): if message is None : message = f \" { resource_type } ` { name } ` already exists!\" super () . __init__ ( message )","title":"AlreadyExistsException"},{"location":"api_docs/exceptions/#zenml.exceptions.ArtifactInterfaceError","text":"Raises exception when interacting with the Artifact interface in an unsupported way. Source code in zenml/exceptions.py class ArtifactInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Artifact interface in an unsupported way.\"\"\"","title":"ArtifactInterfaceError"},{"location":"api_docs/exceptions/#zenml.exceptions.DoesNotExistException","text":"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present. Source code in zenml/exceptions.py class DoesNotExistException ( Exception ): \"\"\"Raises exception when the entity does not exist in the system but an action is being done that requires it to be present.\"\"\" def __init__ ( self , message : str ): super () . __init__ ( message )","title":"DoesNotExistException"},{"location":"api_docs/exceptions/#zenml.exceptions.DuplicateRunNameError","text":"Raises exception when a run with the same name already exists. Source code in zenml/exceptions.py class DuplicateRunNameError ( RuntimeError ): \"\"\"Raises exception when a run with the same name already exists.\"\"\" def __init__ ( self , message : str = \"Unable to run a pipeline with a run name that \" \"already exists.\" , ): super () . __init__ ( message )","title":"DuplicateRunNameError"},{"location":"api_docs/exceptions/#zenml.exceptions.EmptyDatasourceException","text":"Raises exception when a datasource data is accessed without running an associated pipeline. Source code in zenml/exceptions.py class EmptyDatasourceException ( Exception ): \"\"\"Raises exception when a datasource data is accessed without running an associated pipeline.\"\"\" def __init__ ( self , message : str = \"This datasource has not been used in \" \"any pipelines, therefore the associated data has no \" \"versions. Please use this datasource in any ZenML \" \"pipeline with `pipeline.add_datasource(\" \"datasource)`\" , ): super () . __init__ ( message )","title":"EmptyDatasourceException"},{"location":"api_docs/exceptions/#zenml.exceptions.GitException","text":"Raises exception when a problem occurs in git resolution. Source code in zenml/exceptions.py class GitException ( Exception ): \"\"\"Raises exception when a problem occurs in git resolution.\"\"\" def __init__ ( self , message : str = \"There is a problem with git resolution. \" \"Please make sure that all relevant files \" \"are committed.\" , ): super () . __init__ ( message )","title":"GitException"},{"location":"api_docs/exceptions/#zenml.exceptions.InitializationException","text":"Raises exception when a function is run before zenml initialization. Source code in zenml/exceptions.py class InitializationException ( Exception ): \"\"\"Raises exception when a function is run before zenml initialization.\"\"\" def __init__ ( self , message : str = \"ZenML config is none. Did you do `zenml init`?\" ): super () . __init__ ( message )","title":"InitializationException"},{"location":"api_docs/exceptions/#zenml.exceptions.IntegrationError","text":"Raises exceptions when a requested integration can not be activated. Source code in zenml/exceptions.py class IntegrationError ( Exception ): \"\"\"Raises exceptions when a requested integration can not be activated.\"\"\"","title":"IntegrationError"},{"location":"api_docs/exceptions/#zenml.exceptions.MissingStepParameterError","text":"Raises exceptions when a step parameter is missing when running a pipeline. Source code in zenml/exceptions.py class MissingStepParameterError ( Exception ): \"\"\"Raises exceptions when a step parameter is missing when running a pipeline.\"\"\" def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message )","title":"MissingStepParameterError"},{"location":"api_docs/exceptions/#zenml.exceptions.MissingStepParameterError.__init__","text":"Initializes a MissingStepParameterError object. Parameters: Name Type Description Default step_name str Name of the step for which one or more parameters are missing. required missing_parameters List[str] Names of all parameters which are missing. required config_class Type[BaseStepConfig] Class of the configuration object for which the parameters are missing. required Source code in zenml/exceptions.py def __init__ ( self , step_name : str , missing_parameters : List [ str ], config_class : Type [ \"BaseStepConfig\" ], ): \"\"\" Initializes a MissingStepParameterError object. Args: step_name: Name of the step for which one or more parameters are missing. missing_parameters: Names of all parameters which are missing. config_class: Class of the configuration object for which the parameters are missing. \"\"\" message = textwrap . fill ( textwrap . dedent ( f \"\"\" Missing parameters { missing_parameters } for ' { step_name } ' step. There are three ways to solve this issue: (1) Specify a default value in the configuration class ` { config_class . __name__ } ` (2) Specify the parameters in code when creating the pipeline: `my_pipeline( { step_name } (config= { config_class . __name__ } (...))` (3) Specify the parameters in a yaml configuration file and pass it to the pipeline: `my_pipeline(...).with_config('path_to_yaml')` \"\"\" ) ) super () . __init__ ( message )","title":"__init__()"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineConfigurationError","text":"Raises exceptions when a pipeline configuration contains invalid values. Source code in zenml/exceptions.py class PipelineConfigurationError ( Exception ): \"\"\"Raises exceptions when a pipeline configuration contains invalid values.\"\"\"","title":"PipelineConfigurationError"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineInterfaceError","text":"Raises exception when interacting with the Pipeline interface in an unsupported way. Source code in zenml/exceptions.py class PipelineInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Pipeline interface in an unsupported way.\"\"\"","title":"PipelineInterfaceError"},{"location":"api_docs/exceptions/#zenml.exceptions.PipelineNotSucceededException","text":"Raises exception when trying to fetch artifacts from a not succeeded pipeline. Source code in zenml/exceptions.py class PipelineNotSucceededException ( Exception ): \"\"\"Raises exception when trying to fetch artifacts from a not succeeded pipeline.\"\"\" def __init__ ( self , name : str = \"\" , message : str = \" {} is not yet completed successfully.\" , ): super () . __init__ ( message . format ( name ))","title":"PipelineNotSucceededException"},{"location":"api_docs/exceptions/#zenml.exceptions.StepContextError","text":"Raises exception when interacting with a StepContext in an unsupported way. Source code in zenml/exceptions.py class StepContextError ( Exception ): \"\"\"Raises exception when interacting with a StepContext in an unsupported way.\"\"\"","title":"StepContextError"},{"location":"api_docs/exceptions/#zenml.exceptions.StepInterfaceError","text":"Raises exception when interacting with the Step interface in an unsupported way. Source code in zenml/exceptions.py class StepInterfaceError ( Exception ): \"\"\"Raises exception when interacting with the Step interface in an unsupported way.\"\"\"","title":"StepInterfaceError"},{"location":"api_docs/integrations/","text":"Integrations zenml.integrations special The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch. airflow special The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command. AirflowIntegration ( Integration ) Definition of Airflow Integration for ZenML. Source code in zenml/integrations/airflow/__init__.py class AirflowIntegration ( Integration ): \"\"\"Definition of Airflow Integration for ZenML.\"\"\" NAME = AIRFLOW REQUIREMENTS = [ \"apache-airflow==2.2.0\" ] @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa activate () classmethod Activates all classes required for the airflow integration. Source code in zenml/integrations/airflow/__init__.py @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa orchestrators special airflow_component Definition for Airflow component for TFX. AirflowComponent ( PythonOperator ) Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_component.py class AirflowComponent ( python . PythonOperator ): \"\"\"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. \"\"\" def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) __init__ ( self , * , parent_dag , pipeline_node , mlmd_connection , pipeline_info , pipeline_runtime_spec , executor_spec = None , custom_driver_spec = None ) special Constructs an Airflow implementation of TFX component. Parameters: Name Type Description Default parent_dag DAG The airflow DAG that this component is contained in. required pipeline_node PipelineNode The specification of the node to launch. required mlmd_connection Metadata ML metadata connection info. required pipeline_info PipelineInfo The information of the pipeline that this node runs in. required pipeline_runtime_spec PipelineRuntimeSpec The runtime information of the pipeline that this node runs in. required executor_spec Optional[google.protobuf.message.Message] Specification for the executor of the node. None custom_driver_spec Optional[google.protobuf.message.Message] Specification for custom driver. None Source code in zenml/integrations/airflow/orchestrators/airflow_component.py def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) airflow_dag_runner Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes) AirflowDagRunner ( TfxRunner ) Tfx runner on Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Tfx runner on Airflow.\"\"\" def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag def _replace_runtime_params ( self , comp : base_node . BaseNode ) -> base_node . BaseNode : \"\"\"Replaces runtime params for dynamic Airflow parameter execution. Args: comp: TFX component to be parsed. Returns: Returns edited component. \"\"\" for k , prop in comp . exec_properties . copy () . items (): if isinstance ( prop , RuntimeParameter ): # Airflow only supports string parameters. if prop . ptype != str : raise RuntimeError ( f \"RuntimeParameter in Airflow does not support \" f \" { prop . ptype } . The only ptype supported is string.\" ) # If the default is a template, drop the template markers # when inserting it into the .get() default argument below. # Otherwise, provide the default as a quoted string. default = cast ( str , prop . default ) if default . startswith ( \"{{\" ) and default . endswith ( \"}}\" ): default = default [ 2 : - 2 ] else : default = json . dumps ( default ) template_field = '{{ dag_run.conf.get(\" %s \", %s ) }}' % ( prop . name , default , ) comp . exec_properties [ k ] = template_field return comp __init__ ( self , config = None ) special Creates an instance of AirflowDagRunner. Parameters: Name Type Description Default config Union[Dict[str, Any], zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig] Optional Airflow pipeline config for customizing the None Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) run ( self , pipeline , run_name = '' ) Deploys given logical pipeline on Airflow. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and comps. required run_name str Optional name for the run. '' Returns: Type Description airflow.DAG An Airflow DAG. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag AirflowPipelineConfig ( PipelineConfig ) Pipeline config for AirflowDagRunner. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowPipelineConfig ( pipeline_config . PipelineConfig ): \"\"\"Pipeline config for AirflowDagRunner.\"\"\" def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} __init__ ( self , airflow_dag_config = None , ** kwargs ) special Creates an instance of AirflowPipelineConfig. Parameters: Name Type Description Default airflow_dag_config Optional[Dict[str, Any]] Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. None **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} airflow_orchestrator AirflowOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines using Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py class AirflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Airflow.\"\"\" airflow_home : str = \"\" airflow_config : Optional [ Dict [ str , Any ]] = {} schedule_interval_minutes : int = 1 def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values @property def dags_directory ( self ) -> str : \"\"\"Returns path to the airflow dags directory.\"\"\" return os . path . join ( self . airflow_home , \"dags\" ) @property def pid_file ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Returns path to the airflow log file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_orchestrator.log\" ) @property def password_file ( self ) -> str : \"\"\"Returns path to the webserver password file.\"\"\" return os . path . join ( self . airflow_home , \"standalone_admin_password.txt\" ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" from airflow.cli.commands.standalone_command import StandaloneCommand from airflow.jobs.triggerer_job import TriggererJob daemon_running = daemon . check_if_daemon_is_running ( self . pid_file ) command = StandaloneCommand () webserver_port_open = command . port_open ( 8080 ) if not daemon_running : if webserver_port_open : raise RuntimeError ( \"The airflow daemon does not seem to be running but \" \"local port 8080 is occupied. Make sure the port is \" \"available and try again.\" ) # exit early so we don't check non-existing airflow databases return False # we can't use StandaloneCommand().is_ready() here as the # Airflow SequentialExecutor apparently does not send a heartbeat # while running a task which would result in this returning `False` # even if Airflow is running. airflow_running = webserver_port_open and command . job_running ( TriggererJob ) return airflow_running def _set_env ( self ) -> None : \"\"\"Sets environment variables to configure airflow.\"\"\" os . environ [ \"AIRFLOW_HOME\" ] = self . airflow_home os . environ [ \"AIRFLOW__CORE__DAGS_FOLDER\" ] = self . dags_directory os . environ [ \"AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE\" ] = \"false\" os . environ [ \"AIRFLOW__CORE__LOAD_EXAMPLES\" ] = \"false\" # check the DAG folder every 10 seconds for new files os . environ [ \"AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL\" ] = \"10\" def _copy_to_dag_directory_if_necessary ( self , dag_filepath : str ): \"\"\"Copies the DAG module to the airflow DAGs directory if it's not already located there. Args: dag_filepath: Path to the file in which the DAG is defined. \"\"\" dags_directory = fileio . resolve_relative_path ( self . dags_directory ) if dags_directory == os . path . dirname ( dag_filepath ): logger . debug ( \"File is already in airflow DAGs directory.\" ) else : logger . debug ( \"Copying dag file ' %s ' to DAGs directory.\" , dag_filepath ) destination_path = os . path . join ( dags_directory , os . path . basename ( dag_filepath ) ) if fileio . file_exists ( destination_path ): logger . info ( \"File ' %s ' already exists, overwriting with new DAG file\" , destination_path , ) fileio . copy ( dag_filepath , destination_path , overwrite = True ) def _log_webserver_credentials ( self ): \"\"\"Logs URL and credentials to login to the airflow webserver. Raises: FileNotFoundError: If the password file does not exist. \"\"\" if fileio . file_exists ( self . password_file ): with open ( self . password_file ) as file : password = file . read () . strip () else : raise FileNotFoundError ( f \"Can't find password file ' { self . password_file } '\" ) logger . info ( \"To inspect your DAGs, login to http://0.0.0.0:8080 \" \"with username: admin password: %s \" , password , ) def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This contains the airflow DAG that is returned by the `run()` method. Raises: RuntimeError: If airflow is not running. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = caller_filepath ) def up ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. \" \"If you want to start it manually, use the commands described \" \"in the official Airflow quickstart guide for running Airflow locally.\" ) self . down () def down ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> \"airflow.DAG\" : \"\"\"Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused argument to conform with base class signature. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) return runner . run ( tfx_pipeline , run_name = run_name ) dags_directory : str property readonly Returns path to the airflow dags directory. is_running : bool property readonly Returns whether the airflow daemon is currently running. log_file : str property readonly Returns path to the airflow log file. password_file : str property readonly Returns path to the webserver password file. pid_file : str property readonly Returns path to the daemon PID file. __init__ ( self , ** values ) special Sets environment variables to configure airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () down ( self ) Stops the airflow daemon if necessary and tears down resources. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def down ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) pre_run ( self , pipeline , caller_filepath ) Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Parameters: Name Type Description Default pipeline BasePipeline Pipeline that will be run. required caller_filepath str Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. required Exceptions: Type Description RuntimeError If airflow is not running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This contains the airflow DAG that is returned by the `run()` method. Raises: RuntimeError: If airflow is not running. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = caller_filepath ) run ( self , zenml_pipeline , run_name , ** kwargs ) Prepares the pipeline so it can be run in Airflow. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Unused argument to conform with base class signature. {} Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> \"airflow.DAG\" : \"\"\"Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused argument to conform with base class signature. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) return runner . run ( tfx_pipeline , run_name = run_name ) set_airflow_home ( values ) classmethod Sets airflow home according to orchestrator UUID. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values up ( self ) Ensures that Airflow is running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def up ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. \" \"If you want to start it manually, use the commands described \" \"in the official Airflow quickstart guide for running Airflow locally.\" ) self . down () dash special DashIntegration ( Integration ) Definition of Dash integration for ZenML. Source code in zenml/integrations/dash/__init__.py class DashIntegration ( Integration ): \"\"\"Definition of Dash integration for ZenML.\"\"\" NAME = DASH REQUIREMENTS = [ \"dash>=2.0.0\" , \"dash-cytoscape>=0.3.0\" , \"dash-bootstrap-components>=1.0.1\" , ] visualizers special pipeline_run_lineage_visualizer PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ) Implementation of a lineage diagram via the dash and dash-cyctoscape library. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py class PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ): \"\"\"Implementation of a lineage diagram via the [dash]( https://plotly.com/dash/) and [dash-cyctoscape]( https://dash.plotly.com/cytoscape) library.\"\"\" ARTIFACT_PREFIX = \"artifact_\" STEP_PREFIX = \"step_\" STATUS_CLASS_MAPPING = { ExecutionStatus . CACHED : \"green\" , ExecutionStatus . FAILED : \"red\" , ExecutionStatus . RUNNING : \"yellow\" , ExecutionStatus . COMPLETED : \"blue\" , } def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . name } \" , \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . name } \" , \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app evidently special The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file. EvidentlyIntegration ( Integration ) Definition of Evidently integration for ZenML. Source code in zenml/integrations/evidently/__init__.py class EvidentlyIntegration ( Integration ): \"\"\"Definition of [Evidently](https://github.com/evidentlyai/evidently) integration for ZenML.\"\"\" NAME = EVIDENTLY REQUIREMENTS = [ \"evidently>=v0.1.40.dev0\" ] steps special evidently_profile EvidentlyProfileConfig ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] EvidentlyProfileStep ( BaseDriftDetectionStep ) Simple step implementation which implements Evidently's functionality for creating a profile. Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileStep ( BaseDriftDetectionStep ): \"\"\"Simple step implementation which implements Evidently's functionality for creating a profile.\"\"\" OUTPUT_SPEC = { \"profile\" : DataAnalysisArtifact , \"dashboard\" : DataAnalysisArtifact , } def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] CONFIG_CLASS ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Main entrypoint for the Evidently categorical target drift detection step. Parameters: Name Type Description Default reference_dataset DataArtifact a Pandas dataframe required comparison_dataset DataArtifact a Pandas dataframe of new data you wish to compare against the reference data required config EvidentlyProfileConfig the configuration for the step required context StepContext the context of the step required Returns: Type Description profile dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift Source code in zenml/integrations/evidently/steps/evidently_profile.py def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] visualizers special evidently_visualizer EvidentlyVisualizer ( BaseStepVisualizer ) The implementation of an Evidently Visualizer. Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py class EvidentlyVisualizer ( BaseStepVisualizer ): \"\"\"The implementation of an Evidently Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) visualize ( self , object , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) facets special The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment. FacetsIntegration ( Integration ) Definition of Facet integration for ZenML. Source code in zenml/integrations/facets/__init__.py class FacetsIntegration ( Integration ): \"\"\"Definition of [Facet](https://pair-code.github.io/facets/) integration for ZenML.\"\"\" NAME = FACETS REQUIREMENTS = [ \"facets-overview>=1.0.0\" , \"IPython\" ] visualizers special facet_statistics_visualizer FacetStatisticsVisualizer ( BaseStepVisualizer ) The base implementation of a ZenML Visualizer. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py class FacetStatisticsVisualizer ( BaseStepVisualizer ): \"\"\"The base implementation of a ZenML Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ , magic = False ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required magic bool Whether to magically materialize facet in a notebook. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_html ( self , datasets ) Generates html for facet. Parameters: Name Type Description Default datasets List[Dict[str, pandas.core.frame.DataFrame]] List of dicts of dataframes to be visualized as stats. required Returns: Type Description str HTML template with proto string embedded. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ visualize ( self , object , magic = False , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required magic bool Whether to render in a Jupyter notebook or not. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) gcp special The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS). GcpIntegration ( Integration ) Definition of Google Cloud Platform integration for ZenML. Source code in zenml/integrations/gcp/__init__.py class GcpIntegration ( Integration ): \"\"\"Definition of Google Cloud Platform integration for ZenML.\"\"\" NAME = GCP REQUIREMENTS = [ \"gcsfs\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/gcp/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa artifact_stores special gcp_artifact_store GCPArtifactStore ( BaseArtifactStore ) pydantic-model Artifact Store for Google Cloud Storage based artifacts. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py class GCPArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for Google Cloud Storage based artifacts.\"\"\" @validator ( \"path\" ) def must_be_gcs_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a valid gcs path.\"\"\" if not v . startswith ( \"gs://\" ): raise ValueError ( \"Must be a valid gcs path, i.e., starting with `gs://`\" ) return v must_be_gcs_path ( v ) classmethod Validates that the path is a valid gcs path. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py @validator ( \"path\" ) def must_be_gcs_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a valid gcs path.\"\"\" if not v . startswith ( \"gs://\" ): raise ValueError ( \"Must be a valid gcs path, i.e., starting with `gs://`\" ) return v io special gcs_plugin Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs. ZenGCS ( Filesystem ) Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. Source code in zenml/integrations/gcp/io/gcs_plugin.py class ZenGCS ( Filesystem ): \"\"\"Filesystem that delegates to Google Cloud Store using gcsfs. **Note**: To allow TFX to check for various error conditions, we need to raise their custom `NotFoundError` instead of the builtin python FileNotFoundError.\"\"\" SUPPORTED_SCHEMES = [ \"gs://\" ] fs : gcsfs . GCSFileSystem = None @classmethod def _ensure_filesystem_set ( cls ) -> None : \"\"\"Ensures that the filesystem is set.\"\"\" if ZenGCS . fs is None : ZenGCS . fs = gcsfs . GCSFileSystem () @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] copy ( src , dst , overwrite = False ) staticmethod Copy a file. Parameters: Name Type Description Default src Union[bytes, str] The path to copy from. required dst Union[bytes, str] The path to copy to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e exists ( path ) staticmethod Check whether a path exists. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] glob ( pattern ) staticmethod Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Parameters: Name Type Description Default pattern Union[bytes, str] The glob pattern to match, see details above. required Returns: Type Description List[Union[bytes, str]] A list of paths that match the given glob pattern. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] isdir ( path ) staticmethod Check whether a path is a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] listdir ( path ) staticmethod Return a list of files in a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e makedirs ( path ) staticmethod Create a directory at the given path. If needed also create missing parent directories. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) mkdir ( path ) staticmethod Create a directory at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) open ( path , mode = 'r' ) staticmethod Open a file at the given path. Parameters: Name Type Description Default path Union[bytes, str] Path of the file to open. required mode str Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. 'r' Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e remove ( path ) staticmethod Remove the file at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e rename ( src , dst , overwrite = False ) staticmethod Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e rmtree ( path ) staticmethod Remove the given directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e stat ( path ) staticmethod Return stat info for the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e walk ( top , topdown = True , onerror = None ) staticmethod Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Unused argument to conform to interface. True onerror Optional[Callable[..., NoneType]] Unused argument to conform to interface. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] graphviz special GraphvizIntegration ( Integration ) Definition of Graphviz integration for ZenML. Source code in zenml/integrations/graphviz/__init__.py class GraphvizIntegration ( Integration ): \"\"\"Definition of Graphviz integration for ZenML.\"\"\" NAME = GRAPHVIZ REQUIREMENTS = [ \"graphviz>=0.17\" ] SYSTEM_REQUIREMENTS = { \"graphviz\" : \"dot\" } visualizers special pipeline_run_dag_visualizer PipelineRunDagVisualizer ( BasePipelineRunVisualizer ) Visualize the lineage of runs in a pipeline. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py class PipelineRunDagVisualizer ( BasePipelineRunVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline.\"\"\" ARTIFACT_DEFAULT_COLOR = \"blue\" ARTIFACT_CACHED_COLOR = \"green\" ARTIFACT_SHAPE = \"box\" ARTIFACT_PREFIX = \"artifact_\" STEP_COLOR = \"#431D93\" STEP_SHAPE = \"ellipse\" STEP_PREFIX = \"step_\" FONT = \"Roboto\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using graphviz. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot integration Integration Base class for integration in ZenML Source code in zenml/integrations/integration.py class Integration ( metaclass = IntegrationMeta ): \"\"\"Base class for integration in ZenML\"\"\" NAME = \"base_integration\" REQUIREMENTS : List [ str ] = [] SYSTEM_REQUIREMENTS : Dict [ str , str ] = {} @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\" activate () staticmethod Abstract method to activate the integration Source code in zenml/integrations/integration.py @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\" check_installation () classmethod Method to check whether the required packages are installed Source code in zenml/integrations/integration.py @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False IntegrationMeta ( type ) Metaclass responsible for registering different Integration subclasses Source code in zenml/integrations/integration.py class IntegrationMeta ( type ): \"\"\"Metaclass responsible for registering different Integration subclasses\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Hook into creation of an Integration class. Source code in zenml/integrations/integration.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls kubeflow special The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool. KubeflowIntegration ( Integration ) Definition of Kubeflow Integration for ZenML. Source code in zenml/integrations/kubeflow/__init__.py class KubeflowIntegration ( Integration ): \"\"\"Definition of Kubeflow Integration for ZenML.\"\"\" NAME = KUBEFLOW REQUIREMENTS = [ \"kfp==1.8.9\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata # noqa from zenml.integrations.kubeflow import orchestrators # noqa activate () classmethod Activates all classes required for the airflow integration. Source code in zenml/integrations/kubeflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata # noqa from zenml.integrations.kubeflow import orchestrators # noqa container_entrypoint Main entrypoint for containers with Kubeflow TFX component executors. main () Runs a single step defined by the command line arguments. Source code in zenml/integrations/kubeflow/container_entrypoint.py def main () -> None : \"\"\"Runs a single step defined by the command line arguments.\"\"\" # Log to the container's stdout so Kubeflow Pipelines UI can display logs to # the user. logging . basicConfig ( stream = sys . stdout , level = logging . INFO ) logging . getLogger () . setLevel ( logging . INFO ) args = _parse_command_line_arguments () tfx_pipeline = pipeline_pb2 . Pipeline () json_format . Parse ( args . tfx_ir , tfx_pipeline ) _resolve_runtime_parameters ( tfx_pipeline , args . run_name , args . runtime_parameter ) node_id = args . node_id pipeline_node = _get_pipeline_node ( tfx_pipeline , node_id ) deployment_config = runner_utils . extract_local_deployment_config ( tfx_pipeline ) executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) custom_executor_operators = { executable_spec_pb2 . ContainerExecutableSpec : kubernetes_executor_operator . KubernetesExecutorOperator } # make sure all integrations are activated so all materializers etc. are # available integration_registry . activate_integrations () metadata_store = Repository () . get_active_stack () . metadata_store if isinstance ( metadata_store , KubeflowMetadataStore ): # set up the metadata connection so it connects to the internal kubeflow # mysql database connection_config = _get_grpc_metadata_connection_config () else : connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa metadata_connection = metadata . Metadata ( connection_config ) # import the user main module to register all the materializers importlib . import_module ( args . main_module ) if hasattr ( executor_spec , \"class_path\" ): executor_module_parts = getattr ( executor_spec , \"class_path\" ) . split ( \".\" ) executor_class_target_module_name = \".\" . join ( executor_module_parts [: - 1 ]) _create_executor_class ( step_source_module_name = args . step_module , step_function_name = args . step_function_name , executor_class_target_module_name = executor_class_target_module_name , input_artifact_type_mapping = json . loads ( args . input_artifact_types ), ) else : raise RuntimeError ( f \"No class path found inside executor spec: { executor_spec } .\" ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata_connection , pipeline_info = tfx_pipeline . pipeline_info , pipeline_runtime_spec = tfx_pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , custom_executor_operators = custom_executor_operators , ) execution_info = execute_step ( component_launcher ) if execution_info : _dump_ui_metadata ( pipeline_node , execution_info , args . metadata_ui_path ) docker_utils build_docker_image ( build_context_path , image_name , dockerfile_path = None , dockerignore_path = None , requirements = None , use_local_requirements = False , base_image = None ) Builds a docker image. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required image_name str The name to use for the created docker image. required dockerfile_path Optional[str] Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. None dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None requirements Optional[List[str]] Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . None use_local_requirements bool If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. False base_image Optional[str] The image to use as base for the docker image. None Source code in zenml/integrations/kubeflow/docker_utils.py def build_docker_image ( build_context_path : str , image_name : str , dockerfile_path : Optional [ str ] = None , dockerignore_path : Optional [ str ] = None , requirements : Optional [ List [ str ]] = None , use_local_requirements : bool = False , base_image : Optional [ str ] = None , ) -> None : \"\"\"Builds a docker image. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. image_name: The name to use for the created docker image. dockerfile_path: Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. requirements: Optional list of pip requirements to install. This will only be used if no value is given for `dockerfile_path`. use_local_requirements: If `True` and no values are given for `dockerfile_path` and `requirements`, then the packages installed in the environment of the current python processed will be installed in the docker image. base_image: The image to use as base for the docker image. \"\"\" if not requirements and use_local_requirements : local_requirements = get_current_environment_requirements () requirements = [ f \" { package } == { version } \" for package , version in local_requirements . items () if package != \"zenml\" # exclude ZenML ] logger . info ( \"Using requirements from local environment to build \" \"docker image: %s \" , requirements , ) if dockerfile_path : dockerfile_contents = zenml . io . utils . read_file_contents_as_string ( dockerfile_path ) else : dockerfile_contents = generate_dockerfile_contents ( requirements = requirements , base_image = base_image or DEFAULT_BASE_IMAGE , ) build_context = create_custom_build_context ( build_context_path = build_context_path , dockerfile_contents = dockerfile_contents , dockerignore_path = dockerignore_path , ) # If a custom base image is provided, make sure to always pull the # latest version of that image. If no base image is provided, we use # the static default ZenML image so there is no need to constantly pull always_pull_base_image = bool ( base_image ) logger . info ( \"Building docker image ' %s ', this might take a while...\" , image_name ) docker_client = DockerClient . from_env () # We use the client api directly here so we can stream the logs output_stream = docker_client . images . client . api . build ( fileobj = build_context , custom_context = True , tag = image_name , pull = always_pull_base_image , rm = False , # don't remove intermediate containers ) _process_stream ( output_stream ) logger . info ( \"Finished building docker image.\" ) create_custom_build_context ( build_context_path , dockerfile_contents , dockerignore_path = None ) Creates a docker build context. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required dockerfile_contents str File contents of the Dockerfile to use for the build. required dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None Returns: Type Description Any Docker build context that can be passed when building a docker image. Source code in zenml/integrations/kubeflow/docker_utils.py def create_custom_build_context ( build_context_path : str , dockerfile_contents : str , dockerignore_path : Optional [ str ] = None , ) -> Any : \"\"\"Creates a docker build context. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents: File contents of the Dockerfile to use for the build. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. Returns: Docker build context that can be passed when building a docker image. \"\"\" exclude_patterns = [] default_dockerignore_path = os . path . join ( build_context_path , \".dockerignore\" ) if dockerignore_path : exclude_patterns = _parse_dockerignore ( dockerignore_path ) elif fileio . file_exists ( default_dockerignore_path ): logger . info ( \"Using dockerignore found at path ' %s ' to create docker \" \"build context.\" , default_dockerignore_path , ) exclude_patterns = _parse_dockerignore ( default_dockerignore_path ) else : logger . info ( \"No explicit dockerignore specified and no file called \" \".dockerignore exists at the build context root ( %s ).\" \"Creating docker build context with all files inside the build \" \"context root directory.\" , build_context_path , ) logger . debug ( \"Exclude patterns for creating docker build context: %s \" , exclude_patterns , ) no_ignores_found = not exclude_patterns files = docker_build_utils . exclude_paths ( build_context_path , patterns = exclude_patterns ) extra_files = [( \"Dockerfile\" , dockerfile_contents )] context = docker_build_utils . create_archive ( root = build_context_path , files = sorted ( files ), gzip = False , extra_files = extra_files , ) build_context_size = os . path . getsize ( context . name ) if build_context_size > 50 * 1024 * 1024 and no_ignores_found : # The build context exceeds 50MiB and we didn't find any excludes # in dockerignore files -> remind to specify a .dockerignore file logger . warning ( \"Build context size for docker image: %s . If you believe this is \" \"unreasonably large, make sure to include a .dockerignore file at \" \"the root of your build context ( %s ) or specify a custom file \" \"when defining your pipeline.\" , string_utils . get_human_readable_filesize ( build_context_size ), default_dockerignore_path , ) return context generate_dockerfile_contents ( base_image , command = None , requirements = None ) Generates a Dockerfile. Parameters: Name Type Description Default base_image str The image to use as base for the dockerfile. required command Optional[str] The default command that gets executed when running a container of an image created by this dockerfile. None requirements Optional[List[str]] Optional list of pip requirements to install. None Returns: Type Description str Content of a dockerfile. Source code in zenml/integrations/kubeflow/docker_utils.py def generate_dockerfile_contents ( base_image : str , command : Optional [ str ] = None , requirements : Optional [ List [ str ]] = None , ) -> str : \"\"\"Generates a Dockerfile. Args: base_image: The image to use as base for the dockerfile. command: The default command that gets executed when running a container of an image created by this dockerfile. requirements: Optional list of pip requirements to install. Returns: Content of a dockerfile. \"\"\" lines = [ f \"FROM { base_image } \" , \"WORKDIR /app\" ] if requirements : lines . extend ( [ f \"RUN pip install --no-cache { ' ' . join ( requirements ) } \" , ] ) lines . append ( \"COPY . .\" ) if command : lines . append ( f \"CMD { command } \" ) return \" \\n \" . join ( lines ) get_current_environment_requirements () Returns a dict of package requirements for the environment that the current python process is running in. Source code in zenml/integrations/kubeflow/docker_utils.py def get_current_environment_requirements () -> Dict [ str , str ]: \"\"\"Returns a dict of package requirements for the environment that the current python process is running in.\"\"\" return { distribution . key : distribution . version for distribution in pkg_resources . working_set } get_image_digest ( image_name ) Gets the digest of a docker image. Parameters: Name Type Description Default image_name str Name of the image to get the digest for. required Returns: Type Description Optional[str] Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . Source code in zenml/integrations/kubeflow/docker_utils.py def get_image_digest ( image_name : str ) -> Optional [ str ]: \"\"\"Gets the digest of a docker image. Args: image_name: Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns `None`. \"\"\" docker_client = DockerClient . from_env () image = docker_client . images . get ( image_name ) repo_digests = image . attrs [ \"RepoDigests\" ] if len ( repo_digests ) == 1 : return cast ( str , repo_digests [ 0 ]) else : logger . debug ( \"Found zero or more repo digests for docker image ' %s ': %s \" , image_name , repo_digests , ) return None push_docker_image ( image_name ) Pushes a docker image to a container registry. Parameters: Name Type Description Default image_name str The full name (including a tag) of the image to push. required Source code in zenml/integrations/kubeflow/docker_utils.py def push_docker_image ( image_name : str ) -> None : \"\"\"Pushes a docker image to a container registry. Args: image_name: The full name (including a tag) of the image to push. \"\"\" logger . info ( \"Pushing docker image ' %s '.\" , image_name ) docker_client = DockerClient . from_env () output_stream = docker_client . images . push ( image_name , stream = True ) _process_stream ( output_stream ) logger . info ( \"Finished pushing docker image.\" ) metadata special kubeflow_metadata_store KubeflowMetadataStore ( MySQLMetadataStore ) pydantic-model Kubeflow MySQL backend for ZenML metadata store. Source code in zenml/integrations/kubeflow/metadata/kubeflow_metadata_store.py class KubeflowMetadataStore ( MySQLMetadataStore ): \"\"\"Kubeflow MySQL backend for ZenML metadata store.\"\"\" host : str = \"127.0.0.1\" port : int = 3306 database : str = \"metadb\" username : str = \"root\" password : str = \"\" orchestrators special kubeflow_component Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed. KubeflowComponent Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py class KubeflowComponent : \"\"\"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. \"\"\" def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) repo = Repository () artifact_store = repo . get_active_stack () . artifact_store metadata_store = repo . get_active_stack () . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) __init__ ( self , component , depends_on , image , tfx_ir , pod_labels_to_attach , main_module , step_module , step_function_name , runtime_parameters , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' ) special Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Parameters: Name Type Description Default component BaseComponent The logical TFX component to wrap. required depends_on Set[kfp.dsl._container_op.ContainerOp] The set of upstream KFP ContainerOp components that this component will depend on. required image str The container image to use for this component. required tfx_ir Pipeline The TFX intermedia representation of the pipeline. required pod_labels_to_attach Dict[str, str] Dict of pod labels to attach to the GKE pod. required runtime_parameters List[tfx.orchestration.data_types.RuntimeParameter] Runtime parameters of the pipeline. required metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) repo = Repository () artifact_store = repo . get_active_stack () . artifact_store metadata_store = repo . get_active_stack () . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) kubeflow_dag_runner The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation KubeflowDagRunner ( TfxRunner ) Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. \"\"\" def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) def _parse_parameter_from_component ( self , component : tfx_base_component . BaseComponent ) -> None : \"\"\"Extract embedded RuntimeParameter placeholders from a component. Extract embedded RuntimeParameter placeholders from a component, then append the corresponding dsl.PipelineParam to KubeflowDagRunner. Args: component: a TFX component. \"\"\" deduped_parameter_names_for_component = set () for parameter in component . exec_properties . values (): if not isinstance ( parameter , data_types . RuntimeParameter ): continue # Ignore pipeline root because it will be added later. if parameter . name == tfx_pipeline . ROOT_PARAMETER . name : continue if parameter . name in deduped_parameter_names_for_component : continue deduped_parameter_names_for_component . add ( parameter . name ) self . _params_by_component_id [ component . id ] . append ( parameter ) if parameter . name not in self . _deduped_parameter_names : self . _deduped_parameter_names . add ( parameter . name ) dsl_parameter = dsl . PipelineParam ( name = parameter . name , value = str ( parameter . default ) ) self . _params . append ( dsl_parameter ) def _parse_parameter_from_pipeline ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Extract all the RuntimeParameter placeholders from the pipeline.\"\"\" for component in pipeline . components : self . _parse_parameter_from_component ( component ) def _construct_pipeline_graph ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Constructs a Kubeflow Pipeline graph. Args: pipeline: The logical TFX pipeline to base the construction on. pipeline_root: dsl.PipelineParam representing the pipeline root. \"\"\" component_to_kfp_op : Dict [ base_node . BaseNode , dsl . ContainerOp ] = {} tfx_ir = self . _generate_tfx_ir ( pipeline ) # Assumption: There is a partial ordering of components in the list, # i.e. if component A depends on component B and C, then A appears # after B and C in the list. for component in pipeline . components : # Keep track of the set of upstream dsl.ContainerOps for this # component. depends_on = set () for upstream_component in component . upstream_nodes : depends_on . add ( component_to_kfp_op [ upstream_component ]) # remove the extra pipeline node information tfx_node_ir = self . _dehydrate_tfx_ir ( tfx_ir , component . id ) from zenml.utils import source_utils main_module_file = sys . modules [ \"__main__\" ] . __file__ main_module = source_utils . get_module_source_from_file_path ( os . path . abspath ( main_module_file ) ) step_module = component . component_type . split ( \".\" )[: - 1 ] if step_module [ 0 ] == \"__main__\" : step_module = main_module else : step_module = \".\" . join ( step_module ) kfp_component = KubeflowComponent ( main_module = main_module , step_module = step_module , step_function_name = component . id , component = component , depends_on = depends_on , image = self . _kubeflow_config . image , pod_labels_to_attach = self . _pod_labels_to_attach , tfx_ir = tfx_node_ir , metadata_ui_path = self . _kubeflow_config . metadata_ui_path , runtime_parameters = self . _params_by_component_id [ component . id ], ) for operator in self . _kubeflow_config . pipeline_operator_funcs : kfp_component . container_op . apply ( operator ) component_to_kfp_op [ component ] = kfp_component . container_op def _del_unused_field ( self , node_id : str , message_dict : MutableMapping [ str , Any ] ) -> None : \"\"\"Remove fields that are not used by the pipeline.\"\"\" for item in list ( message_dict . keys ()): if item != node_id : del message_dict [ item ] def _dehydrate_tfx_ir ( self , original_pipeline : pipeline_pb2 . Pipeline , node_id : str # type: ignore[valid-type] # noqa ) -> pipeline_pb2 . Pipeline : # type: ignore[valid-type] \"\"\"Dehydrate the TFX IR to remove unused fields.\"\"\" pipeline = copy . deepcopy ( original_pipeline ) for node in pipeline . nodes : # type: ignore[attr-defined] if ( node . WhichOneof ( \"node\" ) == \"pipeline_node\" and node . pipeline_node . node_info . id == node_id ): del pipeline . nodes [:] # type: ignore[attr-defined] pipeline . nodes . extend ([ node ]) # type: ignore[attr-defined] break deployment_config = pipeline_pb2 . IntermediateDeploymentConfig () pipeline . deployment_config . Unpack ( deployment_config ) # type: ignore[attr-defined] # noqa self . _del_unused_field ( node_id , deployment_config . executor_specs ) self . _del_unused_field ( node_id , deployment_config . custom_driver_specs ) self . _del_unused_field ( node_id , deployment_config . node_level_platform_configs ) pipeline . deployment_config . Pack ( deployment_config ) # type: ignore[attr-defined] # noqa return pipeline def _generate_tfx_ir ( self , pipeline : tfx_pipeline . Pipeline ) -> Optional [ pipeline_pb2 . Pipeline ]: # type: ignore[valid-type] \"\"\"Generate the TFX IR from the logical TFX pipeline.\"\"\" result = self . _tfx_compiler . compile ( pipeline ) return result def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) __init__ ( self , config , output_path , pod_labels_to_attach = None ) special Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Parameters: Name Type Description Default config KubeflowDagRunnerConfig A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. required output_path str Path where the pipeline definition file will be stored. required pod_labels_to_attach Optional[Dict[str, str]] Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. None Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) run ( self , pipeline ) Compiles and outputs a Kubeflow Pipeline YAML definition file. Parameters: Name Type Description Default pipeline Pipeline The logical TFX pipeline to use when building the Kubeflow pipeline. required Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) KubeflowDagRunnerConfig ( PipelineConfig ) Runtime configuration parameters specific to execution on Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunnerConfig ( pipeline_config . PipelineConfig ): \"\"\"Runtime configuration parameters specific to execution on Kubeflow.\"\"\" def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path __init__ ( self , image , pipeline_operator_funcs = None , supported_launcher_classes = None , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' , ** kwargs ) special Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Parameters: Name Type Description Default image str The docker image to use in the pipeline. required pipeline_operator_funcs Optional[List[Callable[[kfp.dsl._container_op.ContainerOp], Union[kfp.dsl._container_op.ContainerOp, NoneType]]]] A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. None supported_launcher_classes Optional[List[Type[tfx.orchestration.launcher.base_component_launcher.BaseComponentLauncher]]] A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. None metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path get_default_pipeline_operator_funcs ( use_gcp_sa = False ) Returns a default list of pipeline operator functions. Parameters: Name Type Description Default use_gcp_sa bool If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. False Returns: Type Description List[Callable[[kfp.dsl._container_op.ContainerOp], Optional[kfp.dsl._container_op.ContainerOp]]] A list of functions with type OpFunc. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pipeline_operator_funcs ( use_gcp_sa : bool = False , ) -> List [ OpFunc ]: \"\"\"Returns a default list of pipeline operator functions. Args: use_gcp_sa: If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. \"\"\" # Enables authentication for GCP services if needed. gcp_secret_op = gcp . use_gcp_secret ( _KUBEFLOW_GCP_SECRET_NAME ) # Mounts configmap containing Metadata gRPC server configuration. mount_config_map_op = _mount_config_map_op ( \"metadata-grpc-configmap\" ) if use_gcp_sa : return [ gcp_secret_op , mount_config_map_op ] else : return [ mount_config_map_op ] get_default_pod_labels () Returns the default pod label dict for Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pod_labels () -> Dict [ str , str ]: \"\"\"Returns the default pod label dict for Kubeflow.\"\"\" # KFP default transformers add pod env: # https://github.com/kubeflow/pipelines/blob/0.1.32/sdk/python/kfp/compiler/_default_transformers.py result = { \"add-pod-env\" : \"true\" , telemetry_utils . LABEL_KFP_SDK_ENV : \"tfx\" } return result kubeflow_orchestrator KubeflowOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines using Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py class KubeflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Kubeflow.\"\"\" custom_docker_base_image_name : Optional [ str ] = None kubeflow_pipelines_ui_port : int = DEFAULT_KFP_UI_PORT kubernetes_context : Optional [ str ] = None def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . get_active_stack () . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name @property def root_directory ( self ) -> str : \"\"\"Returns path to the root directory for all files concerning this orchestrator.\"\"\" return os . path . join ( zenml . io . utils . get_global_config_directory (), \"kubeflow\" , str ( self . uuid ), ) @property def pipeline_directory ( self ) -> str : \"\"\"Returns path to a directory in which the kubeflow pipeline files are stored.\"\"\" return os . path . join ( self . root_directory , \"pipelines\" ) def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) repository_root = Repository () . path requirements = ( [ \"kubernetes\" ] + self . _get_stack_requirements () + self . _get_pipeline_requirements ( pipeline ) ) logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = repository_root , image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if Repository () . get_active_stack () . container_registry : push_docker_image ( image_name ) def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> None : \"\"\"Runs the pipeline on Kubeflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused kwargs to conform with base signature \"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( zenml_pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { zenml_pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline ) run_name = run_name or datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = zenml_pipeline . enable_cache , ) def _upload_and_run_pipeline ( self , pipeline_file_path : str , run_name : str , enable_cache : bool ) -> None : \"\"\"Tries to upload and run a KFP pipeline. Args: pipeline_file_path: Path to the pipeline definition file. run_name: A name for the pipeline run that will be started. enable_cache: Whether caching is enabled for this pipeline run. \"\"\" try : if self . kubernetes_context : logger . info ( \"Running in kubernetes context ' %s '.\" , self . kubernetes_context , ) # load kubernetes config to authorize the KFP client config . load_kube_config ( context = self . kubernetes_context ) # upload the pipeline to Kubeflow and start it client = kfp . Client () result = client . create_run_from_pipeline_package ( pipeline_file_path , arguments = {}, run_name = run_name , enable_caching = enable_cache , ) logger . info ( \"Started pipeline run with ID ' %s '.\" , result . run_id ) except urllib3 . exceptions . HTTPError as error : logger . warning ( \"Failed to upload Kubeflow pipeline: %s . \" \"Please make sure your kube config is configured and the \" \"current context is set correctly.\" , error , ) def _get_stack_requirements ( self ) -> List [ str ]: \"\"\"Gets list of requirements for the current active stack.\"\"\" stack = Repository () . get_active_stack () requirements = [] artifact_store_module = stack . artifact_store . __module__ requirements += get_requirements_for_module ( artifact_store_module ) metadata_store_module = stack . metadata_store . __module__ requirements += get_requirements_for_module ( metadata_store_module ) return requirements def _get_pipeline_requirements ( self , pipeline : \"BasePipeline\" ) -> List [ str ]: \"\"\"Gets list of requirements for a pipeline.\"\"\" if pipeline . requirements_file and fileio . file_exists ( pipeline . requirements_file ): logger . debug ( \"Using requirements from file %s .\" , pipeline . requirements_file ) with fileio . open ( pipeline . requirements_file , \"r\" ) as f : return [ requirement . strip () for requirement in f . read () . split ( \" \\n \" ) ] else : return [] @property def _pid_file_path ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Path of the daemon log file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.log\" ) @property def _k3d_cluster_name ( self ) -> str : \"\"\"Returns the K3D cluster name.\"\"\" # K3D only allows cluster names with up to 32 characters, use the # first 8 chars of the orchestrator UUID as identifier return f \"zenml-kubeflow- { str ( self . uuid )[: 8 ] } \" def _get_k3d_registry_name ( self , port : int ) -> str : \"\"\"Returns the K3D registry name.\"\"\" return f \"k3d-zenml-kubeflow-registry.localhost: { port } \" @property def _k3d_registry_config_path ( self ) -> str : \"\"\"Returns the path to the K3D registry config yaml.\"\"\" return os . path . join ( self . root_directory , \"k3d_registry.yaml\" ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is running.\"\"\" if not local_deployment_utils . check_prerequisites (): # if any prerequisites are missing there is certainly no # local deployment running return False return local_deployment_utils . k3d_cluster_exists ( cluster_name = self . _k3d_cluster_name ) def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) def up ( self ) -> None : \"\"\"Spins up a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . get_active_stack () . container_registry if not container_registry : logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Spinning up local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . down () def down ( self ) -> None : \"\"\"Tears down a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment spun down.\" ) is_running : bool property readonly Returns whether the orchestrator is running. log_file : str property readonly Path of the daemon log file. pipeline_directory : str property readonly Returns path to a directory in which the kubeflow pipeline files are stored. root_directory : str property readonly Returns path to the root directory for all files concerning this orchestrator. down ( self ) Tears down a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def down ( self ) -> None : \"\"\"Tears down a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment spun down.\" ) get_docker_image_name ( self , pipeline_name ) Returns the full docker image name including registry and tag. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . get_active_stack () . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name list_manual_setup_steps ( self , container_registry_name , container_registry_path ) Logs manual steps needed to setup the Kubeflow local orchestrator. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) pre_run ( self , pipeline , caller_filepath ) Builds a docker image for the current environment and uploads it to a container registry if configured. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) repository_root = Repository () . path requirements = ( [ \"kubernetes\" ] + self . _get_stack_requirements () + self . _get_pipeline_requirements ( pipeline ) ) logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = repository_root , image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if Repository () . get_active_stack () . container_registry : push_docker_image ( image_name ) run ( self , zenml_pipeline , run_name , ** kwargs ) Runs the pipeline on Kubeflow. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Unused kwargs to conform with base signature {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> None : \"\"\"Runs the pipeline on Kubeflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused kwargs to conform with base signature \"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( zenml_pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { zenml_pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline ) run_name = run_name or datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = zenml_pipeline . enable_cache , ) up ( self ) Spins up a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def up ( self ) -> None : \"\"\"Spins up a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . get_active_stack () . container_registry if not container_registry : logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Spinning up local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . down () kubeflow_utils Common utility for Kubeflow-based orchestrator. replace_placeholder ( component ) Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_utils.py def replace_placeholder ( component : base_node . BaseNode ) -> None : \"\"\"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam.\"\"\" keys = list ( component . exec_properties . keys ()) for key in keys : exec_property = component . exec_properties [ key ] if not isinstance ( exec_property , data_types . RuntimeParameter ): continue component . exec_properties [ key ] = str ( dsl . PipelineParam ( name = exec_property . name ) ) local_deployment_utils check_prerequisites () Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def check_prerequisites () -> bool : \"\"\"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.\"\"\" k3d_installed = shutil . which ( \"k3d\" ) is not None kubectl_installed = shutil . which ( \"kubectl\" ) is not None logger . debug ( \"Local kubeflow deployment prerequisites: K3D - %s , Kubectl - %s \" , k3d_installed , kubectl_installed , ) return k3d_installed and kubectl_installed create_k3d_cluster ( cluster_name , registry_name , registry_config_path ) Creates a K3D cluster. Parameters: Name Type Description Default cluster_name str Name of the cluster to create. required registry_name str Name of the registry to create for this cluster. required registry_config_path str Path to the registry config file. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def create_k3d_cluster ( cluster_name : str , registry_name : str , registry_config_path : str ) -> None : \"\"\"Creates a K3D cluster. Args: cluster_name: Name of the cluster to create. registry_name: Name of the registry to create for this cluster. registry_config_path: Path to the registry config file. \"\"\" logger . info ( \"Creating local K3D cluster ' %s '.\" , cluster_name ) global_config_dir_path = zenml . io . utils . get_global_config_directory () subprocess . check_call ( [ \"k3d\" , \"cluster\" , \"create\" , cluster_name , \"--registry-create\" , registry_name , \"--registry-config\" , registry_config_path , \"--volume\" , f \" { global_config_dir_path } : { global_config_dir_path } \" , ] ) logger . info ( \"Finished K3D cluster creation.\" ) delete_k3d_cluster ( cluster_name ) Deletes a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def delete_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Deletes a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"delete\" , cluster_name ]) logger . info ( \"Deleted local k3d cluster ' %s '.\" , cluster_name ) deploy_kubeflow_pipelines ( kubernetes_context ) Deploys Kubeflow Pipelines. Parameters: Name Type Description Default kubernetes_context str The kubernetes context on which Kubeflow Pipelines should be deployed. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def deploy_kubeflow_pipelines ( kubernetes_context : str ) -> None : \"\"\"Deploys Kubeflow Pipelines. Args: kubernetes_context: The kubernetes context on which Kubeflow Pipelines should be deployed. \"\"\" logger . info ( \"Deploying Kubeflow Pipelines.\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"wait\" , \"--timeout=60s\" , \"--for\" , \"condition=established\" , \"crd/applications.app.k8s.io\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , ] ) logger . info ( \"Waiting for all Kubeflow Pipelines pods to be ready (this might \" \"take a few minutes).\" ) while True : logger . info ( \"Current pod status:\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"get\" , \"pods\" , ] ) if kubeflow_pipelines_ready ( kubernetes_context = kubernetes_context ): break logger . info ( \"One or more pods not ready yet, waiting for 30 seconds...\" ) time . sleep ( 30 ) logger . info ( \"Finished Kubeflow Pipelines setup.\" ) k3d_cluster_exists ( cluster_name ) Checks whether there exists a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_exists ( cluster_name : str ) -> bool : \"\"\"Checks whether there exists a K3D cluster with the given name.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : return True return False kubeflow_pipelines_ready ( kubernetes_context ) Returns whether all Kubeflow Pipelines pods are ready. Parameters: Name Type Description Default kubernetes_context str The kubernetes context in which the pods should be checked. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def kubeflow_pipelines_ready ( kubernetes_context : str ) -> bool : \"\"\"Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context: The kubernetes context in which the pods should be checked. \"\"\" try : subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"wait\" , \"--for\" , \"condition=ready\" , \"--timeout=0s\" , \"pods\" , \"--all\" , ], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL , ) return True except subprocess . CalledProcessError : return False start_kfp_ui_daemon ( pid_file_path , log_file_path , port ) Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Parameters: Name Type Description Default pid_file_path str Path where the file with the daemons process ID should be written. required log_file_path str Path to a file where the daemon logs should be written. required port int Port on which the UI should be accessible. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_kfp_ui_daemon ( pid_file_path : str , log_file_path : str , port : int ) -> None : \"\"\"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path: Path where the file with the daemons process ID should be written. log_file_path: Path to a file where the daemon logs should be written. port: Port on which the UI should be accessible. \"\"\" command = [ \"kubectl\" , \"--namespace\" , \"kubeflow\" , \"port-forward\" , \"svc/ml-pipeline-ui\" , f \" { port } :80\" , ] if not networking_utils . port_available ( port ): modified_command = command . copy () modified_command [ - 1 ] = \"PORT:80\" logger . warning ( \"Unable to port-forward Kubeflow Pipelines UI to local port %d \" \"because the port is occupied. In order to access the Kubeflow \" \"Pipelines UI at http://localhost:PORT/, please run ' %s ' in a \" \"separate command line shell (replace PORT with a free port of \" \"your choice).\" , port , \" \" . join ( modified_command ), ) elif sys . platform == \"win32\" : logger . warning ( \"Daemon functionality not supported on Windows. \" \"In order to access the Kubeflow Pipelines UI at \" \"http://localhost: %d /, please run ' %s ' in a separate command \" \"line shell.\" , port , \" \" . join ( command ), ) else : from zenml.utils import daemon def _daemon_function () -> None : \"\"\"Port-forwards the Kubeflow Pipelines UI pod.\"\"\" subprocess . check_call ( command ) daemon . run_as_daemon ( _daemon_function , pid_file = pid_file_path , log_file = log_file_path ) logger . info ( \"Started Kubeflow Pipelines UI daemon (check the daemon logs at %s \" \"in case you're not able to view the UI). The Kubeflow Pipelines \" \"UI should now be accessible at http://localhost: %d /.\" , log_file_path , port , ) write_local_registry_yaml ( yaml_path , registry_name , registry_uri ) Writes a K3D registry config file. Parameters: Name Type Description Default yaml_path str Path where the config file should be written to. required registry_name str Name of the registry. required registry_uri str URI of the registry. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def write_local_registry_yaml ( yaml_path : str , registry_name : str , registry_uri : str ) -> None : \"\"\"Writes a K3D registry config file. Args: yaml_path: Path where the config file should be written to. registry_name: Name of the registry. registry_uri: URI of the registry. \"\"\" yaml_content = { \"mirrors\" : { registry_uri : { \"endpoint\" : [ f \"http:// { registry_name } \" ]}} } yaml_utils . write_yaml ( yaml_path , yaml_content ) mlflow special The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui MlflowIntegration ( Integration ) Definition of Plotly integration for ZenML. Source code in zenml/integrations/mlflow/__init__.py class MlflowIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = MLFLOW REQUIREMENTS = [ \"mlflow>=1.2.0\" ] mlflow_utils enable_mlflow ( _pipeline , experiment_name = None ) Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Parameters: Name Type Description Default _pipeline Type[zenml.pipelines.base_pipeline.BasePipeline] The decorated pipeline required experiment_name Optional[str] Experiment name to use for mlflow None Returns: Type Description Type[zenml.pipelines.base_pipeline.BasePipeline] the inner decorator which has a pipeline with the two methods extended Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow ( _pipeline : Type [ BasePipeline ], experiment_name : Optional [ str ] = None ) -> Type [ BasePipeline ]: \"\"\"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the __init__ and run method need to be extended accordingly. Args: _pipeline: The decorated pipeline experiment_name: Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended \"\"\" def inner_decorator ( pipeline : Type [ BasePipeline ]) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline with mlflow The __init__ and run method are both extended. Args: pipeline: BasePipeline which will be extended Returns: the class of a newly generated ZenML Pipeline with mlflow \"\"\" return type ( # noqa pipeline . __name__ , ( pipeline ,), { \"__init__\" : enable_mlflow_init ( pipeline . __init__ , experiment_name ), \"run\" : enable_mlflow_run ( pipeline . run ), }, ) return inner_decorator ( _pipeline ) enable_mlflow_init ( original_init , experiment = None ) Outer decorator function for extending the init method for pipelines that should be run using mlflow Parameters: Name Type Description Default original_init Callable[[zenml.pipelines.base_pipeline.BasePipeline, zenml.steps.base_step.BaseStep, Any], NoneType] The init method that should be extended required experiment Optional[str] The users chosen experiment name to use for mlflow None Returns: Type Description Callable[..., NoneType] the inner decorator which extends the init method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_init ( original_init : Callable [[ BasePipeline , BaseStep , Any ], None ], experiment : Optional [ str ] = None , ) -> Callable [ ... , None ]: \"\"\"Outer decorator function for extending the __init__ method for pipelines that should be run using mlflow Args: original_init: The __init__ method that should be extended experiment: The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the __init__ method \"\"\" def inner_decorator ( self : BasePipeline , * args : BaseStep , ** kwargs : Any ) -> None : \"\"\"Inner decorator overwriting the pipeline __init__ Makes sure mlflow is properly set up and all mlflow logging takes place within one mlflow experiment that is associated with the pipeline \"\"\" original_init ( self , * args , ** kwargs ) setup_mlflow ( backend_store_uri = local_mlflow_backend (), experiment_name = experiment if experiment else self . name , ) return inner_decorator enable_mlflow_run ( run ) Outer decorator function for extending the run method for pipelines that should be run using mlflow Parameters: Name Type Description Default run Callable[..., Any] The run method that should be extended required Returns: Type Description Callable[..., Any] the inner decorator which extends the run method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_run ( run : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run: The run method that should be extended Returns: the inner decorator which extends the run method \"\"\" def inner_decorator ( self : BasePipeline , run_name : Optional [ str ] = None ) -> Any : \"\"\"Inner decorator used to extend the run method of a pipeline. This ensures each pipeline run is run within a different mlflow context. Args: self: self of the original pipeline class run_name: Optional name for the run. \"\"\" with mlflow . start_run ( run_name = run_name ): run ( self , run_name ) return inner_decorator local_mlflow_backend () Returns the local mlflow backend inside the global zenml directory Source code in zenml/integrations/mlflow/mlflow_utils.py def local_mlflow_backend () -> str : \"\"\"Returns the local mlflow backend inside the global zenml directory\"\"\" local_mlflow_backend_uri = os . path . join ( get_global_config_directory (), \"local_stores\" , \"mlruns\" ) if not os . path . exists ( local_mlflow_backend_uri ): os . makedirs ( local_mlflow_backend_uri ) # TODO [medium]: safely access (possibly non-existent) artifact stores return \"file:\" + local_mlflow_backend_uri setup_mlflow ( backend_store_uri = None , experiment_name = 'default' ) Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Parameters: Name Type Description Default backend_store_uri Optional[str] The mlflow backend to log to None experiment_name str The experiment name under which all runs will be tracked 'default' Source code in zenml/integrations/mlflow/mlflow_utils.py def setup_mlflow ( backend_store_uri : Optional [ str ] = None , experiment_name : str = \"default\" ) -> None : \"\"\"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri: The mlflow backend to log to experiment_name: The experiment name under which all runs will be tracked \"\"\" # TODO [ENG-316]: Implement a way to get the mlflow token and set # it as env variable at MLFLOW_TRACKING_TOKEN if not backend_store_uri : backend_store_uri = local_mlflow_backend () set_tracking_uri ( backend_store_uri ) # Set which experiment is used within mlflow set_experiment ( experiment_name ) plotly special PlotlyIntegration ( Integration ) Definition of Plotly integration for ZenML. Source code in zenml/integrations/plotly/__init__.py class PlotlyIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = PLOTLY REQUIREMENTS = [ \"plotly>=5.4.0\" ] visualizers special pipeline_lineage_visualizer PipelineLineageVisualizer ( BasePipelineVisualizer ) Visualize the lineage of runs in a pipeline using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py class PipelineLineageVisualizer ( BasePipelineVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline using plotly.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . name : str ( step . id ), } ) if step . name not in dimensions : dimensions . append ( f \" { step . name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . name : str ( step . id ), } ) if step . name not in dimensions : dimensions . append ( f \" { step . name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig pytorch special PytorchIntegration ( Integration ) Definition of PyTorch integration for ZenML. Source code in zenml/integrations/pytorch/__init__.py class PytorchIntegration ( Integration ): \"\"\"Definition of PyTorch integration for ZenML.\"\"\" NAME = PYTORCH REQUIREMENTS = [ \"torch\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/pytorch/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa materializers special pytorch_materializer PyTorchMaterializer ( BaseMaterializer ) Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py class PyTorchMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Module , TorchDict ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) handle_input ( self , data_type ) Reads and returns a PyTorch model. Returns: Type Description Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A loaded pytorch model. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa handle_return ( self , model ) Writes a PyTorch model. Parameters: Name Type Description Default model Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A torch.nn.Module or a dict to pass into model.save required Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) pytorch_types TorchDict ( dict , Generic ) A type of dict that represents saving a model. Source code in zenml/integrations/pytorch/materializers/pytorch_types.py class TorchDict ( Dict [ str , Any ]): \"\"\"A type of dict that represents saving a model.\"\"\" pytorch_lightning special PytorchLightningIntegration ( Integration ) Definition of PyTorch Lightning integration for ZenML. Source code in zenml/integrations/pytorch_lightning/__init__.py class PytorchLightningIntegration ( Integration ): \"\"\"Definition of PyTorch Lightning integration for ZenML.\"\"\" NAME = PYTORCH_L REQUIREMENTS = [ \"pytorch_lightning\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/pytorch_lightning/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa materializers special pytorch_lightning_materializer PyTorchLightningMaterializer ( BaseMaterializer ) Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py class PyTorchLightningMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Trainer ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_input ( self , data_type ) Reads and returns a PyTorch Lightning trainer. Returns: Type Description Trainer A PyTorch Lightning trainer object. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_return ( self , trainer ) Writes a PyTorch Lightning trainer. Parameters: Name Type Description Default trainer Trainer A PyTorch Lightning trainer object. required Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) registry IntegrationRegistry Registry to keep track of ZenML Integrations Source code in zenml/integrations/registry.py class IntegrationRegistry ( object ): \"\"\"Registry to keep track of ZenML Integrations\"\"\" def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} @property def integrations ( self ) -> Dict [ str , Type [ \"Integration\" ]]: \"\"\"Method to get integrations dictionary. Returns: A dict of integration key to type of `Integration`. \"\"\" return self . _integrations @integrations . setter def integrations ( self , i : Any ) -> None : \"\"\"Setter method for the integrations property\"\"\" raise IntegrationError ( \"Please do not manually change the integrations within the \" \"registry. If you would like to register a new integration \" \"manually, please use \" \"`integration_registry.register_integration()`.\" ) def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) @property def list_integration_names ( self ) -> List [ str ]: \"\"\"Get a list of all possible integrations\"\"\" return [ name for name in self . _integrations ] def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" ) integrations : Dict [ str , Type [ Integration ]] property writable Method to get integrations dictionary. Returns: Type Description Dict[str, Type[Integration]] A dict of integration key to type of Integration . list_integration_names : List [ str ] property readonly Get a list of all possible integrations __init__ ( self ) special Initializing the integration registry Source code in zenml/integrations/registry.py def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} activate_integrations ( self ) Method to activate the integrations with are registered in the registry Source code in zenml/integrations/registry.py def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) is_installed ( self , integration_name = None ) Checks if all requirements for an integration are installed Source code in zenml/integrations/registry.py def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" ) register_integration ( self , key , type_ ) Method to register an integration with a given name Source code in zenml/integrations/registry.py def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ select_integration_requirements ( self , integration_name = None ) Select the requirements for a given integration or all integrations Source code in zenml/integrations/registry.py def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] sklearn special SklearnIntegration ( Integration ) Definition of sklearn integration for ZenML. Source code in zenml/integrations/sklearn/__init__.py class SklearnIntegration ( Integration ): \"\"\"Definition of sklearn integration for ZenML.\"\"\" NAME = SKLEARN REQUIREMENTS = [ \"scikit-learn\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/sklearn/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa helpers special digits get_digits () Returns the digits dataset in the form of a tuple of numpy arrays. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits () -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\"Returns the digits dataset in the form of a tuple of numpy arrays.\"\"\" digits = load_digits () # flatten the images n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) return X_train , X_test , y_train , y_test get_digits_model () Creates a support vector classifier for digits dataset. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits_model () -> ClassifierMixin : \"\"\"Creates a support vector classifier for digits dataset.\"\"\" return SVC ( gamma = 0.001 ) materializers special sklearn_materializer SklearnMaterializer ( BaseMaterializer ) Materializer to read data to and from sklearn. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py class SklearnMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from sklearn.\"\"\" ASSOCIATED_TYPES = [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) handle_input ( self , data_type ) Reads a base sklearn model from a pickle file. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf handle_return ( self , clf ) Creates a pickle for a sklearn model. Parameters: Name Type Description Default clf Union[sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin, sklearn.base.ClusterMixin, sklearn.base.BiclusterMixin, sklearn.base.OutlierMixin, sklearn.base.RegressorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.MultiOutputMixin, sklearn.base.DensityMixin, sklearn.base.TransformerMixin] A sklearn model. required Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) steps special sklearn_evaluator SklearnEvaluator ( BaseEvaluatorStep ) A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluator ( BaseEvaluatorStep ): \"\"\"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] CONFIG_CLASS ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str entrypoint ( self , dataset , model , config ) Method which is responsible for the computation of the evaluation Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which represents the test dataset required model Model a trained tensorflow Keras model required config SklearnEvaluatorConfig the configuration for the step required Returns: Type Description dict a dictionary which has the evaluation report Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] SklearnEvaluatorConfig ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str sklearn_splitter SklearnSplitter ( BaseSplitStep ) A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitter ( BaseSplitStep ): \"\"\"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset CONFIG_CLASS ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] entrypoint ( self , dataset , config ) Method which is responsible for the splitting logic Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which entire dataset required config SklearnSplitterConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc3c05130> three dataframes representing the splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset SklearnSplitterConfig ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] sklearn_standard_scaler SklearnStandardScaler ( BasePreprocessorStep ) Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScaler ( BasePreprocessorStep ): \"\"\"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame\"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset CONFIG_CLASS ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config ) Main entrypoint function for the StandardScaler Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required test_dataset DataFrame pd.DataFrame, the test dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required statistics DataFrame pd.DataFrame, the statistics over the train dataset required schema DataFrame pd.DataFrame, the detected schema of the dataset required config SklearnStandardScalerConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc3c05dc0> the transformed train, test and validation datasets as pd.DataFrames Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset SklearnStandardScalerConfig ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] tensorflow special TensorflowIntegration ( Integration ) Definition of Tensorflow integration for ZenML. Source code in zenml/integrations/tensorflow/__init__.py class TensorflowIntegration ( Integration ): \"\"\"Definition of Tensorflow integration for ZenML.\"\"\" NAME = TENSORFLOW REQUIREMENTS = [ \"tensorflow\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa activate () classmethod Activates the integration. Source code in zenml/integrations/tensorflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa materializers special keras_materializer KerasMaterializer ( BaseMaterializer ) Materializer to read/write Keras models. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py class KerasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Keras models.\"\"\" ASSOCIATED_TYPES = [ keras . Model ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) handle_input ( self , data_type ) Reads and returns a Keras model. Returns: Type Description Model A tf.keras.Model model. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) handle_return ( self , model ) Writes a keras model. Parameters: Name Type Description Default model Model A tf.keras.Model model. required Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) tf_dataset_materializer TensorflowDatasetMaterializer ( BaseMaterializer ) Materializer to read data to and from tf.data.Dataset. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py class TensorflowDatasetMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from tf.data.Dataset.\"\"\" ASSOCIATED_TYPES = [ tf . data . Dataset ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) handle_input ( self , data_type ) Reads data into tf.data.Dataset Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) handle_return ( self , dataset ) Persists a tf.data.Dataset object. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) steps special tensorflow_trainer TensorflowBinaryClassifier ( BaseTrainerStep ) Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifier ( BaseTrainerStep ): \"\"\"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset \"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model CONFIG_CLASS ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 entrypoint ( self , train_dataset , validation_dataset , config ) Main entrypoint for the tensorflow trainer Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required config TensorflowBinaryClassifierConfig the configuration of the step required Returns: Type Description Model the trained tf.keras.Model Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model TensorflowBinaryClassifierConfig ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 utils get_integration_for_module ( module_name ) Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_integration_for_module ( module_name : str ) -> Optional [ Type [ Integration ]]: \"\"\"Gets the integration class for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return `None`. If it is part of a ZenML integration, it will return the integration class found inside the integration __init__ file. \"\"\" integration_prefix = \"zenml.integrations.\" if not module_name . startswith ( integration_prefix ): return None integration_module_name = \".\" . join ( module_name . split ( \".\" , 3 )[: 3 ]) try : integration_module = sys . modules [ integration_module_name ] except KeyError : integration_module = importlib . import_module ( integration_module_name ) for name , member in inspect . getmembers ( integration_module ): if ( member is not Integration and isinstance ( member , IntegrationMeta ) and issubclass ( member , Integration ) ): return cast ( Type [ Integration ], member ) return None get_requirements_for_module ( module_name ) Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_requirements_for_module ( module_name : str ) -> List [ str ]: \"\"\"Gets requirements for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration __init__ file. \"\"\" integration = get_integration_for_module ( module_name ) return integration . REQUIREMENTS if integration else []","title":"Integrations"},{"location":"api_docs/integrations/#integrations","text":"","title":"Integrations"},{"location":"api_docs/integrations/#zenml.integrations","text":"The ZenML integrations module contains sub-modules for each integration that we support. This includes orchestrators like Apache Airflow, visualization tools like the facets library, as well as deep learning libraries like PyTorch.","title":"integrations"},{"location":"api_docs/integrations/#zenml.integrations.airflow","text":"The Airflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Airflow orchestrator with the CLI tool, then bootstrap using the zenml orchestrator up command.","title":"airflow"},{"location":"api_docs/integrations/#zenml.integrations.airflow.AirflowIntegration","text":"Definition of Airflow Integration for ZenML. Source code in zenml/integrations/airflow/__init__.py class AirflowIntegration ( Integration ): \"\"\"Definition of Airflow Integration for ZenML.\"\"\" NAME = AIRFLOW REQUIREMENTS = [ \"apache-airflow==2.2.0\" ] @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa","title":"AirflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.airflow.AirflowIntegration.activate","text":"Activates all classes required for the airflow integration. Source code in zenml/integrations/airflow/__init__.py @classmethod def activate ( cls ): \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.airflow import orchestrators # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators","text":"","title":"orchestrators"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_component","text":"Definition for Airflow component for TFX.","title":"airflow_component"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_component.AirflowComponent","text":"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_component.py class AirflowComponent ( python . PythonOperator ): \"\"\"Airflow-specific TFX Component. This class wrap a component run into its own PythonOperator in Airflow. \"\"\" def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , ) __init__ ( self , * , parent_dag , pipeline_node , mlmd_connection , pipeline_info , pipeline_runtime_spec , executor_spec = None , custom_driver_spec = None ) special Constructs an Airflow implementation of TFX component. Parameters: Name Type Description Default parent_dag DAG The airflow DAG that this component is contained in. required pipeline_node PipelineNode The specification of the node to launch. required mlmd_connection Metadata ML metadata connection info. required pipeline_info PipelineInfo The information of the pipeline that this node runs in. required pipeline_runtime_spec PipelineRuntimeSpec The runtime information of the pipeline that this node runs in. required executor_spec Optional[google.protobuf.message.Message] Specification for the executor of the node. None custom_driver_spec Optional[google.protobuf.message.Message] Specification for custom driver. None Source code in zenml/integrations/airflow/orchestrators/airflow_component.py def __init__ ( self , * , parent_dag : airflow . DAG , pipeline_node : pipeline_pb2 . PipelineNode , mlmd_connection : metadata . Metadata , pipeline_info : pipeline_pb2 . PipelineInfo , pipeline_runtime_spec : pipeline_pb2 . PipelineRuntimeSpec , executor_spec : Optional [ message . Message ] = None , custom_driver_spec : Optional [ message . Message ] = None ) -> None : \"\"\"Constructs an Airflow implementation of TFX component. Args: parent_dag: The airflow DAG that this component is contained in. pipeline_node: The specification of the node to launch. mlmd_connection: ML metadata connection info. pipeline_info: The information of the pipeline that this node runs in. pipeline_runtime_spec: The runtime information of the pipeline that this node runs in. executor_spec: Specification for the executor of the node. custom_driver_spec: Specification for custom driver. \"\"\" launcher_callable = functools . partial ( _airflow_component_launcher , pipeline_node = pipeline_node , mlmd_connection = mlmd_connection , pipeline_info = pipeline_info , pipeline_runtime_spec = pipeline_runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) super () . __init__ ( task_id = pipeline_node . node_info . id , provide_context = True , python_callable = launcher_callable , dag = parent_dag , )","title":"AirflowComponent"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner","text":"Definition of Airflow TFX runner. This is an unmodified copy from the TFX source code (outside of superficial, stylistic changes)","title":"airflow_dag_runner"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowDagRunner","text":"Tfx runner on Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Tfx runner on Airflow.\"\"\" def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag def _replace_runtime_params ( self , comp : base_node . BaseNode ) -> base_node . BaseNode : \"\"\"Replaces runtime params for dynamic Airflow parameter execution. Args: comp: TFX component to be parsed. Returns: Returns edited component. \"\"\" for k , prop in comp . exec_properties . copy () . items (): if isinstance ( prop , RuntimeParameter ): # Airflow only supports string parameters. if prop . ptype != str : raise RuntimeError ( f \"RuntimeParameter in Airflow does not support \" f \" { prop . ptype } . The only ptype supported is string.\" ) # If the default is a template, drop the template markers # when inserting it into the .get() default argument below. # Otherwise, provide the default as a quoted string. default = cast ( str , prop . default ) if default . startswith ( \"{{\" ) and default . endswith ( \"}}\" ): default = default [ 2 : - 2 ] else : default = json . dumps ( default ) template_field = '{{ dag_run.conf.get(\" %s \", %s ) }}' % ( prop . name , default , ) comp . exec_properties [ k ] = template_field return comp __init__ ( self , config = None ) special Creates an instance of AirflowDagRunner. Parameters: Name Type Description Default config Union[Dict[str, Any], zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig] Optional Airflow pipeline config for customizing the None Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , config : Optional [ Union [ Dict [ str , Any ], AirflowPipelineConfig ]] = None , ): \"\"\"Creates an instance of AirflowDagRunner. Args: config: Optional Airflow pipeline config for customizing the launching of each component. \"\"\" if isinstance ( config , dict ): warnings . warn ( \"Pass config as a dict type is going to deprecated in 0.1.16. \" \"Use AirflowPipelineConfig type instead.\" , PendingDeprecationWarning , ) config = AirflowPipelineConfig ( airflow_dag_config = config ) super () . __init__ ( config ) run ( self , pipeline , run_name = '' ) Deploys given logical pipeline on Airflow. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and comps. required run_name str Optional name for the run. '' Returns: Type Description airflow.DAG An Airflow DAG. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> \"airflow.DAG\" : \"\"\"Deploys given logical pipeline on Airflow. Args: pipeline: Logical pipeline containing pipeline args and comps. run_name: Optional name for the run. Returns: An Airflow DAG. \"\"\" # Only import these when needed. import airflow # noqa from zenml.integrations.airflow.orchestrators import airflow_component # Merge airflow-specific configs with pipeline args airflow_dag = airflow . DAG ( dag_id = pipeline . pipeline_info . pipeline_name , ** ( typing . cast ( AirflowPipelineConfig , self . _config ) . airflow_dag_config ), is_paused_upon_creation = False , catchup = False , # no backfill ) if \"tmp_dir\" not in pipeline . additional_pipeline_args : tmp_dir = os . path . join ( pipeline . pipeline_info . pipeline_root , \".temp\" , \"\" ) pipeline . additional_pipeline_args [ \"tmp_dir\" ] = tmp_dir for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) self . _replace_runtime_params ( component ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { \"pipeline-run-id\" : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa component_impl_map = {} for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) current_airflow_component = airflow_component . AirflowComponent ( parent_dag = airflow_dag , pipeline_node = pipeline_node , mlmd_connection = connection_config , pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) component_impl_map [ node_id ] = current_airflow_component for upstream_node in node . pipeline_node . upstream_nodes : assert ( upstream_node in component_impl_map ), \"Components is not in topological order\" current_airflow_component . set_upstream ( component_impl_map [ upstream_node ] ) return airflow_dag","title":"AirflowDagRunner"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_dag_runner.AirflowPipelineConfig","text":"Pipeline config for AirflowDagRunner. Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py class AirflowPipelineConfig ( pipeline_config . PipelineConfig ): \"\"\"Pipeline config for AirflowDagRunner.\"\"\" def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {} __init__ ( self , airflow_dag_config = None , ** kwargs ) special Creates an instance of AirflowPipelineConfig. Parameters: Name Type Description Default airflow_dag_config Optional[Dict[str, Any]] Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. None **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/airflow/orchestrators/airflow_dag_runner.py def __init__ ( self , airflow_dag_config : Optional [ Dict [ str , Any ]] = None , ** kwargs : Any ): \"\"\"Creates an instance of AirflowPipelineConfig. Args: airflow_dag_config: Configs of Airflow DAG model. See https://airflow.apache.org/_api/airflow/models/dag/index.html#airflow.models.dag.DAG for the full spec. **kwargs: keyword args for PipelineConfig. \"\"\" super () . __init__ ( ** kwargs ) self . airflow_dag_config = airflow_dag_config or {}","title":"AirflowPipelineConfig"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_orchestrator","text":"","title":"airflow_orchestrator"},{"location":"api_docs/integrations/#zenml.integrations.airflow.orchestrators.airflow_orchestrator.AirflowOrchestrator","text":"Orchestrator responsible for running pipelines using Airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py class AirflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Airflow.\"\"\" airflow_home : str = \"\" airflow_config : Optional [ Dict [ str , Any ]] = {} schedule_interval_minutes : int = 1 def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values @property def dags_directory ( self ) -> str : \"\"\"Returns path to the airflow dags directory.\"\"\" return os . path . join ( self . airflow_home , \"dags\" ) @property def pid_file ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Returns path to the airflow log file.\"\"\" return os . path . join ( self . airflow_home , \"airflow_orchestrator.log\" ) @property def password_file ( self ) -> str : \"\"\"Returns path to the webserver password file.\"\"\" return os . path . join ( self . airflow_home , \"standalone_admin_password.txt\" ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the airflow daemon is currently running.\"\"\" from airflow.cli.commands.standalone_command import StandaloneCommand from airflow.jobs.triggerer_job import TriggererJob daemon_running = daemon . check_if_daemon_is_running ( self . pid_file ) command = StandaloneCommand () webserver_port_open = command . port_open ( 8080 ) if not daemon_running : if webserver_port_open : raise RuntimeError ( \"The airflow daemon does not seem to be running but \" \"local port 8080 is occupied. Make sure the port is \" \"available and try again.\" ) # exit early so we don't check non-existing airflow databases return False # we can't use StandaloneCommand().is_ready() here as the # Airflow SequentialExecutor apparently does not send a heartbeat # while running a task which would result in this returning `False` # even if Airflow is running. airflow_running = webserver_port_open and command . job_running ( TriggererJob ) return airflow_running def _set_env ( self ) -> None : \"\"\"Sets environment variables to configure airflow.\"\"\" os . environ [ \"AIRFLOW_HOME\" ] = self . airflow_home os . environ [ \"AIRFLOW__CORE__DAGS_FOLDER\" ] = self . dags_directory os . environ [ \"AIRFLOW__CORE__DAG_DISCOVERY_SAFE_MODE\" ] = \"false\" os . environ [ \"AIRFLOW__CORE__LOAD_EXAMPLES\" ] = \"false\" # check the DAG folder every 10 seconds for new files os . environ [ \"AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL\" ] = \"10\" def _copy_to_dag_directory_if_necessary ( self , dag_filepath : str ): \"\"\"Copies the DAG module to the airflow DAGs directory if it's not already located there. Args: dag_filepath: Path to the file in which the DAG is defined. \"\"\" dags_directory = fileio . resolve_relative_path ( self . dags_directory ) if dags_directory == os . path . dirname ( dag_filepath ): logger . debug ( \"File is already in airflow DAGs directory.\" ) else : logger . debug ( \"Copying dag file ' %s ' to DAGs directory.\" , dag_filepath ) destination_path = os . path . join ( dags_directory , os . path . basename ( dag_filepath ) ) if fileio . file_exists ( destination_path ): logger . info ( \"File ' %s ' already exists, overwriting with new DAG file\" , destination_path , ) fileio . copy ( dag_filepath , destination_path , overwrite = True ) def _log_webserver_credentials ( self ): \"\"\"Logs URL and credentials to login to the airflow webserver. Raises: FileNotFoundError: If the password file does not exist. \"\"\" if fileio . file_exists ( self . password_file ): with open ( self . password_file ) as file : password = file . read () . strip () else : raise FileNotFoundError ( f \"Can't find password file ' { self . password_file } '\" ) logger . info ( \"To inspect your DAGs, login to http://0.0.0.0:8080 \" \"with username: admin password: %s \" , password , ) def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This contains the airflow DAG that is returned by the `run()` method. Raises: RuntimeError: If airflow is not running. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = caller_filepath ) def up ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. \" \"If you want to start it manually, use the commands described \" \"in the official Airflow quickstart guide for running Airflow locally.\" ) self . down () def down ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> \"airflow.DAG\" : \"\"\"Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused argument to conform with base class signature. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) return runner . run ( tfx_pipeline , run_name = run_name ) dags_directory : str property readonly Returns path to the airflow dags directory. is_running : bool property readonly Returns whether the airflow daemon is currently running. log_file : str property readonly Returns path to the airflow log file. password_file : str property readonly Returns path to the webserver password file. pid_file : str property readonly Returns path to the daemon PID file. __init__ ( self , ** values ) special Sets environment variables to configure airflow. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def __init__ ( self , ** values : Any ): \"\"\"Sets environment variables to configure airflow.\"\"\" super () . __init__ ( ** values ) self . _set_env () down ( self ) Stops the airflow daemon if necessary and tears down resources. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def down ( self ) -> None : \"\"\"Stops the airflow daemon if necessary and tears down resources.\"\"\" if self . is_running : daemon . stop_daemon ( self . pid_file , kill_children = True ) fileio . rm_dir ( self . airflow_home ) logger . info ( \"Airflow spun down.\" ) pre_run ( self , pipeline , caller_filepath ) Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Parameters: Name Type Description Default pipeline BasePipeline Pipeline that will be run. required caller_filepath str Path to the file in which pipeline.run() was called. This contains the airflow DAG that is returned by the run() method. required Exceptions: Type Description RuntimeError If airflow is not running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Checks whether airflow is running and copies the DAG file to the airflow DAGs directory. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This contains the airflow DAG that is returned by the `run()` method. Raises: RuntimeError: If airflow is not running. \"\"\" if not self . is_running : raise RuntimeError ( \"Airflow orchestrator is currently not running. \" \"Run `zenml orchestrator up` to start the \" \"orchestrator of the active stack.\" ) self . _copy_to_dag_directory_if_necessary ( dag_filepath = caller_filepath ) run ( self , zenml_pipeline , run_name , ** kwargs ) Prepares the pipeline so it can be run in Airflow. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Unused argument to conform with base class signature. {} Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> \"airflow.DAG\" : \"\"\"Prepares the pipeline so it can be run in Airflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused argument to conform with base class signature. \"\"\" self . airflow_config = { \"schedule_interval\" : datetime . timedelta ( minutes = self . schedule_interval_minutes ), # We set this in the past and turn catchup off and then it works \"start_date\" : datetime . datetime ( 2019 , 1 , 1 ), } runner = AirflowDagRunner ( AirflowPipelineConfig ( self . airflow_config )) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) return runner . run ( tfx_pipeline , run_name = run_name ) set_airflow_home ( values ) classmethod Sets airflow home according to orchestrator UUID. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py @root_validator def set_airflow_home ( cls , values : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\"Sets airflow home according to orchestrator UUID.\"\"\" if \"uuid\" not in values : raise ValueError ( \"`uuid` needs to exist for AirflowOrchestrator.\" ) values [ \"airflow_home\" ] = os . path . join ( zenml . io . utils . get_global_config_directory (), AIRFLOW_ROOT_DIR , str ( values [ \"uuid\" ]), ) return values up ( self ) Ensures that Airflow is running. Source code in zenml/integrations/airflow/orchestrators/airflow_orchestrator.py def up ( self ) -> None : \"\"\"Ensures that Airflow is running.\"\"\" if self . is_running : logger . info ( \"Airflow is already running.\" ) self . _log_webserver_credentials () return if not fileio . file_exists ( self . dags_directory ): fileio . create_dir_recursive_if_not_exists ( self . dags_directory ) from airflow.cli.commands.standalone_command import StandaloneCommand try : command = StandaloneCommand () # Run the daemon with a working directory inside the current # zenml repo so the same repo will be used to run the DAGs daemon . run_as_daemon ( command . run , pid_file = self . pid_file , log_file = self . log_file , working_directory = zenml . io . utils . get_zenml_dir (), ) while not self . is_running : # Wait until the daemon started all the relevant airflow processes time . sleep ( 0.1 ) self . _log_webserver_credentials () except Exception as e : logger . error ( e ) logger . error ( \"An error occurred while starting the Airflow daemon. \" \"If you want to start it manually, use the commands described \" \"in the official Airflow quickstart guide for running Airflow locally.\" ) self . down ()","title":"AirflowOrchestrator"},{"location":"api_docs/integrations/#zenml.integrations.dash","text":"","title":"dash"},{"location":"api_docs/integrations/#zenml.integrations.dash.DashIntegration","text":"Definition of Dash integration for ZenML. Source code in zenml/integrations/dash/__init__.py class DashIntegration ( Integration ): \"\"\"Definition of Dash integration for ZenML.\"\"\" NAME = DASH REQUIREMENTS = [ \"dash>=2.0.0\" , \"dash-cytoscape>=0.3.0\" , \"dash-bootstrap-components>=1.0.1\" , ]","title":"DashIntegration"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer","text":"","title":"pipeline_run_lineage_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.dash.visualizers.pipeline_run_lineage_visualizer.PipelineRunLineageVisualizer","text":"Implementation of a lineage diagram via the dash and dash-cyctoscape library. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py class PipelineRunLineageVisualizer ( BasePipelineRunVisualizer ): \"\"\"Implementation of a lineage diagram via the [dash]( https://plotly.com/dash/) and [dash-cyctoscape]( https://dash.plotly.com/cytoscape) library.\"\"\" ARTIFACT_PREFIX = \"artifact_\" STEP_PREFIX = \"step_\" STATUS_CLASS_MAPPING = { ExecutionStatus . CACHED : \"green\" , ExecutionStatus . FAILED : \"red\" , ExecutionStatus . RUNNING : \"yellow\" , ExecutionStatus . COMPLETED : \"blue\" , } def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . name } \" , \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. Source code in zenml/integrations/dash/visualizers/pipeline_run_lineage_visualizer.py def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> dash . Dash : \"\"\"Method to visualize pipeline runs via the Dash library. The layout puts every layer of the dag in a column. \"\"\" app = dash . Dash ( __name__ , external_stylesheets = [ dbc . themes . BOOTSTRAP , dbc . icons . BOOTSTRAP , ], ) nodes , edges , first_step_id = [], [], None first_step_id = None for step in object . steps : step_output_artifacts = list ( step . outputs . values ()) execution_id = ( step_output_artifacts [ 0 ] . producer_step . id if step_output_artifacts else step . id ) step_id = self . STEP_PREFIX + str ( step . id ) if first_step_id is None : first_step_id = step_id nodes . append ( { \"data\" : { \"id\" : step_id , \"execution_id\" : execution_id , \"label\" : f \" { execution_id } / { step . name } \" , \"name\" : step . name , # redundant for consistency \"type\" : \"step\" , \"parameters\" : step . parameters , \"inputs\" : { k : v . uri for k , v in step . inputs . items ()}, \"outputs\" : { k : v . uri for k , v in step . outputs . items ()}, }, \"classes\" : self . STATUS_CLASS_MAPPING [ step . status ], } ) for artifact_name , artifact in step . outputs . items (): nodes . append ( { \"data\" : { \"id\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"execution_id\" : artifact . id , \"label\" : f \" { artifact . id } / { artifact_name } (\" f \" { artifact . data_type } )\" , \"type\" : \"artifact\" , \"name\" : artifact_name , \"is_cached\" : artifact . is_cached , \"artifact_type\" : artifact . type , \"artifact_data_type\" : artifact . data_type , \"parent_step_id\" : artifact . parent_step_id , \"producer_step_id\" : artifact . producer_step . id , \"uri\" : artifact . uri , }, \"classes\" : f \"rectangle \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" , } ) edges . append ( { \"data\" : { \"source\" : self . STEP_PREFIX + str ( step . id ), \"target\" : self . ARTIFACT_PREFIX + str ( artifact . id ), }, \"classes\" : f \"edge-arrow \" f \" { self . STATUS_CLASS_MAPPING [ step . status ] } \" + ( \" dashed\" if artifact . is_cached else \" solid\" ), } ) for artifact_name , artifact in step . inputs . items (): edges . append ( { \"data\" : { \"source\" : self . ARTIFACT_PREFIX + str ( artifact . id ), \"target\" : self . STEP_PREFIX + str ( step . id ), }, \"classes\" : \"edge-arrow \" + ( f \" { self . STATUS_CLASS_MAPPING [ ExecutionStatus . CACHED ] } dashed\" if artifact . is_cached else f \" { self . STATUS_CLASS_MAPPING [ step . status ] } solid\" ), } ) app . layout = dbc . Row ( [ dbc . Container ( f \"Run: { object . name } \" , class_name = \"h1\" ), dbc . Row ( [ dbc . Col ( [ dbc . Row ( [ html . Span ( [ html . Span ( [ html . I ( className = \"bi bi-circle-fill me-1\" ), \"Step\" , ], className = \"me-2\" , ), html . Span ( [ html . I ( className = \"bi bi-square-fill me-1\" ), \"Artifact\" , ], className = \"me-4\" , ), dbc . Badge ( \"Completed\" , color = COLOR_BLUE , className = \"me-1\" , ), dbc . Badge ( \"Cached\" , color = COLOR_GREEN , className = \"me-1\" , ), dbc . Badge ( \"Running\" , color = COLOR_YELLOW , className = \"me-1\" , ), dbc . Badge ( \"Failed\" , color = COLOR_RED , className = \"me-1\" , ), ] ), ] ), dbc . Row ( [ cyto . Cytoscape ( id = \"cytoscape\" , layout = { \"name\" : \"breadthfirst\" , \"roots\" : f '[id = \" { first_step_id } \"]' , }, elements = edges + nodes , stylesheet = STYLESHEET , style = { \"width\" : \"100%\" , \"height\" : \"800px\" , }, zoom = 1 , ) ] ), dbc . Row ( [ dbc . Button ( \"Reset\" , id = \"bt-reset\" , color = \"primary\" , className = \"me-1\" , ) ] ), ] ), dbc . Col ( [ dcc . Markdown ( id = \"markdown-selected-node-data\" ), ] ), ] ), ], className = \"p-5\" , ) @app . callback ( # type: ignore[misc] Output ( \"markdown-selected-node-data\" , \"children\" ), Input ( \"cytoscape\" , \"selectedNodeData\" ), ) def display_data ( data_list : List [ Dict [ str , Any ]]) -> str : \"\"\"Callback for the text area below the graph\"\"\" if data_list is None : return \"Click on a node in the diagram.\" text = \"\" for data in data_list : text += f '## { data [ \"execution_id\" ] } / { data [ \"name\" ] } ' + \" \\n\\n \" if data [ \"type\" ] == \"artifact\" : for item in [ \"artifact_data_type\" , \"is_cached\" , \"producer_step_id\" , \"parent_step_id\" , \"uri\" , ]: text += f \"** { item } **: { data [ item ] } \" + \" \\n\\n \" elif data [ \"type\" ] == \"step\" : text += \"### Inputs:\" + \" \\n\\n \" for k , v in data [ \"inputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Outputs:\" + \" \\n\\n \" for k , v in data [ \"outputs\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" text += \"### Params:\" for k , v in data [ \"parameters\" ] . items (): text += f \"** { k } **: { v } \" + \" \\n\\n \" return text @app . callback ( # type: ignore[misc] [ Output ( \"cytoscape\" , \"zoom\" ), Output ( \"cytoscape\" , \"elements\" )], [ Input ( \"bt-reset\" , \"n_clicks\" )], ) def reset_layout ( n_clicks : int , ) -> List [ Union [ int , List [ Dict [ str , Collection [ str ]]]]]: \"\"\"Resets the layout\"\"\" logger . debug ( n_clicks , \"clicked in reset button.\" ) return [ 1 , edges + nodes ] app . run_server () return app","title":"PipelineRunLineageVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.evidently","text":"The Evidently integration provides a way to monitor your models in production. It includes a way to detect data drift and different kinds of model performance issues. The results of Evidently calculations can either be exported as an interactive dashboard (visualized as an html file or in your Jupyter notebook), or as a JSON file.","title":"evidently"},{"location":"api_docs/integrations/#zenml.integrations.evidently.EvidentlyIntegration","text":"Definition of Evidently integration for ZenML. Source code in zenml/integrations/evidently/__init__.py class EvidentlyIntegration ( Integration ): \"\"\"Definition of [Evidently](https://github.com/evidentlyai/evidently) integration for ZenML.\"\"\" NAME = EVIDENTLY REQUIREMENTS = [ \"evidently>=v0.1.40.dev0\" ]","title":"EvidentlyIntegration"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile","text":"","title":"evidently_profile"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile.EvidentlyProfileConfig","text":"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ]","title":"EvidentlyProfileConfig"},{"location":"api_docs/integrations/#zenml.integrations.evidently.steps.evidently_profile.EvidentlyProfileStep","text":"Simple step implementation which implements Evidently's functionality for creating a profile. Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileStep ( BaseDriftDetectionStep ): \"\"\"Simple step implementation which implements Evidently's functionality for creating a profile.\"\"\" OUTPUT_SPEC = { \"profile\" : DataAnalysisArtifact , \"dashboard\" : DataAnalysisArtifact , } def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()] CONFIG_CLASS ( BaseDriftDetectionConfig ) pydantic-model Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used !!! profile_section \"a string that identifies the profile section to be used.\" The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" Source code in zenml/integrations/evidently/steps/evidently_profile.py class EvidentlyProfileConfig ( BaseDriftDetectionConfig ): \"\"\"Config class for Evidently profile steps. column_mapping: properties of the dataframe's columns used profile_section: a string that identifies the profile section to be used. The following are valid options supported by Evidently: - \"datadrift\" - \"categoricaltargetdrift\" - \"numericaltargetdrift\" - \"classificationmodelperformance\" - \"regressionmodelperformance\" - \"probabilisticmodelperformance\" \"\"\" def get_profile_sections_and_tabs ( self , ) -> Tuple [ List [ ProfileSection ], List [ Tab ]]: try : return ( [ profile_mapper [ profile ]() for profile in self . profile_sections ], [ dashboard_mapper [ profile ]() for profile in self . profile_sections ], ) except KeyError : nl = \" \\n \" raise ValueError ( f \"Invalid profile section: { self . profile_sections } \\n\\n \" f \"Valid and supported options are: { nl } - \" f ' { f \" { nl } - \" . join ( list ( profile_mapper . keys ())) } ' ) column_mapping : Optional [ ColumnMapping ] profile_sections : Sequence [ str ] entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Main entrypoint for the Evidently categorical target drift detection step. Parameters: Name Type Description Default reference_dataset DataArtifact a Pandas dataframe required comparison_dataset DataArtifact a Pandas dataframe of new data you wish to compare against the reference data required config EvidentlyProfileConfig the configuration for the step required context StepContext the context of the step required Returns: Type Description profile dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift Source code in zenml/integrations/evidently/steps/evidently_profile.py def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : EvidentlyProfileConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] profile = dict , dashboard = str ): \"\"\"Main entrypoint for the Evidently categorical target drift detection step. Args: reference_dataset: a Pandas dataframe comparison_dataset: a Pandas dataframe of new data you wish to compare against the reference data config: the configuration for the step context: the context of the step Returns: profile: dictionary report extracted from an Evidently Profile generated for the data drift dashboard: HTML report extracted from an Evidently Dashboard generated for the data drift \"\"\" sections , tabs = config . get_profile_sections_and_tabs () data_drift_dashboard = Dashboard ( tabs = tabs ) data_drift_dashboard . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) data_drift_profile = Profile ( sections = sections ) data_drift_profile . calculate ( reference_dataset , comparison_dataset , column_mapping = config . column_mapping or None , ) return [ data_drift_profile . object (), data_drift_dashboard . html ()]","title":"EvidentlyProfileStep"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers.evidently_visualizer","text":"","title":"evidently_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.evidently.visualizers.evidently_visualizer.EvidentlyVisualizer","text":"The implementation of an Evidently Visualizer. Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py class EvidentlyVisualizer ( BaseStepVisualizer ): \"\"\"The implementation of an Evidently Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact ) def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py def generate_facet ( self , html_ : str ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. \"\"\" if self . running_in_notebook (): from IPython.core.display import HTML , display display ( HTML ( html_ )) else : logger . warn ( \"The magic functions are only usable in a Jupyter notebook.\" ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) visualize ( self , object , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required Source code in zenml/integrations/evidently/visualizers/evidently_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). \"\"\" for artifact_view in object . outputs . values (): # filter out anything but data analysis artifacts if ( artifact_view . type == DataAnalysisArtifact . __name__ and artifact_view . data_type == \"builtins.str\" ): artifact = artifact_view . read () self . generate_facet ( artifact )","title":"EvidentlyVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.facets","text":"The Facets integration provides a simple way to visualize post-execution objects like PipelineView , PipelineRunView and StepView . These objects can be extended using the BaseVisualization class. This integration requires facets-overview be installed in your Python environment.","title":"facets"},{"location":"api_docs/integrations/#zenml.integrations.facets.FacetsIntegration","text":"Definition of Facet integration for ZenML. Source code in zenml/integrations/facets/__init__.py class FacetsIntegration ( Integration ): \"\"\"Definition of [Facet](https://pair-code.github.io/facets/) integration for ZenML.\"\"\" NAME = FACETS REQUIREMENTS = [ \"facets-overview>=1.0.0\" , \"IPython\" ]","title":"FacetsIntegration"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers.facet_statistics_visualizer","text":"","title":"facet_statistics_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.facets.visualizers.facet_statistics_visualizer.FacetStatisticsVisualizer","text":"The base implementation of a ZenML Visualizer. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py class FacetStatisticsVisualizer ( BaseStepVisualizer ): \"\"\"The base implementation of a ZenML Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic ) def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_facet ( self , html_ , magic = False ) Generate a Facet Overview Parameters: Name Type Description Default h HTML represented as a string. required magic bool Whether to magically materialize facet in a notebook. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_facet ( self , html_ : str , magic : bool = False ) -> None : \"\"\"Generate a Facet Overview Args: h: HTML represented as a string. magic: Whether to magically materialize facet in a notebook. \"\"\" if magic : if not self . running_in_notebook (): raise EnvironmentError ( \"The magic functions are only usable in a Jupyter notebook.\" ) display ( HTML ( html_ )) else : with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : zenml . io . utils . write_file_contents_as_string ( f . name , html_ ) url = f \"file:/// { f . name } \" logger . info ( \"Opening %s in a new browser..\" % f . name ) webbrowser . open ( url , new = 2 ) generate_html ( self , datasets ) Generates html for facet. Parameters: Name Type Description Default datasets List[Dict[str, pandas.core.frame.DataFrame]] List of dicts of dataframes to be visualized as stats. required Returns: Type Description str HTML template with proto string embedded. Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py def generate_html ( self , datasets : List [ Dict [ Text , pd . DataFrame ]]) -> str : \"\"\"Generates html for facet. Args: datasets: List of dicts of dataframes to be visualized as stats. Returns: HTML template with proto string embedded. \"\"\" proto = GenericFeatureStatisticsGenerator () . ProtoFromDataFrames ( datasets ) protostr = base64 . b64encode ( proto . SerializeToString ()) . decode ( \"utf-8\" ) template = os . path . join ( os . path . abspath ( os . path . dirname ( __file__ )), \"stats.html\" , ) html_template = zenml . io . utils . read_file_contents_as_string ( template ) html_ = html_template . replace ( \"protostr\" , protostr ) return html_ visualize ( self , object , magic = False , * args , ** kwargs ) Method to visualize components Parameters: Name Type Description Default object StepView StepView fetched from run.get_step(). required magic bool Whether to render in a Jupyter notebook or not. False Source code in zenml/integrations/facets/visualizers/facet_statistics_visualizer.py @abstractmethod def visualize ( self , object : StepView , magic : bool = False , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize components Args: object: StepView fetched from run.get_step(). magic: Whether to render in a Jupyter notebook or not. \"\"\" datasets = [] for output_name , artifact_view in object . outputs . items (): df = artifact_view . read () if type ( df ) is not pd . DataFrame : logger . warning ( \"` %s ` is not a pd.DataFrame. You can only visualize \" \"statistics of steps that output pandas dataframes. \" \"Skipping this output..\" % output_name ) else : datasets . append ({ \"name\" : output_name , \"table\" : df }) h = self . generate_html ( datasets ) self . generate_facet ( h , magic )","title":"FacetStatisticsVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.gcp","text":"The GCP integration submodule provides a way to run ZenML pipelines in a cloud environment. Specifically, it allows the use of cloud artifact stores, metadata stores, and an io module to handle file operations on Google Cloud Storage (GCS).","title":"gcp"},{"location":"api_docs/integrations/#zenml.integrations.gcp.GcpIntegration","text":"Definition of Google Cloud Platform integration for ZenML. Source code in zenml/integrations/gcp/__init__.py class GcpIntegration ( Integration ): \"\"\"Definition of Google Cloud Platform integration for ZenML.\"\"\" NAME = GCP REQUIREMENTS = [ \"gcsfs\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa","title":"GcpIntegration"},{"location":"api_docs/integrations/#zenml.integrations.gcp.GcpIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/gcp/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.gcp import artifact_stores # noqa from zenml.integrations.gcp import io # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores","text":"","title":"artifact_stores"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores.gcp_artifact_store","text":"","title":"gcp_artifact_store"},{"location":"api_docs/integrations/#zenml.integrations.gcp.artifact_stores.gcp_artifact_store.GCPArtifactStore","text":"Artifact Store for Google Cloud Storage based artifacts. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py class GCPArtifactStore ( BaseArtifactStore ): \"\"\"Artifact Store for Google Cloud Storage based artifacts.\"\"\" @validator ( \"path\" ) def must_be_gcs_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a valid gcs path.\"\"\" if not v . startswith ( \"gs://\" ): raise ValueError ( \"Must be a valid gcs path, i.e., starting with `gs://`\" ) return v must_be_gcs_path ( v ) classmethod Validates that the path is a valid gcs path. Source code in zenml/integrations/gcp/artifact_stores/gcp_artifact_store.py @validator ( \"path\" ) def must_be_gcs_path ( cls , v : str ) -> str : \"\"\"Validates that the path is a valid gcs path.\"\"\" if not v . startswith ( \"gs://\" ): raise ValueError ( \"Must be a valid gcs path, i.e., starting with `gs://`\" ) return v","title":"GCPArtifactStore"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io","text":"","title":"io"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io.gcs_plugin","text":"Plugin which is created to add Google Cloud Store support to ZenML. It inherits from the base Filesystem created by TFX and overwrites the corresponding functions thanks to gcsfs.","title":"gcs_plugin"},{"location":"api_docs/integrations/#zenml.integrations.gcp.io.gcs_plugin.ZenGCS","text":"Filesystem that delegates to Google Cloud Store using gcsfs. Note : To allow TFX to check for various error conditions, we need to raise their custom NotFoundError instead of the builtin python FileNotFoundError. Source code in zenml/integrations/gcp/io/gcs_plugin.py class ZenGCS ( Filesystem ): \"\"\"Filesystem that delegates to Google Cloud Store using gcsfs. **Note**: To allow TFX to check for various error conditions, we need to raise their custom `NotFoundError` instead of the builtin python FileNotFoundError.\"\"\" SUPPORTED_SCHEMES = [ \"gs://\" ] fs : gcsfs . GCSFileSystem = None @classmethod def _ensure_filesystem_set ( cls ) -> None : \"\"\"Ensures that the filesystem is set.\"\"\" if ZenGCS . fs is None : ZenGCS . fs = gcsfs . GCSFileSystem () @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return] copy ( src , dst , overwrite = False ) staticmethod Copy a file. Parameters: Name Type Description Default src Union[bytes, str] The path to copy from. required dst Union[bytes, str] The path to copy to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file. Args: src: The path to copy from. dst: The path to copy to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to copy to destination ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to copy anyway.\" ) # TODO [ENG-151]: Check if it works with overwrite=True or if we need to # manually remove it first try : ZenGCS . fs . copy ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e exists ( path ) staticmethod Check whether a path exists. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def exists ( path : PathType ) -> bool : \"\"\"Check whether a path exists.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . exists ( path = path ) # type: ignore[no-any-return] glob ( pattern ) staticmethod Return all paths that match the given glob pattern. The glob pattern may include: - ' ' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - ' ' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/ */some_file) Parameters: Name Type Description Default pattern Union[bytes, str] The glob pattern to match, see details above. required Returns: Type Description List[Union[bytes, str]] A list of paths that match the given glob pattern. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return all paths that match the given glob pattern. The glob pattern may include: - '*' to match any number of characters - '?' to match a single character - '[...]' to match one of the characters inside the brackets - '**' as the full name of a path component to match to search in subdirectories of any depth (e.g. '/some_dir/**/some_file) Args: pattern: The glob pattern to match, see details above. Returns: A list of paths that match the given glob pattern. \"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . glob ( path = pattern ) # type: ignore[no-any-return] isdir ( path ) staticmethod Check whether a path is a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def isdir ( path : PathType ) -> bool : \"\"\"Check whether a path is a directory.\"\"\" ZenGCS . _ensure_filesystem_set () return ZenGCS . fs . isdir ( path = path ) # type: ignore[no-any-return] listdir ( path ) staticmethod Return a list of files in a directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def listdir ( path : PathType ) -> List [ PathType ]: \"\"\"Return a list of files in a directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . listdir ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e makedirs ( path ) staticmethod Create a directory at the given path. If needed also create missing parent directories. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def makedirs ( path : PathType ) -> None : \"\"\"Create a directory at the given path. If needed also create missing parent directories.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedirs ( path = path , exist_ok = True ) mkdir ( path ) staticmethod Create a directory at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def mkdir ( path : PathType ) -> None : \"\"\"Create a directory at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () ZenGCS . fs . makedir ( path = path ) open ( path , mode = 'r' ) staticmethod Open a file at the given path. Parameters: Name Type Description Default path Union[bytes, str] Path of the file to open. required mode str Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. 'r' Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def open ( path : PathType , mode : str = \"r\" ) -> Any : \"\"\"Open a file at the given path. Args: path: Path of the file to open. mode: Mode in which to open the file. Currently only 'rb' and 'wb' to read and write binary files are supported. \"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . open ( path = path , mode = mode ) except FileNotFoundError as e : raise NotFoundError () from e remove ( path ) staticmethod Remove the file at the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . rm_file ( path = path ) except FileNotFoundError as e : raise NotFoundError () from e rename ( src , dst , overwrite = False ) staticmethod Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileNotFoundError If the source file does not exist. FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileNotFoundError: If the source file does not exist. FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" ZenGCS . _ensure_filesystem_set () if not overwrite and ZenGCS . fs . exists ( dst ): raise FileExistsError ( f \"Unable to rename file to ' { convert_to_str ( dst ) } ', \" f \"file already exists. Set `overwrite=True` to rename anyway.\" ) # TODO [ENG-152]: Check if it works with overwrite=True or if we need # to manually remove it first try : ZenGCS . fs . rename ( path1 = src , path2 = dst ) except FileNotFoundError as e : raise NotFoundError () from e rmtree ( path ) staticmethod Remove the given directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def rmtree ( path : PathType ) -> None : \"\"\"Remove the given directory.\"\"\" ZenGCS . _ensure_filesystem_set () try : ZenGCS . fs . delete ( path = path , recursive = True ) except FileNotFoundError as e : raise NotFoundError () from e stat ( path ) staticmethod Return stat info for the given path. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def stat ( path : PathType ) -> Dict [ str , Any ]: \"\"\"Return stat info for the given path.\"\"\" ZenGCS . _ensure_filesystem_set () try : return ZenGCS . fs . stat ( path = path ) # type: ignore[no-any-return] except FileNotFoundError as e : raise NotFoundError () from e walk ( top , topdown = True , onerror = None ) staticmethod Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Unused argument to conform to interface. True onerror Optional[Callable[..., NoneType]] Unused argument to conform to interface. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/integrations/gcp/io/gcs_plugin.py @staticmethod def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Unused argument to conform to interface. onerror: Unused argument to conform to interface. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" ZenGCS . _ensure_filesystem_set () # TODO [ENG-153]: Additional params return ZenGCS . fs . walk ( path = top ) # type: ignore[no-any-return]","title":"ZenGCS"},{"location":"api_docs/integrations/#zenml.integrations.graphviz","text":"","title":"graphviz"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.GraphvizIntegration","text":"Definition of Graphviz integration for ZenML. Source code in zenml/integrations/graphviz/__init__.py class GraphvizIntegration ( Integration ): \"\"\"Definition of Graphviz integration for ZenML.\"\"\" NAME = GRAPHVIZ REQUIREMENTS = [ \"graphviz>=0.17\" ] SYSTEM_REQUIREMENTS = { \"graphviz\" : \"dot\" }","title":"GraphvizIntegration"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer","text":"","title":"pipeline_run_dag_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.graphviz.visualizers.pipeline_run_dag_visualizer.PipelineRunDagVisualizer","text":"Visualize the lineage of runs in a pipeline. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py class PipelineRunDagVisualizer ( BasePipelineRunVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline.\"\"\" ARTIFACT_DEFAULT_COLOR = \"blue\" ARTIFACT_CACHED_COLOR = \"green\" ARTIFACT_SHAPE = \"box\" ARTIFACT_PREFIX = \"artifact_\" STEP_COLOR = \"#431D93\" STEP_SHAPE = \"ellipse\" STEP_PREFIX = \"step_\" FONT = \"Roboto\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using graphviz. Source code in zenml/integrations/graphviz/visualizers/pipeline_run_dag_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> graphviz . Digraph : \"\"\"Creates a pipeline lineage diagram using graphviz.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) dot = graphviz . Digraph ( comment = object . name ) # link the steps together for step in object . steps : # add each step as a node dot . node ( self . STEP_PREFIX + str ( step . id ), step . name , shape = self . STEP_SHAPE , ) # for each parent of a step, add an edge for artifact_name , artifact in step . outputs . items (): dot . node ( self . ARTIFACT_PREFIX + str ( artifact . id ), f \" { artifact_name } \\n \" f \"( { artifact . _data_type } )\" , shape = self . ARTIFACT_SHAPE , ) dot . edge ( self . STEP_PREFIX + str ( step . id ), self . ARTIFACT_PREFIX + str ( artifact . id ), ) for artifact_name , artifact in step . inputs . items (): dot . edge ( self . ARTIFACT_PREFIX + str ( artifact . id ), self . STEP_PREFIX + str ( step . id ), ) with tempfile . NamedTemporaryFile ( delete = False , suffix = \".html\" ) as f : dot . render ( filename = f . name , format = \"png\" , view = True , cleanup = True ) return dot","title":"PipelineRunDagVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.integration","text":"","title":"integration"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration","text":"Base class for integration in ZenML Source code in zenml/integrations/integration.py class Integration ( metaclass = IntegrationMeta ): \"\"\"Base class for integration in ZenML\"\"\" NAME = \"base_integration\" REQUIREMENTS : List [ str ] = [] SYSTEM_REQUIREMENTS : Dict [ str , str ] = {} @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\"","title":"Integration"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration.activate","text":"Abstract method to activate the integration Source code in zenml/integrations/integration.py @staticmethod def activate () -> None : \"\"\"Abstract method to activate the integration\"\"\"","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.integration.Integration.check_installation","text":"Method to check whether the required packages are installed Source code in zenml/integrations/integration.py @classmethod def check_installation ( cls ) -> bool : \"\"\"Method to check whether the required packages are installed\"\"\" try : for requirement , command in cls . SYSTEM_REQUIREMENTS . items (): result = shutil . which ( command ) if result is None : logger . debug ( \"Unable to find the required packages for %s on your \" \"system. Please install the packages on your system \" \"and try again.\" , requirement , ) return False for r in cls . REQUIREMENTS : pkg_resources . get_distribution ( r ) logger . debug ( f \"Integration { cls . NAME } is installed correctly with \" f \"requirements { cls . REQUIREMENTS } .\" ) return True except pkg_resources . DistributionNotFound as e : logger . debug ( f \"Unable to find required package ' { e . req } ' for \" f \"integration { cls . NAME } .\" ) return False except pkg_resources . VersionConflict as e : logger . debug ( f \"VersionConflict error when loading installation { cls . NAME } : \" f \" { str ( e ) } \" ) return False","title":"check_installation()"},{"location":"api_docs/integrations/#zenml.integrations.integration.IntegrationMeta","text":"Metaclass responsible for registering different Integration subclasses Source code in zenml/integrations/integration.py class IntegrationMeta ( type ): \"\"\"Metaclass responsible for registering different Integration subclasses\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls","title":"IntegrationMeta"},{"location":"api_docs/integrations/#zenml.integrations.integration.IntegrationMeta.__new__","text":"Hook into creation of an Integration class. Source code in zenml/integrations/integration.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"IntegrationMeta\" : \"\"\"Hook into creation of an Integration class.\"\"\" cls = cast ( Type [ \"Integration\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Integration\" : integration_registry . register_integration ( cls . NAME , cls ) return cls","title":"__new__()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow","text":"The Kubeflow integration sub-module powers an alternative to the local orchestrator. You can enable it by registering the Kubeflow orchestrator with the CLI tool.","title":"kubeflow"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.KubeflowIntegration","text":"Definition of Kubeflow Integration for ZenML. Source code in zenml/integrations/kubeflow/__init__.py class KubeflowIntegration ( Integration ): \"\"\"Definition of Kubeflow Integration for ZenML.\"\"\" NAME = KUBEFLOW REQUIREMENTS = [ \"kfp==1.8.9\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata # noqa from zenml.integrations.kubeflow import orchestrators # noqa","title":"KubeflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.KubeflowIntegration.activate","text":"Activates all classes required for the airflow integration. Source code in zenml/integrations/kubeflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates all classes required for the airflow integration.\"\"\" from zenml.integrations.kubeflow import metadata # noqa from zenml.integrations.kubeflow import orchestrators # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.container_entrypoint","text":"Main entrypoint for containers with Kubeflow TFX component executors.","title":"container_entrypoint"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.container_entrypoint.main","text":"Runs a single step defined by the command line arguments. Source code in zenml/integrations/kubeflow/container_entrypoint.py def main () -> None : \"\"\"Runs a single step defined by the command line arguments.\"\"\" # Log to the container's stdout so Kubeflow Pipelines UI can display logs to # the user. logging . basicConfig ( stream = sys . stdout , level = logging . INFO ) logging . getLogger () . setLevel ( logging . INFO ) args = _parse_command_line_arguments () tfx_pipeline = pipeline_pb2 . Pipeline () json_format . Parse ( args . tfx_ir , tfx_pipeline ) _resolve_runtime_parameters ( tfx_pipeline , args . run_name , args . runtime_parameter ) node_id = args . node_id pipeline_node = _get_pipeline_node ( tfx_pipeline , node_id ) deployment_config = runner_utils . extract_local_deployment_config ( tfx_pipeline ) executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) custom_executor_operators = { executable_spec_pb2 . ContainerExecutableSpec : kubernetes_executor_operator . KubernetesExecutorOperator } # make sure all integrations are activated so all materializers etc. are # available integration_registry . activate_integrations () metadata_store = Repository () . get_active_stack () . metadata_store if isinstance ( metadata_store , KubeflowMetadataStore ): # set up the metadata connection so it connects to the internal kubeflow # mysql database connection_config = _get_grpc_metadata_connection_config () else : connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa metadata_connection = metadata . Metadata ( connection_config ) # import the user main module to register all the materializers importlib . import_module ( args . main_module ) if hasattr ( executor_spec , \"class_path\" ): executor_module_parts = getattr ( executor_spec , \"class_path\" ) . split ( \".\" ) executor_class_target_module_name = \".\" . join ( executor_module_parts [: - 1 ]) _create_executor_class ( step_source_module_name = args . step_module , step_function_name = args . step_function_name , executor_class_target_module_name = executor_class_target_module_name , input_artifact_type_mapping = json . loads ( args . input_artifact_types ), ) else : raise RuntimeError ( f \"No class path found inside executor spec: { executor_spec } .\" ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata_connection , pipeline_info = tfx_pipeline . pipeline_info , pipeline_runtime_spec = tfx_pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , custom_executor_operators = custom_executor_operators , ) execution_info = execute_step ( component_launcher ) if execution_info : _dump_ui_metadata ( pipeline_node , execution_info , args . metadata_ui_path )","title":"main()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils","text":"","title":"docker_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.build_docker_image","text":"Builds a docker image. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required image_name str The name to use for the created docker image. required dockerfile_path Optional[str] Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. None dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None requirements Optional[List[str]] Optional list of pip requirements to install. This will only be used if no value is given for dockerfile_path . None use_local_requirements bool If True and no values are given for dockerfile_path and requirements , then the packages installed in the environment of the current python processed will be installed in the docker image. False base_image Optional[str] The image to use as base for the docker image. None Source code in zenml/integrations/kubeflow/docker_utils.py def build_docker_image ( build_context_path : str , image_name : str , dockerfile_path : Optional [ str ] = None , dockerignore_path : Optional [ str ] = None , requirements : Optional [ List [ str ]] = None , use_local_requirements : bool = False , base_image : Optional [ str ] = None , ) -> None : \"\"\"Builds a docker image. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. image_name: The name to use for the created docker image. dockerfile_path: Optional path to a dockerfile. If no value is given, a temporary dockerfile will be created. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. requirements: Optional list of pip requirements to install. This will only be used if no value is given for `dockerfile_path`. use_local_requirements: If `True` and no values are given for `dockerfile_path` and `requirements`, then the packages installed in the environment of the current python processed will be installed in the docker image. base_image: The image to use as base for the docker image. \"\"\" if not requirements and use_local_requirements : local_requirements = get_current_environment_requirements () requirements = [ f \" { package } == { version } \" for package , version in local_requirements . items () if package != \"zenml\" # exclude ZenML ] logger . info ( \"Using requirements from local environment to build \" \"docker image: %s \" , requirements , ) if dockerfile_path : dockerfile_contents = zenml . io . utils . read_file_contents_as_string ( dockerfile_path ) else : dockerfile_contents = generate_dockerfile_contents ( requirements = requirements , base_image = base_image or DEFAULT_BASE_IMAGE , ) build_context = create_custom_build_context ( build_context_path = build_context_path , dockerfile_contents = dockerfile_contents , dockerignore_path = dockerignore_path , ) # If a custom base image is provided, make sure to always pull the # latest version of that image. If no base image is provided, we use # the static default ZenML image so there is no need to constantly pull always_pull_base_image = bool ( base_image ) logger . info ( \"Building docker image ' %s ', this might take a while...\" , image_name ) docker_client = DockerClient . from_env () # We use the client api directly here so we can stream the logs output_stream = docker_client . images . client . api . build ( fileobj = build_context , custom_context = True , tag = image_name , pull = always_pull_base_image , rm = False , # don't remove intermediate containers ) _process_stream ( output_stream ) logger . info ( \"Finished building docker image.\" )","title":"build_docker_image()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.create_custom_build_context","text":"Creates a docker build context. Parameters: Name Type Description Default build_context_path str Path to a directory that will be sent to the docker daemon as build context. required dockerfile_contents str File contents of the Dockerfile to use for the build. required dockerignore_path Optional[str] Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside build_context_path are included in the build context. None Returns: Type Description Any Docker build context that can be passed when building a docker image. Source code in zenml/integrations/kubeflow/docker_utils.py def create_custom_build_context ( build_context_path : str , dockerfile_contents : str , dockerignore_path : Optional [ str ] = None , ) -> Any : \"\"\"Creates a docker build context. Args: build_context_path: Path to a directory that will be sent to the docker daemon as build context. dockerfile_contents: File contents of the Dockerfile to use for the build. dockerignore_path: Optional path to a dockerignore file. If no value is given, the .dockerignore in the root of the build context will be used if it exists. Otherwise, all files inside `build_context_path` are included in the build context. Returns: Docker build context that can be passed when building a docker image. \"\"\" exclude_patterns = [] default_dockerignore_path = os . path . join ( build_context_path , \".dockerignore\" ) if dockerignore_path : exclude_patterns = _parse_dockerignore ( dockerignore_path ) elif fileio . file_exists ( default_dockerignore_path ): logger . info ( \"Using dockerignore found at path ' %s ' to create docker \" \"build context.\" , default_dockerignore_path , ) exclude_patterns = _parse_dockerignore ( default_dockerignore_path ) else : logger . info ( \"No explicit dockerignore specified and no file called \" \".dockerignore exists at the build context root ( %s ).\" \"Creating docker build context with all files inside the build \" \"context root directory.\" , build_context_path , ) logger . debug ( \"Exclude patterns for creating docker build context: %s \" , exclude_patterns , ) no_ignores_found = not exclude_patterns files = docker_build_utils . exclude_paths ( build_context_path , patterns = exclude_patterns ) extra_files = [( \"Dockerfile\" , dockerfile_contents )] context = docker_build_utils . create_archive ( root = build_context_path , files = sorted ( files ), gzip = False , extra_files = extra_files , ) build_context_size = os . path . getsize ( context . name ) if build_context_size > 50 * 1024 * 1024 and no_ignores_found : # The build context exceeds 50MiB and we didn't find any excludes # in dockerignore files -> remind to specify a .dockerignore file logger . warning ( \"Build context size for docker image: %s . If you believe this is \" \"unreasonably large, make sure to include a .dockerignore file at \" \"the root of your build context ( %s ) or specify a custom file \" \"when defining your pipeline.\" , string_utils . get_human_readable_filesize ( build_context_size ), default_dockerignore_path , ) return context","title":"create_custom_build_context()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.generate_dockerfile_contents","text":"Generates a Dockerfile. Parameters: Name Type Description Default base_image str The image to use as base for the dockerfile. required command Optional[str] The default command that gets executed when running a container of an image created by this dockerfile. None requirements Optional[List[str]] Optional list of pip requirements to install. None Returns: Type Description str Content of a dockerfile. Source code in zenml/integrations/kubeflow/docker_utils.py def generate_dockerfile_contents ( base_image : str , command : Optional [ str ] = None , requirements : Optional [ List [ str ]] = None , ) -> str : \"\"\"Generates a Dockerfile. Args: base_image: The image to use as base for the dockerfile. command: The default command that gets executed when running a container of an image created by this dockerfile. requirements: Optional list of pip requirements to install. Returns: Content of a dockerfile. \"\"\" lines = [ f \"FROM { base_image } \" , \"WORKDIR /app\" ] if requirements : lines . extend ( [ f \"RUN pip install --no-cache { ' ' . join ( requirements ) } \" , ] ) lines . append ( \"COPY . .\" ) if command : lines . append ( f \"CMD { command } \" ) return \" \\n \" . join ( lines )","title":"generate_dockerfile_contents()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.get_current_environment_requirements","text":"Returns a dict of package requirements for the environment that the current python process is running in. Source code in zenml/integrations/kubeflow/docker_utils.py def get_current_environment_requirements () -> Dict [ str , str ]: \"\"\"Returns a dict of package requirements for the environment that the current python process is running in.\"\"\" return { distribution . key : distribution . version for distribution in pkg_resources . working_set }","title":"get_current_environment_requirements()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.get_image_digest","text":"Gets the digest of a docker image. Parameters: Name Type Description Default image_name str Name of the image to get the digest for. required Returns: Type Description Optional[str] Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns None . Source code in zenml/integrations/kubeflow/docker_utils.py def get_image_digest ( image_name : str ) -> Optional [ str ]: \"\"\"Gets the digest of a docker image. Args: image_name: Name of the image to get the digest for. Returns: Returns the repo digest for the given image if there exists exactly one. If there are zero or multiple repo digests, returns `None`. \"\"\" docker_client = DockerClient . from_env () image = docker_client . images . get ( image_name ) repo_digests = image . attrs [ \"RepoDigests\" ] if len ( repo_digests ) == 1 : return cast ( str , repo_digests [ 0 ]) else : logger . debug ( \"Found zero or more repo digests for docker image ' %s ': %s \" , image_name , repo_digests , ) return None","title":"get_image_digest()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.docker_utils.push_docker_image","text":"Pushes a docker image to a container registry. Parameters: Name Type Description Default image_name str The full name (including a tag) of the image to push. required Source code in zenml/integrations/kubeflow/docker_utils.py def push_docker_image ( image_name : str ) -> None : \"\"\"Pushes a docker image to a container registry. Args: image_name: The full name (including a tag) of the image to push. \"\"\" logger . info ( \"Pushing docker image ' %s '.\" , image_name ) docker_client = DockerClient . from_env () output_stream = docker_client . images . push ( image_name , stream = True ) _process_stream ( output_stream ) logger . info ( \"Finished pushing docker image.\" )","title":"push_docker_image()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata","text":"","title":"metadata"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata.kubeflow_metadata_store","text":"","title":"kubeflow_metadata_store"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.metadata.kubeflow_metadata_store.KubeflowMetadataStore","text":"Kubeflow MySQL backend for ZenML metadata store. Source code in zenml/integrations/kubeflow/metadata/kubeflow_metadata_store.py class KubeflowMetadataStore ( MySQLMetadataStore ): \"\"\"Kubeflow MySQL backend for ZenML metadata store.\"\"\" host : str = \"127.0.0.1\" port : int = 3306 database : str = \"metadb\" username : str = \"root\" password : str = \"\"","title":"KubeflowMetadataStore"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators","text":"","title":"orchestrators"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_component","text":"Kubeflow Pipelines based implementation of TFX components. These components are lightweight wrappers around the KFP DSL's ContainerOp, and ensure that the container gets called with the right set of input arguments. It also ensures that each component exports named output attributes that are consistent with those provided by the native TFX components, thus ensuring that both types of pipeline definitions are compatible. Note: This requires Kubeflow Pipelines SDK to be installed.","title":"kubeflow_component"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_component.KubeflowComponent","text":"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py class KubeflowComponent : \"\"\"Base component for all Kubeflow pipelines TFX components. Returns a wrapper around a KFP DSL ContainerOp class, and adds named output attributes that match the output names for the corresponding native TFX components. \"\"\" def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) repo = Repository () artifact_store = repo . get_active_stack () . artifact_store metadata_store = repo . get_active_stack () . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v ) __init__ ( self , component , depends_on , image , tfx_ir , pod_labels_to_attach , main_module , step_module , step_function_name , runtime_parameters , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' ) special Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Parameters: Name Type Description Default component BaseComponent The logical TFX component to wrap. required depends_on Set[kfp.dsl._container_op.ContainerOp] The set of upstream KFP ContainerOp components that this component will depend on. required image str The container image to use for this component. required tfx_ir Pipeline The TFX intermedia representation of the pipeline. required pod_labels_to_attach Dict[str, str] Dict of pod labels to attach to the GKE pod. required runtime_parameters List[tfx.orchestration.data_types.RuntimeParameter] Runtime parameters of the pipeline. required metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_component.py def __init__ ( self , component : tfx_base_component . BaseComponent , depends_on : Set [ dsl . ContainerOp ], image : str , tfx_ir : pipeline_pb2 . Pipeline , # type: ignore[valid-type] pod_labels_to_attach : Dict [ str , str ], main_module : str , step_module : str , step_function_name : str , runtime_parameters : List [ data_types . RuntimeParameter ], metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ): \"\"\"Creates a new Kubeflow-based component. This class essentially wraps a dsl.ContainerOp construct in Kubeflow Pipelines. Args: component: The logical TFX component to wrap. depends_on: The set of upstream KFP ContainerOp components that this component will depend on. image: The container image to use for this component. tfx_ir: The TFX intermedia representation of the pipeline. pod_labels_to_attach: Dict of pod labels to attach to the GKE pod. runtime_parameters: Runtime parameters of the pipeline. metadata_ui_path: File location for metadata-ui-metadata.json file. \"\"\" utils . replace_placeholder ( component ) input_artifact_type_mapping = _get_input_artifact_type_mapping ( component ) arguments = [ \"--node_id\" , component . id , \"--tfx_ir\" , json_format . MessageToJson ( tfx_ir ), \"--metadata_ui_path\" , metadata_ui_path , \"--main_module\" , main_module , \"--step_module\" , step_module , \"--step_function_name\" , step_function_name , \"--input_artifact_types\" , json . dumps ( input_artifact_type_mapping ), \"--run_name\" , \"{{workflow.annotations.pipelines.kubeflow.org/run_name}}\" , ] for param in runtime_parameters : arguments . append ( \"--runtime_parameter\" ) arguments . append ( _encode_runtime_parameter ( param )) repo = Repository () artifact_store = repo . get_active_stack () . artifact_store metadata_store = repo . get_active_stack () . metadata_store volumes : Dict [ str , k8s_client . V1Volume ] = {} has_local_repos = False if isinstance ( artifact_store , LocalArtifactStore ): has_local_repos = True host_path = k8s_client . V1HostPathVolumeSource ( path = artifact_store . path , type = \"Directory\" ) volumes [ artifact_store . path ] = k8s_client . V1Volume ( name = \"local-artifact-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local artifact store (path: %s ) \" \"in kubeflow pipelines container.\" , artifact_store . path , ) if isinstance ( metadata_store , SQLiteMetadataStore ): has_local_repos = True metadata_store_dir = os . path . dirname ( metadata_store . uri ) host_path = k8s_client . V1HostPathVolumeSource ( path = metadata_store_dir , type = \"Directory\" ) volumes [ metadata_store_dir ] = k8s_client . V1Volume ( name = \"local-metadata-store\" , host_path = host_path ) logger . debug ( \"Adding host path volume for local metadata store (uri: %s ) \" \"in kubeflow pipelines container.\" , metadata_store . uri , ) self . container_op = dsl . ContainerOp ( name = component . id , command = CONTAINER_ENTRYPOINT_COMMAND , image = image , arguments = arguments , output_artifact_paths = { \"mlpipeline-ui-metadata\" : metadata_ui_path , }, pvolumes = volumes , ) if has_local_repos : if sys . platform == \"win32\" : # File permissions are not checked on Windows. This if clause # prevents mypy from complaining about unused 'type: ignore' # statements pass else : # Run KFP containers in the context of the local UID/GID # to ensure that the artifact and metadata stores can be shared # with the local pipeline runs. self . container_op . container . security_context = ( k8s_client . V1SecurityContext ( run_as_user = os . getuid (), run_as_group = os . getgid (), ) ) logger . debug ( \"Setting security context UID and GID to local user/group \" \"in kubeflow pipelines container.\" ) for op in depends_on : self . container_op . after ( op ) self . container_op . container . add_env_variable ( k8s_client . V1EnvVar ( name = ENV_ZENML_PREVENT_PIPELINE_EXECUTION , value = \"True\" ) ) for k , v in pod_labels_to_attach . items (): self . container_op . add_pod_label ( k , v )","title":"KubeflowComponent"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner","text":"The below code is copied from the TFX source repo with minor changes. All credits goes to the TFX team for the core implementation","title":"kubeflow_dag_runner"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.KubeflowDagRunner","text":"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunner ( tfx_runner . TfxRunner ): \"\"\"Kubeflow Pipelines runner. Constructs a pipeline definition YAML file based on the TFX logical pipeline. \"\"\" def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) def _parse_parameter_from_component ( self , component : tfx_base_component . BaseComponent ) -> None : \"\"\"Extract embedded RuntimeParameter placeholders from a component. Extract embedded RuntimeParameter placeholders from a component, then append the corresponding dsl.PipelineParam to KubeflowDagRunner. Args: component: a TFX component. \"\"\" deduped_parameter_names_for_component = set () for parameter in component . exec_properties . values (): if not isinstance ( parameter , data_types . RuntimeParameter ): continue # Ignore pipeline root because it will be added later. if parameter . name == tfx_pipeline . ROOT_PARAMETER . name : continue if parameter . name in deduped_parameter_names_for_component : continue deduped_parameter_names_for_component . add ( parameter . name ) self . _params_by_component_id [ component . id ] . append ( parameter ) if parameter . name not in self . _deduped_parameter_names : self . _deduped_parameter_names . add ( parameter . name ) dsl_parameter = dsl . PipelineParam ( name = parameter . name , value = str ( parameter . default ) ) self . _params . append ( dsl_parameter ) def _parse_parameter_from_pipeline ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Extract all the RuntimeParameter placeholders from the pipeline.\"\"\" for component in pipeline . components : self . _parse_parameter_from_component ( component ) def _construct_pipeline_graph ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Constructs a Kubeflow Pipeline graph. Args: pipeline: The logical TFX pipeline to base the construction on. pipeline_root: dsl.PipelineParam representing the pipeline root. \"\"\" component_to_kfp_op : Dict [ base_node . BaseNode , dsl . ContainerOp ] = {} tfx_ir = self . _generate_tfx_ir ( pipeline ) # Assumption: There is a partial ordering of components in the list, # i.e. if component A depends on component B and C, then A appears # after B and C in the list. for component in pipeline . components : # Keep track of the set of upstream dsl.ContainerOps for this # component. depends_on = set () for upstream_component in component . upstream_nodes : depends_on . add ( component_to_kfp_op [ upstream_component ]) # remove the extra pipeline node information tfx_node_ir = self . _dehydrate_tfx_ir ( tfx_ir , component . id ) from zenml.utils import source_utils main_module_file = sys . modules [ \"__main__\" ] . __file__ main_module = source_utils . get_module_source_from_file_path ( os . path . abspath ( main_module_file ) ) step_module = component . component_type . split ( \".\" )[: - 1 ] if step_module [ 0 ] == \"__main__\" : step_module = main_module else : step_module = \".\" . join ( step_module ) kfp_component = KubeflowComponent ( main_module = main_module , step_module = step_module , step_function_name = component . id , component = component , depends_on = depends_on , image = self . _kubeflow_config . image , pod_labels_to_attach = self . _pod_labels_to_attach , tfx_ir = tfx_node_ir , metadata_ui_path = self . _kubeflow_config . metadata_ui_path , runtime_parameters = self . _params_by_component_id [ component . id ], ) for operator in self . _kubeflow_config . pipeline_operator_funcs : kfp_component . container_op . apply ( operator ) component_to_kfp_op [ component ] = kfp_component . container_op def _del_unused_field ( self , node_id : str , message_dict : MutableMapping [ str , Any ] ) -> None : \"\"\"Remove fields that are not used by the pipeline.\"\"\" for item in list ( message_dict . keys ()): if item != node_id : del message_dict [ item ] def _dehydrate_tfx_ir ( self , original_pipeline : pipeline_pb2 . Pipeline , node_id : str # type: ignore[valid-type] # noqa ) -> pipeline_pb2 . Pipeline : # type: ignore[valid-type] \"\"\"Dehydrate the TFX IR to remove unused fields.\"\"\" pipeline = copy . deepcopy ( original_pipeline ) for node in pipeline . nodes : # type: ignore[attr-defined] if ( node . WhichOneof ( \"node\" ) == \"pipeline_node\" and node . pipeline_node . node_info . id == node_id ): del pipeline . nodes [:] # type: ignore[attr-defined] pipeline . nodes . extend ([ node ]) # type: ignore[attr-defined] break deployment_config = pipeline_pb2 . IntermediateDeploymentConfig () pipeline . deployment_config . Unpack ( deployment_config ) # type: ignore[attr-defined] # noqa self . _del_unused_field ( node_id , deployment_config . executor_specs ) self . _del_unused_field ( node_id , deployment_config . custom_driver_specs ) self . _del_unused_field ( node_id , deployment_config . node_level_platform_configs ) pipeline . deployment_config . Pack ( deployment_config ) # type: ignore[attr-defined] # noqa return pipeline def _generate_tfx_ir ( self , pipeline : tfx_pipeline . Pipeline ) -> Optional [ pipeline_pb2 . Pipeline ]: # type: ignore[valid-type] \"\"\"Generate the TFX IR from the logical TFX pipeline.\"\"\" result = self . _tfx_compiler . compile ( pipeline ) return result def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , ) __init__ ( self , config , output_path , pod_labels_to_attach = None ) special Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Parameters: Name Type Description Default config KubeflowDagRunnerConfig A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. required output_path str Path where the pipeline definition file will be stored. required pod_labels_to_attach Optional[Dict[str, str]] Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. None Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , config : KubeflowDagRunnerConfig , output_path : str , pod_labels_to_attach : Optional [ Dict [ str , str ]] = None , ): \"\"\"Initializes KubeflowDagRunner for compiling a Kubeflow Pipeline. Args: config: A KubeflowDagRunnerConfig object to specify runtime configuration when running the pipeline under Kubeflow. output_path: Path where the pipeline definition file will be stored. pod_labels_to_attach: Optional set of pod labels to attach to GKE pod spinned up for this pipeline. Default to the 3 labels: 1. add-pod-env: true, 2. pipeline SDK type, 3. pipeline unique ID, where 2 and 3 are instrumentation of usage tracking. \"\"\" super () . __init__ ( config ) self . _kubeflow_config = config self . _output_path = output_path self . _compiler = compiler . Compiler () self . _tfx_compiler = tfx_compiler . Compiler () self . _params : List [ dsl . PipelineParam ] = [] self . _params_by_component_id : Dict [ str , List [ data_types . RuntimeParameter ] ] = collections . defaultdict ( list ) self . _deduped_parameter_names : Set [ str ] = set () self . _pod_labels_to_attach = ( pod_labels_to_attach or get_default_pod_labels () ) run ( self , pipeline ) Compiles and outputs a Kubeflow Pipeline YAML definition file. Parameters: Name Type Description Default pipeline Pipeline The logical TFX pipeline to use when building the Kubeflow pipeline. required Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline ) -> None : \"\"\"Compiles and outputs a Kubeflow Pipeline YAML definition file. Args: pipeline: The logical TFX pipeline to use when building the Kubeflow pipeline. \"\"\" for component in pipeline . components : # TODO(b/187122662): Pass through pip dependencies as a first-class # component flag. if isinstance ( component , tfx_base_component . BaseComponent ): component . _resolve_pip_dependencies ( # pylint: disable=protected-access pipeline . pipeline_info . pipeline_root ) def _construct_pipeline () -> None : \"\"\"Creates Kubeflow ContainerOps for each TFX component encountered in the pipeline definition.\"\"\" self . _construct_pipeline_graph ( pipeline ) # Need to run this first to get self._params populated. Then KFP # compiler can correctly match default value with PipelineParam. self . _parse_parameter_from_pipeline ( pipeline ) # Create workflow spec and write out to package. self . _compiler . _create_and_write_workflow ( # pylint: disable=protected-access pipeline_func = _construct_pipeline , pipeline_name = pipeline . pipeline_info . pipeline_name , params_list = self . _params , package_path = self . _output_path , ) logger . info ( \"Finished writing kubeflow pipeline definition file ' %s '.\" , self . _output_path , )","title":"KubeflowDagRunner"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.KubeflowDagRunnerConfig","text":"Runtime configuration parameters specific to execution on Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py class KubeflowDagRunnerConfig ( pipeline_config . PipelineConfig ): \"\"\"Runtime configuration parameters specific to execution on Kubeflow.\"\"\" def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path __init__ ( self , image , pipeline_operator_funcs = None , supported_launcher_classes = None , metadata_ui_path = '/tmp/mlpipeline-ui-metadata.json' , ** kwargs ) special Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Parameters: Name Type Description Default image str The docker image to use in the pipeline. required pipeline_operator_funcs Optional[List[Callable[[kfp.dsl._container_op.ContainerOp], Union[kfp.dsl._container_op.ContainerOp, NoneType]]]] A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. None supported_launcher_classes Optional[List[Type[tfx.orchestration.launcher.base_component_launcher.BaseComponentLauncher]]] A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. None metadata_ui_path str File location for metadata-ui-metadata.json file. '/tmp/mlpipeline-ui-metadata.json' **kwargs Any keyword args for PipelineConfig. {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def __init__ ( self , image : str , pipeline_operator_funcs : Optional [ List [ OpFunc ]] = None , supported_launcher_classes : Optional [ List [ Type [ base_component_launcher . BaseComponentLauncher ]] ] = None , metadata_ui_path : str = \"/tmp/mlpipeline-ui-metadata.json\" , ** kwargs : Any ): \"\"\"Creates a KubeflowDagRunnerConfig object. The user can use pipeline_operator_funcs to apply modifications to ContainerOps used in the pipeline. For example, to ensure the pipeline steps mount a GCP secret, and a Persistent Volume, one can create config object like so: from kfp import gcp, onprem mount_secret_op = gcp.use_secret('my-secret-name) mount_volume_op = onprem.mount_pvc( \"my-persistent-volume-claim\", \"my-volume-name\", \"/mnt/volume-mount-path\") config = KubeflowDagRunnerConfig( pipeline_operator_funcs=[mount_secret_op, mount_volume_op] ) Args: image: The docker image to use in the pipeline. pipeline_operator_funcs: A list of ContainerOp modifying functions that will be applied to every container step in the pipeline. supported_launcher_classes: A list of component launcher classes that are supported by the current pipeline. List sequence determines the order in which launchers are chosen for each component being run. metadata_ui_path: File location for metadata-ui-metadata.json file. **kwargs: keyword args for PipelineConfig. \"\"\" supported_launcher_classes = supported_launcher_classes or [ in_process_component_launcher . InProcessComponentLauncher , kubernetes_component_launcher . KubernetesComponentLauncher , ] super () . __init__ ( supported_launcher_classes = supported_launcher_classes , ** kwargs ) self . pipeline_operator_funcs = ( pipeline_operator_funcs or get_default_pipeline_operator_funcs () ) self . image = image self . metadata_ui_path = metadata_ui_path","title":"KubeflowDagRunnerConfig"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.get_default_pipeline_operator_funcs","text":"Returns a default list of pipeline operator functions. Parameters: Name Type Description Default use_gcp_sa bool If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. False Returns: Type Description List[Callable[[kfp.dsl._container_op.ContainerOp], Optional[kfp.dsl._container_op.ContainerOp]]] A list of functions with type OpFunc. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pipeline_operator_funcs ( use_gcp_sa : bool = False , ) -> List [ OpFunc ]: \"\"\"Returns a default list of pipeline operator functions. Args: use_gcp_sa: If true, mount a GCP service account secret to each pod, with the name _KUBEFLOW_GCP_SECRET_NAME. Returns: A list of functions with type OpFunc. \"\"\" # Enables authentication for GCP services if needed. gcp_secret_op = gcp . use_gcp_secret ( _KUBEFLOW_GCP_SECRET_NAME ) # Mounts configmap containing Metadata gRPC server configuration. mount_config_map_op = _mount_config_map_op ( \"metadata-grpc-configmap\" ) if use_gcp_sa : return [ gcp_secret_op , mount_config_map_op ] else : return [ mount_config_map_op ]","title":"get_default_pipeline_operator_funcs()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_dag_runner.get_default_pod_labels","text":"Returns the default pod label dict for Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_dag_runner.py def get_default_pod_labels () -> Dict [ str , str ]: \"\"\"Returns the default pod label dict for Kubeflow.\"\"\" # KFP default transformers add pod env: # https://github.com/kubeflow/pipelines/blob/0.1.32/sdk/python/kfp/compiler/_default_transformers.py result = { \"add-pod-env\" : \"true\" , telemetry_utils . LABEL_KFP_SDK_ENV : \"tfx\" } return result","title":"get_default_pod_labels()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_orchestrator","text":"","title":"kubeflow_orchestrator"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_orchestrator.KubeflowOrchestrator","text":"Orchestrator responsible for running pipelines using Kubeflow. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py class KubeflowOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines using Kubeflow.\"\"\" custom_docker_base_image_name : Optional [ str ] = None kubeflow_pipelines_ui_port : int = DEFAULT_KFP_UI_PORT kubernetes_context : Optional [ str ] = None def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . get_active_stack () . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name @property def root_directory ( self ) -> str : \"\"\"Returns path to the root directory for all files concerning this orchestrator.\"\"\" return os . path . join ( zenml . io . utils . get_global_config_directory (), \"kubeflow\" , str ( self . uuid ), ) @property def pipeline_directory ( self ) -> str : \"\"\"Returns path to a directory in which the kubeflow pipeline files are stored.\"\"\" return os . path . join ( self . root_directory , \"pipelines\" ) def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) repository_root = Repository () . path requirements = ( [ \"kubernetes\" ] + self . _get_stack_requirements () + self . _get_pipeline_requirements ( pipeline ) ) logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = repository_root , image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if Repository () . get_active_stack () . container_registry : push_docker_image ( image_name ) def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> None : \"\"\"Runs the pipeline on Kubeflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused kwargs to conform with base signature \"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( zenml_pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { zenml_pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline ) run_name = run_name or datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = zenml_pipeline . enable_cache , ) def _upload_and_run_pipeline ( self , pipeline_file_path : str , run_name : str , enable_cache : bool ) -> None : \"\"\"Tries to upload and run a KFP pipeline. Args: pipeline_file_path: Path to the pipeline definition file. run_name: A name for the pipeline run that will be started. enable_cache: Whether caching is enabled for this pipeline run. \"\"\" try : if self . kubernetes_context : logger . info ( \"Running in kubernetes context ' %s '.\" , self . kubernetes_context , ) # load kubernetes config to authorize the KFP client config . load_kube_config ( context = self . kubernetes_context ) # upload the pipeline to Kubeflow and start it client = kfp . Client () result = client . create_run_from_pipeline_package ( pipeline_file_path , arguments = {}, run_name = run_name , enable_caching = enable_cache , ) logger . info ( \"Started pipeline run with ID ' %s '.\" , result . run_id ) except urllib3 . exceptions . HTTPError as error : logger . warning ( \"Failed to upload Kubeflow pipeline: %s . \" \"Please make sure your kube config is configured and the \" \"current context is set correctly.\" , error , ) def _get_stack_requirements ( self ) -> List [ str ]: \"\"\"Gets list of requirements for the current active stack.\"\"\" stack = Repository () . get_active_stack () requirements = [] artifact_store_module = stack . artifact_store . __module__ requirements += get_requirements_for_module ( artifact_store_module ) metadata_store_module = stack . metadata_store . __module__ requirements += get_requirements_for_module ( metadata_store_module ) return requirements def _get_pipeline_requirements ( self , pipeline : \"BasePipeline\" ) -> List [ str ]: \"\"\"Gets list of requirements for a pipeline.\"\"\" if pipeline . requirements_file and fileio . file_exists ( pipeline . requirements_file ): logger . debug ( \"Using requirements from file %s .\" , pipeline . requirements_file ) with fileio . open ( pipeline . requirements_file , \"r\" ) as f : return [ requirement . strip () for requirement in f . read () . split ( \" \\n \" ) ] else : return [] @property def _pid_file_path ( self ) -> str : \"\"\"Returns path to the daemon PID file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.pid\" ) @property def log_file ( self ) -> str : \"\"\"Path of the daemon log file.\"\"\" return os . path . join ( self . root_directory , \"kubeflow_daemon.log\" ) @property def _k3d_cluster_name ( self ) -> str : \"\"\"Returns the K3D cluster name.\"\"\" # K3D only allows cluster names with up to 32 characters, use the # first 8 chars of the orchestrator UUID as identifier return f \"zenml-kubeflow- { str ( self . uuid )[: 8 ] } \" def _get_k3d_registry_name ( self , port : int ) -> str : \"\"\"Returns the K3D registry name.\"\"\" return f \"k3d-zenml-kubeflow-registry.localhost: { port } \" @property def _k3d_registry_config_path ( self ) -> str : \"\"\"Returns the path to the K3D registry config yaml.\"\"\" return os . path . join ( self . root_directory , \"k3d_registry.yaml\" ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is running.\"\"\" if not local_deployment_utils . check_prerequisites (): # if any prerequisites are missing there is certainly no # local deployment running return False return local_deployment_utils . k3d_cluster_exists ( cluster_name = self . _k3d_cluster_name ) def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) def up ( self ) -> None : \"\"\"Spins up a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . get_active_stack () . container_registry if not container_registry : logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Spinning up local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . down () def down ( self ) -> None : \"\"\"Tears down a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment spun down.\" ) is_running : bool property readonly Returns whether the orchestrator is running. log_file : str property readonly Path of the daemon log file. pipeline_directory : str property readonly Returns path to a directory in which the kubeflow pipeline files are stored. root_directory : str property readonly Returns path to the root directory for all files concerning this orchestrator. down ( self ) Tears down a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def down ( self ) -> None : \"\"\"Tears down a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : local_deployment_utils . delete_k3d_cluster ( cluster_name = self . _k3d_cluster_name ) if fileio . file_exists ( self . _pid_file_path ): if sys . platform == \"win32\" : # Daemon functionality is not supported on Windows, so the PID # file won't exist. This if clause exists just for mypy to not # complain about missing functions pass else : from zenml.utils import daemon daemon . stop_daemon ( self . _pid_file_path , kill_children = True ) fileio . remove ( self . _pid_file_path ) if fileio . file_exists ( self . log_file ): fileio . remove ( self . log_file ) logger . info ( \"Local kubeflow pipelines deployment spun down.\" ) get_docker_image_name ( self , pipeline_name ) Returns the full docker image name including registry and tag. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def get_docker_image_name ( self , pipeline_name : str ) -> str : \"\"\"Returns the full docker image name including registry and tag.\"\"\" base_image_name = f \"zenml-kubeflow: { pipeline_name } \" container_registry = Repository () . get_active_stack () . container_registry if container_registry : registry_uri = container_registry . uri . rstrip ( \"/\" ) return f \" { registry_uri } / { base_image_name } \" else : return base_image_name list_manual_setup_steps ( self , container_registry_name , container_registry_path ) Logs manual steps needed to setup the Kubeflow local orchestrator. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def list_manual_setup_steps ( self , container_registry_name : str , container_registry_path : str ) -> None : \"\"\"Logs manual steps needed to setup the Kubeflow local orchestrator.\"\"\" global_config_dir_path = zenml . io . utils . get_global_config_directory () kubeflow_commands = [ f \"> k3d cluster create CLUSTER_NAME --registry-create { container_registry_name } --registry-config { container_registry_path } --volume { global_config_dir_path } : { global_config_dir_path } \\n \" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , \"> kubectl --context CLUSTER_NAME wait --timeout=60s --for condition=established crd/applications.app.k8s.io\" , f \"> kubectl --context CLUSTER_NAME apply -k github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , f \"> kubectl --namespace kubeflow port-forward svc/ml-pipeline-ui { self . kubeflow_pipelines_ui_port } :80\" , ] logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment.\" ) logger . info ( \"If you wish to spin up this Kubeflow local orchestrator manually, \" \"please enter the following commands (substituting where appropriate): \\n \" ) logger . info ( \" \\n \" . join ( kubeflow_commands )) pre_run ( self , pipeline , caller_filepath ) Builds a docker image for the current environment and uploads it to a container registry if configured. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Builds a docker image for the current environment and uploads it to a container registry if configured. \"\"\" from zenml.integrations.kubeflow.docker_utils import ( build_docker_image , push_docker_image , ) image_name = self . get_docker_image_name ( pipeline . name ) repository_root = Repository () . path requirements = ( [ \"kubernetes\" ] + self . _get_stack_requirements () + self . _get_pipeline_requirements ( pipeline ) ) logger . debug ( \"Kubeflow docker container requirements: %s \" , requirements ) build_docker_image ( build_context_path = repository_root , image_name = image_name , dockerignore_path = pipeline . dockerignore_file , requirements = requirements , base_image = self . custom_docker_base_image_name , ) if Repository () . get_active_stack () . container_registry : push_docker_image ( image_name ) run ( self , zenml_pipeline , run_name , ** kwargs ) Runs the pipeline on Kubeflow. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Unused kwargs to conform with base signature {} Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any , ) -> None : \"\"\"Runs the pipeline on Kubeflow. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Unused kwargs to conform with base signature \"\"\" from zenml.integrations.kubeflow.docker_utils import get_image_digest image_name = self . get_docker_image_name ( zenml_pipeline . name ) image_name = get_image_digest ( image_name ) or image_name fileio . make_dirs ( self . pipeline_directory ) pipeline_file_path = os . path . join ( self . pipeline_directory , f \" { zenml_pipeline . name } .yaml\" ) runner_config = KubeflowDagRunnerConfig ( image = image_name ) runner = KubeflowDagRunner ( config = runner_config , output_path = pipeline_file_path ) tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline ) run_name = run_name or datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) self . _upload_and_run_pipeline ( pipeline_file_path = pipeline_file_path , run_name = run_name , enable_cache = zenml_pipeline . enable_cache , ) up ( self ) Spins up a local Kubeflow Pipelines deployment. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_orchestrator.py def up ( self ) -> None : \"\"\"Spins up a local Kubeflow Pipelines deployment.\"\"\" if self . is_running : logger . info ( \"Found already existing local Kubeflow Pipelines deployment. \" \"If there are any issues with the existing deployment, please \" \"run 'zenml orchestrator down' to delete it.\" ) return if not local_deployment_utils . check_prerequisites (): logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Please install 'k3d' and 'kubectl' and try again.\" ) return container_registry = Repository () . get_active_stack () . container_registry if not container_registry : logger . error ( \"Unable to spin up local Kubeflow Pipelines deployment: \" \"Missing container registry in current stack.\" ) return logger . info ( \"Spinning up local Kubeflow Pipelines deployment...\" ) fileio . make_dirs ( self . root_directory ) container_registry_port = int ( container_registry . uri . split ( \":\" )[ - 1 ]) container_registry_name = self . _get_k3d_registry_name ( port = container_registry_port ) local_deployment_utils . write_local_registry_yaml ( yaml_path = self . _k3d_registry_config_path , registry_name = container_registry_name , registry_uri = container_registry . uri , ) try : local_deployment_utils . create_k3d_cluster ( cluster_name = self . _k3d_cluster_name , registry_name = container_registry_name , registry_config_path = self . _k3d_registry_config_path , ) kubernetes_context = f \"k3d- { self . _k3d_cluster_name } \" local_deployment_utils . deploy_kubeflow_pipelines ( kubernetes_context = kubernetes_context ) port = self . kubeflow_pipelines_ui_port if ( port == DEFAULT_KFP_UI_PORT and not networking_utils . port_available ( port ) ): # if the user didn't specify a specific port and the default # port is occupied, fallback to a random open port port = networking_utils . find_available_port () local_deployment_utils . start_kfp_ui_daemon ( pid_file_path = self . _pid_file_path , log_file_path = self . log_file , port = port , ) except Exception as e : logger . error ( e ) self . list_manual_setup_steps ( container_registry_name , self . _k3d_registry_config_path ) self . down ()","title":"KubeflowOrchestrator"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_utils","text":"Common utility for Kubeflow-based orchestrator.","title":"kubeflow_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.kubeflow_utils.replace_placeholder","text":"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam. Source code in zenml/integrations/kubeflow/orchestrators/kubeflow_utils.py def replace_placeholder ( component : base_node . BaseNode ) -> None : \"\"\"Replaces the RuntimeParameter placeholders with kfp.dsl.PipelineParam.\"\"\" keys = list ( component . exec_properties . keys ()) for key in keys : exec_property = component . exec_properties [ key ] if not isinstance ( exec_property , data_types . RuntimeParameter ): continue component . exec_properties [ key ] = str ( dsl . PipelineParam ( name = exec_property . name ) )","title":"replace_placeholder()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils","text":"","title":"local_deployment_utils"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.check_prerequisites","text":"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def check_prerequisites () -> bool : \"\"\"Checks whether all prerequisites for a local kubeflow pipelines deployment are installed.\"\"\" k3d_installed = shutil . which ( \"k3d\" ) is not None kubectl_installed = shutil . which ( \"kubectl\" ) is not None logger . debug ( \"Local kubeflow deployment prerequisites: K3D - %s , Kubectl - %s \" , k3d_installed , kubectl_installed , ) return k3d_installed and kubectl_installed","title":"check_prerequisites()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.create_k3d_cluster","text":"Creates a K3D cluster. Parameters: Name Type Description Default cluster_name str Name of the cluster to create. required registry_name str Name of the registry to create for this cluster. required registry_config_path str Path to the registry config file. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def create_k3d_cluster ( cluster_name : str , registry_name : str , registry_config_path : str ) -> None : \"\"\"Creates a K3D cluster. Args: cluster_name: Name of the cluster to create. registry_name: Name of the registry to create for this cluster. registry_config_path: Path to the registry config file. \"\"\" logger . info ( \"Creating local K3D cluster ' %s '.\" , cluster_name ) global_config_dir_path = zenml . io . utils . get_global_config_directory () subprocess . check_call ( [ \"k3d\" , \"cluster\" , \"create\" , cluster_name , \"--registry-create\" , registry_name , \"--registry-config\" , registry_config_path , \"--volume\" , f \" { global_config_dir_path } : { global_config_dir_path } \" , ] ) logger . info ( \"Finished K3D cluster creation.\" )","title":"create_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.delete_k3d_cluster","text":"Deletes a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def delete_k3d_cluster ( cluster_name : str ) -> None : \"\"\"Deletes a K3D cluster with the given name.\"\"\" subprocess . check_call ([ \"k3d\" , \"cluster\" , \"delete\" , cluster_name ]) logger . info ( \"Deleted local k3d cluster ' %s '.\" , cluster_name )","title":"delete_k3d_cluster()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.deploy_kubeflow_pipelines","text":"Deploys Kubeflow Pipelines. Parameters: Name Type Description Default kubernetes_context str The kubernetes context on which Kubeflow Pipelines should be deployed. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def deploy_kubeflow_pipelines ( kubernetes_context : str ) -> None : \"\"\"Deploys Kubeflow Pipelines. Args: kubernetes_context: The kubernetes context on which Kubeflow Pipelines should be deployed. \"\"\" logger . info ( \"Deploying Kubeflow Pipelines.\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/cluster-scoped-resources?ref= { KFP_VERSION } &timeout=1m\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"wait\" , \"--timeout=60s\" , \"--for\" , \"condition=established\" , \"crd/applications.app.k8s.io\" , ] ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"apply\" , \"-k\" , f \"github.com/kubeflow/pipelines/manifests/kustomize/env/platform-agnostic-pns?ref= { KFP_VERSION } &timeout=1m\" , ] ) logger . info ( \"Waiting for all Kubeflow Pipelines pods to be ready (this might \" \"take a few minutes).\" ) while True : logger . info ( \"Current pod status:\" ) subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"get\" , \"pods\" , ] ) if kubeflow_pipelines_ready ( kubernetes_context = kubernetes_context ): break logger . info ( \"One or more pods not ready yet, waiting for 30 seconds...\" ) time . sleep ( 30 ) logger . info ( \"Finished Kubeflow Pipelines setup.\" )","title":"deploy_kubeflow_pipelines()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.k3d_cluster_exists","text":"Checks whether there exists a K3D cluster with the given name. Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def k3d_cluster_exists ( cluster_name : str ) -> bool : \"\"\"Checks whether there exists a K3D cluster with the given name.\"\"\" output = subprocess . check_output ( [ \"k3d\" , \"cluster\" , \"list\" , \"--output\" , \"json\" ] ) clusters = json . loads ( output ) for cluster in clusters : if cluster [ \"name\" ] == cluster_name : return True return False","title":"k3d_cluster_exists()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.kubeflow_pipelines_ready","text":"Returns whether all Kubeflow Pipelines pods are ready. Parameters: Name Type Description Default kubernetes_context str The kubernetes context in which the pods should be checked. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def kubeflow_pipelines_ready ( kubernetes_context : str ) -> bool : \"\"\"Returns whether all Kubeflow Pipelines pods are ready. Args: kubernetes_context: The kubernetes context in which the pods should be checked. \"\"\" try : subprocess . check_call ( [ \"kubectl\" , \"--context\" , kubernetes_context , \"--namespace\" , \"kubeflow\" , \"wait\" , \"--for\" , \"condition=ready\" , \"--timeout=0s\" , \"pods\" , \"--all\" , ], stdout = subprocess . DEVNULL , stderr = subprocess . DEVNULL , ) return True except subprocess . CalledProcessError : return False","title":"kubeflow_pipelines_ready()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.start_kfp_ui_daemon","text":"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Parameters: Name Type Description Default pid_file_path str Path where the file with the daemons process ID should be written. required log_file_path str Path to a file where the daemon logs should be written. required port int Port on which the UI should be accessible. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def start_kfp_ui_daemon ( pid_file_path : str , log_file_path : str , port : int ) -> None : \"\"\"Starts a daemon process that forwards ports so the Kubeflow Pipelines UI is accessible in the browser. Args: pid_file_path: Path where the file with the daemons process ID should be written. log_file_path: Path to a file where the daemon logs should be written. port: Port on which the UI should be accessible. \"\"\" command = [ \"kubectl\" , \"--namespace\" , \"kubeflow\" , \"port-forward\" , \"svc/ml-pipeline-ui\" , f \" { port } :80\" , ] if not networking_utils . port_available ( port ): modified_command = command . copy () modified_command [ - 1 ] = \"PORT:80\" logger . warning ( \"Unable to port-forward Kubeflow Pipelines UI to local port %d \" \"because the port is occupied. In order to access the Kubeflow \" \"Pipelines UI at http://localhost:PORT/, please run ' %s ' in a \" \"separate command line shell (replace PORT with a free port of \" \"your choice).\" , port , \" \" . join ( modified_command ), ) elif sys . platform == \"win32\" : logger . warning ( \"Daemon functionality not supported on Windows. \" \"In order to access the Kubeflow Pipelines UI at \" \"http://localhost: %d /, please run ' %s ' in a separate command \" \"line shell.\" , port , \" \" . join ( command ), ) else : from zenml.utils import daemon def _daemon_function () -> None : \"\"\"Port-forwards the Kubeflow Pipelines UI pod.\"\"\" subprocess . check_call ( command ) daemon . run_as_daemon ( _daemon_function , pid_file = pid_file_path , log_file = log_file_path ) logger . info ( \"Started Kubeflow Pipelines UI daemon (check the daemon logs at %s \" \"in case you're not able to view the UI). The Kubeflow Pipelines \" \"UI should now be accessible at http://localhost: %d /.\" , log_file_path , port , )","title":"start_kfp_ui_daemon()"},{"location":"api_docs/integrations/#zenml.integrations.kubeflow.orchestrators.local_deployment_utils.write_local_registry_yaml","text":"Writes a K3D registry config file. Parameters: Name Type Description Default yaml_path str Path where the config file should be written to. required registry_name str Name of the registry. required registry_uri str URI of the registry. required Source code in zenml/integrations/kubeflow/orchestrators/local_deployment_utils.py def write_local_registry_yaml ( yaml_path : str , registry_name : str , registry_uri : str ) -> None : \"\"\"Writes a K3D registry config file. Args: yaml_path: Path where the config file should be written to. registry_name: Name of the registry. registry_uri: URI of the registry. \"\"\" yaml_content = { \"mirrors\" : { registry_uri : { \"endpoint\" : [ f \"http:// { registry_name } \" ]}} } yaml_utils . write_yaml ( yaml_path , yaml_content )","title":"write_local_registry_yaml()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow","text":"The mlflow integrations currently enables you to use mlflow tracking as a convenient way to visualize your experiment runs within the mlflow ui","title":"mlflow"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.MlflowIntegration","text":"Definition of Plotly integration for ZenML. Source code in zenml/integrations/mlflow/__init__.py class MlflowIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = MLFLOW REQUIREMENTS = [ \"mlflow>=1.2.0\" ]","title":"MlflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils","text":"","title":"mlflow_utils"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow","text":"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the init and run method need to be extended accordingly. Parameters: Name Type Description Default _pipeline Type[zenml.pipelines.base_pipeline.BasePipeline] The decorated pipeline required experiment_name Optional[str] Experiment name to use for mlflow None Returns: Type Description Type[zenml.pipelines.base_pipeline.BasePipeline] the inner decorator which has a pipeline with the two methods extended Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow ( _pipeline : Type [ BasePipeline ], experiment_name : Optional [ str ] = None ) -> Type [ BasePipeline ]: \"\"\"Outer decorator function for the creation of a ZenML pipeline with mlflow tracking enabled. In order for a pipeline to run within the context of mlflow, the mlflow experiment should be associated with the pipeline directly. Each separate pipeline run needs to be associated directly with a pipeline run. For this, the __init__ and run method need to be extended accordingly. Args: _pipeline: The decorated pipeline experiment_name: Experiment name to use for mlflow Returns: the inner decorator which has a pipeline with the two methods extended \"\"\" def inner_decorator ( pipeline : Type [ BasePipeline ]) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline with mlflow The __init__ and run method are both extended. Args: pipeline: BasePipeline which will be extended Returns: the class of a newly generated ZenML Pipeline with mlflow \"\"\" return type ( # noqa pipeline . __name__ , ( pipeline ,), { \"__init__\" : enable_mlflow_init ( pipeline . __init__ , experiment_name ), \"run\" : enable_mlflow_run ( pipeline . run ), }, ) return inner_decorator ( _pipeline )","title":"enable_mlflow()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow_init","text":"Outer decorator function for extending the init method for pipelines that should be run using mlflow Parameters: Name Type Description Default original_init Callable[[zenml.pipelines.base_pipeline.BasePipeline, zenml.steps.base_step.BaseStep, Any], NoneType] The init method that should be extended required experiment Optional[str] The users chosen experiment name to use for mlflow None Returns: Type Description Callable[..., NoneType] the inner decorator which extends the init method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_init ( original_init : Callable [[ BasePipeline , BaseStep , Any ], None ], experiment : Optional [ str ] = None , ) -> Callable [ ... , None ]: \"\"\"Outer decorator function for extending the __init__ method for pipelines that should be run using mlflow Args: original_init: The __init__ method that should be extended experiment: The users chosen experiment name to use for mlflow Returns: the inner decorator which extends the __init__ method \"\"\" def inner_decorator ( self : BasePipeline , * args : BaseStep , ** kwargs : Any ) -> None : \"\"\"Inner decorator overwriting the pipeline __init__ Makes sure mlflow is properly set up and all mlflow logging takes place within one mlflow experiment that is associated with the pipeline \"\"\" original_init ( self , * args , ** kwargs ) setup_mlflow ( backend_store_uri = local_mlflow_backend (), experiment_name = experiment if experiment else self . name , ) return inner_decorator","title":"enable_mlflow_init()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.enable_mlflow_run","text":"Outer decorator function for extending the run method for pipelines that should be run using mlflow Parameters: Name Type Description Default run Callable[..., Any] The run method that should be extended required Returns: Type Description Callable[..., Any] the inner decorator which extends the run method Source code in zenml/integrations/mlflow/mlflow_utils.py def enable_mlflow_run ( run : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Outer decorator function for extending the run method for pipelines that should be run using mlflow Args: run: The run method that should be extended Returns: the inner decorator which extends the run method \"\"\" def inner_decorator ( self : BasePipeline , run_name : Optional [ str ] = None ) -> Any : \"\"\"Inner decorator used to extend the run method of a pipeline. This ensures each pipeline run is run within a different mlflow context. Args: self: self of the original pipeline class run_name: Optional name for the run. \"\"\" with mlflow . start_run ( run_name = run_name ): run ( self , run_name ) return inner_decorator","title":"enable_mlflow_run()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.local_mlflow_backend","text":"Returns the local mlflow backend inside the global zenml directory Source code in zenml/integrations/mlflow/mlflow_utils.py def local_mlflow_backend () -> str : \"\"\"Returns the local mlflow backend inside the global zenml directory\"\"\" local_mlflow_backend_uri = os . path . join ( get_global_config_directory (), \"local_stores\" , \"mlruns\" ) if not os . path . exists ( local_mlflow_backend_uri ): os . makedirs ( local_mlflow_backend_uri ) # TODO [medium]: safely access (possibly non-existent) artifact stores return \"file:\" + local_mlflow_backend_uri","title":"local_mlflow_backend()"},{"location":"api_docs/integrations/#zenml.integrations.mlflow.mlflow_utils.setup_mlflow","text":"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Parameters: Name Type Description Default backend_store_uri Optional[str] The mlflow backend to log to None experiment_name str The experiment name under which all runs will be tracked 'default' Source code in zenml/integrations/mlflow/mlflow_utils.py def setup_mlflow ( backend_store_uri : Optional [ str ] = None , experiment_name : str = \"default\" ) -> None : \"\"\"Setup all mlflow related configurations. This includes specifying which mlflow tracking uri should b e used and which experiment the tracking will be associated with. Args: backend_store_uri: The mlflow backend to log to experiment_name: The experiment name under which all runs will be tracked \"\"\" # TODO [ENG-316]: Implement a way to get the mlflow token and set # it as env variable at MLFLOW_TRACKING_TOKEN if not backend_store_uri : backend_store_uri = local_mlflow_backend () set_tracking_uri ( backend_store_uri ) # Set which experiment is used within mlflow set_experiment ( experiment_name )","title":"setup_mlflow()"},{"location":"api_docs/integrations/#zenml.integrations.plotly","text":"","title":"plotly"},{"location":"api_docs/integrations/#zenml.integrations.plotly.PlotlyIntegration","text":"Definition of Plotly integration for ZenML. Source code in zenml/integrations/plotly/__init__.py class PlotlyIntegration ( Integration ): \"\"\"Definition of Plotly integration for ZenML.\"\"\" NAME = PLOTLY REQUIREMENTS = [ \"plotly>=5.4.0\" ]","title":"PlotlyIntegration"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers","text":"","title":"visualizers"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers.pipeline_lineage_visualizer","text":"","title":"pipeline_lineage_visualizer"},{"location":"api_docs/integrations/#zenml.integrations.plotly.visualizers.pipeline_lineage_visualizer.PipelineLineageVisualizer","text":"Visualize the lineage of runs in a pipeline using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py class PipelineLineageVisualizer ( BasePipelineVisualizer ): \"\"\"Visualize the lineage of runs in a pipeline using plotly.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . name : str ( step . id ), } ) if step . name not in dimensions : dimensions . append ( f \" { step . name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig visualize ( self , object , * args , ** kwargs ) Creates a pipeline lineage diagram using plotly. Source code in zenml/integrations/plotly/visualizers/pipeline_lineage_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Figure : \"\"\"Creates a pipeline lineage diagram using plotly.\"\"\" logger . warning ( \"This integration is not completed yet. Results might be unexpected.\" ) category_dict = {} dimensions = [ \"run\" ] for run in object . runs : category_dict [ run . name ] = { \"run\" : run . name } for step in run . steps : category_dict [ run . name ] . update ( { step . name : str ( step . id ), } ) if step . name not in dimensions : dimensions . append ( f \" { step . name } \" ) category_df = pd . DataFrame . from_dict ( category_dict , orient = \"index\" ) category_df = category_df . reset_index () fig = px . parallel_categories ( category_df , dimensions , color = None , labels = \"status\" , ) fig . show () return fig","title":"PipelineLineageVisualizer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch","text":"","title":"pytorch"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.PytorchIntegration","text":"Definition of PyTorch integration for ZenML. Source code in zenml/integrations/pytorch/__init__.py class PytorchIntegration ( Integration ): \"\"\"Definition of PyTorch integration for ZenML.\"\"\" NAME = PYTORCH REQUIREMENTS = [ \"torch\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa","title":"PytorchIntegration"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.PytorchIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/pytorch/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_materializer","text":"","title":"pytorch_materializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_materializer.PyTorchMaterializer","text":"Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py class PyTorchMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Module , TorchDict ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) handle_input ( self , data_type ) Reads and returns a PyTorch model. Returns: Type Description Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A loaded pytorch model. Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Union [ Module , TorchDict ]: \"\"\"Reads and returns a PyTorch model. Returns: A loaded pytorch model. \"\"\" super () . handle_input ( data_type ) return torch . load ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME )) # type: ignore[no-untyped-call] # noqa handle_return ( self , model ) Writes a PyTorch model. Parameters: Name Type Description Default model Union[torch.nn.modules.module.Module, zenml.integrations.pytorch.materializers.pytorch_types.TorchDict] A torch.nn.Module or a dict to pass into model.save required Source code in zenml/integrations/pytorch/materializers/pytorch_materializer.py def handle_return ( self , model : Union [ Module , TorchDict ]) -> None : \"\"\"Writes a PyTorch model. Args: model: A torch.nn.Module or a dict to pass into model.save \"\"\" super () . handle_return ( model ) torch . save ( model , os . path . join ( self . artifact . uri , DEFAULT_FILENAME ))","title":"PyTorchMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_types","text":"","title":"pytorch_types"},{"location":"api_docs/integrations/#zenml.integrations.pytorch.materializers.pytorch_types.TorchDict","text":"A type of dict that represents saving a model. Source code in zenml/integrations/pytorch/materializers/pytorch_types.py class TorchDict ( Dict [ str , Any ]): \"\"\"A type of dict that represents saving a model.\"\"\"","title":"TorchDict"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning","text":"","title":"pytorch_lightning"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.PytorchLightningIntegration","text":"Definition of PyTorch Lightning integration for ZenML. Source code in zenml/integrations/pytorch_lightning/__init__.py class PytorchLightningIntegration ( Integration ): \"\"\"Definition of PyTorch Lightning integration for ZenML.\"\"\" NAME = PYTORCH_L REQUIREMENTS = [ \"pytorch_lightning\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa","title":"PytorchLightningIntegration"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.PytorchLightningIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/pytorch_lightning/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.pytorch_lightning import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers.pytorch_lightning_materializer","text":"","title":"pytorch_lightning_materializer"},{"location":"api_docs/integrations/#zenml.integrations.pytorch_lightning.materializers.pytorch_lightning_materializer.PyTorchLightningMaterializer","text":"Materializer to read/write Pytorch models. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py class PyTorchLightningMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Pytorch models.\"\"\" ASSOCIATED_TYPES = [ Trainer ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_input ( self , data_type ) Reads and returns a PyTorch Lightning trainer. Returns: Type Description Trainer A PyTorch Lightning trainer object. Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Trainer : \"\"\"Reads and returns a PyTorch Lightning trainer. Returns: A PyTorch Lightning trainer object. \"\"\" super () . handle_input ( data_type ) return Trainer ( resume_from_checkpoint = os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) ) handle_return ( self , trainer ) Writes a PyTorch Lightning trainer. Parameters: Name Type Description Default trainer Trainer A PyTorch Lightning trainer object. required Source code in zenml/integrations/pytorch_lightning/materializers/pytorch_lightning_materializer.py def handle_return ( self , trainer : Trainer ) -> None : \"\"\"Writes a PyTorch Lightning trainer. Args: trainer: A PyTorch Lightning trainer object. \"\"\" super () . handle_return ( trainer ) trainer . save_checkpoint ( os . path . join ( self . artifact . uri , CHECKPOINT_NAME ) )","title":"PyTorchLightningMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.registry","text":"","title":"registry"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry","text":"Registry to keep track of ZenML Integrations Source code in zenml/integrations/registry.py class IntegrationRegistry ( object ): \"\"\"Registry to keep track of ZenML Integrations\"\"\" def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {} @property def integrations ( self ) -> Dict [ str , Type [ \"Integration\" ]]: \"\"\"Method to get integrations dictionary. Returns: A dict of integration key to type of `Integration`. \"\"\" return self . _integrations @integrations . setter def integrations ( self , i : Any ) -> None : \"\"\"Setter method for the integrations property\"\"\" raise IntegrationError ( \"Please do not manually change the integrations within the \" \"registry. If you would like to register a new integration \" \"manually, please use \" \"`integration_registry.register_integration()`.\" ) def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_ def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" ) @property def list_integration_names ( self ) -> List [ str ]: \"\"\"Get a list of all possible integrations\"\"\" return [ name for name in self . _integrations ] def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ] def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" )","title":"IntegrationRegistry"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.integrations","text":"Method to get integrations dictionary. Returns: Type Description Dict[str, Type[Integration]] A dict of integration key to type of Integration .","title":"integrations"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.list_integration_names","text":"Get a list of all possible integrations","title":"list_integration_names"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.__init__","text":"Initializing the integration registry Source code in zenml/integrations/registry.py def __init__ ( self ) -> None : \"\"\"Initializing the integration registry\"\"\" self . _integrations : Dict [ str , Type [ \"Integration\" ]] = {}","title":"__init__()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.activate_integrations","text":"Method to activate the integrations with are registered in the registry Source code in zenml/integrations/registry.py def activate_integrations ( self ) -> None : \"\"\"Method to activate the integrations with are registered in the registry\"\"\" for name , integration in self . _integrations . items (): if integration . check_installation (): integration . activate () logger . debug ( f \"Integration ` { name } ` is activated.\" ) else : logger . debug ( f \"Integration ` { name } ` could not be activated.\" )","title":"activate_integrations()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.is_installed","text":"Checks if all requirements for an integration are installed Source code in zenml/integrations/registry.py def is_installed ( self , integration_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if all requirements for an integration are installed\"\"\" if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . check_installation () elif not integration_name : all_installed = [ self . _integrations [ item ] . check_installation () for item in self . list_integration_names ] return any ( all_installed ) else : raise KeyError ( f \"Integration ' { integration_name } ' not found. \" f \"Currently the following integrations are available: \" f \" { self . list_integration_names } \" )","title":"is_installed()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.register_integration","text":"Method to register an integration with a given name Source code in zenml/integrations/registry.py def register_integration ( self , key : str , type_ : Type [ \"Integration\" ] ) -> None : \"\"\"Method to register an integration with a given name\"\"\" self . _integrations [ key ] = type_","title":"register_integration()"},{"location":"api_docs/integrations/#zenml.integrations.registry.IntegrationRegistry.select_integration_requirements","text":"Select the requirements for a given integration or all integrations Source code in zenml/integrations/registry.py def select_integration_requirements ( self , integration_name : Optional [ str ] = None ) -> List [ str ]: \"\"\"Select the requirements for a given integration or all integrations\"\"\" if integration_name : if integration_name in self . list_integration_names : return self . _integrations [ integration_name ] . REQUIREMENTS else : raise KeyError ( f \"Version { integration_name } does not exist. \" f \"Currently the following integrations are implemented. \" f \" { self . list_integration_names } \" ) else : return [ requirement for name in self . list_integration_names for requirement in self . _integrations [ name ] . REQUIREMENTS ]","title":"select_integration_requirements()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn","text":"","title":"sklearn"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.SklearnIntegration","text":"Definition of sklearn integration for ZenML. Source code in zenml/integrations/sklearn/__init__.py class SklearnIntegration ( Integration ): \"\"\"Definition of sklearn integration for ZenML.\"\"\" NAME = SKLEARN REQUIREMENTS = [ \"scikit-learn\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa","title":"SklearnIntegration"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.SklearnIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/sklearn/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.sklearn import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers","text":"","title":"helpers"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits","text":"","title":"digits"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits.get_digits","text":"Returns the digits dataset in the form of a tuple of numpy arrays. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits () -> Tuple [ np . ndarray , np . ndarray , np . ndarray , np . ndarray ]: \"\"\"Returns the digits dataset in the form of a tuple of numpy arrays.\"\"\" digits = load_digits () # flatten the images n_samples = len ( digits . images ) data = digits . images . reshape (( n_samples , - 1 )) # Split data into 50% train and 50% test subsets X_train , X_test , y_train , y_test = train_test_split ( data , digits . target , test_size = 0.5 , shuffle = False ) return X_train , X_test , y_train , y_test","title":"get_digits()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.helpers.digits.get_digits_model","text":"Creates a support vector classifier for digits dataset. Source code in zenml/integrations/sklearn/helpers/digits.py def get_digits_model () -> ClassifierMixin : \"\"\"Creates a support vector classifier for digits dataset.\"\"\" return SVC ( gamma = 0.001 )","title":"get_digits_model()"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers.sklearn_materializer","text":"","title":"sklearn_materializer"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.materializers.sklearn_materializer.SklearnMaterializer","text":"Materializer to read data to and from sklearn. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py class SklearnMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from sklearn.\"\"\" ASSOCIATED_TYPES = [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid ) handle_input ( self , data_type ) Reads a base sklearn model from a pickle file. Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_input ( self , data_type : Type [ Any ] ) -> Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ]: \"\"\"Reads a base sklearn model from a pickle file.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"rb\" ) as fid : clf = pickle . load ( fid ) return clf handle_return ( self , clf ) Creates a pickle for a sklearn model. Parameters: Name Type Description Default clf Union[sklearn.base.BaseEstimator, sklearn.base.ClassifierMixin, sklearn.base.ClusterMixin, sklearn.base.BiclusterMixin, sklearn.base.OutlierMixin, sklearn.base.RegressorMixin, sklearn.base.MetaEstimatorMixin, sklearn.base.MultiOutputMixin, sklearn.base.DensityMixin, sklearn.base.TransformerMixin] A sklearn model. required Source code in zenml/integrations/sklearn/materializers/sklearn_materializer.py def handle_return ( self , clf : Union [ BaseEstimator , ClassifierMixin , ClusterMixin , BiclusterMixin , OutlierMixin , RegressorMixin , MetaEstimatorMixin , MultiOutputMixin , DensityMixin , TransformerMixin , ], ) -> None : \"\"\"Creates a pickle for a sklearn model. Args: clf: A sklearn model. \"\"\" super () . handle_return ( clf ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) with fileio . open ( filepath , \"wb\" ) as fid : pickle . dump ( clf , fid )","title":"SklearnMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator","text":"","title":"sklearn_evaluator"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator.SklearnEvaluator","text":"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluator ( BaseEvaluatorStep ): \"\"\"A simple step implementation which utilizes sklearn to evaluate the performance of a given model on a given test dataset\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return] CONFIG_CLASS ( BaseEvaluatorConfig ) pydantic-model Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str entrypoint ( self , dataset , model , config ) Method which is responsible for the computation of the evaluation Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which represents the test dataset required model Model a trained tensorflow Keras model required config SklearnEvaluatorConfig the configuration for the step required Returns: Type Description dict a dictionary which has the evaluation report Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , model : tf . keras . Model , config : SklearnEvaluatorConfig , ) -> dict : # type: ignore[type-arg] \"\"\"Method which is responsible for the computation of the evaluation Args: dataset: a pandas Dataframe which represents the test dataset model: a trained tensorflow Keras model config: the configuration for the step Returns: a dictionary which has the evaluation report \"\"\" labels = dataset . pop ( config . label_class_column ) predictions = model . predict ( dataset ) predicted_classes = [ 1 if v > 0.5 else 0 for v in predictions ] report = classification_report ( labels , predicted_classes , output_dict = True ) return report # type: ignore[no-any-return]","title":"SklearnEvaluator"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_evaluator.SklearnEvaluatorConfig","text":"Config class for the sklearn evaluator Source code in zenml/integrations/sklearn/steps/sklearn_evaluator.py class SklearnEvaluatorConfig ( BaseEvaluatorConfig ): \"\"\"Config class for the sklearn evaluator\"\"\" label_class_column : str","title":"SklearnEvaluatorConfig"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter","text":"","title":"sklearn_splitter"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter.SklearnSplitter","text":"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitter ( BaseSplitStep ): \"\"\"A simple step implementation which utilizes sklearn to split a given dataset into train, test and validation splits\"\"\" def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset CONFIG_CLASS ( BaseSplitStepConfig ) pydantic-model Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ] entrypoint ( self , dataset , config ) Method which is responsible for the splitting logic Parameters: Name Type Description Default dataset DataFrame a pandas Dataframe which entire dataset required config SklearnSplitterConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc3c05130> three dataframes representing the splits Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : SklearnSplitterConfig , ) -> Output ( # type:ignore[valid-type] train = pd . DataFrame , test = pd . DataFrame , validation = pd . DataFrame ): \"\"\"Method which is responsible for the splitting logic Args: dataset: a pandas Dataframe which entire dataset config: the configuration for the step Returns: three dataframes representing the splits \"\"\" if ( any ( [ split not in config . ratios for split in [ \"train\" , \"test\" , \"validation\" ] ] ) or len ( config . ratios ) != 3 ): raise KeyError ( f \"Make sure that you only use 'train', 'test' and \" f \"'validation' as keys in the ratios dict. Current keys: \" f \" { config . ratios . keys () } \" ) if sum ( config . ratios . values ()) != 1 : raise ValueError ( f \"Make sure that the ratios sum up to 1. Current \" f \"ratios: { config . ratios } \" ) train_dataset , test_dataset = train_test_split ( dataset , test_size = config . ratios [ \"test\" ] ) train_dataset , val_dataset = train_test_split ( train_dataset , test_size = ( config . ratios [ \"validation\" ] / ( config . ratios [ \"validation\" ] + config . ratios [ \"train\" ]) ), ) return train_dataset , test_dataset , val_dataset","title":"SklearnSplitter"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_splitter.SklearnSplitterConfig","text":"Config class for the sklearn splitter Source code in zenml/integrations/sklearn/steps/sklearn_splitter.py class SklearnSplitterConfig ( BaseSplitStepConfig ): \"\"\"Config class for the sklearn splitter\"\"\" ratios : Dict [ str , float ]","title":"SklearnSplitterConfig"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler","text":"","title":"sklearn_standard_scaler"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler.SklearnStandardScaler","text":"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScaler ( BasePreprocessorStep ): \"\"\"Simple step implementation which utilizes the StandardScaler from sklearn to transform the numeric columns of a pd.DataFrame\"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset CONFIG_CLASS ( BasePreprocessorConfig ) pydantic-model Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = [] entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config ) Main entrypoint function for the StandardScaler Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required test_dataset DataFrame pd.DataFrame, the test dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required statistics DataFrame pd.DataFrame, the statistics over the train dataset required schema DataFrame pd.DataFrame, the detected schema of the dataset required config SklearnStandardScalerConfig the configuration for the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc3c05dc0> the transformed train, test and validation datasets as pd.DataFrames Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , test_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , statistics : pd . DataFrame , schema : pd . DataFrame , config : SklearnStandardScalerConfig , ) -> Output ( # type:ignore[valid-type] train_transformed = pd . DataFrame , test_transformed = pd . DataFrame , validation_transformed = pd . DataFrame , ): \"\"\"Main entrypoint function for the StandardScaler Args: train_dataset: pd.DataFrame, the training dataset test_dataset: pd.DataFrame, the test dataset validation_dataset: pd.DataFrame, the validation dataset statistics: pd.DataFrame, the statistics over the train dataset schema: pd.DataFrame, the detected schema of the dataset config: the configuration for the step Returns: the transformed train, test and validation datasets as pd.DataFrames \"\"\" schema_dict = { k : v [ 0 ] for k , v in schema . to_dict () . items ()} # Exclude columns feature_set = set ( train_dataset . columns ) - set ( config . exclude_columns ) for feature , feature_type in schema_dict . items (): if feature_type != \"int64\" and feature_type != \"float64\" : feature_set . remove ( feature ) logger . warning ( f \" { feature } column is a not numeric, thus it is excluded \" f \"from the standard scaling.\" ) transform_feature_set = feature_set - set ( config . ignore_columns ) # Transform the datasets scaler = StandardScaler () scaler . mean_ = statistics [ \"mean\" ][ transform_feature_set ] scaler . scale_ = statistics [ \"std\" ][ transform_feature_set ] train_dataset [ list ( transform_feature_set )] = scaler . transform ( train_dataset [ transform_feature_set ] ) test_dataset [ list ( transform_feature_set )] = scaler . transform ( test_dataset [ transform_feature_set ] ) validation_dataset [ list ( transform_feature_set )] = scaler . transform ( validation_dataset [ transform_feature_set ] ) return train_dataset , test_dataset , validation_dataset","title":"SklearnStandardScaler"},{"location":"api_docs/integrations/#zenml.integrations.sklearn.steps.sklearn_standard_scaler.SklearnStandardScalerConfig","text":"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset Source code in zenml/integrations/sklearn/steps/sklearn_standard_scaler.py class SklearnStandardScalerConfig ( BasePreprocessorConfig ): \"\"\"Config class for the sklearn standard scaler ignore_columns: a list of column names which should not be scaled exclude_columns: a list of column names to be excluded from the dataset \"\"\" ignore_columns : List [ str ] = [] exclude_columns : List [ str ] = []","title":"SklearnStandardScalerConfig"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow","text":"","title":"tensorflow"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.TensorflowIntegration","text":"Definition of Tensorflow integration for ZenML. Source code in zenml/integrations/tensorflow/__init__.py class TensorflowIntegration ( Integration ): \"\"\"Definition of Tensorflow integration for ZenML.\"\"\" NAME = TENSORFLOW REQUIREMENTS = [ \"tensorflow\" ] @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa","title":"TensorflowIntegration"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.TensorflowIntegration.activate","text":"Activates the integration. Source code in zenml/integrations/tensorflow/__init__.py @classmethod def activate ( cls ) -> None : \"\"\"Activates the integration.\"\"\" from zenml.integrations.tensorflow import materializers # noqa","title":"activate()"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers","text":"","title":"materializers"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.keras_materializer","text":"","title":"keras_materializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.keras_materializer.KerasMaterializer","text":"Materializer to read/write Keras models. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py class KerasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read/write Keras models.\"\"\" ASSOCIATED_TYPES = [ keras . Model ] ASSOCIATED_ARTIFACT_TYPES = [ ModelArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri ) handle_input ( self , data_type ) Reads and returns a Keras model. Returns: Type Description Model A tf.keras.Model model. Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> keras . Model : \"\"\"Reads and returns a Keras model. Returns: A tf.keras.Model model. \"\"\" super () . handle_input ( data_type ) return keras . models . load_model ( self . artifact . uri ) handle_return ( self , model ) Writes a keras model. Parameters: Name Type Description Default model Model A tf.keras.Model model. required Source code in zenml/integrations/tensorflow/materializers/keras_materializer.py def handle_return ( self , model : keras . Model ) -> None : \"\"\"Writes a keras model. Args: model: A tf.keras.Model model. \"\"\" super () . handle_return ( model ) model . save ( self . artifact . uri )","title":"KerasMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.tf_dataset_materializer","text":"","title":"tf_dataset_materializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.materializers.tf_dataset_materializer.TensorflowDatasetMaterializer","text":"Materializer to read data to and from tf.data.Dataset. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py class TensorflowDatasetMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from tf.data.Dataset.\"\"\" ASSOCIATED_TYPES = [ tf . data . Dataset ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None ) handle_input ( self , data_type ) Reads data into tf.data.Dataset Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads data into tf.data.Dataset\"\"\" super () . handle_input ( data_type ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) return tf . data . experimental . load ( path ) handle_return ( self , dataset ) Persists a tf.data.Dataset object. Source code in zenml/integrations/tensorflow/materializers/tf_dataset_materializer.py def handle_return ( self , dataset : tf . data . Dataset ) -> None : \"\"\"Persists a tf.data.Dataset object.\"\"\" super () . handle_return ( dataset ) path = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) tf . data . experimental . save ( dataset , path , compression = None , shard_func = None )","title":"TensorflowDatasetMaterializer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps","text":"","title":"steps"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer","text":"","title":"tensorflow_trainer"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer.TensorflowBinaryClassifier","text":"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifier ( BaseTrainerStep ): \"\"\"Simple step implementation which creates a simple tensorflow feedforward neural network and trains it on a given pd.DataFrame dataset \"\"\" def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model CONFIG_CLASS ( BaseTrainerConfig ) pydantic-model Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8 entrypoint ( self , train_dataset , validation_dataset , config ) Main entrypoint for the tensorflow trainer Parameters: Name Type Description Default train_dataset DataFrame pd.DataFrame, the training dataset required validation_dataset DataFrame pd.DataFrame, the validation dataset required config TensorflowBinaryClassifierConfig the configuration of the step required Returns: Type Description Model the trained tf.keras.Model Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py def entrypoint ( # type: ignore[override] self , train_dataset : pd . DataFrame , validation_dataset : pd . DataFrame , config : TensorflowBinaryClassifierConfig , ) -> tf . keras . Model : \"\"\"Main entrypoint for the tensorflow trainer Args: train_dataset: pd.DataFrame, the training dataset validation_dataset: pd.DataFrame, the validation dataset config: the configuration of the step Returns: the trained tf.keras.Model \"\"\" model = tf . keras . Sequential () model . add ( tf . keras . layers . InputLayer ( input_shape = config . input_shape )) model . add ( tf . keras . layers . Flatten ()) last_layer = config . layers . pop () for i , layer in enumerate ( config . layers ): model . add ( tf . keras . layers . Dense ( layer , activation = \"relu\" )) model . add ( tf . keras . layers . Dense ( last_layer , activation = \"sigmoid\" )) model . compile ( optimizer = tf . keras . optimizers . Adam ( config . learning_rate ), loss = tf . keras . losses . BinaryCrossentropy (), metrics = config . metrics , ) train_target = train_dataset . pop ( config . target_column ) validation_target = validation_dataset . pop ( config . target_column ) model . fit ( x = train_dataset , y = train_target , validation_data = ( validation_dataset , validation_target ), batch_size = config . batch_size , epochs = config . epochs , ) model . summary () return model","title":"TensorflowBinaryClassifier"},{"location":"api_docs/integrations/#zenml.integrations.tensorflow.steps.tensorflow_trainer.TensorflowBinaryClassifierConfig","text":"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch Source code in zenml/integrations/tensorflow/steps/tensorflow_trainer.py class TensorflowBinaryClassifierConfig ( BaseTrainerConfig ): \"\"\"Config class for the tensorflow trainer target_column: the name of the label column layers: the number of units in the fully connected layers input_shape: the shape of the input learning_rate: the learning rate metrics: the list of metrics to be computed epochs: the number of epochs batch_size: the size of the batch \"\"\" target_column : str layers : List [ int ] = [ 256 , 64 , 1 ] input_shape : Tuple [ int ] = ( 8 ,) learning_rate : float = 0.001 metrics : List [ str ] = [ \"accuracy\" ] epochs : int = 50 batch_size : int = 8","title":"TensorflowBinaryClassifierConfig"},{"location":"api_docs/integrations/#zenml.integrations.utils","text":"","title":"utils"},{"location":"api_docs/integrations/#zenml.integrations.utils.get_integration_for_module","text":"Gets the integration class for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return None . If it is part of a ZenML integration, it will return the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_integration_for_module ( module_name : str ) -> Optional [ Type [ Integration ]]: \"\"\"Gets the integration class for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return `None`. If it is part of a ZenML integration, it will return the integration class found inside the integration __init__ file. \"\"\" integration_prefix = \"zenml.integrations.\" if not module_name . startswith ( integration_prefix ): return None integration_module_name = \".\" . join ( module_name . split ( \".\" , 3 )[: 3 ]) try : integration_module = sys . modules [ integration_module_name ] except KeyError : integration_module = importlib . import_module ( integration_module_name ) for name , member in inspect . getmembers ( integration_module ): if ( member is not Integration and isinstance ( member , IntegrationMeta ) and issubclass ( member , Integration ) ): return cast ( Type [ Integration ], member ) return None","title":"get_integration_for_module()"},{"location":"api_docs/integrations/#zenml.integrations.utils.get_requirements_for_module","text":"Gets requirements for a module inside an integration. If the module given by module_name is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration init file. Source code in zenml/integrations/utils.py def get_requirements_for_module ( module_name : str ) -> List [ str ]: \"\"\"Gets requirements for a module inside an integration. If the module given by `module_name` is not part of a ZenML integration, this method will return an empty list. If it is part of a ZenML integration, it will return the list of requirements specified inside the integration class found inside the integration __init__ file. \"\"\" integration = get_integration_for_module ( module_name ) return integration . REQUIREMENTS if integration else []","title":"get_requirements_for_module()"},{"location":"api_docs/io/","text":"Io zenml.io special The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx . fileio append_file ( file_path , file_contents ) Appends file_contents to file. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. required Source code in zenml/io/fileio.py def append_file ( file_path : str , file_contents : str ) -> None : \"\"\"Appends file_contents to file. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # with file_io.FileIO(file_path, mode='a') as f: # f.write(file_contents) raise NotImplementedError convert_to_str ( path ) Converts a PathType to a str using UTF-8. Source code in zenml/io/fileio.py def convert_to_str ( path : PathType ) -> str : \"\"\"Converts a PathType to a str using UTF-8.\"\"\" if isinstance ( path , str ): return path else : return path . decode ( \"utf-8\" ) copy ( src , dst , overwrite = False ) Copy a file from the source to the destination. Source code in zenml/io/fileio.py def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file from the source to the destination.\"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . copy ( src , dst , overwrite = overwrite ) else : if not overwrite and file_exists ( dst ): raise FileExistsError ( f \"Destination file ' { convert_to_str ( dst ) } ' already exists \" f \"and `overwrite` is false.\" ) contents = open ( src , mode = \"rb\" ) . read () open ( dst , mode = \"wb\" ) . write ( contents ) copy_dir ( source_dir , destination_dir , overwrite = False ) Copies dir from source to destination. Parameters: Name Type Description Default source_dir str Path to copy from. required destination_dir str Path to copy to. required overwrite bool Boolean. If false, function throws an error before overwrite. False Source code in zenml/io/fileio.py def copy_dir ( source_dir : str , destination_dir : str , overwrite : bool = False ) -> None : \"\"\"Copies dir from source to destination. Args: source_dir: Path to copy from. destination_dir: Path to copy to. overwrite: Boolean. If false, function throws an error before overwrite. \"\"\" for source_file in list_dir ( source_dir ): source_file_path = Path ( source_file ) destination_name = os . path . join ( destination_dir , source_file_path . name ) if is_dir ( source_file ): copy_dir ( source_file , destination_name , overwrite ) else : create_dir_recursive_if_not_exists ( str ( Path ( destination_name ) . parent ) ) copy ( str ( source_file_path ), str ( destination_name ), overwrite ) create_dir_if_not_exists ( dir_path ) Creates directory if it does not exist. Parameters: Name Type Description Default dir_path(str) Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory if it does not exist. Args: dir_path(str): Local path in filesystem. \"\"\" if not is_dir ( dir_path ): mkdir ( dir_path ) create_dir_recursive_if_not_exists ( dir_path ) Creates directory recursively if it does not exist. Parameters: Name Type Description Default dir_path str Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_recursive_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory recursively if it does not exist. Args: dir_path: Local path in filesystem. \"\"\" if not is_dir ( dir_path ): make_dirs ( dir_path ) create_file_if_not_exists ( file_path , file_contents = ' {} ' ) Creates file if it does not exist. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. '{}' Source code in zenml/io/fileio.py def create_file_if_not_exists ( file_path : str , file_contents : str = \" {} \" ) -> None : \"\"\"Creates file if it does not exist. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # if not fileio.exists(file_path): # fileio.(file_path, file_contents) full_path = Path ( file_path ) create_dir_recursive_if_not_exists ( str ( full_path . parent )) with open ( str ( full_path ), \"w\" ) as f : f . write ( file_contents ) file_exists ( path ) Returns True if the given path exists. Source code in zenml/io/fileio.py def file_exists ( path : PathType ) -> bool : \"\"\"Returns `True` if the given path exists.\"\"\" return _get_filesystem ( path ) . exists ( path ) find_files ( dir_path , pattern ) Find files in a directory that match pattern. Parameters: Name Type Description Default dir_path Union[bytes, str] Path to directory. required pattern str pattern like *.png. required Yields: Type Description Iterable[str] All matching filenames if found, else None. Source code in zenml/io/fileio.py def find_files ( dir_path : PathType , pattern : str ) -> Iterable [ str ]: # TODO [ENG-189]: correct docstring since 'None' is never returned \"\"\"Find files in a directory that match pattern. Args: dir_path: Path to directory. pattern: pattern like *.png. Yields: All matching filenames if found, else None. \"\"\" for root , dirs , files in walk ( dir_path ): for basename in files : if fnmatch . fnmatch ( convert_to_str ( basename ), pattern ): filename = os . path . join ( convert_to_str ( root ), convert_to_str ( basename ) ) yield filename get_grandparent ( dir_path ) Get grandparent of dir. Parameters: Name Type Description Default dir_path str Path to directory. required Returns: Type Description str The input paths parents parent. Source code in zenml/io/fileio.py def get_grandparent ( dir_path : str ) -> str : \"\"\"Get grandparent of dir. Args: dir_path: Path to directory. Returns: The input paths parents parent. \"\"\" return Path ( dir_path ) . parent . parent . stem get_parent ( dir_path ) Get parent of dir. Parameters: Name Type Description Default dir_path(str) Path to directory. required Returns: Type Description str Parent (stem) of the dir as a string. Source code in zenml/io/fileio.py def get_parent ( dir_path : str ) -> str : \"\"\"Get parent of dir. Args: dir_path(str): Path to directory. Returns: Parent (stem) of the dir as a string. \"\"\" return Path ( dir_path ) . parent . stem glob ( pattern ) Return the paths that match a glob pattern. Source code in zenml/io/fileio.py def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return the paths that match a glob pattern.\"\"\" return _get_filesystem ( pattern ) . glob ( pattern ) is_dir ( path ) Returns whether the given path points to a directory. Source code in zenml/io/fileio.py def is_dir ( path : PathType ) -> bool : \"\"\"Returns whether the given path points to a directory.\"\"\" return _get_filesystem ( path ) . isdir ( path ) is_remote ( path ) Returns True if path exists remotely. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if remote path, else False. Source code in zenml/io/fileio.py def is_remote ( path : str ) -> bool : \"\"\"Returns True if path exists remotely. Args: path: Any path as a string. Returns: True if remote path, else False. \"\"\" return any ( path . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX ) is_root ( path ) Returns true if path has no parent in local filesystem. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description bool True if root, else False. Source code in zenml/io/fileio.py def is_root ( path : str ) -> bool : \"\"\"Returns true if path has no parent in local filesystem. Args: path: Local path in filesystem. Returns: True if root, else False. \"\"\" return Path ( path ) . parent == Path ( path ) list_dir ( dir_path , only_file_names = False ) Returns a list of files under dir. Parameters: Name Type Description Default dir_path str Path in filesystem. required only_file_names bool Returns only file names if True. False Returns: Type Description List[str] List of full qualified paths. Source code in zenml/io/fileio.py def list_dir ( dir_path : str , only_file_names : bool = False ) -> List [ str ]: \"\"\"Returns a list of files under dir. Args: dir_path: Path in filesystem. only_file_names: Returns only file names if True. Returns: List of full qualified paths. \"\"\" try : return [ os . path . join ( dir_path , convert_to_str ( f )) if not only_file_names else convert_to_str ( f ) for f in _get_filesystem ( dir_path ) . listdir ( dir_path ) ] except IOError : logger . debug ( f \"Dir { dir_path } not found.\" ) return [] make_dirs ( path ) Make a directory at the given path, recursively creating parents. Source code in zenml/io/fileio.py def make_dirs ( path : PathType ) -> None : \"\"\"Make a directory at the given path, recursively creating parents.\"\"\" _get_filesystem ( path ) . makedirs ( path ) mkdir ( path ) Make a directory at the given path; parent directory must exist. Source code in zenml/io/fileio.py def mkdir ( path : PathType ) -> None : \"\"\"Make a directory at the given path; parent directory must exist.\"\"\" _get_filesystem ( path ) . mkdir ( path ) move ( source , destination , overwrite = False ) Moves dir or file from source to destination. Can be used to rename. Parameters: Name Type Description Default source str Local path to copy from. required destination str Local path to copy to. required overwrite bool boolean, if false, then throws an error before overwrite. False Source code in zenml/io/fileio.py def move ( source : str , destination : str , overwrite : bool = False ) -> None : \"\"\"Moves dir or file from source to destination. Can be used to rename. Args: source: Local path to copy from. destination: Local path to copy to. overwrite: boolean, if false, then throws an error before overwrite. \"\"\" rename ( source , destination , overwrite ) open ( path , mode = 'r' ) Open a file at the given path. Source code in zenml/io/fileio.py def open ( path : PathType , mode : str = \"r\" ) -> Any : # noqa \"\"\"Open a file at the given path.\"\"\" return _get_filesystem ( path ) . open ( path , mode = mode ) remove ( path ) Remove the file at the given path. Dangerous operation. Source code in zenml/io/fileio.py def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path. Dangerous operation.\"\"\" if not file_exists ( path ): raise FileNotFoundError ( f \" { convert_to_str ( path ) } does not exist!\" ) _get_filesystem ( path ) . remove ( path ) rename ( src , dst , overwrite = False ) Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/io/fileio.py def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . rename ( src , dst , overwrite = overwrite ) else : raise NotImplementedError ( f \"Renaming from { convert_to_str ( src ) } to { convert_to_str ( dst ) } \" f \"using different filesystems plugins is currently not supported.\" ) resolve_relative_path ( path ) Takes relative path and resolves it absolutely. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description str Resolved path. Source code in zenml/io/fileio.py def resolve_relative_path ( path : str ) -> str : \"\"\"Takes relative path and resolves it absolutely. Args: path: Local path in filesystem. Returns: Resolved path. \"\"\" if is_remote ( path ): return path return str ( Path ( path ) . resolve ()) rm_dir ( dir_path ) Deletes dir recursively. Dangerous operation. Parameters: Name Type Description Default dir_path str Dir to delete. required Source code in zenml/io/fileio.py def rm_dir ( dir_path : str ) -> None : \"\"\"Deletes dir recursively. Dangerous operation. Args: dir_path: Dir to delete. \"\"\" _get_filesystem ( dir_path ) . rmtree ( dir_path ) stat ( path ) Return the stat descriptor for a given file path. Source code in zenml/io/fileio.py def stat ( path : PathType ) -> Any : \"\"\"Return the stat descriptor for a given file path.\"\"\" return _get_filesystem ( path ) . stat ( path ) walk ( top , topdown = True , onerror = None ) Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Whether to walk directories topdown or bottom-up. True onerror Optional[Callable[..., NoneType]] Callable that gets called if an error occurs. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/io/fileio.py def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Whether to walk directories topdown or bottom-up. onerror: Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" return _get_filesystem ( top ) . walk ( top , topdown = topdown , onerror = onerror ) fileio_registry Filesystem registry managing filesystem plugins. FileIORegistry Registry of pluggable filesystem implementations used in TFX components. Source code in zenml/io/fileio_registry.py class FileIORegistry : \"\"\"Registry of pluggable filesystem implementations used in TFX components.\"\"\" def __init__ ( self ) -> None : self . _filesystems : Dict [ PathType , Type [ Filesystem ]] = {} self . _registration_lock = threading . Lock () def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme ) get_filesystem_for_path ( self , path ) Get filesystem plugin for given path. Source code in zenml/io/fileio_registry.py def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme ) get_filesystem_for_scheme ( self , scheme ) Get filesystem plugin for given scheme string. Source code in zenml/io/fileio_registry.py def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] register ( self , filesystem_cls ) Register a filesystem implementation. Parameters: Name Type Description Default filesystem_cls Type[tfx.dsl.io.filesystem.Filesystem] Subclass of tfx.dsl.io.filesystem.Filesystem . required Source code in zenml/io/fileio_registry.py def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls filesystem FileSystemMeta ( type ) Metaclass which is responsible for registering the defined filesystem in the default fileio registry. Source code in zenml/io/filesystem.py class FileSystemMeta ( type ): \"\"\"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Creates the filesystem class and registers it Source code in zenml/io/filesystem.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls Filesystem ( Filesystem ) Abstract Filesystem class. Source code in zenml/io/filesystem.py class Filesystem ( BaseFileSystem , metaclass = FileSystemMeta ): \"\"\"Abstract Filesystem class.\"\"\" NotFoundError ( OSError ) Auxiliary not found error Source code in zenml/io/filesystem.py class NotFoundError ( IOError ): \"\"\"Auxiliary not found error\"\"\" utils create_tarfile ( source_dir , output_filename = 'zipped.tar.gz' , exclude_function = None ) Create a compressed representation of source_dir. Parameters: Name Type Description Default source_dir str Path to source dir. required output_filename str Name of outputted gz. 'zipped.tar.gz' exclude_function Optional[Callable[[tarfile.TarInfo], Union[tarfile.TarInfo, NoneType]]] Function that determines whether to exclude file. None Source code in zenml/io/utils.py def create_tarfile ( source_dir : str , output_filename : str = \"zipped.tar.gz\" , exclude_function : Optional [ Callable [[ tarfile . TarInfo ], Optional [ tarfile . TarInfo ]] ] = None , ) -> None : \"\"\"Create a compressed representation of source_dir. Args: source_dir: Path to source dir. output_filename: Name of outputted gz. exclude_function: Function that determines whether to exclude file. \"\"\" if exclude_function is None : # default is to exclude the .zenml directory def exclude_function ( tarinfo : tarfile . TarInfo , ) -> Optional [ tarfile . TarInfo ]: \"\"\"Exclude files from tar. Args: tarinfo: Any Returns: tarinfo required for exclude. \"\"\" filename = tarinfo . name if \".zenml/\" in filename or \"venv/\" in filename : return None else : return tarinfo with tarfile . open ( output_filename , \"w:gz\" ) as tar : tar . add ( source_dir , arcname = \"\" , filter = exclude_function ) extract_tarfile ( source_tar , output_dir ) Extracts all files in a compressed tar file to output_dir. Parameters: Name Type Description Default source_tar str Path to a tar compressed file. required output_dir str Directory where to extract. required Source code in zenml/io/utils.py def extract_tarfile ( source_tar : str , output_dir : str ) -> None : \"\"\"Extracts all files in a compressed tar file to output_dir. Args: source_tar: Path to a tar compressed file. output_dir: Directory where to extract. \"\"\" if is_remote ( source_tar ): raise NotImplementedError ( \"Use local tars for now.\" ) with tarfile . open ( source_tar , \"r:gz\" ) as tar : tar . extractall ( output_dir ) get_global_config_directory () Returns the global config directory for ZenML. Source code in zenml/io/utils.py def get_global_config_directory () -> str : \"\"\"Returns the global config directory for ZenML.\"\"\" return click . get_app_dir ( APP_NAME ) get_zenml_config_dir ( path = None ) Recursive function to find the zenml config starting from path. Parameters: Name Type Description Default path Default value = os.getcwd Path to check. None Returns: Type Description str The full path with the resolved zenml directory. Source code in zenml/io/utils.py def get_zenml_config_dir ( path : Optional [ str ] = None ) -> str : \"\"\"Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS. \"\"\" return os . path . join ( get_zenml_dir ( path ), ZENML_DIR_NAME ) get_zenml_dir ( path = None ) Returns path to a ZenML repository directory. Parameters: Name Type Description Default path Optional[str] Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Returns: Type Description str Absolute path to a ZenML repository directory. Exceptions: Type Description InitializationException If no ZenML repository is found. Source code in zenml/io/utils.py def get_zenml_dir ( path : Optional [ str ] = None ) -> str : \"\"\"Returns path to a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable path = os . getenv ( ENV_ZENML_REPOSITORY_PATH , None ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure to \" f \"create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the # current working directory path = os . getcwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { os . getcwd () } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( repo_path : str ) -> str : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if is_zenml_dir ( repo_path ): return repo_path if not search_parent_directories or is_root ( repo_path ): raise InitializationException ( error_message ) return _find_repo_helper ( str ( Path ( repo_path ) . parent )) path = _find_repo_helper ( path ) return str ( Path ( path ) . resolve ()) is_gcs_path ( path ) Returns True if path is on Google Cloud Storage. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if gcs path, else False. Source code in zenml/io/utils.py def is_gcs_path ( path : str ) -> bool : \"\"\"Returns True if path is on Google Cloud Storage. Args: path: Any path as a string. Returns: True if gcs path, else False. \"\"\" return path . startswith ( \"gs://\" ) is_zenml_dir ( path ) Check if dir is a zenml dir or not. Parameters: Name Type Description Default path str Path to the root. required Returns: Type Description bool True if path contains a zenml dir, False if not. Source code in zenml/io/utils.py def is_zenml_dir ( path : str ) -> bool : \"\"\"Check if dir is a zenml dir or not. Args: path: Path to the root. Returns: True if path contains a zenml dir, False if not. \"\"\" config_dir_path = os . path . join ( path , ZENML_DIR_NAME ) return bool ( is_dir ( config_dir_path )) read_file_contents_as_string ( file_path ) Reads contents of file. Parameters: Name Type Description Default file_path str Path to file. required Source code in zenml/io/utils.py def read_file_contents_as_string ( file_path : str ) -> str : \"\"\"Reads contents of file. Args: file_path: Path to file. \"\"\" if not file_exists ( file_path ): raise FileNotFoundError ( f \" { file_path } does not exist!\" ) return open ( file_path ) . read () # type: ignore[no-any-return] write_file_contents_as_string ( file_path , content ) Writes contents of file. Parameters: Name Type Description Default file_path str Path to file. required content str Contents of file. required Source code in zenml/io/utils.py def write_file_contents_as_string ( file_path : str , content : str ) -> None : \"\"\"Writes contents of file. Args: file_path: Path to file. content: Contents of file. \"\"\" with open ( file_path , \"w\" ) as f : f . write ( content )","title":"Io"},{"location":"api_docs/io/#io","text":"","title":"Io"},{"location":"api_docs/io/#zenml.io","text":"The io module handles file operations for the ZenML package. It offers a standard interface for reading, writing and manipulating files and directories. It is heavily influenced and inspired by the io module of tfx .","title":"io"},{"location":"api_docs/io/#zenml.io.fileio","text":"","title":"fileio"},{"location":"api_docs/io/#zenml.io.fileio.append_file","text":"Appends file_contents to file. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. required Source code in zenml/io/fileio.py def append_file ( file_path : str , file_contents : str ) -> None : \"\"\"Appends file_contents to file. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # with file_io.FileIO(file_path, mode='a') as f: # f.write(file_contents) raise NotImplementedError","title":"append_file()"},{"location":"api_docs/io/#zenml.io.fileio.convert_to_str","text":"Converts a PathType to a str using UTF-8. Source code in zenml/io/fileio.py def convert_to_str ( path : PathType ) -> str : \"\"\"Converts a PathType to a str using UTF-8.\"\"\" if isinstance ( path , str ): return path else : return path . decode ( \"utf-8\" )","title":"convert_to_str()"},{"location":"api_docs/io/#zenml.io.fileio.copy","text":"Copy a file from the source to the destination. Source code in zenml/io/fileio.py def copy ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Copy a file from the source to the destination.\"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . copy ( src , dst , overwrite = overwrite ) else : if not overwrite and file_exists ( dst ): raise FileExistsError ( f \"Destination file ' { convert_to_str ( dst ) } ' already exists \" f \"and `overwrite` is false.\" ) contents = open ( src , mode = \"rb\" ) . read () open ( dst , mode = \"wb\" ) . write ( contents )","title":"copy()"},{"location":"api_docs/io/#zenml.io.fileio.copy_dir","text":"Copies dir from source to destination. Parameters: Name Type Description Default source_dir str Path to copy from. required destination_dir str Path to copy to. required overwrite bool Boolean. If false, function throws an error before overwrite. False Source code in zenml/io/fileio.py def copy_dir ( source_dir : str , destination_dir : str , overwrite : bool = False ) -> None : \"\"\"Copies dir from source to destination. Args: source_dir: Path to copy from. destination_dir: Path to copy to. overwrite: Boolean. If false, function throws an error before overwrite. \"\"\" for source_file in list_dir ( source_dir ): source_file_path = Path ( source_file ) destination_name = os . path . join ( destination_dir , source_file_path . name ) if is_dir ( source_file ): copy_dir ( source_file , destination_name , overwrite ) else : create_dir_recursive_if_not_exists ( str ( Path ( destination_name ) . parent ) ) copy ( str ( source_file_path ), str ( destination_name ), overwrite )","title":"copy_dir()"},{"location":"api_docs/io/#zenml.io.fileio.create_dir_if_not_exists","text":"Creates directory if it does not exist. Parameters: Name Type Description Default dir_path(str) Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory if it does not exist. Args: dir_path(str): Local path in filesystem. \"\"\" if not is_dir ( dir_path ): mkdir ( dir_path )","title":"create_dir_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.create_dir_recursive_if_not_exists","text":"Creates directory recursively if it does not exist. Parameters: Name Type Description Default dir_path str Local path in filesystem. required Source code in zenml/io/fileio.py def create_dir_recursive_if_not_exists ( dir_path : str ) -> None : \"\"\"Creates directory recursively if it does not exist. Args: dir_path: Local path in filesystem. \"\"\" if not is_dir ( dir_path ): make_dirs ( dir_path )","title":"create_dir_recursive_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.create_file_if_not_exists","text":"Creates file if it does not exist. Parameters: Name Type Description Default file_path str Local path in filesystem. required file_contents str Contents of file. '{}' Source code in zenml/io/fileio.py def create_file_if_not_exists ( file_path : str , file_contents : str = \" {} \" ) -> None : \"\"\"Creates file if it does not exist. Args: file_path: Local path in filesystem. file_contents: Contents of file. \"\"\" # if not fileio.exists(file_path): # fileio.(file_path, file_contents) full_path = Path ( file_path ) create_dir_recursive_if_not_exists ( str ( full_path . parent )) with open ( str ( full_path ), \"w\" ) as f : f . write ( file_contents )","title":"create_file_if_not_exists()"},{"location":"api_docs/io/#zenml.io.fileio.file_exists","text":"Returns True if the given path exists. Source code in zenml/io/fileio.py def file_exists ( path : PathType ) -> bool : \"\"\"Returns `True` if the given path exists.\"\"\" return _get_filesystem ( path ) . exists ( path )","title":"file_exists()"},{"location":"api_docs/io/#zenml.io.fileio.find_files","text":"Find files in a directory that match pattern. Parameters: Name Type Description Default dir_path Union[bytes, str] Path to directory. required pattern str pattern like *.png. required Yields: Type Description Iterable[str] All matching filenames if found, else None. Source code in zenml/io/fileio.py def find_files ( dir_path : PathType , pattern : str ) -> Iterable [ str ]: # TODO [ENG-189]: correct docstring since 'None' is never returned \"\"\"Find files in a directory that match pattern. Args: dir_path: Path to directory. pattern: pattern like *.png. Yields: All matching filenames if found, else None. \"\"\" for root , dirs , files in walk ( dir_path ): for basename in files : if fnmatch . fnmatch ( convert_to_str ( basename ), pattern ): filename = os . path . join ( convert_to_str ( root ), convert_to_str ( basename ) ) yield filename","title":"find_files()"},{"location":"api_docs/io/#zenml.io.fileio.get_grandparent","text":"Get grandparent of dir. Parameters: Name Type Description Default dir_path str Path to directory. required Returns: Type Description str The input paths parents parent. Source code in zenml/io/fileio.py def get_grandparent ( dir_path : str ) -> str : \"\"\"Get grandparent of dir. Args: dir_path: Path to directory. Returns: The input paths parents parent. \"\"\" return Path ( dir_path ) . parent . parent . stem","title":"get_grandparent()"},{"location":"api_docs/io/#zenml.io.fileio.get_parent","text":"Get parent of dir. Parameters: Name Type Description Default dir_path(str) Path to directory. required Returns: Type Description str Parent (stem) of the dir as a string. Source code in zenml/io/fileio.py def get_parent ( dir_path : str ) -> str : \"\"\"Get parent of dir. Args: dir_path(str): Path to directory. Returns: Parent (stem) of the dir as a string. \"\"\" return Path ( dir_path ) . parent . stem","title":"get_parent()"},{"location":"api_docs/io/#zenml.io.fileio.glob","text":"Return the paths that match a glob pattern. Source code in zenml/io/fileio.py def glob ( pattern : PathType ) -> List [ PathType ]: \"\"\"Return the paths that match a glob pattern.\"\"\" return _get_filesystem ( pattern ) . glob ( pattern )","title":"glob()"},{"location":"api_docs/io/#zenml.io.fileio.is_dir","text":"Returns whether the given path points to a directory. Source code in zenml/io/fileio.py def is_dir ( path : PathType ) -> bool : \"\"\"Returns whether the given path points to a directory.\"\"\" return _get_filesystem ( path ) . isdir ( path )","title":"is_dir()"},{"location":"api_docs/io/#zenml.io.fileio.is_remote","text":"Returns True if path exists remotely. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if remote path, else False. Source code in zenml/io/fileio.py def is_remote ( path : str ) -> bool : \"\"\"Returns True if path exists remotely. Args: path: Any path as a string. Returns: True if remote path, else False. \"\"\" return any ( path . startswith ( prefix ) for prefix in REMOTE_FS_PREFIX )","title":"is_remote()"},{"location":"api_docs/io/#zenml.io.fileio.is_root","text":"Returns true if path has no parent in local filesystem. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description bool True if root, else False. Source code in zenml/io/fileio.py def is_root ( path : str ) -> bool : \"\"\"Returns true if path has no parent in local filesystem. Args: path: Local path in filesystem. Returns: True if root, else False. \"\"\" return Path ( path ) . parent == Path ( path )","title":"is_root()"},{"location":"api_docs/io/#zenml.io.fileio.list_dir","text":"Returns a list of files under dir. Parameters: Name Type Description Default dir_path str Path in filesystem. required only_file_names bool Returns only file names if True. False Returns: Type Description List[str] List of full qualified paths. Source code in zenml/io/fileio.py def list_dir ( dir_path : str , only_file_names : bool = False ) -> List [ str ]: \"\"\"Returns a list of files under dir. Args: dir_path: Path in filesystem. only_file_names: Returns only file names if True. Returns: List of full qualified paths. \"\"\" try : return [ os . path . join ( dir_path , convert_to_str ( f )) if not only_file_names else convert_to_str ( f ) for f in _get_filesystem ( dir_path ) . listdir ( dir_path ) ] except IOError : logger . debug ( f \"Dir { dir_path } not found.\" ) return []","title":"list_dir()"},{"location":"api_docs/io/#zenml.io.fileio.make_dirs","text":"Make a directory at the given path, recursively creating parents. Source code in zenml/io/fileio.py def make_dirs ( path : PathType ) -> None : \"\"\"Make a directory at the given path, recursively creating parents.\"\"\" _get_filesystem ( path ) . makedirs ( path )","title":"make_dirs()"},{"location":"api_docs/io/#zenml.io.fileio.mkdir","text":"Make a directory at the given path; parent directory must exist. Source code in zenml/io/fileio.py def mkdir ( path : PathType ) -> None : \"\"\"Make a directory at the given path; parent directory must exist.\"\"\" _get_filesystem ( path ) . mkdir ( path )","title":"mkdir()"},{"location":"api_docs/io/#zenml.io.fileio.move","text":"Moves dir or file from source to destination. Can be used to rename. Parameters: Name Type Description Default source str Local path to copy from. required destination str Local path to copy to. required overwrite bool boolean, if false, then throws an error before overwrite. False Source code in zenml/io/fileio.py def move ( source : str , destination : str , overwrite : bool = False ) -> None : \"\"\"Moves dir or file from source to destination. Can be used to rename. Args: source: Local path to copy from. destination: Local path to copy to. overwrite: boolean, if false, then throws an error before overwrite. \"\"\" rename ( source , destination , overwrite )","title":"move()"},{"location":"api_docs/io/#zenml.io.fileio.open","text":"Open a file at the given path. Source code in zenml/io/fileio.py def open ( path : PathType , mode : str = \"r\" ) -> Any : # noqa \"\"\"Open a file at the given path.\"\"\" return _get_filesystem ( path ) . open ( path , mode = mode )","title":"open()"},{"location":"api_docs/io/#zenml.io.fileio.remove","text":"Remove the file at the given path. Dangerous operation. Source code in zenml/io/fileio.py def remove ( path : PathType ) -> None : \"\"\"Remove the file at the given path. Dangerous operation.\"\"\" if not file_exists ( path ): raise FileNotFoundError ( f \" { convert_to_str ( path ) } does not exist!\" ) _get_filesystem ( path ) . remove ( path )","title":"remove()"},{"location":"api_docs/io/#zenml.io.fileio.rename","text":"Rename source file to destination file. Parameters: Name Type Description Default src Union[bytes, str] The path of the file to rename. required dst Union[bytes, str] The path to rename the source file to. required overwrite bool If a file already exists at the destination, this method will overwrite it if overwrite= True and raise a FileExistsError otherwise. False Exceptions: Type Description FileExistsError If a file already exists at the destination and overwrite is not set to True . Source code in zenml/io/fileio.py def rename ( src : PathType , dst : PathType , overwrite : bool = False ) -> None : \"\"\"Rename source file to destination file. Args: src: The path of the file to rename. dst: The path to rename the source file to. overwrite: If a file already exists at the destination, this method will overwrite it if overwrite=`True` and raise a FileExistsError otherwise. Raises: FileExistsError: If a file already exists at the destination and overwrite is not set to `True`. \"\"\" src_fs = _get_filesystem ( src ) dst_fs = _get_filesystem ( dst ) if src_fs is dst_fs : src_fs . rename ( src , dst , overwrite = overwrite ) else : raise NotImplementedError ( f \"Renaming from { convert_to_str ( src ) } to { convert_to_str ( dst ) } \" f \"using different filesystems plugins is currently not supported.\" )","title":"rename()"},{"location":"api_docs/io/#zenml.io.fileio.resolve_relative_path","text":"Takes relative path and resolves it absolutely. Parameters: Name Type Description Default path str Local path in filesystem. required Returns: Type Description str Resolved path. Source code in zenml/io/fileio.py def resolve_relative_path ( path : str ) -> str : \"\"\"Takes relative path and resolves it absolutely. Args: path: Local path in filesystem. Returns: Resolved path. \"\"\" if is_remote ( path ): return path return str ( Path ( path ) . resolve ())","title":"resolve_relative_path()"},{"location":"api_docs/io/#zenml.io.fileio.rm_dir","text":"Deletes dir recursively. Dangerous operation. Parameters: Name Type Description Default dir_path str Dir to delete. required Source code in zenml/io/fileio.py def rm_dir ( dir_path : str ) -> None : \"\"\"Deletes dir recursively. Dangerous operation. Args: dir_path: Dir to delete. \"\"\" _get_filesystem ( dir_path ) . rmtree ( dir_path )","title":"rm_dir()"},{"location":"api_docs/io/#zenml.io.fileio.stat","text":"Return the stat descriptor for a given file path. Source code in zenml/io/fileio.py def stat ( path : PathType ) -> Any : \"\"\"Return the stat descriptor for a given file path.\"\"\" return _get_filesystem ( path ) . stat ( path )","title":"stat()"},{"location":"api_docs/io/#zenml.io.fileio.walk","text":"Return an iterator that walks the contents of the given directory. Parameters: Name Type Description Default top Union[bytes, str] Path of directory to walk. required topdown bool Whether to walk directories topdown or bottom-up. True onerror Optional[Callable[..., NoneType]] Callable that gets called if an error occurs. None Returns: Type Description Iterable[Tuple[Union[bytes, str], List[Union[bytes, str]], List[Union[bytes, str]]]] An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. Source code in zenml/io/fileio.py def walk ( top : PathType , topdown : bool = True , onerror : Optional [ Callable [ ... , None ]] = None , ) -> Iterable [ Tuple [ PathType , List [ PathType ], List [ PathType ]]]: \"\"\"Return an iterator that walks the contents of the given directory. Args: top: Path of directory to walk. topdown: Whether to walk directories topdown or bottom-up. onerror: Callable that gets called if an error occurs. Returns: An Iterable of Tuples, each of which contain the path of the current directory path, a list of directories inside the current directory and a list of files inside the current directory. \"\"\" return _get_filesystem ( top ) . walk ( top , topdown = topdown , onerror = onerror )","title":"walk()"},{"location":"api_docs/io/#zenml.io.fileio_registry","text":"Filesystem registry managing filesystem plugins.","title":"fileio_registry"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry","text":"Registry of pluggable filesystem implementations used in TFX components. Source code in zenml/io/fileio_registry.py class FileIORegistry : \"\"\"Registry of pluggable filesystem implementations used in TFX components.\"\"\" def __init__ ( self ) -> None : self . _filesystems : Dict [ PathType , Type [ Filesystem ]] = {} self . _registration_lock = threading . Lock () def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ] def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme )","title":"FileIORegistry"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.get_filesystem_for_path","text":"Get filesystem plugin for given path. Source code in zenml/io/fileio_registry.py def get_filesystem_for_path ( self , path : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given path.\"\"\" # Assume local path by default, but extract filesystem prefix if available. if isinstance ( path , str ): path_bytes = path . encode ( \"utf-8\" ) elif isinstance ( path , bytes ): path_bytes = path else : raise ValueError ( \"Invalid path type: %r .\" % path ) result = re . match ( b \"^([a-z0-9]+://)\" , path_bytes ) if result : scheme = result . group ( 1 ) . decode ( \"utf-8\" ) else : scheme = \"\" return self . get_filesystem_for_scheme ( scheme )","title":"get_filesystem_for_path()"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.get_filesystem_for_scheme","text":"Get filesystem plugin for given scheme string. Source code in zenml/io/fileio_registry.py def get_filesystem_for_scheme ( self , scheme : PathType ) -> Type [ Filesystem ]: \"\"\"Get filesystem plugin for given scheme string.\"\"\" if isinstance ( scheme , bytes ): scheme = scheme . decode ( \"utf-8\" ) if scheme not in self . _filesystems : raise Exception ( f \"No filesystems were found for the scheme: \" f \" { scheme } . Please make sure that you are using \" f \"the right path and the all the necessary \" f \"integrations are properly installed.\" ) return self . _filesystems [ scheme ]","title":"get_filesystem_for_scheme()"},{"location":"api_docs/io/#zenml.io.fileio_registry.FileIORegistry.register","text":"Register a filesystem implementation. Parameters: Name Type Description Default filesystem_cls Type[tfx.dsl.io.filesystem.Filesystem] Subclass of tfx.dsl.io.filesystem.Filesystem . required Source code in zenml/io/fileio_registry.py def register ( self , filesystem_cls : Type [ Filesystem ]) -> None : \"\"\"Register a filesystem implementation. Args: filesystem_cls: Subclass of `tfx.dsl.io.filesystem.Filesystem`. \"\"\" with self . _registration_lock : for scheme in filesystem_cls . SUPPORTED_SCHEMES : current_preferred = self . _filesystems . get ( scheme ) if current_preferred is not None : # TODO: [LOW] Decide what to do here. Do we overwrite, # give out a warning or do we fail? pass self . _filesystems [ scheme ] = filesystem_cls","title":"register()"},{"location":"api_docs/io/#zenml.io.filesystem","text":"","title":"filesystem"},{"location":"api_docs/io/#zenml.io.filesystem.FileSystemMeta","text":"Metaclass which is responsible for registering the defined filesystem in the default fileio registry. Source code in zenml/io/filesystem.py class FileSystemMeta ( type ): \"\"\"Metaclass which is responsible for registering the defined filesystem in the default fileio registry.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls","title":"FileSystemMeta"},{"location":"api_docs/io/#zenml.io.filesystem.FileSystemMeta.__new__","text":"Creates the filesystem class and registers it Source code in zenml/io/filesystem.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"FileSystemMeta\" : \"\"\"Creates the filesystem class and registers it\"\"\" cls = cast ( Type [ \"Filesystem\" ], super () . __new__ ( mcs , name , bases , dct )) if name != \"Filesystem\" : assert cls . SUPPORTED_SCHEMES , ( \"You should specify a list of SUPPORTED_SCHEMES when creating \" \"a filesystem\" ) default_fileio_registry . register ( cls ) return cls","title":"__new__()"},{"location":"api_docs/io/#zenml.io.filesystem.Filesystem","text":"Abstract Filesystem class. Source code in zenml/io/filesystem.py class Filesystem ( BaseFileSystem , metaclass = FileSystemMeta ): \"\"\"Abstract Filesystem class.\"\"\"","title":"Filesystem"},{"location":"api_docs/io/#zenml.io.filesystem.NotFoundError","text":"Auxiliary not found error Source code in zenml/io/filesystem.py class NotFoundError ( IOError ): \"\"\"Auxiliary not found error\"\"\"","title":"NotFoundError"},{"location":"api_docs/io/#zenml.io.utils","text":"","title":"utils"},{"location":"api_docs/io/#zenml.io.utils.create_tarfile","text":"Create a compressed representation of source_dir. Parameters: Name Type Description Default source_dir str Path to source dir. required output_filename str Name of outputted gz. 'zipped.tar.gz' exclude_function Optional[Callable[[tarfile.TarInfo], Union[tarfile.TarInfo, NoneType]]] Function that determines whether to exclude file. None Source code in zenml/io/utils.py def create_tarfile ( source_dir : str , output_filename : str = \"zipped.tar.gz\" , exclude_function : Optional [ Callable [[ tarfile . TarInfo ], Optional [ tarfile . TarInfo ]] ] = None , ) -> None : \"\"\"Create a compressed representation of source_dir. Args: source_dir: Path to source dir. output_filename: Name of outputted gz. exclude_function: Function that determines whether to exclude file. \"\"\" if exclude_function is None : # default is to exclude the .zenml directory def exclude_function ( tarinfo : tarfile . TarInfo , ) -> Optional [ tarfile . TarInfo ]: \"\"\"Exclude files from tar. Args: tarinfo: Any Returns: tarinfo required for exclude. \"\"\" filename = tarinfo . name if \".zenml/\" in filename or \"venv/\" in filename : return None else : return tarinfo with tarfile . open ( output_filename , \"w:gz\" ) as tar : tar . add ( source_dir , arcname = \"\" , filter = exclude_function )","title":"create_tarfile()"},{"location":"api_docs/io/#zenml.io.utils.extract_tarfile","text":"Extracts all files in a compressed tar file to output_dir. Parameters: Name Type Description Default source_tar str Path to a tar compressed file. required output_dir str Directory where to extract. required Source code in zenml/io/utils.py def extract_tarfile ( source_tar : str , output_dir : str ) -> None : \"\"\"Extracts all files in a compressed tar file to output_dir. Args: source_tar: Path to a tar compressed file. output_dir: Directory where to extract. \"\"\" if is_remote ( source_tar ): raise NotImplementedError ( \"Use local tars for now.\" ) with tarfile . open ( source_tar , \"r:gz\" ) as tar : tar . extractall ( output_dir )","title":"extract_tarfile()"},{"location":"api_docs/io/#zenml.io.utils.get_global_config_directory","text":"Returns the global config directory for ZenML. Source code in zenml/io/utils.py def get_global_config_directory () -> str : \"\"\"Returns the global config directory for ZenML.\"\"\" return click . get_app_dir ( APP_NAME )","title":"get_global_config_directory()"},{"location":"api_docs/io/#zenml.io.utils.get_zenml_config_dir","text":"Recursive function to find the zenml config starting from path. Parameters: Name Type Description Default path Default value = os.getcwd Path to check. None Returns: Type Description str The full path with the resolved zenml directory. Source code in zenml/io/utils.py def get_zenml_config_dir ( path : Optional [ str ] = None ) -> str : \"\"\"Recursive function to find the zenml config starting from path. Args: path (Default value = os.getcwd()): Path to check. Returns: The full path with the resolved zenml directory. Raises: InitializationException if directory not found until root of OS. \"\"\" return os . path . join ( get_zenml_dir ( path ), ZENML_DIR_NAME )","title":"get_zenml_config_dir()"},{"location":"api_docs/io/#zenml.io.utils.get_zenml_dir","text":"Returns path to a ZenML repository directory. Parameters: Name Type Description Default path Optional[str] Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable ZENML_REPOSITORY_PATH (if set) and recursively searching in the parent directories of the current working directory. None Returns: Type Description str Absolute path to a ZenML repository directory. Exceptions: Type Description InitializationException If no ZenML repository is found. Source code in zenml/io/utils.py def get_zenml_dir ( path : Optional [ str ] = None ) -> str : \"\"\"Returns path to a ZenML repository directory. Args: path: Optional path to look for the repository. If no path is given, this function tries to find the repository using the environment variable `ZENML_REPOSITORY_PATH` (if set) and recursively searching in the parent directories of the current working directory. Returns: Absolute path to a ZenML repository directory. Raises: InitializationException: If no ZenML repository is found. \"\"\" if not path : # try to get path from the environment variable path = os . getenv ( ENV_ZENML_REPOSITORY_PATH , None ) if path : # explicit path via parameter or environment variable, don't search # parent directories search_parent_directories = False error_message = ( f \"Unable to find ZenML repository at path ' { path } '. Make sure to \" f \"create a ZenML repository by calling `zenml init` when \" f \"specifying an explicit repository path in code or via the \" f \"environment variable ' { ENV_ZENML_REPOSITORY_PATH } '.\" ) else : # try to find the repo in the parent directories of the # current working directory path = os . getcwd () search_parent_directories = True error_message = ( f \"Unable to find ZenML repository in your current working \" f \"directory ( { os . getcwd () } ) or any parent directories. If you \" f \"want to use an existing repository which is in a different \" f \"location, set the environment variable \" f \"' { ENV_ZENML_REPOSITORY_PATH } '. If you want to create a new \" f \"repository, run `zenml init`.\" ) def _find_repo_helper ( repo_path : str ) -> str : \"\"\"Helper function to recursively search parent directories for a ZenML repository.\"\"\" if is_zenml_dir ( repo_path ): return repo_path if not search_parent_directories or is_root ( repo_path ): raise InitializationException ( error_message ) return _find_repo_helper ( str ( Path ( repo_path ) . parent )) path = _find_repo_helper ( path ) return str ( Path ( path ) . resolve ())","title":"get_zenml_dir()"},{"location":"api_docs/io/#zenml.io.utils.is_gcs_path","text":"Returns True if path is on Google Cloud Storage. Parameters: Name Type Description Default path str Any path as a string. required Returns: Type Description bool True if gcs path, else False. Source code in zenml/io/utils.py def is_gcs_path ( path : str ) -> bool : \"\"\"Returns True if path is on Google Cloud Storage. Args: path: Any path as a string. Returns: True if gcs path, else False. \"\"\" return path . startswith ( \"gs://\" )","title":"is_gcs_path()"},{"location":"api_docs/io/#zenml.io.utils.is_zenml_dir","text":"Check if dir is a zenml dir or not. Parameters: Name Type Description Default path str Path to the root. required Returns: Type Description bool True if path contains a zenml dir, False if not. Source code in zenml/io/utils.py def is_zenml_dir ( path : str ) -> bool : \"\"\"Check if dir is a zenml dir or not. Args: path: Path to the root. Returns: True if path contains a zenml dir, False if not. \"\"\" config_dir_path = os . path . join ( path , ZENML_DIR_NAME ) return bool ( is_dir ( config_dir_path ))","title":"is_zenml_dir()"},{"location":"api_docs/io/#zenml.io.utils.read_file_contents_as_string","text":"Reads contents of file. Parameters: Name Type Description Default file_path str Path to file. required Source code in zenml/io/utils.py def read_file_contents_as_string ( file_path : str ) -> str : \"\"\"Reads contents of file. Args: file_path: Path to file. \"\"\" if not file_exists ( file_path ): raise FileNotFoundError ( f \" { file_path } does not exist!\" ) return open ( file_path ) . read () # type: ignore[no-any-return]","title":"read_file_contents_as_string()"},{"location":"api_docs/io/#zenml.io.utils.write_file_contents_as_string","text":"Writes contents of file. Parameters: Name Type Description Default file_path str Path to file. required content str Contents of file. required Source code in zenml/io/utils.py def write_file_contents_as_string ( file_path : str , content : str ) -> None : \"\"\"Writes contents of file. Args: file_path: Path to file. content: Contents of file. \"\"\" with open ( file_path , \"w\" ) as f : f . write ( content )","title":"write_file_contents_as_string()"},{"location":"api_docs/logger/","text":"Logger zenml.logger CustomFormatter ( Formatter ) Formats logs according to custom specifications. Source code in zenml/logger.py class CustomFormatter ( logging . Formatter ): \"\"\"Formats logs according to custom specifications.\"\"\" grey : str = \" \\x1b [38;21m\" pink : str = \" \\x1b [35m\" green : str = \" \\x1b [32m\" yellow : str = \" \\x1b [33;21m\" red : str = \" \\x1b [31;21m\" bold_red : str = \" \\x1b [31;1m\" purple : str = \" \\x1b [1;35m\" reset : str = \" \\x1b [0m\" format_template : str = ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s (%(\" \"filename)s: %(lineno)d )\" if LoggingLevels [ ZENML_LOGGING_VERBOSITY ] == LoggingLevels . DEBUG else \" %(message)s \" ) COLORS : Dict [ LoggingLevels , str ] = { LoggingLevels . DEBUG : grey , LoggingLevels . INFO : purple , LoggingLevels . WARN : yellow , LoggingLevels . ERROR : red , LoggingLevels . CRITICAL : bold_red , } def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message format ( self , record ) Converts a log record to a (colored) string Parameters: Name Type Description Default record LogRecord LogRecord generated by the code. required Returns: Type Description str A string formatted according to specifications. Source code in zenml/logger.py def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message get_console_handler () Get console handler for logging. Source code in zenml/logger.py def get_console_handler () -> Any : \"\"\"Get console handler for logging.\"\"\" console_handler = logging . StreamHandler ( sys . stdout ) console_handler . setFormatter ( CustomFormatter ()) return console_handler get_file_handler () Return a file handler for logging. Source code in zenml/logger.py def get_file_handler () -> Any : \"\"\"Return a file handler for logging.\"\"\" file_handler = TimedRotatingFileHandler ( LOG_FILE , when = \"midnight\" ) file_handler . setFormatter ( CustomFormatter ()) return file_handler get_logger ( logger_name ) Main function to get logger name,. Parameters: Name Type Description Default logger_name str Name of logger to initialize. required Returns: Type Description Logger A logger object. Source code in zenml/logger.py def get_logger ( logger_name : str ) -> logging . Logger : \"\"\"Main function to get logger name,. Args: logger_name: Name of logger to initialize. Returns: A logger object. \"\"\" logger = logging . getLogger ( logger_name ) logger . setLevel ( get_logging_level () . value ) logger . addHandler ( get_console_handler ()) # TODO [ENG-130]: Add a file handler for persistent handling # logger.addHandler(get_file_handler()) # with this pattern, it's rarely necessary to propagate the error up to # parent logger . propagate = False return logger get_logging_level () Get logging level from the env variable. Source code in zenml/logger.py def get_logging_level () -> LoggingLevels : \"\"\"Get logging level from the env variable.\"\"\" verbosity = ZENML_LOGGING_VERBOSITY . upper () if verbosity not in LoggingLevels . __members__ : raise KeyError ( f \"Verbosity must be one of { list ( LoggingLevels . __members__ . keys ()) } \" ) return LoggingLevels [ verbosity ] init_logging () Initialize logging with default levels. Source code in zenml/logger.py def init_logging () -> None : \"\"\"Initialize logging with default levels.\"\"\" # Mute tensorflow cuda warnings os . environ [ \"TF_CPP_MIN_LOG_LEVEL\" ] = \"3\" set_root_verbosity () # Mute apache_beam muted_logger_names = [ \"apache_beam\" , \"rdbms_metadata_access_object\" , \"apache_beam.io.gcp.bigquery\" , \"backoff\" , \"segment\" , ] for logger_name in muted_logger_names : logging . getLogger ( logger_name ) . setLevel ( logging . WARNING ) logging . getLogger ( logger_name ) . disabled = True # set absl logging absl_logging . set_verbosity ( ABSL_LOGGING_VERBOSITY ) set_root_verbosity () Set the root verbosity. Source code in zenml/logger.py def set_root_verbosity () -> None : \"\"\"Set the root verbosity.\"\"\" level = get_logging_level () if level != LoggingLevels . NOTSET : logging . basicConfig ( level = level . value ) get_logger ( __name__ ) . debug ( f \"Logging set to level: \" f \" { logging . getLevelName ( level . value ) } \" ) else : logging . disable ( sys . maxsize ) logging . getLogger () . disabled = True get_logger ( __name__ ) . debug ( \"Logging NOTSET\" )","title":"Logger"},{"location":"api_docs/logger/#logger","text":"","title":"Logger"},{"location":"api_docs/logger/#zenml.logger","text":"","title":"logger"},{"location":"api_docs/logger/#zenml.logger.CustomFormatter","text":"Formats logs according to custom specifications. Source code in zenml/logger.py class CustomFormatter ( logging . Formatter ): \"\"\"Formats logs according to custom specifications.\"\"\" grey : str = \" \\x1b [38;21m\" pink : str = \" \\x1b [35m\" green : str = \" \\x1b [32m\" yellow : str = \" \\x1b [33;21m\" red : str = \" \\x1b [31;21m\" bold_red : str = \" \\x1b [31;1m\" purple : str = \" \\x1b [1;35m\" reset : str = \" \\x1b [0m\" format_template : str = ( \" %(asctime)s - %(name)s - %(levelname)s - %(message)s (%(\" \"filename)s: %(lineno)d )\" if LoggingLevels [ ZENML_LOGGING_VERBOSITY ] == LoggingLevels . DEBUG else \" %(message)s \" ) COLORS : Dict [ LoggingLevels , str ] = { LoggingLevels . DEBUG : grey , LoggingLevels . INFO : purple , LoggingLevels . WARN : yellow , LoggingLevels . ERROR : red , LoggingLevels . CRITICAL : bold_red , } def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message","title":"CustomFormatter"},{"location":"api_docs/logger/#zenml.logger.CustomFormatter.format","text":"Converts a log record to a (colored) string Parameters: Name Type Description Default record LogRecord LogRecord generated by the code. required Returns: Type Description str A string formatted according to specifications. Source code in zenml/logger.py def format ( self , record : logging . LogRecord ) -> str : \"\"\"Converts a log record to a (colored) string Args: record: LogRecord generated by the code. Returns: A string formatted according to specifications. \"\"\" log_fmt = ( self . COLORS [ LoggingLevels [ ZENML_LOGGING_VERBOSITY ]] + self . format_template + self . reset ) formatter = logging . Formatter ( log_fmt ) formatted_message = formatter . format ( record ) quoted_groups = re . findall ( \"`([^`]*)`\" , formatted_message ) for quoted in quoted_groups : formatted_message = formatted_message . replace ( \"`\" + quoted + \"`\" , \"`\" + self . reset + self . yellow + quoted + \"`\" + self . COLORS . get ( LoggingLevels [ ZENML_LOGGING_VERBOSITY ]), ) return formatted_message","title":"format()"},{"location":"api_docs/logger/#zenml.logger.get_console_handler","text":"Get console handler for logging. Source code in zenml/logger.py def get_console_handler () -> Any : \"\"\"Get console handler for logging.\"\"\" console_handler = logging . StreamHandler ( sys . stdout ) console_handler . setFormatter ( CustomFormatter ()) return console_handler","title":"get_console_handler()"},{"location":"api_docs/logger/#zenml.logger.get_file_handler","text":"Return a file handler for logging. Source code in zenml/logger.py def get_file_handler () -> Any : \"\"\"Return a file handler for logging.\"\"\" file_handler = TimedRotatingFileHandler ( LOG_FILE , when = \"midnight\" ) file_handler . setFormatter ( CustomFormatter ()) return file_handler","title":"get_file_handler()"},{"location":"api_docs/logger/#zenml.logger.get_logger","text":"Main function to get logger name,. Parameters: Name Type Description Default logger_name str Name of logger to initialize. required Returns: Type Description Logger A logger object. Source code in zenml/logger.py def get_logger ( logger_name : str ) -> logging . Logger : \"\"\"Main function to get logger name,. Args: logger_name: Name of logger to initialize. Returns: A logger object. \"\"\" logger = logging . getLogger ( logger_name ) logger . setLevel ( get_logging_level () . value ) logger . addHandler ( get_console_handler ()) # TODO [ENG-130]: Add a file handler for persistent handling # logger.addHandler(get_file_handler()) # with this pattern, it's rarely necessary to propagate the error up to # parent logger . propagate = False return logger","title":"get_logger()"},{"location":"api_docs/logger/#zenml.logger.get_logging_level","text":"Get logging level from the env variable. Source code in zenml/logger.py def get_logging_level () -> LoggingLevels : \"\"\"Get logging level from the env variable.\"\"\" verbosity = ZENML_LOGGING_VERBOSITY . upper () if verbosity not in LoggingLevels . __members__ : raise KeyError ( f \"Verbosity must be one of { list ( LoggingLevels . __members__ . keys ()) } \" ) return LoggingLevels [ verbosity ]","title":"get_logging_level()"},{"location":"api_docs/logger/#zenml.logger.init_logging","text":"Initialize logging with default levels. Source code in zenml/logger.py def init_logging () -> None : \"\"\"Initialize logging with default levels.\"\"\" # Mute tensorflow cuda warnings os . environ [ \"TF_CPP_MIN_LOG_LEVEL\" ] = \"3\" set_root_verbosity () # Mute apache_beam muted_logger_names = [ \"apache_beam\" , \"rdbms_metadata_access_object\" , \"apache_beam.io.gcp.bigquery\" , \"backoff\" , \"segment\" , ] for logger_name in muted_logger_names : logging . getLogger ( logger_name ) . setLevel ( logging . WARNING ) logging . getLogger ( logger_name ) . disabled = True # set absl logging absl_logging . set_verbosity ( ABSL_LOGGING_VERBOSITY )","title":"init_logging()"},{"location":"api_docs/logger/#zenml.logger.set_root_verbosity","text":"Set the root verbosity. Source code in zenml/logger.py def set_root_verbosity () -> None : \"\"\"Set the root verbosity.\"\"\" level = get_logging_level () if level != LoggingLevels . NOTSET : logging . basicConfig ( level = level . value ) get_logger ( __name__ ) . debug ( f \"Logging set to level: \" f \" { logging . getLevelName ( level . value ) } \" ) else : logging . disable ( sys . maxsize ) logging . getLogger () . disabled = True get_logger ( __name__ ) . debug ( \"Logging NOTSET\" )","title":"set_root_verbosity()"},{"location":"api_docs/materializers/","text":"Materializers zenml.materializers special Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class. base_materializer BaseMaterializer Base Materializer to realize artifact data. Source code in zenml/materializers/base_materializer.py class BaseMaterializer ( metaclass = BaseMaterializerMeta ): \"\"\"Base Materializer to realize artifact data.\"\"\" ASSOCIATED_ARTIFACT_TYPES : ClassVar [ List [ Type [ \"BaseArtifact\" ]]] = [] ASSOCIATED_TYPES : ClassVar [ List [ Type [ Any ]]] = [] def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # ) __init__ ( self , artifact ) special Initializes a materializer with the given artifact. Source code in zenml/materializers/base_materializer.py def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact handle_input ( self , data_type ) Write logic here to handle input of the step function. Parameters: Name Type Description Default data_type Type[Any] What type the input should be materialized as. required Returns: Type Description Any Any object that is to be passed into the relevant artifact in the step. Source code in zenml/materializers/base_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) handle_return ( self , data ) Write logic here to handle return of the step function. Source code in zenml/materializers/base_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # ) BaseMaterializerMeta ( type ) Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. Source code in zenml/materializers/base_materializer.py class BaseMaterializerMeta ( type ): \"\"\"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Creates a Materializer class and registers it at the MaterializerRegistry . Source code in zenml/materializers/base_materializer.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls beam_materializer BeamMaterializer ( BaseMaterializer ) Materializer to read data to and from beam. Source code in zenml/materializers/beam_materializer.py class BeamMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from beam.\"\"\" ASSOCIATED_TYPES = [ beam . PCollection ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run() handle_input ( self , data_type ) Reads all files inside the artifact directory and materializes them as a beam compatible output. Source code in zenml/materializers/beam_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) handle_return ( self , pipeline ) Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Parameters: Name Type Description Default pipeline Pipeline A beam.pipeline object. required Source code in zenml/materializers/beam_materializer.py def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run() built_in_materializer BuiltInMaterializer ( BaseMaterializer ) Read/Write JSON files. Source code in zenml/materializers/built_in_materializer.py class BuiltInMaterializer ( BaseMaterializer ): \"\"\"Read/Write JSON files.\"\"\" # TODO [ENG-322]: consider adding typing.Dict and typing.List # since these are the 'correct' way to annotate these types. ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , DataAnalysisArtifact , ] ASSOCIATED_TYPES = [ int , str , bytes , dict , float , list , tuple , bool , ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data ) handle_input ( self , data_type ) Reads basic primitive types from json. Source code in zenml/materializers/built_in_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents handle_return ( self , data ) Handles basic built-in types and stores them as json Source code in zenml/materializers/built_in_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data ) default_materializer_registry MaterializerRegistry Matches a python type to a default materializer. Source code in zenml/materializers/default_materializer_registry.py class MaterializerRegistry : \"\"\"Matches a python type to a default materializer.\"\"\" def __init__ ( self ) -> None : self . materializer_types : Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]] = {} def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types __getitem__ ( self , key ) special Get a single materializers based on the key. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required Returns: Type Description Type[BaseMaterializer] BaseMaterializer subclass that was registered for this key. Source code in zenml/materializers/default_materializer_registry.py def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) get_materializer_types ( self ) Get all registered materializer types. Source code in zenml/materializers/default_materializer_registry.py def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types is_registered ( self , key ) Returns if a materializer class is registered for the given type. Source code in zenml/materializers/default_materializer_registry.py def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types register_and_overwrite_type ( self , key , type_ ) Registers a new materializer and also overwrites a default if set. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) register_materializer_type ( self , key , type_ ) Registers a new materializer. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) numpy_materializer NumpyMaterializer ( BaseMaterializer ) Materializer to read data to and from pandas. Source code in zenml/materializers/numpy_materializer.py class NumpyMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ np . ndarray ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream ) handle_input ( self , data_type ) Reads numpy array from parquet file. Source code in zenml/materializers/numpy_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) handle_return ( self , arr ) Writes a np.ndarray to the artifact store as a parquet file. Parameters: Name Type Description Default arr ndarray The numpy array to write. required Source code in zenml/materializers/numpy_materializer.py def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream ) pandas_materializer PandasMaterializer ( BaseMaterializer ) Materializer to read data to and from pandas. Source code in zenml/materializers/pandas_materializer.py class PandasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ pd . DataFrame ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , StatisticsArtifact , SchemaArtifact , ] def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE ) handle_input ( self , data_type ) Reads pd.Dataframe from a parquet file. Source code in zenml/materializers/pandas_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) handle_return ( self , df ) Writes a pandas dataframe to the specified filename. Parameters: Name Type Description Default df DataFrame The pandas dataframe to write. required Source code in zenml/materializers/pandas_materializer.py def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"Materializers"},{"location":"api_docs/materializers/#materializers","text":"","title":"Materializers"},{"location":"api_docs/materializers/#zenml.materializers","text":"Materializers are used to convert a ZenML artifact into a specific format. They are most often used to handle the input or output of ZenML steps, and can be extended by building on the BaseMaterializer class.","title":"materializers"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer","text":"","title":"base_materializer"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer","text":"Base Materializer to realize artifact data. Source code in zenml/materializers/base_materializer.py class BaseMaterializer ( metaclass = BaseMaterializerMeta ): \"\"\"Base Materializer to realize artifact data.\"\"\" ASSOCIATED_ARTIFACT_TYPES : ClassVar [ List [ Type [ \"BaseArtifact\" ]]] = [] ASSOCIATED_TYPES : ClassVar [ List [ Type [ Any ]]] = [] def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # ) def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # )","title":"BaseMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.__init__","text":"Initializes a materializer with the given artifact. Source code in zenml/materializers/base_materializer.py def __init__ ( self , artifact : \"BaseArtifact\" ): \"\"\"Initializes a materializer with the given artifact.\"\"\" self . artifact = artifact","title":"__init__()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.handle_input","text":"Write logic here to handle input of the step function. Parameters: Name Type Description Default data_type Type[Any] What type the input should be materialized as. required Returns: Type Description Any Any object that is to be passed into the relevant artifact in the step. Source code in zenml/materializers/base_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Write logic here to handle input of the step function. Args: data_type: What type the input should be materialized as. Returns: Any object that is to be passed into the relevant artifact in the step. \"\"\" # TODO [ENG-140]: Add type checking for materializer handle_input # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__name__}. Supported types: {self.ASSOCIATED_TYPES}\" # )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializer.handle_return","text":"Write logic here to handle return of the step function. Source code in zenml/materializers/base_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Write logic here to handle return of the step function. Args: Any object that is specified as an input artifact of the step. \"\"\" # TODO [ENG-141]: Put proper type checking # if data_type not in self.ASSOCIATED_TYPES: # raise ValueError( # f\"Data type {data_type} not supported by materializer \" # f\"{self.__class__.__name__}. Supported types: \" # f\"{self.ASSOCIATED_TYPES}\" # )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializerMeta","text":"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts. Source code in zenml/materializers/base_materializer.py class BaseMaterializerMeta ( type ): \"\"\"Metaclass responsible for registering different BaseMaterializer subclasses for reading/writing artifacts.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls","title":"BaseMaterializerMeta"},{"location":"api_docs/materializers/#zenml.materializers.base_materializer.BaseMaterializerMeta.__new__","text":"Creates a Materializer class and registers it at the MaterializerRegistry . Source code in zenml/materializers/base_materializer.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseMaterializerMeta\" : \"\"\"Creates a Materializer class and registers it at the `MaterializerRegistry`.\"\"\" cls = cast ( Type [ \"BaseMaterializer\" ], super () . __new__ ( mcs , name , bases , dct ) ) if name != \"BaseMaterializer\" : assert cls . ASSOCIATED_TYPES , ( \"You should specify a list of ASSOCIATED_TYPES when creating a \" \"Materializer!\" ) for associated_type in cls . ASSOCIATED_TYPES : default_materializer_registry . register_materializer_type ( associated_type , cls ) if cls . ASSOCIATED_ARTIFACT_TYPES : type_registry . register_integration ( associated_type , cls . ASSOCIATED_ARTIFACT_TYPES ) else : from zenml.artifacts.base_artifact import BaseArtifact type_registry . register_integration ( associated_type , [ BaseArtifact ] ) return cls","title":"__new__()"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer","text":"","title":"beam_materializer"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer","text":"Materializer to read data to and from beam. Source code in zenml/materializers/beam_materializer.py class BeamMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from beam.\"\"\" ASSOCIATED_TYPES = [ beam . PCollection ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type ) def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run()","title":"BeamMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer.handle_input","text":"Reads all files inside the artifact directory and materializes them as a beam compatible output. Source code in zenml/materializers/beam_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads all files inside the artifact directory and materializes them as a beam compatible output.\"\"\" # TODO [ENG-138]: Implement beam reading super () . handle_input ( data_type )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.beam_materializer.BeamMaterializer.handle_return","text":"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Parameters: Name Type Description Default pipeline Pipeline A beam.pipeline object. required Source code in zenml/materializers/beam_materializer.py def handle_return ( self , pipeline : beam . Pipeline ) -> None : \"\"\"Appends a beam.io.WriteToParquet at the end of a beam pipeline and therefore persists the results. Args: pipeline: A beam.pipeline object. \"\"\" # TODO [ENG-139]: Implement beam writing super () . handle_return ( pipeline ) pipeline | beam . ParDo () pipeline . run () # pipeline | beam.io.WriteToParquet(self.artifact.uri) # pipeline.run()","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer","text":"","title":"built_in_materializer"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer","text":"Read/Write JSON files. Source code in zenml/materializers/built_in_materializer.py class BuiltInMaterializer ( BaseMaterializer ): \"\"\"Read/Write JSON files.\"\"\" # TODO [ENG-322]: consider adding typing.Dict and typing.List # since these are the 'correct' way to annotate these types. ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , DataAnalysisArtifact , ] ASSOCIATED_TYPES = [ int , str , bytes , dict , float , list , tuple , bool , ] def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data )","title":"BuiltInMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer.handle_input","text":"Reads basic primitive types from json. Source code in zenml/materializers/built_in_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> Any : \"\"\"Reads basic primitive types from json.\"\"\" super () . handle_input ( data_type ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) contents = yaml_utils . read_json ( filepath ) if type ( contents ) != data_type : # TODO [ENG-142]: Raise error or try to coerce logger . debug ( f \"Contents { contents } was type { type ( contents ) } but expected \" f \" { data_type } \" ) return contents","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.built_in_materializer.BuiltInMaterializer.handle_return","text":"Handles basic built-in types and stores them as json Source code in zenml/materializers/built_in_materializer.py def handle_return ( self , data : Any ) -> None : \"\"\"Handles basic built-in types and stores them as json\"\"\" super () . handle_return ( data ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) yaml_utils . write_json ( filepath , data )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry","text":"","title":"default_materializer_registry"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry","text":"Matches a python type to a default materializer. Source code in zenml/materializers/default_materializer_registry.py class MaterializerRegistry : \"\"\"Matches a python type to a default materializer.\"\"\" def __init__ ( self ) -> None : self . materializer_types : Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]] = {} def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" ) def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" ) def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types","title":"MaterializerRegistry"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.__getitem__","text":"Get a single materializers based on the key. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required Returns: Type Description Type[BaseMaterializer] BaseMaterializer subclass that was registered for this key. Source code in zenml/materializers/default_materializer_registry.py def __getitem__ ( self , key : Type [ Any ]) -> Type [ \"BaseMaterializer\" ]: \"\"\"Get a single materializers based on the key. Args: key: Indicates the type of an object. Returns: `BaseMaterializer` subclass that was registered for this key. \"\"\" if key in self . materializer_types : return self . materializer_types [ key ] else : raise KeyError ( f \"Type { key } does not have a default `Materializer`! Please \" f \"specify your own `Materializer`.\" )","title":"__getitem__()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.get_materializer_types","text":"Get all registered materializer types. Source code in zenml/materializers/default_materializer_registry.py def get_materializer_types ( self , ) -> Dict [ Type [ Any ], Type [ \"BaseMaterializer\" ]]: \"\"\"Get all registered materializer types.\"\"\" return self . materializer_types","title":"get_materializer_types()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.is_registered","text":"Returns if a materializer class is registered for the given type. Source code in zenml/materializers/default_materializer_registry.py def is_registered ( self , key : Type [ Any ]) -> bool : \"\"\"Returns if a materializer class is registered for the given type.\"\"\" return key in self . materializer_types","title":"is_registered()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.register_and_overwrite_type","text":"Registers a new materializer and also overwrites a default if set. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_and_overwrite_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer and also overwrites a default if set. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" )","title":"register_and_overwrite_type()"},{"location":"api_docs/materializers/#zenml.materializers.default_materializer_registry.MaterializerRegistry.register_materializer_type","text":"Registers a new materializer. Parameters: Name Type Description Default key Type[Any] Indicates the type of an object. required type_ Type[BaseMaterializer] A BaseMaterializer subclass. required Source code in zenml/materializers/default_materializer_registry.py def register_materializer_type ( self , key : Type [ Any ], type_ : Type [ \"BaseMaterializer\" ] ) -> None : \"\"\"Registers a new materializer. Args: key: Indicates the type of an object. type_: A BaseMaterializer subclass. \"\"\" if key not in self . materializer_types : self . materializer_types [ key ] = type_ logger . debug ( f \"Registered materializer { type_ } for { key } \" ) else : logger . debug ( f \" { key } already registered with \" f \" { self . materializer_types [ key ] } . Cannot register { type_ } .\" )","title":"register_materializer_type()"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer","text":"","title":"numpy_materializer"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer","text":"Materializer to read data to and from pandas. Source code in zenml/materializers/numpy_materializer.py class NumpyMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ np . ndarray ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact ] def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple ) def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream )","title":"NumpyMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer.handle_input","text":"Reads numpy array from parquet file. Source code in zenml/materializers/numpy_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> np . ndarray : \"\"\"Reads numpy array from parquet file.\"\"\" super () . handle_input ( data_type ) shape_dict = yaml_utils . read_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ) ) shape_tuple = tuple ( shape_dict . values ()) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"rb\" ) as f : input_stream = pa . input_stream ( f ) data = pq . read_table ( input_stream ) vals = getattr ( data . to_pandas (), DATA_VAR ) . values return np . reshape ( vals , shape_tuple )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.numpy_materializer.NumpyMaterializer.handle_return","text":"Writes a np.ndarray to the artifact store as a parquet file. Parameters: Name Type Description Default arr ndarray The numpy array to write. required Source code in zenml/materializers/numpy_materializer.py def handle_return ( self , arr : np . ndarray ) -> None : \"\"\"Writes a np.ndarray to the artifact store as a parquet file. Args: arr: The numpy array to write. \"\"\" super () . handle_return ( arr ) yaml_utils . write_json ( os . path . join ( self . artifact . uri , SHAPE_FILENAME ), { str ( i ): x for i , x in enumerate ( arr . shape )}, ) pa_table = pa . table ({ DATA_VAR : arr . flatten ()}) with fileio . open ( os . path . join ( self . artifact . uri , DATA_FILENAME ), \"wb\" ) as f : stream = pa . output_stream ( f ) pq . write_table ( pa_table , stream )","title":"handle_return()"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer","text":"","title":"pandas_materializer"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer","text":"Materializer to read data to and from pandas. Source code in zenml/materializers/pandas_materializer.py class PandasMaterializer ( BaseMaterializer ): \"\"\"Materializer to read data to and from pandas.\"\"\" ASSOCIATED_TYPES = [ pd . DataFrame ] ASSOCIATED_ARTIFACT_TYPES = [ DataArtifact , StatisticsArtifact , SchemaArtifact , ] def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) ) def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"PandasMaterializer"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer.handle_input","text":"Reads pd.Dataframe from a parquet file. Source code in zenml/materializers/pandas_materializer.py def handle_input ( self , data_type : Type [ Any ]) -> pd . DataFrame : \"\"\"Reads pd.Dataframe from a parquet file.\"\"\" super () . handle_input ( data_type ) return pd . read_parquet ( os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) )","title":"handle_input()"},{"location":"api_docs/materializers/#zenml.materializers.pandas_materializer.PandasMaterializer.handle_return","text":"Writes a pandas dataframe to the specified filename. Parameters: Name Type Description Default df DataFrame The pandas dataframe to write. required Source code in zenml/materializers/pandas_materializer.py def handle_return ( self , df : pd . DataFrame ) -> None : \"\"\"Writes a pandas dataframe to the specified filename. Args: df: The pandas dataframe to write. \"\"\" super () . handle_return ( df ) filepath = os . path . join ( self . artifact . uri , DEFAULT_FILENAME ) df . to_parquet ( filepath , compression = COMPRESSION_TYPE )","title":"handle_return()"},{"location":"api_docs/metadata_stores/","text":"Metadata Stores zenml.metadata_stores special The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store. base_metadata_store BaseMetadataStore ( BaseComponent ) pydantic-model Metadata store base class to track metadata of zenml first class citizens. Source code in zenml/metadata_stores/base_metadata_store.py class BaseMetadataStore ( BaseComponent ): \"\"\"Metadata store base class to track metadata of zenml first class citizens.\"\"\" _run_type_name : str = \"pipeline_run\" _node_type_name : str = \"node\" _METADATA_STORE_DIR_NAME = \"metadata_stores\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseMetadataStore instance. Args: repo_path: Path to the repository of this metadata store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _METADATA_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @property def store ( self ) -> metadata_store . MetadataStore : \"\"\"General property that hooks into TFX metadata store.\"\"\" # TODO [ENG-133]: this always gets recreated, is this intended? return metadata_store . MetadataStore ( self . get_tfx_metadata_config (), enable_upgrade_migration = True ) @abstractmethod def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError @property def step_type_mapping ( self ) -> Dict [ int , str ]: \"\"\"Maps type_id's to step names.\"\"\" return { type_ . id : type_ . name for type_ in self . store . get_execution_types () } def _check_if_executions_belong_to_pipeline ( self , executions : List [ proto . Execution ], pipeline : PipelineView , ) -> bool : \"\"\"Returns `True` if the executions are associated with the pipeline context.\"\"\" for execution in executions : associated_contexts = self . store . get_contexts_by_execution ( execution . id ) for context in associated_contexts : if context . id == pipeline . _id : # noqa return True return False def _get_step_view_from_execution ( self , execution : proto . Execution ) -> StepView : \"\"\"Get original StepView from an execution. Args: execution: proto.Execution object from mlmd store. Returns: Original `StepView` derived from the proto.Execution. \"\"\" impl_name = self . step_type_mapping [ execution . type_id ] . split ( \".\" )[ - 1 ] step_name_property = execution . custom_properties . get ( INTERNAL_EXECUTION_PARAMETER_PREFIX + PARAM_PIPELINE_PARAMETER_NAME , None , ) if step_name_property : step_name = json . loads ( step_name_property . string_value ) else : raise KeyError ( f \"Step name missing for execution with ID { execution . id } . \" f \"This error probably occurs because you're using ZenML \" f \"version 0.5.4 or newer but your metadata store contains \" f \"data from previous versions.\" ) step_parameters = {} for k , v in execution . custom_properties . items (): if not k . startswith ( INTERNAL_EXECUTION_PARAMETER_PREFIX ): try : step_parameters [ k ] = json . loads ( v . string_value ) except JSONDecodeError : # this means there is a property in there that is neither # an internal one or one created by zenml. Therefore, we can # ignore it pass # TODO [ENG-222]: This is a lot of querying to the metadata store. We # should refactor and make it nicer. Probably it makes more sense # to first get `executions_ids_for_current_run` and then filter on # `event.execution_id in execution_ids_for_current_run`. # Core logic here is that we get the event of this particular execution # id that gives us the artifacts of this execution. We then go through # all `input` artifacts of this execution and get all events related to # that artifact. This in turn gives us other events for which this # artifact was an `output` artifact. Then we simply need to sort by # time to get the most recent execution (i.e. step) that produced that # particular artifact. events_for_execution = self . store . get_events_by_execution_ids ( [ execution . id ] ) parents_step_ids = set () for current_event in events_for_execution : if current_event . type == current_event . INPUT : # this means the artifact is an input artifact events_for_input_artifact = [ e for e in self . store . get_events_by_artifact_ids ( [ current_event . artifact_id ] ) # should be output type and should NOT be the same id as # the execution we are querying and it should be BEFORE # the time of the current event. if e . type == e . OUTPUT and e . execution_id != current_event . execution_id and e . milliseconds_since_epoch < current_event . milliseconds_since_epoch ] # sort by time events_for_input_artifact . sort ( key = lambda x : x . milliseconds_since_epoch # type: ignore[no-any-return] # noqa ) # take the latest one and add execution to the parents. parents_step_ids . add ( events_for_input_artifact [ - 1 ] . execution_id ) return StepView ( id_ = execution . id , parents_step_ids = list ( parents_step_ids ), name = impl_name , pipeline_step_name = step_name , parameters = step_parameters , metadata_store = self , ) def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . pipeline_step_name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . name , ) return inputs , outputs def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution ) class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_metadata_store_\" step_type_mapping : Dict [ int , str ] property readonly Maps type_id's to step names. store : MetadataStore property readonly General property that hooks into TFX metadata store. Config Configuration of settings. Source code in zenml/metadata_stores/base_metadata_store.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_metadata_store_\" __init__ ( self , repo_path , ** kwargs ) special Initializes a BaseMetadataStore instance. Parameters: Name Type Description Default repo_path str Path to the repository of this metadata store. required Source code in zenml/metadata_stores/base_metadata_store.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseMetadataStore instance. Args: repo_path: Path to the repository of this metadata store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _METADATA_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) get_pipeline ( self , pipeline_name ) Returns a pipeline for the given name. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None get_pipeline_run ( self , pipeline , run_name ) Gets a specific run for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None get_pipeline_run_steps ( self , pipeline_run ) Gets all steps for the given pipeline run. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . pipeline_step_name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps get_pipeline_runs ( self , pipeline ) Gets all runs for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs get_pipelines ( self ) Returns a list of all pipelines stored in this metadata store. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines get_producer_step_from_artifact ( self , artifact ) Returns original StepView from an ArtifactView. Parameters: Name Type Description Default artifact ArtifactView ArtifactView to be queried. required Returns: Type Description StepView Original StepView that produced the artifact. Source code in zenml/metadata_stores/base_metadata_store.py def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution ) get_step_artifacts ( self , step ) Returns input and output artifacts for the given step. Parameters: Name Type Description Default step StepView The step for which to get the artifacts. required Returns: Type Description Tuple[Dict[str, zenml.post_execution.artifact.ArtifactView], Dict[str, zenml.post_execution.artifact.ArtifactView]] A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . name , ) return inputs , outputs get_step_by_id ( self , step_id ) Gets a StepView by its ID Source code in zenml/metadata_stores/base_metadata_store.py def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) get_step_status ( self , step ) Gets the execution status of a single step. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED get_tfx_metadata_config ( self ) Return tfx metadata config. Source code in zenml/metadata_stores/base_metadata_store.py @abstractmethod def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError mysql_metadata_store MySQLMetadataStore ( BaseMetadataStore ) pydantic-model MySQL backend for ZenML metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py class MySQLMetadataStore ( BaseMetadataStore ): \"\"\"MySQL backend for ZenML metadata store.\"\"\" host : str port : int database : str username : str password : str def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , ) get_tfx_metadata_config ( self ) Return tfx metadata config for mysql metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , ) sqlite_metadata_store SQLiteMetadataStore ( BaseMetadataStore ) pydantic-model SQLite backend for ZenML metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py class SQLiteMetadataStore ( BaseMetadataStore ): \"\"\"SQLite backend for ZenML metadata store.\"\"\" uri : str def __init__ ( self , ** data : Any ): \"\"\"Constructor for MySQL MetadataStore for ZenML.\"\"\" super () . __init__ ( ** data ) # TODO [ENG-131]: Replace with proper custom validator. if fileio . is_remote ( self . uri ): raise Exception ( f \"URI { self . uri } is a non-local path. A sqlite store \" f \"can only be local paths\" ) # Resolve URI if relative URI provided # self.uri = fileio.resolve_relative_path(uri) def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri ) @validator ( \"uri\" ) def uri_must_be_local ( cls , v : str ) -> str : \"\"\"Validator to ensure uri is local\"\"\" if fileio . is_remote ( v ): raise ValueError ( f \"URI { v } is a non-local path. A sqlite store \" f \"can only be local paths\" ) return v __init__ ( self , ** data ) special Constructor for MySQL MetadataStore for ZenML. Source code in zenml/metadata_stores/sqlite_metadata_store.py def __init__ ( self , ** data : Any ): \"\"\"Constructor for MySQL MetadataStore for ZenML.\"\"\" super () . __init__ ( ** data ) # TODO [ENG-131]: Replace with proper custom validator. if fileio . is_remote ( self . uri ): raise Exception ( f \"URI { self . uri } is a non-local path. A sqlite store \" f \"can only be local paths\" ) # Resolve URI if relative URI provided # self.uri = fileio.resolve_relative_path(uri) get_tfx_metadata_config ( self ) Return tfx metadata config for sqlite metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri ) uri_must_be_local ( v ) classmethod Validator to ensure uri is local Source code in zenml/metadata_stores/sqlite_metadata_store.py @validator ( \"uri\" ) def uri_must_be_local ( cls , v : str ) -> str : \"\"\"Validator to ensure uri is local\"\"\" if fileio . is_remote ( v ): raise ValueError ( f \"URI { v } is a non-local path. A sqlite store \" f \"can only be local paths\" ) return v","title":"Metadata Stores"},{"location":"api_docs/metadata_stores/#metadata-stores","text":"","title":"Metadata Stores"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores","text":"The configuration of each pipeline, step, backend, and produced artifacts are all tracked within the metadata store. The metadata store is an SQL database, and can be sqlite or mysql . Metadata are the pieces of information tracked about the pipelines, experiments and configurations that you are running with ZenML. Metadata are stored inside the metadata store.","title":"metadata_stores"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store","text":"","title":"base_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore","text":"Metadata store base class to track metadata of zenml first class citizens. Source code in zenml/metadata_stores/base_metadata_store.py class BaseMetadataStore ( BaseComponent ): \"\"\"Metadata store base class to track metadata of zenml first class citizens.\"\"\" _run_type_name : str = \"pipeline_run\" _node_type_name : str = \"node\" _METADATA_STORE_DIR_NAME = \"metadata_stores\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseMetadataStore instance. Args: repo_path: Path to the repository of this metadata store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _METADATA_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @property def store ( self ) -> metadata_store . MetadataStore : \"\"\"General property that hooks into TFX metadata store.\"\"\" # TODO [ENG-133]: this always gets recreated, is this intended? return metadata_store . MetadataStore ( self . get_tfx_metadata_config (), enable_upgrade_migration = True ) @abstractmethod def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError @property def step_type_mapping ( self ) -> Dict [ int , str ]: \"\"\"Maps type_id's to step names.\"\"\" return { type_ . id : type_ . name for type_ in self . store . get_execution_types () } def _check_if_executions_belong_to_pipeline ( self , executions : List [ proto . Execution ], pipeline : PipelineView , ) -> bool : \"\"\"Returns `True` if the executions are associated with the pipeline context.\"\"\" for execution in executions : associated_contexts = self . store . get_contexts_by_execution ( execution . id ) for context in associated_contexts : if context . id == pipeline . _id : # noqa return True return False def _get_step_view_from_execution ( self , execution : proto . Execution ) -> StepView : \"\"\"Get original StepView from an execution. Args: execution: proto.Execution object from mlmd store. Returns: Original `StepView` derived from the proto.Execution. \"\"\" impl_name = self . step_type_mapping [ execution . type_id ] . split ( \".\" )[ - 1 ] step_name_property = execution . custom_properties . get ( INTERNAL_EXECUTION_PARAMETER_PREFIX + PARAM_PIPELINE_PARAMETER_NAME , None , ) if step_name_property : step_name = json . loads ( step_name_property . string_value ) else : raise KeyError ( f \"Step name missing for execution with ID { execution . id } . \" f \"This error probably occurs because you're using ZenML \" f \"version 0.5.4 or newer but your metadata store contains \" f \"data from previous versions.\" ) step_parameters = {} for k , v in execution . custom_properties . items (): if not k . startswith ( INTERNAL_EXECUTION_PARAMETER_PREFIX ): try : step_parameters [ k ] = json . loads ( v . string_value ) except JSONDecodeError : # this means there is a property in there that is neither # an internal one or one created by zenml. Therefore, we can # ignore it pass # TODO [ENG-222]: This is a lot of querying to the metadata store. We # should refactor and make it nicer. Probably it makes more sense # to first get `executions_ids_for_current_run` and then filter on # `event.execution_id in execution_ids_for_current_run`. # Core logic here is that we get the event of this particular execution # id that gives us the artifacts of this execution. We then go through # all `input` artifacts of this execution and get all events related to # that artifact. This in turn gives us other events for which this # artifact was an `output` artifact. Then we simply need to sort by # time to get the most recent execution (i.e. step) that produced that # particular artifact. events_for_execution = self . store . get_events_by_execution_ids ( [ execution . id ] ) parents_step_ids = set () for current_event in events_for_execution : if current_event . type == current_event . INPUT : # this means the artifact is an input artifact events_for_input_artifact = [ e for e in self . store . get_events_by_artifact_ids ( [ current_event . artifact_id ] ) # should be output type and should NOT be the same id as # the execution we are querying and it should be BEFORE # the time of the current event. if e . type == e . OUTPUT and e . execution_id != current_event . execution_id and e . milliseconds_since_epoch < current_event . milliseconds_since_epoch ] # sort by time events_for_input_artifact . sort ( key = lambda x : x . milliseconds_since_epoch # type: ignore[no-any-return] # noqa ) # take the latest one and add execution to the parents. parents_step_ids . add ( events_for_input_artifact [ - 1 ] . execution_id ) return StepView ( id_ = execution . id , parents_step_ids = list ( parents_step_ids ), name = impl_name , pipeline_step_name = step_name , parameters = step_parameters , metadata_store = self , ) def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . pipeline_step_name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution ) def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . name , ) return inputs , outputs def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution ) class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_metadata_store_\"","title":"BaseMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.step_type_mapping","text":"Maps type_id's to step names.","title":"step_type_mapping"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.store","text":"General property that hooks into TFX metadata store.","title":"store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.Config","text":"Configuration of settings. Source code in zenml/metadata_stores/base_metadata_store.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_metadata_store_\"","title":"Config"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.__init__","text":"Initializes a BaseMetadataStore instance. Parameters: Name Type Description Default repo_path str Path to the repository of this metadata store. required Source code in zenml/metadata_stores/base_metadata_store.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseMetadataStore instance. Args: repo_path: Path to the repository of this metadata store. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _METADATA_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs )","title":"__init__()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline","text":"Returns a pipeline for the given name. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline ( self , pipeline_name : str ) -> Optional [ PipelineView ]: \"\"\"Returns a pipeline for the given name.\"\"\" pipeline_context = self . store . get_context_by_type_and_name ( PIPELINE_CONTEXT_TYPE_NAME , pipeline_name ) if pipeline_context : logger . debug ( \"Fetched pipeline with name ' %s '\" , pipeline_name ) return PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) else : logger . info ( \"No pipelines found for name ' %s '\" , pipeline_name ) return None","title":"get_pipeline()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_run","text":"Gets a specific run for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run ( self , pipeline : PipelineView , run_name : str ) -> Optional [ PipelineRunView ]: \"\"\"Gets a specific run for the given pipeline.\"\"\" run = self . store . get_context_by_type_and_name ( PIPELINE_RUN_CONTEXT_TYPE_NAME , run_name ) if not run : # No context found for the given run name return None executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): logger . debug ( \"Fetched pipeline run with name ' %s '\" , run_name ) return PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) logger . info ( \"No pipeline run found for name ' %s '\" , run_name ) return None","title":"get_pipeline_run()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_run_steps","text":"Gets all steps for the given pipeline run. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_run_steps ( self , pipeline_run : PipelineRunView ) -> Dict [ str , StepView ]: \"\"\"Gets all steps for the given pipeline run.\"\"\" steps : Dict [ str , StepView ] = OrderedDict () # reverse the executions as they get returned in reverse chronological # order from the metadata store for execution in reversed ( pipeline_run . _executions ): # noqa step = self . _get_step_view_from_execution ( execution ) steps [ step . pipeline_step_name ] = step logger . debug ( \"Fetched %d steps for pipeline run ' %s '.\" , len ( steps ), pipeline_run . name , ) return steps","title":"get_pipeline_run_steps()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipeline_runs","text":"Gets all runs for the given pipeline. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipeline_runs ( self , pipeline : PipelineView ) -> Dict [ str , PipelineRunView ]: \"\"\"Gets all runs for the given pipeline.\"\"\" all_pipeline_runs = self . store . get_contexts_by_type ( PIPELINE_RUN_CONTEXT_TYPE_NAME ) runs : Dict [ str , PipelineRunView ] = OrderedDict () for run in all_pipeline_runs : executions = self . store . get_executions_by_context ( run . id ) if self . _check_if_executions_belong_to_pipeline ( executions , pipeline ): run_view = PipelineRunView ( id_ = run . id , name = run . name , executions = executions , metadata_store = self , ) runs [ run . name ] = run_view logger . debug ( \"Fetched %d pipeline runs for pipeline named ' %s '.\" , len ( runs ), pipeline . name , ) return runs","title":"get_pipeline_runs()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_pipelines","text":"Returns a list of all pipelines stored in this metadata store. Source code in zenml/metadata_stores/base_metadata_store.py def get_pipelines ( self ) -> List [ PipelineView ]: \"\"\"Returns a list of all pipelines stored in this metadata store.\"\"\" pipelines = [] for pipeline_context in self . store . get_contexts_by_type ( PIPELINE_CONTEXT_TYPE_NAME ): pipeline = PipelineView ( id_ = pipeline_context . id , name = pipeline_context . name , metadata_store = self , ) pipelines . append ( pipeline ) logger . debug ( \"Fetched %d pipelines.\" , len ( pipelines )) return pipelines","title":"get_pipelines()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_producer_step_from_artifact","text":"Returns original StepView from an ArtifactView. Parameters: Name Type Description Default artifact ArtifactView ArtifactView to be queried. required Returns: Type Description StepView Original StepView that produced the artifact. Source code in zenml/metadata_stores/base_metadata_store.py def get_producer_step_from_artifact ( self , artifact : ArtifactView ) -> StepView : \"\"\"Returns original StepView from an ArtifactView. Args: artifact: ArtifactView to be queried. Returns: Original StepView that produced the artifact. \"\"\" executions_ids = set ( event . execution_id for event in self . store . get_events_by_artifact_ids ([ artifact . id ]) if event . type == event . OUTPUT ) execution = self . store . get_executions_by_id ( executions_ids )[ 0 ] return self . _get_step_view_from_execution ( execution )","title":"get_producer_step_from_artifact()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_artifacts","text":"Returns input and output artifacts for the given step. Parameters: Name Type Description Default step StepView The step for which to get the artifacts. required Returns: Type Description Tuple[Dict[str, zenml.post_execution.artifact.ArtifactView], Dict[str, zenml.post_execution.artifact.ArtifactView]] A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_artifacts ( self , step : StepView ) -> Tuple [ Dict [ str , ArtifactView ], Dict [ str , ArtifactView ]]: \"\"\"Returns input and output artifacts for the given step. Args: step: The step for which to get the artifacts. Returns: A tuple (inputs, outputs) where inputs and outputs are both Dicts mapping artifact names to the input and output artifacts respectively. \"\"\" # maps artifact types to their string representation artifact_type_mapping = { type_ . id : type_ . name for type_ in self . store . get_artifact_types () } events = self . store . get_events_by_execution_ids ([ step . _id ]) # noqa artifacts = self . store . get_artifacts_by_id ( [ event . artifact_id for event in events ] ) inputs : Dict [ str , ArtifactView ] = {} outputs : Dict [ str , ArtifactView ] = {} # sort them according to artifact_id's so that the zip works. events . sort ( key = lambda x : x . artifact_id ) artifacts . sort ( key = lambda x : x . id ) for event_proto , artifact_proto in zip ( events , artifacts ): artifact_type = artifact_type_mapping [ artifact_proto . type_id ] artifact_name = event_proto . path . steps [ 0 ] . key materializer = artifact_proto . properties [ MATERIALIZER_PROPERTY_KEY ] . string_value data_type = artifact_proto . properties [ DATATYPE_PROPERTY_KEY ] . string_value parent_step_id = step . id if event_proto . type == event_proto . INPUT : # In the case that this is an input event, we actually need # to resolve it via its parents outputs. for parent in step . parent_steps : for a in parent . outputs . values (): if artifact_proto . id == a . id : parent_step_id = parent . id artifact = ArtifactView ( id_ = event_proto . artifact_id , type_ = artifact_type , uri = artifact_proto . uri , materializer = materializer , data_type = data_type , metadata_store = self , parent_step_id = parent_step_id , ) if event_proto . type == event_proto . INPUT : inputs [ artifact_name ] = artifact elif event_proto . type == event_proto . OUTPUT : outputs [ artifact_name ] = artifact logger . debug ( \"Fetched %d inputs and %d outputs for step ' %s '.\" , len ( inputs ), len ( outputs ), step . name , ) return inputs , outputs","title":"get_step_artifacts()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_by_id","text":"Gets a StepView by its ID Source code in zenml/metadata_stores/base_metadata_store.py def get_step_by_id ( self , step_id : int ) -> StepView : \"\"\"Gets a `StepView` by its ID\"\"\" execution = self . store . get_executions_by_id ([ step_id ])[ 0 ] return self . _get_step_view_from_execution ( execution )","title":"get_step_by_id()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_step_status","text":"Gets the execution status of a single step. Source code in zenml/metadata_stores/base_metadata_store.py def get_step_status ( self , step : StepView ) -> ExecutionStatus : \"\"\"Gets the execution status of a single step.\"\"\" proto = self . store . get_executions_by_id ([ step . _id ])[ 0 ] # noqa state = proto . last_known_state if state == proto . COMPLETE : return ExecutionStatus . COMPLETED elif state == proto . RUNNING : return ExecutionStatus . RUNNING elif state == proto . CACHED : return ExecutionStatus . CACHED else : return ExecutionStatus . FAILED","title":"get_step_status()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.base_metadata_store.BaseMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config. Source code in zenml/metadata_stores/base_metadata_store.py @abstractmethod def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config.\"\"\" raise NotImplementedError","title":"get_tfx_metadata_config()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store","text":"","title":"mysql_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store.MySQLMetadataStore","text":"MySQL backend for ZenML metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py class MySQLMetadataStore ( BaseMetadataStore ): \"\"\"MySQL backend for ZenML metadata store.\"\"\" host : str port : int database : str username : str password : str def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , )","title":"MySQLMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.mysql_metadata_store.MySQLMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config for mysql metadata store. Source code in zenml/metadata_stores/mysql_metadata_store.py def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for mysql metadata store.\"\"\" return metadata . mysql_metadata_connection_config ( host = self . host , port = self . port , database = self . database , username = self . username , password = self . password , )","title":"get_tfx_metadata_config()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store","text":"","title":"sqlite_metadata_store"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore","text":"SQLite backend for ZenML metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py class SQLiteMetadataStore ( BaseMetadataStore ): \"\"\"SQLite backend for ZenML metadata store.\"\"\" uri : str def __init__ ( self , ** data : Any ): \"\"\"Constructor for MySQL MetadataStore for ZenML.\"\"\" super () . __init__ ( ** data ) # TODO [ENG-131]: Replace with proper custom validator. if fileio . is_remote ( self . uri ): raise Exception ( f \"URI { self . uri } is a non-local path. A sqlite store \" f \"can only be local paths\" ) # Resolve URI if relative URI provided # self.uri = fileio.resolve_relative_path(uri) def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri ) @validator ( \"uri\" ) def uri_must_be_local ( cls , v : str ) -> str : \"\"\"Validator to ensure uri is local\"\"\" if fileio . is_remote ( v ): raise ValueError ( f \"URI { v } is a non-local path. A sqlite store \" f \"can only be local paths\" ) return v","title":"SQLiteMetadataStore"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.__init__","text":"Constructor for MySQL MetadataStore for ZenML. Source code in zenml/metadata_stores/sqlite_metadata_store.py def __init__ ( self , ** data : Any ): \"\"\"Constructor for MySQL MetadataStore for ZenML.\"\"\" super () . __init__ ( ** data ) # TODO [ENG-131]: Replace with proper custom validator. if fileio . is_remote ( self . uri ): raise Exception ( f \"URI { self . uri } is a non-local path. A sqlite store \" f \"can only be local paths\" ) # Resolve URI if relative URI provided # self.uri = fileio.resolve_relative_path(uri)","title":"__init__()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.get_tfx_metadata_config","text":"Return tfx metadata config for sqlite metadata store. Source code in zenml/metadata_stores/sqlite_metadata_store.py def get_tfx_metadata_config ( self ) -> metadata_store_pb2 . ConnectionConfig : \"\"\"Return tfx metadata config for sqlite metadata store.\"\"\" return metadata . sqlite_metadata_connection_config ( self . uri )","title":"get_tfx_metadata_config()"},{"location":"api_docs/metadata_stores/#zenml.metadata_stores.sqlite_metadata_store.SQLiteMetadataStore.uri_must_be_local","text":"Validator to ensure uri is local Source code in zenml/metadata_stores/sqlite_metadata_store.py @validator ( \"uri\" ) def uri_must_be_local ( cls , v : str ) -> str : \"\"\"Validator to ensure uri is local\"\"\" if fileio . is_remote ( v ): raise ValueError ( f \"URI { v } is a non-local path. A sqlite store \" f \"can only be local paths\" ) return v","title":"uri_must_be_local()"},{"location":"api_docs/orchestrators/","text":"Orchestrators zenml.orchestrators special An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline. base_orchestrator BaseOrchestrator ( BaseComponent ) pydantic-model Base Orchestrator class to orchestrate ZenML pipelines. Source code in zenml/orchestrators/base_orchestrator.py class BaseOrchestrator ( BaseComponent ): \"\"\"Base Orchestrator class to orchestrate ZenML pipelines.\"\"\" _ORCHESTRATOR_STORE_DIR_NAME : str = \"orchestrators\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseOrchestrator instance. Args: repo_path: Path to the repository of this orchestrator. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ORCHESTRATOR_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @abstractmethod def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any ) -> Any : \"\"\"Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Potential additional parameters used in subclass implementations. \"\"\" raise NotImplementedError @property @abstractmethod def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is currently running.\"\"\" @property def log_file ( self ) -> Optional [ str ]: \"\"\"Returns path to a log file if available.\"\"\" # TODO [ENG-136]: make this more generic in case an orchestrator has # multiple log files, e.g. change to a monitor() method which yields # new logs to output to the CLI return None def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Should be run before the `run()` function to prepare orchestrator. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This is necessary for airflow so we know the file in which the DAG is defined. \"\"\" def post_run ( self ) -> None : \"\"\"Should be run after the `run()` to clean up.\"\"\" def up ( self ) -> None : \"\"\"Provisions resources for the orchestrator.\"\"\" def down ( self ) -> None : \"\"\"Destroys resources for the orchestrator.\"\"\" class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_orchestrator_\" is_running : bool property readonly Returns whether the orchestrator is currently running. log_file : Optional [ str ] property readonly Returns path to a log file if available. Config Configuration of settings. Source code in zenml/orchestrators/base_orchestrator.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_orchestrator_\" __init__ ( self , repo_path , ** kwargs ) special Initializes a BaseOrchestrator instance. Parameters: Name Type Description Default repo_path str Path to the repository of this orchestrator. required Source code in zenml/orchestrators/base_orchestrator.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseOrchestrator instance. Args: repo_path: Path to the repository of this orchestrator. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ORCHESTRATOR_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) down ( self ) Destroys resources for the orchestrator. Source code in zenml/orchestrators/base_orchestrator.py def down ( self ) -> None : \"\"\"Destroys resources for the orchestrator.\"\"\" post_run ( self ) Should be run after the run() to clean up. Source code in zenml/orchestrators/base_orchestrator.py def post_run ( self ) -> None : \"\"\"Should be run after the `run()` to clean up.\"\"\" pre_run ( self , pipeline , caller_filepath ) Should be run before the run() function to prepare orchestrator. Parameters: Name Type Description Default pipeline BasePipeline Pipeline that will be run. required caller_filepath str Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined. required Source code in zenml/orchestrators/base_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Should be run before the `run()` function to prepare orchestrator. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This is necessary for airflow so we know the file in which the DAG is defined. \"\"\" run ( self , zenml_pipeline , run_name , ** kwargs ) Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Potential additional parameters used in subclass implementations. {} Source code in zenml/orchestrators/base_orchestrator.py @abstractmethod def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any ) -> Any : \"\"\"Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Potential additional parameters used in subclass implementations. \"\"\" raise NotImplementedError up ( self ) Provisions resources for the orchestrator. Source code in zenml/orchestrators/base_orchestrator.py def up ( self ) -> None : \"\"\"Provisions resources for the orchestrator.\"\"\" local special local_dag_runner Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py LocalDagRunner ( TfxRunner ) Local TFX DAG runner. Source code in zenml/orchestrators/local/local_dag_runner.py class LocalDagRunner ( tfx_runner . TfxRunner ): \"\"\"Local TFX DAG runner.\"\"\" def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher ) __init__ ( self ) special Initializes LocalDagRunner as a TFX orchestrator. Source code in zenml/orchestrators/local/local_dag_runner.py def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" run ( self , pipeline , run_name = '' ) Runs given logical pipeline locally. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and components. required run_name str Name of the pipeline run. '' Source code in zenml/orchestrators/local/local_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher ) local_orchestrator LocalOrchestrator ( BaseOrchestrator ) pydantic-model Orchestrator responsible for running pipelines locally. Source code in zenml/orchestrators/local/local_orchestrator.py class LocalOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines locally.\"\"\" _is_running : bool = PrivateAttr ( default = False ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is currently running.\"\"\" return self . _is_running def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** pipeline_args : Any ) -> None : \"\"\"Runs a pipeline locally. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **pipeline_args: Unused kwargs to conform with base signature. \"\"\" self . _is_running = True runner = LocalDagRunner () tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline , run_name ) self . _is_running = False is_running : bool property readonly Returns whether the orchestrator is currently running. run ( self , zenml_pipeline , run_name , ** pipeline_args ) Runs a pipeline locally. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **pipeline_args Any Unused kwargs to conform with base signature. {} Source code in zenml/orchestrators/local/local_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** pipeline_args : Any ) -> None : \"\"\"Runs a pipeline locally. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **pipeline_args: Unused kwargs to conform with base signature. \"\"\" self . _is_running = True runner = LocalDagRunner () tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline , run_name ) self . _is_running = False utils create_tfx_pipeline ( zenml_pipeline ) Creates a tfx pipeline from a ZenML pipeline. Source code in zenml/orchestrators/utils.py def create_tfx_pipeline ( zenml_pipeline : \"BasePipeline\" , ) -> tfx_pipeline . Pipeline : \"\"\"Creates a tfx pipeline from a ZenML pipeline.\"\"\" # Connect the inputs/outputs of all steps in the pipeline zenml_pipeline . connect ( ** zenml_pipeline . steps ) tfx_components = [ step . component for step in zenml_pipeline . steps . values ()] artifact_store = zenml_pipeline . stack . artifact_store metadata_store = zenml_pipeline . stack . metadata_store return tfx_pipeline . Pipeline ( pipeline_name = zenml_pipeline . name , components = tfx_components , # type: ignore[arg-type] pipeline_root = artifact_store . path , metadata_connection_config = metadata_store . get_tfx_metadata_config (), enable_cache = zenml_pipeline . enable_cache , ) execute_step ( tfx_launcher ) Executes a tfx component. Parameters: Name Type Description Default tfx_launcher Launcher A tfx launcher to execute the component. required Returns: Type Description Optional[tfx.orchestration.portable.data_types.ExecutionInfo] Optional execution info returned by the launcher. Source code in zenml/orchestrators/utils.py def execute_step ( tfx_launcher : launcher . Launcher , ) -> Optional [ data_types . ExecutionInfo ]: \"\"\"Executes a tfx component. Args: tfx_launcher: A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. \"\"\" step_name = tfx_launcher . _pipeline_node . node_info . id # type: ignore[attr-defined] # noqa start_time = time . time () logger . info ( f \"Step ` { step_name } ` has started.\" ) try : execution_info = tfx_launcher . launch () except RuntimeError as e : if \"execution has already succeeded\" in str ( e ): # Hacky workaround to catch the error that a pipeline run with # this name already exists. Raise an error with a more descriptive # message instead. raise DuplicateRunNameError () else : raise run_duration = time . time () - start_time logger . info ( \"Step ` %s ` has finished in %s .\" , step_name , string_utils . get_human_readable_time ( run_duration ), ) return execution_info","title":"Orchestrators"},{"location":"api_docs/orchestrators/#orchestrators","text":"","title":"Orchestrators"},{"location":"api_docs/orchestrators/#zenml.orchestrators","text":"An orchestrator is a special kind of backend that manages the running of each step of the pipeline. Orchestrators administer the actual pipeline runs. You can think of it as the 'root' of any pipeline job that you run during your experimentation. ZenML supports a local orchestrator out of the box which allows you to run your pipelines in a local environment. We also support using Apache Airflow as the orchestrator to handle the steps of your pipeline.","title":"orchestrators"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator","text":"","title":"base_orchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator","text":"Base Orchestrator class to orchestrate ZenML pipelines. Source code in zenml/orchestrators/base_orchestrator.py class BaseOrchestrator ( BaseComponent ): \"\"\"Base Orchestrator class to orchestrate ZenML pipelines.\"\"\" _ORCHESTRATOR_STORE_DIR_NAME : str = \"orchestrators\" def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseOrchestrator instance. Args: repo_path: Path to the repository of this orchestrator. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ORCHESTRATOR_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs ) @abstractmethod def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any ) -> Any : \"\"\"Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Potential additional parameters used in subclass implementations. \"\"\" raise NotImplementedError @property @abstractmethod def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is currently running.\"\"\" @property def log_file ( self ) -> Optional [ str ]: \"\"\"Returns path to a log file if available.\"\"\" # TODO [ENG-136]: make this more generic in case an orchestrator has # multiple log files, e.g. change to a monitor() method which yields # new logs to output to the CLI return None def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Should be run before the `run()` function to prepare orchestrator. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This is necessary for airflow so we know the file in which the DAG is defined. \"\"\" def post_run ( self ) -> None : \"\"\"Should be run after the `run()` to clean up.\"\"\" def up ( self ) -> None : \"\"\"Provisions resources for the orchestrator.\"\"\" def down ( self ) -> None : \"\"\"Destroys resources for the orchestrator.\"\"\" class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_orchestrator_\"","title":"BaseOrchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.is_running","text":"Returns whether the orchestrator is currently running.","title":"is_running"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.log_file","text":"Returns path to a log file if available.","title":"log_file"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.Config","text":"Configuration of settings. Source code in zenml/orchestrators/base_orchestrator.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_orchestrator_\"","title":"Config"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.__init__","text":"Initializes a BaseOrchestrator instance. Parameters: Name Type Description Default repo_path str Path to the repository of this orchestrator. required Source code in zenml/orchestrators/base_orchestrator.py def __init__ ( self , repo_path : str , ** kwargs : Any ) -> None : \"\"\"Initializes a BaseOrchestrator instance. Args: repo_path: Path to the repository of this orchestrator. \"\"\" serialization_dir = os . path . join ( get_zenml_config_dir ( repo_path ), self . _ORCHESTRATOR_STORE_DIR_NAME , ) super () . __init__ ( serialization_dir = serialization_dir , ** kwargs )","title":"__init__()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.down","text":"Destroys resources for the orchestrator. Source code in zenml/orchestrators/base_orchestrator.py def down ( self ) -> None : \"\"\"Destroys resources for the orchestrator.\"\"\"","title":"down()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.post_run","text":"Should be run after the run() to clean up. Source code in zenml/orchestrators/base_orchestrator.py def post_run ( self ) -> None : \"\"\"Should be run after the `run()` to clean up.\"\"\"","title":"post_run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.pre_run","text":"Should be run before the run() function to prepare orchestrator. Parameters: Name Type Description Default pipeline BasePipeline Pipeline that will be run. required caller_filepath str Path to the file in which pipeline.run() was called. This is necessary for airflow so we know the file in which the DAG is defined. required Source code in zenml/orchestrators/base_orchestrator.py def pre_run ( self , pipeline : \"BasePipeline\" , caller_filepath : str ) -> None : \"\"\"Should be run before the `run()` function to prepare orchestrator. Args: pipeline: Pipeline that will be run. caller_filepath: Path to the file in which `pipeline.run()` was called. This is necessary for airflow so we know the file in which the DAG is defined. \"\"\"","title":"pre_run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.run","text":"Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **kwargs Any Potential additional parameters used in subclass implementations. {} Source code in zenml/orchestrators/base_orchestrator.py @abstractmethod def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** kwargs : Any ) -> Any : \"\"\"Abstract method to run a pipeline. Overwrite this in subclasses with a concrete implementation on how to run the given pipeline. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **kwargs: Potential additional parameters used in subclass implementations. \"\"\" raise NotImplementedError","title":"run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.base_orchestrator.BaseOrchestrator.up","text":"Provisions resources for the orchestrator. Source code in zenml/orchestrators/base_orchestrator.py def up ( self ) -> None : \"\"\"Provisions resources for the orchestrator.\"\"\"","title":"up()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local","text":"","title":"local"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner","text":"Inspired by local dag runner implementation by Google at: https://github.com/tensorflow/tfx/blob/master/tfx/orchestration /local/local_dag_runner.py","title":"local_dag_runner"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner","text":"Local TFX DAG runner. Source code in zenml/orchestrators/local/local_dag_runner.py class LocalDagRunner ( tfx_runner . TfxRunner ): \"\"\"Local TFX DAG runner.\"\"\" def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\" def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher )","title":"LocalDagRunner"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner.__init__","text":"Initializes LocalDagRunner as a TFX orchestrator. Source code in zenml/orchestrators/local/local_dag_runner.py def __init__ ( self ) -> None : \"\"\"Initializes LocalDagRunner as a TFX orchestrator.\"\"\"","title":"__init__()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_dag_runner.LocalDagRunner.run","text":"Runs given logical pipeline locally. Parameters: Name Type Description Default pipeline Pipeline Logical pipeline containing pipeline args and components. required run_name str Name of the pipeline run. '' Source code in zenml/orchestrators/local/local_dag_runner.py def run ( self , pipeline : tfx_pipeline . Pipeline , run_name : str = \"\" ) -> None : \"\"\"Runs given logical pipeline locally. Args: pipeline: Logical pipeline containing pipeline args and components. run_name: Name of the pipeline run. \"\"\" for component in pipeline . components : if isinstance ( component , base_component . BaseComponent ): component . _resolve_pip_dependencies ( pipeline . pipeline_info . pipeline_root ) c = compiler . Compiler () pipeline = c . compile ( pipeline ) # Substitute the runtime parameter to be a concrete run_id runtime_parameter_utils . substitute_runtime_parameter ( pipeline , { PIPELINE_RUN_ID_PARAMETER_NAME : run_name , }, ) deployment_config = runner_utils . extract_local_deployment_config ( pipeline ) connection_config = deployment_config . metadata_connection_config # type: ignore[attr-defined] # noqa logger . debug ( f \"Using deployment config: \\n { deployment_config } \" ) logger . debug ( f \"Using connection config: \\n { connection_config } \" ) # Run each component. Note that the pipeline.components list is in # topological order. for node in pipeline . nodes : pipeline_node = node . pipeline_node node_id = pipeline_node . node_info . id executor_spec = runner_utils . extract_executor_spec ( deployment_config , node_id ) custom_driver_spec = runner_utils . extract_custom_driver_spec ( deployment_config , node_id ) component_launcher = launcher . Launcher ( pipeline_node = pipeline_node , mlmd_connection = metadata . Metadata ( connection_config ), pipeline_info = pipeline . pipeline_info , pipeline_runtime_spec = pipeline . runtime_spec , executor_spec = executor_spec , custom_driver_spec = custom_driver_spec , ) execute_step ( component_launcher )","title":"run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator","text":"","title":"local_orchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator","text":"Orchestrator responsible for running pipelines locally. Source code in zenml/orchestrators/local/local_orchestrator.py class LocalOrchestrator ( BaseOrchestrator ): \"\"\"Orchestrator responsible for running pipelines locally.\"\"\" _is_running : bool = PrivateAttr ( default = False ) @property def is_running ( self ) -> bool : \"\"\"Returns whether the orchestrator is currently running.\"\"\" return self . _is_running def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** pipeline_args : Any ) -> None : \"\"\"Runs a pipeline locally. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **pipeline_args: Unused kwargs to conform with base signature. \"\"\" self . _is_running = True runner = LocalDagRunner () tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline , run_name ) self . _is_running = False","title":"LocalOrchestrator"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator.is_running","text":"Returns whether the orchestrator is currently running.","title":"is_running"},{"location":"api_docs/orchestrators/#zenml.orchestrators.local.local_orchestrator.LocalOrchestrator.run","text":"Runs a pipeline locally. Parameters: Name Type Description Default zenml_pipeline BasePipeline The pipeline to run. required run_name str Name of the pipeline run. required **pipeline_args Any Unused kwargs to conform with base signature. {} Source code in zenml/orchestrators/local/local_orchestrator.py def run ( self , zenml_pipeline : \"BasePipeline\" , run_name : str , ** pipeline_args : Any ) -> None : \"\"\"Runs a pipeline locally. Args: zenml_pipeline: The pipeline to run. run_name: Name of the pipeline run. **pipeline_args: Unused kwargs to conform with base signature. \"\"\" self . _is_running = True runner = LocalDagRunner () tfx_pipeline = create_tfx_pipeline ( zenml_pipeline ) runner . run ( tfx_pipeline , run_name ) self . _is_running = False","title":"run()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils","text":"","title":"utils"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils.create_tfx_pipeline","text":"Creates a tfx pipeline from a ZenML pipeline. Source code in zenml/orchestrators/utils.py def create_tfx_pipeline ( zenml_pipeline : \"BasePipeline\" , ) -> tfx_pipeline . Pipeline : \"\"\"Creates a tfx pipeline from a ZenML pipeline.\"\"\" # Connect the inputs/outputs of all steps in the pipeline zenml_pipeline . connect ( ** zenml_pipeline . steps ) tfx_components = [ step . component for step in zenml_pipeline . steps . values ()] artifact_store = zenml_pipeline . stack . artifact_store metadata_store = zenml_pipeline . stack . metadata_store return tfx_pipeline . Pipeline ( pipeline_name = zenml_pipeline . name , components = tfx_components , # type: ignore[arg-type] pipeline_root = artifact_store . path , metadata_connection_config = metadata_store . get_tfx_metadata_config (), enable_cache = zenml_pipeline . enable_cache , )","title":"create_tfx_pipeline()"},{"location":"api_docs/orchestrators/#zenml.orchestrators.utils.execute_step","text":"Executes a tfx component. Parameters: Name Type Description Default tfx_launcher Launcher A tfx launcher to execute the component. required Returns: Type Description Optional[tfx.orchestration.portable.data_types.ExecutionInfo] Optional execution info returned by the launcher. Source code in zenml/orchestrators/utils.py def execute_step ( tfx_launcher : launcher . Launcher , ) -> Optional [ data_types . ExecutionInfo ]: \"\"\"Executes a tfx component. Args: tfx_launcher: A tfx launcher to execute the component. Returns: Optional execution info returned by the launcher. \"\"\" step_name = tfx_launcher . _pipeline_node . node_info . id # type: ignore[attr-defined] # noqa start_time = time . time () logger . info ( f \"Step ` { step_name } ` has started.\" ) try : execution_info = tfx_launcher . launch () except RuntimeError as e : if \"execution has already succeeded\" in str ( e ): # Hacky workaround to catch the error that a pipeline run with # this name already exists. Raise an error with a more descriptive # message instead. raise DuplicateRunNameError () else : raise run_duration = time . time () - start_time logger . info ( \"Step ` %s ` has finished in %s .\" , step_name , string_utils . get_human_readable_time ( run_duration ), ) return execution_info","title":"execute_step()"},{"location":"api_docs/pipelines/","text":"Pipelines zenml.pipelines special A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator. base_pipeline BasePipeline Abstract base class for all ZenML pipelines. Attributes: Name Type Description name The name of this pipeline. enable_cache A boolean indicating if caching is enabled for this pipeline. requirements_file Optional path to a pip requirements file that contains all requirements to run the pipeline. Source code in zenml/pipelines/base_pipeline.py class BasePipeline ( metaclass = BasePipelineMeta ): \"\"\"Abstract base class for all ZenML pipelines. Attributes: name: The name of this pipeline. enable_cache: A boolean indicating if caching is enabled for this pipeline. requirements_file: Optional path to a pip requirements file that contains all requirements to run the pipeline. \"\"\" STEP_SPEC : ClassVar [ Dict [ str , Any ]] = None # type: ignore[assignment] INSTANCE_CONFIGURATION : Dict [ Text , Any ] = {} def __init__ ( self , * args : BaseStep , ** kwargs : Any ) -> None : try : self . __stack = Repository () . get_active_stack () except DoesNotExistException as exc : raise DoesNotExistException ( \"Could not retrieve any active stack. Make sure to set a \" \"stack active via `zenml stack set STACK_NAME`\" ) from exc kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , True ) self . requirements_file = kwargs . pop ( PARAM_REQUIREMENTS_FILE , None ) self . dockerignore_file = kwargs . pop ( PARAM_DOCKERIGNORE_FILE , None ) self . name = self . __class__ . __name__ logger . info ( \"Creating run for pipeline: ` %s `\" , self . name ) logger . info ( f 'Cache { \"enabled\" if self . enable_cache else \"disabled\" } for ' f \"pipeline ` { self . name } `\" ) self . __steps : Dict [ str , BaseStep ] = {} self . _verify_arguments ( * args , ** kwargs ) def _verify_arguments ( self , * steps : BaseStep , ** kw_steps : BaseStep ) -> None : \"\"\"Verifies the initialization args and kwargs of this pipeline. This method makes sure that no missing/unexpected arguments or arguments of a wrong type are passed when creating a pipeline. If all arguments are correct, saves the steps to `self.__steps`. Args: *steps: The args passed to the init method of this pipeline. **kw_steps: The kwargs passed to the init method of this pipeline. Raises: PipelineInterfaceError: If there are too many/few arguments or arguments with a wrong name/type. \"\"\" input_step_keys = list ( self . STEP_SPEC . keys ()) if len ( steps ) > len ( input_step_keys ): raise PipelineInterfaceError ( f \"Too many input steps for pipeline ' { self . name } '. \" f \"This pipeline expects { len ( input_step_keys ) } step(s) \" f \"but got { len ( steps ) + len ( kw_steps ) } .\" ) combined_steps = {} step_cls_args : Set [ Type [ BaseStep ]] = set () for i , step in enumerate ( steps ): step_class = type ( step ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for positional \" f \"argument { i } of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) key = input_step_keys [ i ] step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_args . add ( step_class ) step_cls_kwargs : Dict [ Type [ BaseStep ], str ] = {} for key , step in kw_steps . items (): step_class = type ( step ) if key in combined_steps : # a step for this key was already set by # the positional input steps raise PipelineInterfaceError ( f \"Unexpected keyword argument ' { key } ' for pipeline \" f \"' { self . name } '. A step for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for argument \" f \"' { key } ' of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_kwargs : prev_key = step_cls_kwargs [ step_class ] raise PipelineInterfaceError ( f \"Same step object (` { step_class } `) passed for arguments \" f \"' { key } ' and ' { prev_key } '. Step objects should be \" f \"unique for each argument.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_kwargs [ step_class ] = key # check if there are any missing or unexpected steps expected_steps = set ( self . STEP_SPEC . keys ()) actual_steps = set ( combined_steps . keys ()) missing_steps = expected_steps - actual_steps unexpected_steps = actual_steps - expected_steps if missing_steps : raise PipelineInterfaceError ( f \"Missing input step(s) for pipeline \" f \"' { self . name } ': { missing_steps } .\" ) if unexpected_steps : raise PipelineInterfaceError ( f \"Unexpected input step(s) for pipeline \" f \"' { self . name } ': { unexpected_steps } . This pipeline \" f \"only requires the following steps: { expected_steps } .\" ) self . __steps = combined_steps @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError @property def stack ( self ) -> BaseStack : \"\"\"Returns the stack for this pipeline.\"\"\" return self . __stack @stack . setter def stack ( self , stack : BaseStack ) -> NoReturn : \"\"\"Setting the stack property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"The stack will be automatically inferred from your environment. \" \"Please do no attempt to manually change it.\" ) @property def steps ( self ) -> Dict [ str , BaseStep ]: \"\"\"Returns a dictionary of pipeline steps.\"\"\" return self . __steps @steps . setter def steps ( self , steps : Dict [ str , BaseStep ]) -> NoReturn : \"\"\"Setting the steps property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"Cannot set steps manually!\" ) def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name: Optional name for the run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () if not run_name : run_name = ( f \" { self . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"stack_type\" : self . stack . stack_type , \"total_steps\" : len ( self . steps ), }, ) start_time = time . time () logger . info ( \"Using stack ` %s ` for pipeline ` %s `. Running pipeline..\" , Repository () . get_active_stack_key (), self . name , ) # filepath of the file where pipeline.run() was called caller_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) self . stack . orchestrator . pre_run ( pipeline = self , caller_filepath = caller_filepath ) ret = self . stack . orchestrator . run ( self , run_name = run_name ) self . stack . orchestrator . post_run () run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) return ret def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self def _read_config_steps ( self , steps : Dict [ str , Dict [ str , Any ]], overwrite : bool = False ) -> None : \"\"\"Reads and sets step parameters from a config file. Args: steps: Maps step names to dicts of parameter names and values. overwrite: If `True`, overwrite previously set step parameters. \"\"\" for step_name , step_dict in steps . items (): StepConfigurationKeys . key_check ( step_dict ) if step_name not in self . __steps : raise PipelineConfigurationError ( f \"Found ' { step_name } ' step in configuration yaml but it \" f \"doesn't exist in the pipeline steps \" f \" { list ( self . __steps . keys ()) } .\" ) step = self . __steps [ step_name ] step_parameters = ( step . CONFIG_CLASS . __fields__ . keys () if step . CONFIG_CLASS else {} ) parameters = step_dict . get ( StepConfigurationKeys . PARAMETERS_ , {}) for parameter , value in parameters . items (): if parameter not in step_parameters : raise PipelineConfigurationError ( f \"Found parameter ' { parameter } ' for ' { step_name } ' step \" f \"in configuration yaml but it doesn't exist in the \" f \"configuration class ` { step . CONFIG_CLASS } `. Available \" f \"parameters for this step: \" f \" { list ( step_parameters ) } .\" ) previous_value = step . PARAM_SPEC . get ( parameter , None ) if overwrite : step . PARAM_SPEC [ parameter ] = value else : step . PARAM_SPEC . setdefault ( parameter , value ) if overwrite or not previous_value : logger . debug ( \"Setting parameter %s = %s for step ' %s '.\" , parameter , value , step_name , ) if previous_value and not overwrite : logger . warning ( \"Parameter ' %s ' from configuration yaml will NOT be \" \"set as a configuration object was given when \" \"creating the step. Set `overwrite_step_parameters=\" \"True` when setting the configuration yaml to always \" \"use the options specified in the yaml file.\" , parameter , ) stack : BaseStack property writable Returns the stack for this pipeline. steps : Dict [ str , zenml . steps . base_step . BaseStep ] property writable Returns a dictionary of pipeline steps. connect ( self , * args , ** kwargs ) Function that connects inputs and outputs of the pipeline steps. Source code in zenml/pipelines/base_pipeline.py @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError run ( self , run_name = None ) Runs the pipeline using the orchestrator of the pipeline stack. Parameters: Name Type Description Default run_name Optional[str] Optional name for the run. None Source code in zenml/pipelines/base_pipeline.py def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name: Optional name for the run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () if not run_name : run_name = ( f \" { self . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"stack_type\" : self . stack . stack_type , \"total_steps\" : len ( self . steps ), }, ) start_time = time . time () logger . info ( \"Using stack ` %s ` for pipeline ` %s `. Running pipeline..\" , Repository () . get_active_stack_key (), self . name , ) # filepath of the file where pipeline.run() was called caller_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) self . stack . orchestrator . pre_run ( pipeline = self , caller_filepath = caller_filepath ) ret = self . stack . orchestrator . run ( self , run_name = run_name ) self . stack . orchestrator . post_run () run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) return ret with_config ( self , config_file , overwrite_step_parameters = False ) Configures this pipeline using a yaml file. Parameters: Name Type Description Default config_file str Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. required overwrite_step_parameters bool If set to True , values from the configuration file will overwrite configuration parameters passed in code. False Returns: Type Description ~T The pipeline object that this method was called on. Source code in zenml/pipelines/base_pipeline.py def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self BasePipelineMeta ( type ) Pipeline Metaclass responsible for validating the pipeline definition. Source code in zenml/pipelines/base_pipeline.py class BasePipelineMeta ( type ): \"\"\"Pipeline Metaclass responsible for validating the pipeline definition.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Saves argument names for later verification purposes Source code in zenml/pipelines/base_pipeline.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls builtin_pipelines special training_pipeline TrainingPipeline ( BasePipeline ) Class for the classic training pipeline implementation Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py class TrainingPipeline ( BasePipeline ): \"\"\"Class for the classic training pipeline implementation\"\"\" def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore connect ( self , datasource , splitter , analyzer , preprocessor , trainer , evaluator ) Main connect method for the standard training pipelines Parameters: Name Type Description Default datasource BaseDatasourceStep the step responsible for the data ingestion required splitter BaseSplitStep the step responsible for splitting the dataset into train, test, val required analyzer BaseAnalyzerStep the step responsible for extracting the statistics and the schema required preprocessor BasePreprocessorStep the step responsible for preprocessing the data required trainer BaseTrainerStep the step responsible for training a model required evaluator BaseEvaluatorStep the step responsible for computing the evaluation of the trained model required Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore pipeline_decorator pipeline ( _func = None , * , name = None , enable_cache = True , requirements_file = None , dockerignore_file = None ) Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. None enable_cache bool Whether to use caching or not. True requirements_file Optional[str] Optional path to a pip requirements file that contains requirements to run the pipeline. None dockerignore_file Optional[str] Optional path to a dockerignore file to use when building docker images for running this pipeline. Note : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. None Returns: Type Description Union[Type[zenml.pipelines.base_pipeline.BasePipeline], Callable[[~F], Type[zenml.pipelines.base_pipeline.BasePipeline]]] the inner decorator which creates the pipeline class based on the ZenML BasePipeline Source code in zenml/pipelines/pipeline_decorator.py def pipeline ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : bool = True , requirements_file : Optional [ str ] = None , dockerignore_file : Optional [ str ] = None , ) -> Union [ Type [ BasePipeline ], Callable [[ F ], Type [ BasePipeline ]]]: \"\"\"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func: The decorated function. name: The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Whether to use caching or not. requirements_file: Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file: Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note**: If you pass a file, make sure it does not include the `.zen` directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline \"\"\" def inner_decorator ( func : F ) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline Args: func: types.FunctionType, this function will be used as the \"connect\" method of the generated Pipeline Returns: the class of a newly generated ZenML Pipeline \"\"\" return type ( # noqa name if name else func . __name__ , ( BasePipeline ,), { PIPELINE_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_REQUIREMENTS_FILE : requirements_file , PARAM_DOCKERIGNORE_FILE : dockerignore_file , }, }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"Pipelines"},{"location":"api_docs/pipelines/#pipelines","text":"","title":"Pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines","text":"A ZenML pipeline is a sequence of tasks that execute in a specific order and yield artifacts. The artifacts are stored within the artifact store and indexed via the metadata store. Each individual task within a pipeline is known as a step. The standard pipelines within ZenML are designed to have easy interfaces to add pre-decided steps, with the order also pre-decided. Other sorts of pipelines can be created as well from scratch, building on the BasePipeline class. Pipelines can be written as simple functions. They are created by using decorators appropriate to the specific use case you have. The moment it is run , a pipeline is compiled and passed directly to the orchestrator.","title":"pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline","text":"","title":"base_pipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline","text":"Abstract base class for all ZenML pipelines. Attributes: Name Type Description name The name of this pipeline. enable_cache A boolean indicating if caching is enabled for this pipeline. requirements_file Optional path to a pip requirements file that contains all requirements to run the pipeline. Source code in zenml/pipelines/base_pipeline.py class BasePipeline ( metaclass = BasePipelineMeta ): \"\"\"Abstract base class for all ZenML pipelines. Attributes: name: The name of this pipeline. enable_cache: A boolean indicating if caching is enabled for this pipeline. requirements_file: Optional path to a pip requirements file that contains all requirements to run the pipeline. \"\"\" STEP_SPEC : ClassVar [ Dict [ str , Any ]] = None # type: ignore[assignment] INSTANCE_CONFIGURATION : Dict [ Text , Any ] = {} def __init__ ( self , * args : BaseStep , ** kwargs : Any ) -> None : try : self . __stack = Repository () . get_active_stack () except DoesNotExistException as exc : raise DoesNotExistException ( \"Could not retrieve any active stack. Make sure to set a \" \"stack active via `zenml stack set STACK_NAME`\" ) from exc kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , True ) self . requirements_file = kwargs . pop ( PARAM_REQUIREMENTS_FILE , None ) self . dockerignore_file = kwargs . pop ( PARAM_DOCKERIGNORE_FILE , None ) self . name = self . __class__ . __name__ logger . info ( \"Creating run for pipeline: ` %s `\" , self . name ) logger . info ( f 'Cache { \"enabled\" if self . enable_cache else \"disabled\" } for ' f \"pipeline ` { self . name } `\" ) self . __steps : Dict [ str , BaseStep ] = {} self . _verify_arguments ( * args , ** kwargs ) def _verify_arguments ( self , * steps : BaseStep , ** kw_steps : BaseStep ) -> None : \"\"\"Verifies the initialization args and kwargs of this pipeline. This method makes sure that no missing/unexpected arguments or arguments of a wrong type are passed when creating a pipeline. If all arguments are correct, saves the steps to `self.__steps`. Args: *steps: The args passed to the init method of this pipeline. **kw_steps: The kwargs passed to the init method of this pipeline. Raises: PipelineInterfaceError: If there are too many/few arguments or arguments with a wrong name/type. \"\"\" input_step_keys = list ( self . STEP_SPEC . keys ()) if len ( steps ) > len ( input_step_keys ): raise PipelineInterfaceError ( f \"Too many input steps for pipeline ' { self . name } '. \" f \"This pipeline expects { len ( input_step_keys ) } step(s) \" f \"but got { len ( steps ) + len ( kw_steps ) } .\" ) combined_steps = {} step_cls_args : Set [ Type [ BaseStep ]] = set () for i , step in enumerate ( steps ): step_class = type ( step ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for positional \" f \"argument { i } of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) key = input_step_keys [ i ] step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_args . add ( step_class ) step_cls_kwargs : Dict [ Type [ BaseStep ], str ] = {} for key , step in kw_steps . items (): step_class = type ( step ) if key in combined_steps : # a step for this key was already set by # the positional input steps raise PipelineInterfaceError ( f \"Unexpected keyword argument ' { key } ' for pipeline \" f \"' { self . name } '. A step for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( step , BaseStep ): raise PipelineInterfaceError ( f \"Wrong argument type (` { step_class } `) for argument \" f \"' { key } ' of pipeline ' { self . name } '. Only \" f \"`@step` decorated functions or instances of `BaseStep` \" f \"subclasses can be used as arguments when creating \" f \"a pipeline.\" ) if step_class in step_cls_kwargs : prev_key = step_cls_kwargs [ step_class ] raise PipelineInterfaceError ( f \"Same step object (` { step_class } `) passed for arguments \" f \"' { key } ' and ' { prev_key } '. Step objects should be \" f \"unique for each argument.\" ) if step_class in step_cls_args : raise PipelineInterfaceError ( f \"Step object (` { step_class } `) has been used twice. Step \" f \"objects should be unique for each argument.\" ) step . pipeline_parameter_name = key combined_steps [ key ] = step step_cls_kwargs [ step_class ] = key # check if there are any missing or unexpected steps expected_steps = set ( self . STEP_SPEC . keys ()) actual_steps = set ( combined_steps . keys ()) missing_steps = expected_steps - actual_steps unexpected_steps = actual_steps - expected_steps if missing_steps : raise PipelineInterfaceError ( f \"Missing input step(s) for pipeline \" f \"' { self . name } ': { missing_steps } .\" ) if unexpected_steps : raise PipelineInterfaceError ( f \"Unexpected input step(s) for pipeline \" f \"' { self . name } ': { unexpected_steps } . This pipeline \" f \"only requires the following steps: { expected_steps } .\" ) self . __steps = combined_steps @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError @property def stack ( self ) -> BaseStack : \"\"\"Returns the stack for this pipeline.\"\"\" return self . __stack @stack . setter def stack ( self , stack : BaseStack ) -> NoReturn : \"\"\"Setting the stack property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"The stack will be automatically inferred from your environment. \" \"Please do no attempt to manually change it.\" ) @property def steps ( self ) -> Dict [ str , BaseStep ]: \"\"\"Returns a dictionary of pipeline steps.\"\"\" return self . __steps @steps . setter def steps ( self , steps : Dict [ str , BaseStep ]) -> NoReturn : \"\"\"Setting the steps property is not allowed. This method always raises a PipelineInterfaceError. \"\"\" raise PipelineInterfaceError ( \"Cannot set steps manually!\" ) def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name: Optional name for the run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () if not run_name : run_name = ( f \" { self . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"stack_type\" : self . stack . stack_type , \"total_steps\" : len ( self . steps ), }, ) start_time = time . time () logger . info ( \"Using stack ` %s ` for pipeline ` %s `. Running pipeline..\" , Repository () . get_active_stack_key (), self . name , ) # filepath of the file where pipeline.run() was called caller_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) self . stack . orchestrator . pre_run ( pipeline = self , caller_filepath = caller_filepath ) ret = self . stack . orchestrator . run ( self , run_name = run_name ) self . stack . orchestrator . post_run () run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) return ret def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self def _read_config_steps ( self , steps : Dict [ str , Dict [ str , Any ]], overwrite : bool = False ) -> None : \"\"\"Reads and sets step parameters from a config file. Args: steps: Maps step names to dicts of parameter names and values. overwrite: If `True`, overwrite previously set step parameters. \"\"\" for step_name , step_dict in steps . items (): StepConfigurationKeys . key_check ( step_dict ) if step_name not in self . __steps : raise PipelineConfigurationError ( f \"Found ' { step_name } ' step in configuration yaml but it \" f \"doesn't exist in the pipeline steps \" f \" { list ( self . __steps . keys ()) } .\" ) step = self . __steps [ step_name ] step_parameters = ( step . CONFIG_CLASS . __fields__ . keys () if step . CONFIG_CLASS else {} ) parameters = step_dict . get ( StepConfigurationKeys . PARAMETERS_ , {}) for parameter , value in parameters . items (): if parameter not in step_parameters : raise PipelineConfigurationError ( f \"Found parameter ' { parameter } ' for ' { step_name } ' step \" f \"in configuration yaml but it doesn't exist in the \" f \"configuration class ` { step . CONFIG_CLASS } `. Available \" f \"parameters for this step: \" f \" { list ( step_parameters ) } .\" ) previous_value = step . PARAM_SPEC . get ( parameter , None ) if overwrite : step . PARAM_SPEC [ parameter ] = value else : step . PARAM_SPEC . setdefault ( parameter , value ) if overwrite or not previous_value : logger . debug ( \"Setting parameter %s = %s for step ' %s '.\" , parameter , value , step_name , ) if previous_value and not overwrite : logger . warning ( \"Parameter ' %s ' from configuration yaml will NOT be \" \"set as a configuration object was given when \" \"creating the step. Set `overwrite_step_parameters=\" \"True` when setting the configuration yaml to always \" \"use the options specified in the yaml file.\" , parameter , )","title":"BasePipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.stack","text":"Returns the stack for this pipeline.","title":"stack"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.steps","text":"Returns a dictionary of pipeline steps.","title":"steps"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.connect","text":"Function that connects inputs and outputs of the pipeline steps. Source code in zenml/pipelines/base_pipeline.py @abstractmethod def connect ( self , * args : BaseStep , ** kwargs : BaseStep ) -> None : \"\"\"Function that connects inputs and outputs of the pipeline steps.\"\"\" raise NotImplementedError","title":"connect()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.run","text":"Runs the pipeline using the orchestrator of the pipeline stack. Parameters: Name Type Description Default run_name Optional[str] Optional name for the run. None Source code in zenml/pipelines/base_pipeline.py def run ( self , run_name : Optional [ str ] = None ) -> Any : \"\"\"Runs the pipeline using the orchestrator of the pipeline stack. Args: run_name: Optional name for the run. \"\"\" if SHOULD_PREVENT_PIPELINE_EXECUTION : # An environment variable was set to stop the execution of # pipelines. This is done to prevent execution of module-level # pipeline.run() calls inside docker containers which should only # run a single step. logger . info ( \"Preventing execution of pipeline ' %s '. If this is not \" \"intended behavior, make sure to unset the environment \" \"variable ' %s '.\" , self . name , ENV_ZENML_PREVENT_PIPELINE_EXECUTION , ) return # Activating the built-in integrations through lazy loading from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () if not run_name : run_name = ( f \" { self . name } -\" f ' { datetime . now () . strftime ( \" %d _%h_%y-%H_%M_%S_ %f \" ) } ' ) analytics_utils . track_event ( event = analytics_utils . RUN_PIPELINE , metadata = { \"stack_type\" : self . stack . stack_type , \"total_steps\" : len ( self . steps ), }, ) start_time = time . time () logger . info ( \"Using stack ` %s ` for pipeline ` %s `. Running pipeline..\" , Repository () . get_active_stack_key (), self . name , ) # filepath of the file where pipeline.run() was called caller_filepath = fileio . resolve_relative_path ( inspect . currentframe () . f_back . f_code . co_filename # type: ignore[union-attr] # noqa ) self . stack . orchestrator . pre_run ( pipeline = self , caller_filepath = caller_filepath ) ret = self . stack . orchestrator . run ( self , run_name = run_name ) self . stack . orchestrator . post_run () run_duration = time . time () - start_time logger . info ( \"Pipeline run ` %s ` has finished in %s .\" , run_name , string_utils . get_human_readable_time ( run_duration ), ) return ret","title":"run()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipeline.with_config","text":"Configures this pipeline using a yaml file. Parameters: Name Type Description Default config_file str Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. required overwrite_step_parameters bool If set to True , values from the configuration file will overwrite configuration parameters passed in code. False Returns: Type Description ~T The pipeline object that this method was called on. Source code in zenml/pipelines/base_pipeline.py def with_config ( self : T , config_file : str , overwrite_step_parameters : bool = False ) -> T : \"\"\"Configures this pipeline using a yaml file. Args: config_file: Path to a yaml file which contains configuration options for running this pipeline. See https://docs.zenml.io/features/pipeline-configuration#setting-step-parameters-using-a-config-file for details regarding the specification of this file. overwrite_step_parameters: If set to `True`, values from the configuration file will overwrite configuration parameters passed in code. Returns: The pipeline object that this method was called on. \"\"\" config_yaml = yaml_utils . read_yaml ( config_file ) if PipelineConfigurationKeys . STEPS in config_yaml : self . _read_config_steps ( config_yaml [ PipelineConfigurationKeys . STEPS ], overwrite = overwrite_step_parameters , ) return self","title":"with_config()"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipelineMeta","text":"Pipeline Metaclass responsible for validating the pipeline definition. Source code in zenml/pipelines/base_pipeline.py class BasePipelineMeta ( type ): \"\"\"Pipeline Metaclass responsible for validating the pipeline definition.\"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls","title":"BasePipelineMeta"},{"location":"api_docs/pipelines/#zenml.pipelines.base_pipeline.BasePipelineMeta.__new__","text":"Saves argument names for later verification purposes Source code in zenml/pipelines/base_pipeline.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BasePipelineMeta\" : \"\"\"Saves argument names for later verification purposes\"\"\" cls = cast ( Type [ \"BasePipeline\" ], super () . __new__ ( mcs , name , bases , dct )) cls . STEP_SPEC = {} connect_spec = inspect . getfullargspec ( getattr ( cls , PIPELINE_INNER_FUNC_NAME ) ) connect_args = connect_spec . args if connect_args and connect_args [ 0 ] == \"self\" : connect_args . pop ( 0 ) for arg in connect_args : arg_type = connect_spec . annotations . get ( arg , None ) cls . STEP_SPEC . update ({ arg : arg_type }) return cls","title":"__new__()"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines","text":"","title":"builtin_pipelines"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline","text":"","title":"training_pipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline.TrainingPipeline","text":"Class for the classic training pipeline implementation Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py class TrainingPipeline ( BasePipeline ): \"\"\"Class for the classic training pipeline implementation\"\"\" def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore","title":"TrainingPipeline"},{"location":"api_docs/pipelines/#zenml.pipelines.builtin_pipelines.training_pipeline.TrainingPipeline.connect","text":"Main connect method for the standard training pipelines Parameters: Name Type Description Default datasource BaseDatasourceStep the step responsible for the data ingestion required splitter BaseSplitStep the step responsible for splitting the dataset into train, test, val required analyzer BaseAnalyzerStep the step responsible for extracting the statistics and the schema required preprocessor BasePreprocessorStep the step responsible for preprocessing the data required trainer BaseTrainerStep the step responsible for training a model required evaluator BaseEvaluatorStep the step responsible for computing the evaluation of the trained model required Source code in zenml/pipelines/builtin_pipelines/training_pipeline.py def connect ( # type: ignore[override] self , datasource : step_interfaces . BaseDatasourceStep , splitter : step_interfaces . BaseSplitStep , analyzer : step_interfaces . BaseAnalyzerStep , preprocessor : step_interfaces . BasePreprocessorStep , trainer : step_interfaces . BaseTrainerStep , evaluator : step_interfaces . BaseEvaluatorStep , ) -> None : \"\"\"Main connect method for the standard training pipelines Args: datasource: the step responsible for the data ingestion splitter: the step responsible for splitting the dataset into train, test, val analyzer: the step responsible for extracting the statistics and the schema preprocessor: the step responsible for preprocessing the data trainer: the step responsible for training a model evaluator: the step responsible for computing the evaluation of the trained model \"\"\" # Ingesting the datasource dataset = datasource () # Splitting the data train , test , validation = splitter ( dataset = dataset ) # type:ignore # Analyzing the train dataset statistics , schema = analyzer ( dataset = train ) # type:ignore # Preprocessing the splits train_t , test_t , validation_t = preprocessor ( # type:ignore train_dataset = train , test_dataset = test , validation_dataset = validation , statistics = statistics , schema = schema , ) # Training the model model = trainer ( train_dataset = train_t , validation_dataset = validation_t ) # Evaluating the trained model evaluator ( model = model , dataset = test_t ) # type:ignore","title":"connect()"},{"location":"api_docs/pipelines/#zenml.pipelines.pipeline_decorator","text":"","title":"pipeline_decorator"},{"location":"api_docs/pipelines/#zenml.pipelines.pipeline_decorator.pipeline","text":"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. None enable_cache bool Whether to use caching or not. True requirements_file Optional[str] Optional path to a pip requirements file that contains requirements to run the pipeline. None dockerignore_file Optional[str] Optional path to a dockerignore file to use when building docker images for running this pipeline. Note : If you pass a file, make sure it does not include the .zen directory as it is needed to run ZenML inside the container. None Returns: Type Description Union[Type[zenml.pipelines.base_pipeline.BasePipeline], Callable[[~F], Type[zenml.pipelines.base_pipeline.BasePipeline]]] the inner decorator which creates the pipeline class based on the ZenML BasePipeline Source code in zenml/pipelines/pipeline_decorator.py def pipeline ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : bool = True , requirements_file : Optional [ str ] = None , dockerignore_file : Optional [ str ] = None , ) -> Union [ Type [ BasePipeline ], Callable [[ F ], Type [ BasePipeline ]]]: \"\"\"Outer decorator function for the creation of a ZenML pipeline In order to be able to work with parameters such as \"name\", it features a nested decorator structure. Args: _func: The decorated function. name: The name of the pipeline. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Whether to use caching or not. requirements_file: Optional path to a pip requirements file that contains requirements to run the pipeline. dockerignore_file: Optional path to a dockerignore file to use when building docker images for running this pipeline. **Note**: If you pass a file, make sure it does not include the `.zen` directory as it is needed to run ZenML inside the container. Returns: the inner decorator which creates the pipeline class based on the ZenML BasePipeline \"\"\" def inner_decorator ( func : F ) -> Type [ BasePipeline ]: \"\"\"Inner decorator function for the creation of a ZenML Pipeline Args: func: types.FunctionType, this function will be used as the \"connect\" method of the generated Pipeline Returns: the class of a newly generated ZenML Pipeline \"\"\" return type ( # noqa name if name else func . __name__ , ( BasePipeline ,), { PIPELINE_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_REQUIREMENTS_FILE : requirements_file , PARAM_DOCKERIGNORE_FILE : dockerignore_file , }, }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"pipeline()"},{"location":"api_docs/post_execution/","text":"Post Execution zenml.post_execution special After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object. artifact ArtifactView Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. Source code in zenml/post_execution/artifact.py class ArtifactView : \"\"\"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. \"\"\" def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id @property def id ( self ) -> int : \"\"\"Returns the artifact id.\"\"\" return self . _id @property def type ( self ) -> str : \"\"\"Returns the artifact type.\"\"\" return self . _type @property def data_type ( self ) -> str : \"\"\"Returns the data type of the artifact.\"\"\" return self . _data_type @property def uri ( self ) -> str : \"\"\"Returns the URI where the artifact data is stored.\"\"\" return self . _uri @property def parent_step_id ( self ) -> int : \"\"\"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.\"\"\" return self . _parent_step_id @property def producer_step ( self ) -> \"StepView\" : \"\"\"Returns the original StepView that produced the artifact.\"\"\" # TODO [ENG-174]: Replace with artifact.id instead of passing self if # required. return self . _metadata_store . get_producer_step_from_artifact ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns True if artifact was cached in a previous run, else False.\"\"\" # self._metadata_store. return self . producer_step . id != self . parent_step_id def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented data_type : str property readonly Returns the data type of the artifact. id : int property readonly Returns the artifact id. is_cached : bool property readonly Returns True if artifact was cached in a previous run, else False. parent_step_id : int property readonly Returns the ID of the parent step. This need not be equivalent to the ID of the producer step. producer_step : StepView property readonly Returns the original StepView that produced the artifact. type : str property readonly Returns the artifact type. uri : str property readonly Returns the URI where the artifact data is stored. __eq__ ( self , other ) special Returns whether the other object is referring to the same artifact. Source code in zenml/post_execution/artifact.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented __init__ ( self , id_ , type_ , uri , materializer , data_type , metadata_store , parent_step_id ) special Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Parameters: Name Type Description Default id_ int The artifact id. required type_ str The type of this artifact. required uri str Specifies where the artifact data is stored. required materializer str Information needed to restore the materializer that was used to write this artifact. required data_type str The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required parent_step_id int The ID of the parent step. required Source code in zenml/post_execution/artifact.py def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id __repr__ ( self ) special Returns a string representation of this artifact. Source code in zenml/post_execution/artifact.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) read ( self , output_data_type = None , materializer_class = None ) Materializes the data stored in this artifact. Parameters: Name Type Description Default output_data_type Optional[Type[Any]] The datatype to which the materializer should read, will be passed to the materializers handle_input method. None materializer_class Optional[Type[BaseMaterializer]] The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. None Returns: Type Description Any The materialized data. Source code in zenml/post_execution/artifact.py def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) pipeline PipelineView Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. Source code in zenml/post_execution/pipeline.py class PipelineView : \"\"\"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. \"\"\" def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline.\"\"\" return self . _name @property def runs ( self ) -> List [ \"PipelineRunView\" ]: \"\"\"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. \"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . values ()) def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented name : str property readonly Returns the name of the pipeline. runs : List [ PipelineRunView ] property readonly Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. __eq__ ( self , other ) special Returns whether the other object is referring to the same pipeline. Source code in zenml/post_execution/pipeline.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , name , metadata_store ) special Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Parameters: Name Type Description Default id_ int The context id of this pipeline. required name str The name of this pipeline. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required Source code in zenml/post_execution/pipeline.py def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store __repr__ ( self ) special Returns a string representation of this pipeline. Source code in zenml/post_execution/pipeline.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) get_run ( self , name ) Returns a run for the given name. Parameters: Name Type Description Default name str The name of the run to return. required Exceptions: Type Description KeyError If there is no run with the given name. Source code in zenml/post_execution/pipeline.py def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run get_run_names ( self ) Returns a list of all run names. Source code in zenml/post_execution/pipeline.py def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) pipeline_run PipelineRunView Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. Source code in zenml/post_execution/pipeline_run.py class PipelineRunView : \"\"\"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. \"\"\" def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline run.\"\"\" return self . _name @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the pipeline run.\"\"\" step_statuses = ( step . status for step in self . steps ) if any ( status == ExecutionStatus . FAILED for status in step_statuses ): return ExecutionStatus . FAILED elif all ( status == ExecutionStatus . COMPLETED or status == ExecutionStatus . CACHED for status in step_statuses ): return ExecutionStatus . COMPLETED else : return ExecutionStatus . RUNNING @property def steps ( self ) -> List [ StepView ]: \"\"\"Returns all steps that were executed as part of this pipeline run.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . values ()) def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) def _ensure_steps_fetched ( self ) -> None : \"\"\"Fetches all steps for this pipeline run from the metadata store.\"\"\" if self . _steps : # we already fetched the steps, no need to do anything return self . _steps = self . _metadata_store . get_pipeline_run_steps ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented name : str property readonly Returns the name of the pipeline run. status : ExecutionStatus property readonly Returns the current status of the pipeline run. steps : List [ zenml . post_execution . step . StepView ] property readonly Returns all steps that were executed as part of this pipeline run. __eq__ ( self , other ) special Returns whether the other object is referring to the same pipeline run. Source code in zenml/post_execution/pipeline_run.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , name , executions , metadata_store ) special Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Parameters: Name Type Description Default id_ int The context id of this pipeline run. required name str The name of this pipeline run. required executions List[ml_metadata.proto.metadata_store_pb2.Execution] All executions associated with this pipeline run. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline run. required Source code in zenml/post_execution/pipeline_run.py def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () __repr__ ( self ) special Returns a string representation of this pipeline run. Source code in zenml/post_execution/pipeline_run.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) get_step ( self , name ) Returns a step for the given name. Parameters: Name Type Description Default name str The name of the step to return. required Exceptions: Type Description KeyError If there is no step with the given name. Source code in zenml/post_execution/pipeline_run.py def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) get_step_names ( self ) Returns a list of all step names. Source code in zenml/post_execution/pipeline_run.py def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) step StepView Post-execution step class which can be used to query artifact information associated with a pipeline step. Source code in zenml/post_execution/step.py class StepView : \"\"\"Post-execution step class which can be used to query artifact information associated with a pipeline step. \"\"\" def __init__ ( self , id_ : int , parents_step_ids : List [ int ], name : str , pipeline_step_name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. name: The name of this step. pipeline_step_name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _name = name self . _pipeline_step_name = pipeline_step_name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} @property def id ( self ) -> int : \"\"\"Returns the step id.\"\"\" return self . _id @property def parents_step_ids ( self ) -> List [ int ]: \"\"\"Returns a list of ID's of all parents of this step.\"\"\" return self . _parents_step_ids @property def parent_steps ( self ) -> List [ \"StepView\" ]: \"\"\"Returns a list of all parent steps of this step.\"\"\" steps = [ self . _metadata_store . get_step_by_id ( s ) for s in self . parents_step_ids ] return steps @property def name ( self ) -> str : \"\"\"Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...) \"\"\" return self . _name @property def pipeline_step_name ( self ) -> str : \"\"\"Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be `step_a` \"\"\" return self . _pipeline_step_name @property def parameters ( self ) -> Dict [ str , Any ]: \"\"\"The parameters used to run this step.\"\"\" return self . _parameters @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the step.\"\"\" return self . _metadata_store . get_step_status ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns whether the step is cached or not.\"\"\" return self . status == ExecutionStatus . CACHED @property def inputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all input artifacts that were used to run this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _inputs @property def input ( self ) -> ArtifactView : \"\"\"Returns the input artifact that was used to run this step. Raises: ValueError: If there were zero or multiple inputs to this step. \"\"\" if len ( self . inputs ) != 1 : raise ValueError ( \"Can't use the `StepView.input` property for steps with zero \" \"or multiple inputs, use `StepView.inputs` instead.\" ) return next ( iter ( self . inputs . values ())) @property def outputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all output artifacts that were written by this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _outputs @property def output ( self ) -> ArtifactView : \"\"\"Returns the output artifact that was written by this step. Raises: ValueError: If there were zero or multiple step outputs. \"\"\" if len ( self . outputs ) != 1 : raise ValueError ( \"Can't use the `StepView.output` property for steps with zero \" \"or multiple outputs, use `StepView.outputs` instead.\" ) return next ( iter ( self . outputs . values ())) def _ensure_inputs_outputs_fetched ( self ) -> None : \"\"\"Fetches all step inputs and outputs from the metadata store.\"\"\" if self . _inputs or self . _outputs : # we already fetched inputs/outputs, no need to do anything return self . _inputs , self . _outputs = self . _metadata_store . get_step_artifacts ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ', parameters= { self . _parameters } )\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented id : int property readonly Returns the step id. input : ArtifactView property readonly Returns the input artifact that was used to run this step. Exceptions: Type Description ValueError If there were zero or multiple inputs to this step. inputs : Dict [ str , zenml . post_execution . artifact . ArtifactView ] property readonly Returns all input artifacts that were used to run this step. is_cached : bool property readonly Returns whether the step is cached or not. name : str property readonly Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) the step name will be \"my_step_function\" @step def my_step_function(...) output : ArtifactView property readonly Returns the output artifact that was written by this step. Exceptions: Type Description ValueError If there were zero or multiple step outputs. outputs : Dict [ str , zenml . post_execution . artifact . ArtifactView ] property readonly Returns all output artifacts that were written by this step. parameters : Dict [ str , Any ] property readonly The parameters used to run this step. parent_steps : List [ StepView ] property readonly Returns a list of all parent steps of this step. parents_step_ids : List [ int ] property readonly Returns a list of ID's of all parents of this step. pipeline_step_name : str property readonly Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a status : ExecutionStatus property readonly Returns the current status of the step. __eq__ ( self , other ) special Returns whether the other object is referring to the same step. Source code in zenml/post_execution/step.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented __init__ ( self , id_ , parents_step_ids , name , pipeline_step_name , parameters , metadata_store ) special Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Parameters: Name Type Description Default id_ int The execution id of this step. required parents_step_ids List[int] The execution ids of the parents of this step. required name str The name of this step. required pipeline_step_name str The name of this step within the pipeline required parameters Dict[str, Any] Parameters that were used to run this step. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this step. required Source code in zenml/post_execution/step.py def __init__ ( self , id_ : int , parents_step_ids : List [ int ], name : str , pipeline_step_name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. name: The name of this step. pipeline_step_name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _name = name self . _pipeline_step_name = pipeline_step_name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} __repr__ ( self ) special Returns a string representation of this step. Source code in zenml/post_execution/step.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ', parameters= { self . _parameters } )\" )","title":"Post Execution"},{"location":"api_docs/post_execution/#post-execution","text":"","title":"Post Execution"},{"location":"api_docs/post_execution/#zenml.post_execution","text":"After executing a pipeline, the user needs to be able to fetch it from history and perform certain tasks. The post_execution submodule provides a set of interfaces with which the user can interact with artifacts, the pipeline, steps, and the post-run pipeline object.","title":"post_execution"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact","text":"","title":"artifact"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView","text":"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. Source code in zenml/post_execution/artifact.py class ArtifactView : \"\"\"Post-execution artifact class which can be used to read artifact data that was created during a pipeline execution. \"\"\" def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id @property def id ( self ) -> int : \"\"\"Returns the artifact id.\"\"\" return self . _id @property def type ( self ) -> str : \"\"\"Returns the artifact type.\"\"\" return self . _type @property def data_type ( self ) -> str : \"\"\"Returns the data type of the artifact.\"\"\" return self . _data_type @property def uri ( self ) -> str : \"\"\"Returns the URI where the artifact data is stored.\"\"\" return self . _uri @property def parent_step_id ( self ) -> int : \"\"\"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.\"\"\" return self . _parent_step_id @property def producer_step ( self ) -> \"StepView\" : \"\"\"Returns the original StepView that produced the artifact.\"\"\" # TODO [ENG-174]: Replace with artifact.id instead of passing self if # required. return self . _metadata_store . get_producer_step_from_artifact ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns True if artifact was cached in a previous run, else False.\"\"\" # self._metadata_store. return self . producer_step . id != self . parent_step_id def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented","title":"ArtifactView"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.data_type","text":"Returns the data type of the artifact.","title":"data_type"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.id","text":"Returns the artifact id.","title":"id"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.is_cached","text":"Returns True if artifact was cached in a previous run, else False.","title":"is_cached"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.parent_step_id","text":"Returns the ID of the parent step. This need not be equivalent to the ID of the producer step.","title":"parent_step_id"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.producer_step","text":"Returns the original StepView that produced the artifact.","title":"producer_step"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.type","text":"Returns the artifact type.","title":"type"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.uri","text":"Returns the URI where the artifact data is stored.","title":"uri"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__eq__","text":"Returns whether the other object is referring to the same artifact. Source code in zenml/post_execution/artifact.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same artifact.\"\"\" if isinstance ( other , ArtifactView ): return self . _id == other . _id and self . _uri == other . _uri return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__init__","text":"Initializes a post-execution artifact object. In most cases ArtifactView objects should not be created manually but retrieved from a StepView via the inputs or outputs properties. Parameters: Name Type Description Default id_ int The artifact id. required type_ str The type of this artifact. required uri str Specifies where the artifact data is stored. required materializer str Information needed to restore the materializer that was used to write this artifact. required data_type str The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required parent_step_id int The ID of the parent step. required Source code in zenml/post_execution/artifact.py def __init__ ( self , id_ : int , type_ : str , uri : str , materializer : str , data_type : str , metadata_store : \"BaseMetadataStore\" , parent_step_id : int , ): \"\"\"Initializes a post-execution artifact object. In most cases `ArtifactView` objects should not be created manually but retrieved from a `StepView` via the `inputs` or `outputs` properties. Args: id_: The artifact id. type_: The type of this artifact. uri: Specifies where the artifact data is stored. materializer: Information needed to restore the materializer that was used to write this artifact. data_type: The type of data that was passed to the materializer when writing that artifact. Will be used as a default type to read the artifact. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. parent_step_id: The ID of the parent step. \"\"\" self . _id = id_ self . _type = type_ self . _uri = uri self . _materializer = materializer self . _data_type = data_type self . _metadata_store = metadata_store self . _parent_step_id = parent_step_id","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.__repr__","text":"Returns a string representation of this artifact. Source code in zenml/post_execution/artifact.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this artifact.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"type=' { self . _type } ', uri=' { self . _uri } ', \" f \"materializer=' { self . _materializer } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.artifact.ArtifactView.read","text":"Materializes the data stored in this artifact. Parameters: Name Type Description Default output_data_type Optional[Type[Any]] The datatype to which the materializer should read, will be passed to the materializers handle_input method. None materializer_class Optional[Type[BaseMaterializer]] The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. None Returns: Type Description Any The materialized data. Source code in zenml/post_execution/artifact.py def read ( self , output_data_type : Optional [ Type [ Any ]] = None , materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> Any : \"\"\"Materializes the data stored in this artifact. Args: output_data_type: The datatype to which the materializer should read, will be passed to the materializers `handle_input` method. materializer_class: The class of the materializer that should be used to read the artifact data. If no materializer class is given, we use the materializer that was used to write the artifact during execution of the pipeline. Returns: The materialized data. \"\"\" if not materializer_class : materializer_class = source_utils . load_source_path_class ( self . _materializer ) if not output_data_type : output_data_type = source_utils . load_source_path_class ( self . _data_type ) logger . debug ( \"Using ' %s ' to read ' %s ' (uri: %s ).\" , materializer_class . __qualname__ , self . _type , self . _uri , ) # TODO [ENG-162]: passing in `self` to initialize the materializer only # works because materializers only require a `.uri` property at the # moment. materializer = materializer_class ( self ) # type: ignore[arg-type] return materializer . handle_input ( output_data_type )","title":"read()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline","text":"","title":"pipeline"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView","text":"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. Source code in zenml/post_execution/pipeline.py class PipelineView : \"\"\"Post-execution pipeline class which can be used to query pipeline-related information from the metadata store. \"\"\" def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline.\"\"\" return self . _name @property def runs ( self ) -> List [ \"PipelineRunView\" ]: \"\"\"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list. \"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . values ()) def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ()) def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"PipelineView"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.name","text":"Returns the name of the pipeline.","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.runs","text":"Returns all stored runs of this pipeline. The runs are returned in chronological order, so the latest run will be the last element in this list.","title":"runs"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__eq__","text":"Returns whether the other object is referring to the same pipeline. Source code in zenml/post_execution/pipeline.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline.\"\"\" if isinstance ( other , PipelineView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__init__","text":"Initializes a post-execution pipeline object. In most cases PipelineView objects should not be created manually but retrieved using the get_pipelines() method of a zenml.core.repo.Repository instead. Parameters: Name Type Description Default id_ int The context id of this pipeline. required name str The name of this pipeline. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline. required Source code in zenml/post_execution/pipeline.py def __init__ ( self , id_ : int , name : str , metadata_store : \"BaseMetadataStore\" ): \"\"\"Initializes a post-execution pipeline object. In most cases `PipelineView` objects should not be created manually but retrieved using the `get_pipelines()` method of a `zenml.core.repo.Repository` instead. Args: id_: The context id of this pipeline. name: The name of this pipeline. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.__repr__","text":"Returns a string representation of this pipeline. Source code in zenml/post_execution/pipeline.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.get_run","text":"Returns a run for the given name. Parameters: Name Type Description Default name str The name of the run to return. required Exceptions: Type Description KeyError If there is no run with the given name. Source code in zenml/post_execution/pipeline.py def get_run ( self , name : str ) -> \"PipelineRunView\" : \"\"\"Returns a run for the given name. Args: name: The name of the run to return. Raises: KeyError: If there is no run with the given name. \"\"\" run = self . _metadata_store . get_pipeline_run ( self , name ) if not run : raise KeyError ( f \"No run found for name ` { name } `. This pipeline \" f \"only has runs with the following \" f \"names: ` { self . get_run_names () } `\" ) return run","title":"get_run()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline.PipelineView.get_run_names","text":"Returns a list of all run names. Source code in zenml/post_execution/pipeline.py def get_run_names ( self ) -> List [ str ]: \"\"\"Returns a list of all run names.\"\"\" # Do not cache runs as new runs might appear during this objects # lifecycle runs = self . _metadata_store . get_pipeline_runs ( self ) return list ( runs . keys ())","title":"get_run_names()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run","text":"","title":"pipeline_run"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView","text":"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. Source code in zenml/post_execution/pipeline_run.py class PipelineRunView : \"\"\"Post-execution pipeline run class which can be used to query steps and artifact information associated with a pipeline execution. \"\"\" def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict () @property def name ( self ) -> str : \"\"\"Returns the name of the pipeline run.\"\"\" return self . _name @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the pipeline run.\"\"\" step_statuses = ( step . status for step in self . steps ) if any ( status == ExecutionStatus . FAILED for status in step_statuses ): return ExecutionStatus . FAILED elif all ( status == ExecutionStatus . COMPLETED or status == ExecutionStatus . CACHED for status in step_statuses ): return ExecutionStatus . COMPLETED else : return ExecutionStatus . RUNNING @property def steps ( self ) -> List [ StepView ]: \"\"\"Returns all steps that were executed as part of this pipeline run.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . values ()) def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ()) def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" ) def _ensure_steps_fetched ( self ) -> None : \"\"\"Fetches all steps for this pipeline run from the metadata store.\"\"\" if self . _steps : # we already fetched the steps, no need to do anything return self . _steps = self . _metadata_store . get_pipeline_run_steps ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"PipelineRunView"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.name","text":"Returns the name of the pipeline run.","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.status","text":"Returns the current status of the pipeline run.","title":"status"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.steps","text":"Returns all steps that were executed as part of this pipeline run.","title":"steps"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__eq__","text":"Returns whether the other object is referring to the same pipeline run. Source code in zenml/post_execution/pipeline_run.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same pipeline run.\"\"\" if isinstance ( other , PipelineRunView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__init__","text":"Initializes a post-execution pipeline run object. In most cases PipelineRunView objects should not be created manually but retrieved from a PipelineView object instead. Parameters: Name Type Description Default id_ int The context id of this pipeline run. required name str The name of this pipeline run. required executions List[ml_metadata.proto.metadata_store_pb2.Execution] All executions associated with this pipeline run. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this pipeline run. required Source code in zenml/post_execution/pipeline_run.py def __init__ ( self , id_ : int , name : str , executions : List [ proto . Execution ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution pipeline run object. In most cases `PipelineRunView` objects should not be created manually but retrieved from a `PipelineView` object instead. Args: id_: The context id of this pipeline run. name: The name of this pipeline run. executions: All executions associated with this pipeline run. metadata_store: The metadata store which should be used to fetch additional information related to this pipeline run. \"\"\" self . _id = id_ self . _name = name self . _metadata_store = metadata_store self . _executions = executions self . _steps : Dict [ str , StepView ] = OrderedDict ()","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.__repr__","text":"Returns a string representation of this pipeline run. Source code in zenml/post_execution/pipeline_run.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this pipeline run.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ')\" )","title":"__repr__()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.get_step","text":"Returns a step for the given name. Parameters: Name Type Description Default name str The name of the step to return. required Exceptions: Type Description KeyError If there is no step with the given name. Source code in zenml/post_execution/pipeline_run.py def get_step ( self , name : str ) -> StepView : \"\"\"Returns a step for the given name. Args: name: The name of the step to return. Raises: KeyError: If there is no step with the given name. \"\"\" self . _ensure_steps_fetched () try : return self . _steps [ name ] except KeyError : raise KeyError ( f \"No step found for name ` { name } `. This pipeline \" f \"run only has steps with the following \" f \"names: ` { self . get_step_names () } `\" )","title":"get_step()"},{"location":"api_docs/post_execution/#zenml.post_execution.pipeline_run.PipelineRunView.get_step_names","text":"Returns a list of all step names. Source code in zenml/post_execution/pipeline_run.py def get_step_names ( self ) -> List [ str ]: \"\"\"Returns a list of all step names.\"\"\" self . _ensure_steps_fetched () return list ( self . _steps . keys ())","title":"get_step_names()"},{"location":"api_docs/post_execution/#zenml.post_execution.step","text":"","title":"step"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView","text":"Post-execution step class which can be used to query artifact information associated with a pipeline step. Source code in zenml/post_execution/step.py class StepView : \"\"\"Post-execution step class which can be used to query artifact information associated with a pipeline step. \"\"\" def __init__ ( self , id_ : int , parents_step_ids : List [ int ], name : str , pipeline_step_name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. name: The name of this step. pipeline_step_name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _name = name self . _pipeline_step_name = pipeline_step_name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {} @property def id ( self ) -> int : \"\"\"Returns the step id.\"\"\" return self . _id @property def parents_step_ids ( self ) -> List [ int ]: \"\"\"Returns a list of ID's of all parents of this step.\"\"\" return self . _parents_step_ids @property def parent_steps ( self ) -> List [ \"StepView\" ]: \"\"\"Returns a list of all parent steps of this step.\"\"\" steps = [ self . _metadata_store . get_step_by_id ( s ) for s in self . parents_step_ids ] return steps @property def name ( self ) -> str : \"\"\"Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples: # the step name will be \"my_step\" @step(name=\"my_step\") def my_step_function(...) # the step name will be \"my_step_function\" @step def my_step_function(...) \"\"\" return self . _name @property def pipeline_step_name ( self ) -> str : \"\"\"Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be `step_a` \"\"\" return self . _pipeline_step_name @property def parameters ( self ) -> Dict [ str , Any ]: \"\"\"The parameters used to run this step.\"\"\" return self . _parameters @property def status ( self ) -> ExecutionStatus : \"\"\"Returns the current status of the step.\"\"\" return self . _metadata_store . get_step_status ( self ) @property def is_cached ( self ) -> bool : \"\"\"Returns whether the step is cached or not.\"\"\" return self . status == ExecutionStatus . CACHED @property def inputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all input artifacts that were used to run this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _inputs @property def input ( self ) -> ArtifactView : \"\"\"Returns the input artifact that was used to run this step. Raises: ValueError: If there were zero or multiple inputs to this step. \"\"\" if len ( self . inputs ) != 1 : raise ValueError ( \"Can't use the `StepView.input` property for steps with zero \" \"or multiple inputs, use `StepView.inputs` instead.\" ) return next ( iter ( self . inputs . values ())) @property def outputs ( self ) -> Dict [ str , ArtifactView ]: \"\"\"Returns all output artifacts that were written by this step.\"\"\" self . _ensure_inputs_outputs_fetched () return self . _outputs @property def output ( self ) -> ArtifactView : \"\"\"Returns the output artifact that was written by this step. Raises: ValueError: If there were zero or multiple step outputs. \"\"\" if len ( self . outputs ) != 1 : raise ValueError ( \"Can't use the `StepView.output` property for steps with zero \" \"or multiple outputs, use `StepView.outputs` instead.\" ) return next ( iter ( self . outputs . values ())) def _ensure_inputs_outputs_fetched ( self ) -> None : \"\"\"Fetches all step inputs and outputs from the metadata store.\"\"\" if self . _inputs or self . _outputs : # we already fetched inputs/outputs, no need to do anything return self . _inputs , self . _outputs = self . _metadata_store . get_step_artifacts ( self ) def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ', parameters= { self . _parameters } )\" ) def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"StepView"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.id","text":"Returns the step id.","title":"id"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.input","text":"Returns the input artifact that was used to run this step. Exceptions: Type Description ValueError If there were zero or multiple inputs to this step.","title":"input"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.inputs","text":"Returns all input artifacts that were used to run this step.","title":"inputs"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.is_cached","text":"Returns whether the step is cached or not.","title":"is_cached"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.name","text":"Returns the step name. This name is equal to the name argument passed to the @step decorator or the actual function name if no explicit name was given. Examples:","title":"name"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.name--the-step-name-will-be-my_step","text":"@step(name=\"my_step\") def my_step_function(...)","title":"the step name will be \"my_step\""},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.name--the-step-name-will-be-my_step_function","text":"@step def my_step_function(...)","title":"the step name will be \"my_step_function\""},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.output","text":"Returns the output artifact that was written by this step. Exceptions: Type Description ValueError If there were zero or multiple step outputs.","title":"output"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.outputs","text":"Returns all output artifacts that were written by this step.","title":"outputs"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parameters","text":"The parameters used to run this step.","title":"parameters"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parent_steps","text":"Returns a list of all parent steps of this step.","title":"parent_steps"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.parents_step_ids","text":"Returns a list of ID's of all parents of this step.","title":"parents_step_ids"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.pipeline_step_name","text":"Returns the pipeline step name as it is defined in the pipeline. This name is equal to the name given to the step within the pipeline context Examples: @step() def my_step_function(...) @pipeline def my_pipeline_function(step_a) p = my_pipeline_function( step_a = my_step_function() ) The pipeline step name will be step_a","title":"pipeline_step_name"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.status","text":"Returns the current status of the step.","title":"status"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__eq__","text":"Returns whether the other object is referring to the same step. Source code in zenml/post_execution/step.py def __eq__ ( self , other : Any ) -> bool : \"\"\"Returns whether the other object is referring to the same step.\"\"\" if isinstance ( other , StepView ): return ( self . _id == other . _id and self . _metadata_store . uuid == other . _metadata_store . uuid ) return NotImplemented","title":"__eq__()"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__init__","text":"Initializes a post-execution step object. In most cases StepView objects should not be created manually but retrieved from a PipelineRunView object instead. Parameters: Name Type Description Default id_ int The execution id of this step. required parents_step_ids List[int] The execution ids of the parents of this step. required name str The name of this step. required pipeline_step_name str The name of this step within the pipeline required parameters Dict[str, Any] Parameters that were used to run this step. required metadata_store BaseMetadataStore The metadata store which should be used to fetch additional information related to this step. required Source code in zenml/post_execution/step.py def __init__ ( self , id_ : int , parents_step_ids : List [ int ], name : str , pipeline_step_name : str , parameters : Dict [ str , Any ], metadata_store : \"BaseMetadataStore\" , ): \"\"\"Initializes a post-execution step object. In most cases `StepView` objects should not be created manually but retrieved from a `PipelineRunView` object instead. Args: id_: The execution id of this step. parents_step_ids: The execution ids of the parents of this step. name: The name of this step. pipeline_step_name: The name of this step within the pipeline parameters: Parameters that were used to run this step. metadata_store: The metadata store which should be used to fetch additional information related to this step. \"\"\" self . _id = id_ self . _parents_step_ids = parents_step_ids self . _name = name self . _pipeline_step_name = pipeline_step_name self . _parameters = parameters self . _metadata_store = metadata_store self . _inputs : Dict [ str , ArtifactView ] = {} self . _outputs : Dict [ str , ArtifactView ] = {}","title":"__init__()"},{"location":"api_docs/post_execution/#zenml.post_execution.step.StepView.__repr__","text":"Returns a string representation of this step. Source code in zenml/post_execution/step.py def __repr__ ( self ) -> str : \"\"\"Returns a string representation of this step.\"\"\" return ( f \" { self . __class__ . __qualname__ } (id= { self . _id } , \" f \"name=' { self . _name } ', parameters= { self . _parameters } )\" )","title":"__repr__()"},{"location":"api_docs/stacks/","text":"Stacks zenml.stacks special A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it. base_stack BaseStack ( BaseSettings ) pydantic-model Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. You can set it through the config yaml file. You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. Environment variables, e.g. zenml_var as described above. Variables loaded from a config yaml file. The default field values. Source code in zenml/stacks/base_stack.py class BaseStack ( BaseSettings ): \"\"\"Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic `BaseSettings` class, which means that there are multiple ways to use it. * You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): * Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values. \"\"\" stack_type : StackTypes = StackTypes . base metadata_store_name : str artifact_store_name : str orchestrator_name : str container_registry_name : Optional [ str ] = None _repo_path : Optional [ str ] = PrivateAttr ( default = None ) def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) } @property def orchestrator ( self ) -> \"BaseOrchestrator\" : \"\"\"Returns the orchestrator of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_orchestrator ( self . orchestrator_name ) @property def artifact_store ( self ) -> \"BaseArtifactStore\" : \"\"\"Returns the artifact store of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_artifact_store ( self . artifact_store_name ) @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"Returns the metadata store of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_metadata_store ( self . metadata_store_name ) @property def container_registry ( self ) -> Optional [ \"BaseContainerRegistry\" ]: \"\"\"Returns the optional container registry of this stack.\"\"\" if self . container_registry_name : from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_container_registry ( self . container_registry_name ) else : return None class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_stack_\" artifact_store : BaseArtifactStore property readonly Returns the artifact store of this stack. container_registry : Optional [ BaseContainerRegistry ] property readonly Returns the optional container registry of this stack. metadata_store : BaseMetadataStore property readonly Returns the metadata store of this stack. orchestrator : BaseOrchestrator property readonly Returns the orchestrator of this stack. Config Configuration of settings. Source code in zenml/stacks/base_stack.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_stack_\" dict ( self , ** kwargs ) Removes private attributes from pydantic dict so they don't get stored in our config files. Source code in zenml/stacks/base_stack.py def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) }","title":"Stacks"},{"location":"api_docs/stacks/#stacks","text":"","title":"Stacks"},{"location":"api_docs/stacks/#zenml.stacks","text":"A stack is made up of the following three core components: an Artifact Store, a Metadata Store, and an Orchestrator (backend). A ZenML stack also happens to be a Pydantic BaseSettings class, which means that there are multiple ways to use it.","title":"stacks"},{"location":"api_docs/stacks/#zenml.stacks.base_stack","text":"","title":"base_stack"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack","text":"Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic BaseSettings class, which means that there are multiple ways to use it. You can set it via env variables. You can set it through the config yaml file. You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): Arguments passed to the Settings class initializer. Environment variables, e.g. zenml_var as described above. Variables loaded from a config yaml file. The default field values. Source code in zenml/stacks/base_stack.py class BaseStack ( BaseSettings ): \"\"\"Base stack for ZenML. A ZenML stack brings together a Metadata Store, an Artifact Store, and an Orchestrator, the trifecta of the environment required to run a ZenML pipeline. A ZenML stack also happens to be a pydantic `BaseSettings` class, which means that there are multiple ways to use it. * You can set it via env variables. * You can set it through the config yaml file. * You can set it in code by initializing an object of this class, and passing it to pipelines as a configuration. In the case where a value is specified for the same Settings field in multiple ways, the selected value is determined as follows (in descending order of priority): * Arguments passed to the Settings class initializer. * Environment variables, e.g. zenml_var as described above. * Variables loaded from a config yaml file. * The default field values. \"\"\" stack_type : StackTypes = StackTypes . base metadata_store_name : str artifact_store_name : str orchestrator_name : str container_registry_name : Optional [ str ] = None _repo_path : Optional [ str ] = PrivateAttr ( default = None ) def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) } @property def orchestrator ( self ) -> \"BaseOrchestrator\" : \"\"\"Returns the orchestrator of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_orchestrator ( self . orchestrator_name ) @property def artifact_store ( self ) -> \"BaseArtifactStore\" : \"\"\"Returns the artifact store of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_artifact_store ( self . artifact_store_name ) @property def metadata_store ( self ) -> \"BaseMetadataStore\" : \"\"\"Returns the metadata store of this stack.\"\"\" from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_metadata_store ( self . metadata_store_name ) @property def container_registry ( self ) -> Optional [ \"BaseContainerRegistry\" ]: \"\"\"Returns the optional container registry of this stack.\"\"\" if self . container_registry_name : from zenml.core.repo import Repository return Repository ( self . _repo_path ) . service . get_container_registry ( self . container_registry_name ) else : return None class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_stack_\"","title":"BaseStack"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.artifact_store","text":"Returns the artifact store of this stack.","title":"artifact_store"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.container_registry","text":"Returns the optional container registry of this stack.","title":"container_registry"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.metadata_store","text":"Returns the metadata store of this stack.","title":"metadata_store"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.orchestrator","text":"Returns the orchestrator of this stack.","title":"orchestrator"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.Config","text":"Configuration of settings. Source code in zenml/stacks/base_stack.py class Config : \"\"\"Configuration of settings.\"\"\" env_prefix = \"zenml_stack_\"","title":"Config"},{"location":"api_docs/stacks/#zenml.stacks.base_stack.BaseStack.dict","text":"Removes private attributes from pydantic dict so they don't get stored in our config files. Source code in zenml/stacks/base_stack.py def dict ( self , ** kwargs : Any ) -> Dict [ str , Any ]: \"\"\"Removes private attributes from pydantic dict so they don't get stored in our config files.\"\"\" return { key : value for key , value in super () . dict ( ** kwargs ) . items () if not key . startswith ( \"_\" ) }","title":"dict()"},{"location":"api_docs/steps/","text":"Steps zenml.steps special A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator. base_step BaseStep Abstract base class for all ZenML steps. Attributes: Name Type Description name The name of this step. pipeline_parameter_name Optional[str] The name of the pipeline parameter for which this step was passed as an argument. enable_cache A boolean indicating if caching is enabled for this step. requires_context A boolean indicating if this step requires a StepContext object during execution. Source code in zenml/steps/base_step.py class BaseStep ( metaclass = BaseStepMeta ): \"\"\"Abstract base class for all ZenML steps. Attributes: name: The name of this step. pipeline_parameter_name: The name of the pipeline parameter for which this step was passed as an argument. enable_cache: A boolean indicating if caching is enabled for this step. requires_context: A boolean indicating if this step requires a `StepContext` object during execution. \"\"\" # TODO [ENG-156]: Ensure these are ordered INPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa OUTPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa CONFIG_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None CONFIG_CLASS : ClassVar [ Optional [ Type [ BaseStepConfig ]]] = None CONTEXT_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None PARAM_SPEC : Dict [ str , Any ] = {} INPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} OUTPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} INSTANCE_CONFIGURATION : Dict [ str , Any ] = {} def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : self . name = self . __class__ . __name__ self . pipeline_parameter_name : Optional [ str ] = None kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . requires_context = bool ( self . CONTEXT_PARAMETER_NAME ) self . _created_by_functional_api = kwargs . pop ( PARAM_CREATED_BY_FUNCTIONAL_API , False ) enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , None ) if enable_cache is None : if self . requires_context : # Using the StepContext inside a step provides access to # external resources which might influence the step execution. # We therefore disable caching unless it is explicitly enabled enable_cache = False logger . debug ( \"Step ' %s ': Step context required and caching not \" \"explicitly enabled.\" , self . name , ) else : # Default to cache enabled if not explicitly set enable_cache = True logger . debug ( \"Step ' %s ': Caching %s .\" , self . name , \"enabled\" if enable_cache else \"disabled\" , ) self . enable_cache = enable_cache self . _explicit_materializers : Dict [ str , Type [ BaseMaterializer ]] = {} self . _component : Optional [ _ZenMLSimpleComponent ] = None self . _verify_init_arguments ( * args , ** kwargs ) self . _verify_output_spec () @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers @property def _internal_execution_parameters ( self ) -> Dict [ str , Any ]: \"\"\"ZenML internal execution parameters for this step.\"\"\" parameters = { PARAM_PIPELINE_PARAMETER_NAME : self . pipeline_parameter_name } if self . enable_cache : # Caching is enabled so we compute a hash of the step function code # and materializers to catch changes in the step behavior def _get_hashed_source ( value : Any ) -> str : \"\"\"Returns a hash of the objects source code.\"\"\" source_code = inspect . getsource ( value ) return hashlib . sha256 ( source_code . encode ( \"utf-8\" )) . hexdigest () # If the step was defined using the functional api, only track # changes to the entrypoint function. Otherwise track changes to # the entire step class. source_object = ( self . entrypoint if self . _created_by_functional_api else self . __class__ ) parameters [ \"step_source\" ] = _get_hashed_source ( source_object ) for name , materializer in self . get_materializers () . items (): key = f \" { name } _materializer_source\" parameters [ key ] = _get_hashed_source ( materializer ) else : # Add a random string to the execution properties to disable caching random_string = f \" { random . getrandbits ( 128 ) : 032x } \" parameters [ \"disable_cache\" ] = random_string return { INTERNAL_EXECUTION_PARAMETER_PREFIX + key : value for key , value in parameters . items () } def _verify_init_arguments ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Verifies the initialization args and kwargs of this step. This method makes sure that there is only a config object passed at initialization and that it was passed using the correct name and type specified in the step declaration. If the correct config object was found, additionally saves the config parameters to `self.PARAM_SPEC`. Args: *args: The args passed to the init method of this step. **kwargs: The kwargs passed to the init method of this step. Raises: StepInterfaceError: If there are too many arguments or arguments with a wrong name/type. \"\"\" maximum_arg_count = 1 if self . CONFIG_CLASS else 0 arg_count = len ( args ) + len ( kwargs ) if arg_count > maximum_arg_count : raise StepInterfaceError ( f \"Too many arguments ( { arg_count } , expected: \" f \" { maximum_arg_count } ) passed when creating a \" f \"' { self . name } ' step.\" ) if self . CONFIG_PARAMETER_NAME and self . CONFIG_CLASS : if args : config = args [ 0 ] elif kwargs : key , config = kwargs . popitem () if key != self . CONFIG_PARAMETER_NAME : raise StepInterfaceError ( f \"Unknown keyword argument ' { key } ' when creating a \" f \"' { self . name } ' step, only expected a single \" f \"argument with key ' { self . CONFIG_PARAMETER_NAME } '.\" ) else : # This step requires configuration parameters but no config # object was passed as an argument. The parameters might be # set via default values in the config class or in a # configuration file, so we continue for now and verify # that all parameters are set before running the step return if not isinstance ( config , self . CONFIG_CLASS ): raise StepInterfaceError ( f \"` { config } ` object passed when creating a \" f \"' { self . name } ' step is not a \" f \"` { self . CONFIG_CLASS . __name__ } ` instance.\" ) self . PARAM_SPEC = config . dict () def _verify_output_spec ( self ) -> None : \"\"\"Verifies the explicitly set output artifact types of this step. Raises: StepInterfaceError: If an output artifact type is specified for a non-existent step output or the artifact type is not allowed for the corresponding output type. \"\"\" for output_name , artifact_type in self . OUTPUT_SPEC . items (): if output_name not in self . OUTPUT_SIGNATURE : raise StepInterfaceError ( f \"Found explicit artifact type for unrecognized output \" f \"' { output_name } ' in step ' { self . name } '. Output \" f \"artifact types can only be specified for the outputs \" f \"of this step: { set ( self . OUTPUT_SIGNATURE ) } .\" ) if not issubclass ( artifact_type , BaseArtifact ): raise StepInterfaceError ( f \"Invalid artifact type ( { artifact_type } ) for output \" f \"' { output_name } ' of step ' { self . name } '. Only \" f \"`BaseArtifact` subclasses are allowed as artifact types.\" ) output_type = self . OUTPUT_SIGNATURE [ output_name ] allowed_artifact_types = set ( type_registry . get_artifact_type ( output_type ) ) if artifact_type not in allowed_artifact_types : raise StepInterfaceError ( f \"Artifact type ` { artifact_type } ` for output \" f \"' { output_name } ' of step ' { self . name } ' is not an \" f \"allowed artifact type for the defined output type \" f \"` { output_type } `. Allowed artifact types: \" f \" { allowed_artifact_types } . If you want to extend the \" f \"allowed artifact types, implement a custom \" f \"`BaseMaterializer` subclass and set its \" f \"`ASSOCIATED_ARTIFACT_TYPES` and `ASSOCIATED_TYPES` \" f \"accordingly.\" ) def _update_and_verify_parameter_spec ( self ) -> None : \"\"\"Verifies and prepares the config parameters for running this step. When the step requires config parameters, this method: - checks if config parameters were set via a config object or file - tries to set missing config parameters from default values of the config class Raises: MissingStepParameterError: If no value could be found for one or more config parameters. StepInterfaceError: If a config parameter value couldn't be serialized to json. \"\"\" if self . CONFIG_CLASS : # we need to store a value for all config keys inside the # metadata store to make sure caching works as expected missing_keys = [] for name , field in self . CONFIG_CLASS . __fields__ . items (): if name in self . PARAM_SPEC : # a value for this parameter has been set already continue if field . required : # this field has no default value set and therefore needs # to be passed via an initialized config object missing_keys . append ( name ) else : # use default value from the pydantic config class self . PARAM_SPEC [ name ] = field . default if missing_keys : raise MissingStepParameterError ( self . name , missing_keys , self . CONFIG_CLASS ) def _prepare_input_artifacts ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Dict [ str , Channel ]: \"\"\"Verifies and prepares the input artifacts for running this step. Args: *artifacts: Positional input artifacts passed to the __call__ method. **kw_artifacts: Keyword input artifacts passed to the __call__ method. Returns: Dictionary containing both the positional and keyword input artifacts. Raises: StepInterfaceError: If there are too many or too few artifacts. \"\"\" input_artifact_keys = list ( self . INPUT_SIGNATURE . keys ()) if len ( artifacts ) > len ( input_artifact_keys ): raise StepInterfaceError ( f \"Too many input artifacts for step ' { self . name } '. \" f \"This step expects { len ( input_artifact_keys ) } artifact(s) \" f \"but got { len ( artifacts ) + len ( kw_artifacts ) } .\" ) combined_artifacts = {} for i , artifact in enumerate ( artifacts ): if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for positional \" f \"argument { i } of step ' { self . name } '. Only outputs \" f \"from previous steps can be used as arguments when \" f \"connecting steps.\" ) key = input_artifact_keys [ i ] combined_artifacts [ key ] = artifact for key , artifact in kw_artifacts . items (): if key in combined_artifacts : # an artifact for this key was already set by # the positional input artifacts raise StepInterfaceError ( f \"Unexpected keyword argument ' { key } ' for step \" f \"' { self . name } '. An artifact for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for argument \" f \"' { key } ' of step ' { self . name } '. Only outputs from \" f \"previous steps can be used as arguments when \" f \"connecting steps.\" ) combined_artifacts [ key ] = artifact # check if there are any missing or unexpected artifacts expected_artifacts = set ( self . INPUT_SIGNATURE . keys ()) actual_artifacts = set ( combined_artifacts . keys ()) missing_artifacts = expected_artifacts - actual_artifacts unexpected_artifacts = actual_artifacts - expected_artifacts if missing_artifacts : raise StepInterfaceError ( f \"Missing input artifact(s) for step \" f \"' { self . name } ': { missing_artifacts } .\" ) if unexpected_artifacts : raise StepInterfaceError ( f \"Unexpected input artifact(s) for step \" f \"' { self . name } ': { unexpected_artifacts } . This step \" f \"only requires the following artifacts: { expected_artifacts } .\" ) return combined_artifacts def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns @property def component ( self ) -> _ZenMLSimpleComponent : \"\"\"Returns a TFX component.\"\"\" if not self . _component : raise StepInterfaceError ( \"Trying to access the step component \" \"before creating it via calling the step.\" ) return self . _component def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self component : _ZenMLSimpleComponent property readonly Returns a TFX component. __call__ ( self , * artifacts , ** kw_artifacts ) special Generates a component when called. Source code in zenml/steps/base_step.py def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns entrypoint ( self , * args , ** kwargs ) Abstract method for core step logic. Source code in zenml/steps/base_step.py @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" get_materializers ( self , ensure_complete = False ) Returns available materializers for the outputs of this step. Parameters: Name Type Description Default ensure_complete bool If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. False Returns: Type Description Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Exceptions: Type Description StepInterfaceError (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. Source code in zenml/steps/base_step.py def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers with_return_materializers ( self , materializers ) Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Parameters: Name Type Description Default materializers Union[Type[zenml.materializers.base_materializer.BaseMaterializer], Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]]] The materializers for the outputs of this step. required Returns: Type Description ~T The object that this method was called on. Exceptions: Type Description StepInterfaceError If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. Source code in zenml/steps/base_step.py def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self BaseStepMeta ( type ) Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class Source code in zenml/steps/base_step.py class BaseStepMeta ( type ): \"\"\"Metaclass for `BaseStep`. Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class \"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls __new__ ( mcs , name , bases , dct ) special staticmethod Set up a new class with a qualified spec. Source code in zenml/steps/base_step.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls base_step_config BaseStepConfig ( BaseModel ) pydantic-model Base configuration class to pass execution params into a step. Source code in zenml/steps/base_step_config.py class BaseStepConfig ( BaseModel ): \"\"\"Base configuration class to pass execution params into a step.\"\"\" builtin_steps special pandas_analyzer PandasAnalyzer ( BaseAnalyzerStep ) Simple step implementation which analyzes a given pd.DataFrame Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzer ( BaseAnalyzerStep ): \"\"\"Simple step implementation which analyzes a given pd.DataFrame\"\"\" # Manually defining the type of the output artifacts OUTPUT_SPEC = { \"statistics\" : StatisticsArtifact , \"schema\" : SchemaArtifact } def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema CONFIG_CLASS ( BaseAnalyzerConfig ) pydantic-model Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None entrypoint ( self , dataset , config ) Main entrypoint function for the pandas analyzer Parameters: Name Type Description Default dataset DataFrame pd.DataFrame, the given dataset required config PandasAnalyzerConfig the configuration of the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc4168b50> the statistics and the schema of the given dataframe Source code in zenml/steps/builtin_steps/pandas_analyzer.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema PandasAnalyzerConfig ( BaseAnalyzerConfig ) pydantic-model Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None pandas_datasource PandasDatasource ( BaseDatasourceStep ) Simple step implementation to ingest from a csv file using pandas Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasource ( BaseDatasourceStep ): \"\"\"Simple step implementation to ingest from a csv file using pandas\"\"\" def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , ) CONFIG_CLASS ( BaseDatasourceConfig ) pydantic-model Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None entrypoint ( self , config ) Main entrypoint method for the PandasDatasource Parameters: Name Type Description Default config PandasDatasourceConfig the configuration of the step required Returns: Type Description DataFrame the resulting dataframe Source code in zenml/steps/builtin_steps/pandas_datasource.py def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , ) PandasDatasourceConfig ( BaseDatasourceConfig ) pydantic-model Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None step_context StepContext Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. Source code in zenml/steps/step_context.py class StepContext : \"\"\"Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a `StepContext` object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a `StepContext` object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the `StepContext` and automatically pass it when executing your step. **Note**: When using a `StepContext` inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the `@step` decorator or when initializing your custom step class. \"\"\" def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepInterfaceError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } def _get_output ( self , output_name : Optional [ str ] = None ) -> StepContextOutput : \"\"\"Returns the materializer and artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the materializer and URI. Returns: Tuple containing the materializer and artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" output_count = len ( self . _outputs ) if output_count == 0 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step does not have any outputs.\" ) if not output_name and output_count > 1 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step has multiple outputs ( { set ( self . _outputs ) } ), \" f \"please specify which output to return.\" ) if output_name : if output_name not in self . _outputs : raise StepContextError ( f \"Unable to get step output ' { output_name } ' for \" f \"step ' { self . step_name } '. This step does not have an \" f \"output with the given name, please specify one of the \" f \"available outputs: { set ( self . _outputs ) } .\" ) return self . _outputs [ output_name ] else : return next ( iter ( self . _outputs . values ())) def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri ) __init__ ( self , step_name , output_materializers , output_artifacts ) special Initializes a StepContext instance. Parameters: Name Type Description Default step_name str The name of the step that this context is used in. required output_materializers Dict[str, Type[BaseMaterializer]] The output materializers of the step that this context is used in. required output_artifacts Dict[str, BaseArtifact] The output artifacts of the step that this context is used in. required Exceptions: Type Description StepInterfaceError If the keys of the output materializers and Source code in zenml/steps/step_context.py def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepInterfaceError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } get_output_artifact_uri ( self , output_name = None ) Returns the artifact URI for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. None Returns: Type Description str Artifact URI for the given output. Exceptions: Type Description StepInterfaceError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri ) get_output_materializer ( self , output_name = None , custom_materializer_class = None ) Returns a materializer for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. None custom_materializer_class Optional[Type[BaseMaterializer]] If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. None Returns: Type Description BaseMaterializer A materializer initialized with the output artifact for the given output. Exceptions: Type Description StepInterfaceError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) StepContextOutput ( tuple ) Tuple containing materializer class and artifact for a step output. Source code in zenml/steps/step_context.py class StepContextOutput ( NamedTuple ): \"\"\"Tuple containing materializer class and artifact for a step output.\"\"\" materializer_class : Type [ \"BaseMaterializer\" ] artifact : \"BaseArtifact\" __getnewargs__ ( self ) special Return self as a plain tuple. Used by copy and pickle. Source code in zenml/steps/step_context.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self ) __new__ ( _cls , materializer_class , artifact ) special staticmethod Create new instance of StepContextOutput(materializer_class, artifact) __repr__ ( self ) special Return a nicely formatted representation string Source code in zenml/steps/step_context.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self step_decorator step ( _func = None , * , name = None , enable_cache = None , output_types = None ) Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the step. If left empty, the name of the decorated function will be used as a fallback. None enable_cache Optional[bool] Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class: zenml.steps.step_context.StepContext for more information). None output_types Optional[Dict[str, Type[BaseArtifact]]] A dictionary which sets different outputs to non-default artifact types None Returns: Type Description Union[Type[zenml.steps.base_step.BaseStep], Callable[[~F], Type[zenml.steps.base_step.BaseStep]]] the inner decorator which creates the step class based on the ZenML BaseStep Source code in zenml/steps/step_decorator.py def step ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : Optional [ bool ] = None , output_types : Optional [ Dict [ str , Type [ \"BaseArtifact\" ]]] = None , ) -> Union [ Type [ BaseStep ], Callable [[ F ], Type [ BaseStep ]]]: \"\"\"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as `name`, it features a nested decorator structure. Args: _func: The decorated function. name: The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a `StepContext` (see :class:`zenml.steps.step_context.StepContext` for more information). output_types: A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep \"\"\" def inner_decorator ( func : F ) -> Type [ BaseStep ]: \"\"\"Inner decorator function for the creation of a ZenML Step Args: func: types.FunctionType, this function will be used as the \"process\" method of the generated Step Returns: The class of a newly generated ZenML Step. \"\"\" step_name = name or func . __name__ output_spec = output_types or {} return type ( # noqa step_name , ( BaseStep ,), { STEP_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_CREATED_BY_FUNCTIONAL_API : True , }, OUTPUT_SPEC : output_spec , \"__module__\" : func . __module__ , }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func ) step_interfaces special base_analyzer_step BaseAnalyzerConfig ( BaseStepConfig ) pydantic-model Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\" BaseAnalyzerStep ( BaseStep ) Base step implementation for any analyzer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerStep ( BaseStep ): \"\"\"Base step implementation for any analyzer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\" entrypoint ( self , dataset , config , context ) Base entrypoint for any analyzer implementation Source code in zenml/steps/step_interfaces/base_analyzer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\" base_datasource_step BaseDatasourceConfig ( BaseStepConfig ) pydantic-model Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\" BaseDatasourceStep ( BaseStep ) Base step implementation for any datasource step implementation on ZenML Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceStep ( BaseStep ): \"\"\"Base step implementation for any datasource step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\" entrypoint ( self , config , context ) Base entrypoint for any datasource implementation Source code in zenml/steps/step_interfaces/base_datasource_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\" base_drift_detection_step BaseDriftDetectionConfig ( BaseStepConfig ) pydantic-model Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\" BaseDriftDetectionStep ( BaseStep ) Base step implementation for any drift detection step implementation on ZenML Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionStep ( BaseStep ): \"\"\"Base step implementation for any drift detection step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\" entrypoint ( self , reference_dataset , comparison_dataset , config , context ) Base entrypoint for any drift detection implementation Source code in zenml/steps/step_interfaces/base_drift_detection_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\" base_evaluator_step BaseEvaluatorConfig ( BaseStepConfig ) pydantic-model Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\" BaseEvaluatorStep ( BaseStep ) Base step implementation for any evaluator step implementation on ZenML Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorStep ( BaseStep ): \"\"\"Base step implementation for any evaluator step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\" entrypoint ( self , dataset , model , config , context ) Base entrypoint for any evaluator implementation Source code in zenml/steps/step_interfaces/base_evaluator_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\" base_preprocessor_step BasePreprocessorConfig ( BaseStepConfig ) pydantic-model Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\" BasePreprocessorStep ( BaseStep ) Base step implementation for any Preprocessor step implementation on ZenML Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorStep ( BaseStep ): \"\"\"Base step implementation for any Preprocessor step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\" entrypoint ( self , train_dataset , test_dataset , validation_dataset , statistics , schema , config , context ) Base entrypoint for any Preprocessor implementation Source code in zenml/steps/step_interfaces/base_preprocessor_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\" base_split_step BaseSplitStep ( BaseStep ) Base step implementation for any split step implementation on ZenML Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStep ( BaseStep ): \"\"\"Base step implementation for any split step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\" entrypoint ( self , dataset , config , context ) Entrypoint for a function for the split steps to run Source code in zenml/steps/step_interfaces/base_split_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\" BaseSplitStepConfig ( BaseStepConfig ) pydantic-model Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\" base_trainer_step BaseTrainerConfig ( BaseStepConfig ) pydantic-model Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\" BaseTrainerStep ( BaseStep ) Base step implementation for any Trainer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerStep ( BaseStep ): \"\"\"Base step implementation for any Trainer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\" CONFIG_CLASS ( BaseStepConfig ) pydantic-model Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\" entrypoint ( self , train_dataset , validation_dataset , config , context ) Base entrypoint for any Trainer implementation Source code in zenml/steps/step_interfaces/base_trainer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\" step_output Output A named tuple with a default name that cannot be overridden. Source code in zenml/steps/step_output.py class Output ( object ): \"\"\"A named tuple with a default name that cannot be overridden.\"\"\" def __init__ ( self , ** kwargs : Type [ Any ]): # TODO [ENG-161]: do we even need the named tuple here or is # a list of tuples (name, Type) sufficient? self . outputs = NamedTuple ( \"ZenOutput\" , ** kwargs ) # type: ignore[misc] def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items () items ( self ) Yields a tuple of type (output_name, output_type). Source code in zenml/steps/step_output.py def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items () utils The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML. do_types_match ( type_a , type_b ) Check whether type_a and type_b match. Parameters: Name Type Description Default type_a Type[Any] First Type to check. required type_b Type[Any] Second Type to check. required Returns: Type Description bool True if types match, otherwise False. Source code in zenml/steps/utils.py def do_types_match ( type_a : Type [ Any ], type_b : Type [ Any ]) -> bool : \"\"\"Check whether type_a and type_b match. Args: type_a: First Type to check. type_b: Second Type to check. Returns: True if types match, otherwise False. \"\"\" # TODO [ENG-158]: Check more complicated cases where type_a can be a sub-type # of type_b return type_a == type_b generate_component_class ( step_name , step_module , input_spec , output_spec , execution_parameter_names , step_function , materializers ) Generates a TFX component class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required step_module str Module in which the step class is defined. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required step_function Callable[..., Any] The actual function to execute when running the step. required materializers Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] Materializer classes for all outputs of the step. required Returns: Type Description Type[_ZenMLSimpleComponent] A TFX component class. Source code in zenml/steps/utils.py def generate_component_class ( step_name : str , step_module : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], step_function : Callable [ ... , Any ], materializers : Dict [ str , Type [ BaseMaterializer ]], ) -> Type [ \"_ZenMLSimpleComponent\" ]: \"\"\"Generates a TFX component class for a ZenML step. Args: step_name: Name of the step for which the component will be created. step_module: Module in which the step class is defined. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. step_function: The actual function to execute when running the step. materializers: Materializer classes for all outputs of the step. Returns: A TFX component class. \"\"\" component_spec_class = generate_component_spec_class ( step_name = step_name , input_spec = input_spec , output_spec = output_spec , execution_parameter_names = execution_parameter_names , ) # Create executor class executor_class_name = f \" { step_name } _Executor\" executor_class = type ( executor_class_name , ( _FunctionExecutor ,), { \"_FUNCTION\" : staticmethod ( step_function ), \"__module__\" : step_module , \"materializers\" : materializers , PARAM_STEP_NAME : step_name , }, ) # Add the executor class to the module in which the step was defined module = sys . modules [ step_module ] setattr ( module , executor_class_name , executor_class ) return type ( step_name , ( _ZenMLSimpleComponent ,), { \"SPEC_CLASS\" : component_spec_class , \"EXECUTOR_SPEC\" : ExecutorClassSpec ( executor_class = executor_class ), \"__module__\" : step_module , }, ) generate_component_spec_class ( step_name , input_spec , output_spec , execution_parameter_names ) Generates a TFX component spec class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required Returns: Type Description Type[tfx.types.component_spec.ComponentSpec] A TFX component spec class. Source code in zenml/steps/utils.py def generate_component_spec_class ( step_name : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], ) -> Type [ component_spec . ComponentSpec ]: \"\"\"Generates a TFX component spec class for a ZenML step. Args: step_name: Name of the step for which the component will be created. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. Returns: A TFX component spec class. \"\"\" inputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in input_spec . items () } outputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in output_spec . items () } parameters = { key : component_spec . ExecutionParameter ( type = str ) # type: ignore[no-untyped-call] # noqa for key in execution_parameter_names } return type ( f \" { step_name } _Spec\" , ( component_spec . ComponentSpec ,), { \"INPUTS\" : inputs , \"OUTPUTS\" : outputs , \"PARAMETERS\" : parameters , }, )","title":"Steps"},{"location":"api_docs/steps/#steps","text":"","title":"Steps"},{"location":"api_docs/steps/#zenml.steps","text":"A step is a single piece or stage of a ZenML pipeline. Think of each step as being one of the nodes of a Directed Acyclic Graph (or DAG). Steps are responsible for one aspect of processing or interacting with the data / artifacts in the pipeline. ZenML currently implements a basic step interface, but there will be other more customized interfaces (layered in a hierarchy) for specialized implementations. Conceptually, a Step is a discrete and independent part of a pipeline that is responsible for one particular aspect of data manipulation inside a ZenML pipeline. Steps can be subclassed from the BaseStep class, or used via our @step decorator.","title":"steps"},{"location":"api_docs/steps/#zenml.steps.base_step","text":"","title":"base_step"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep","text":"Abstract base class for all ZenML steps. Attributes: Name Type Description name The name of this step. pipeline_parameter_name Optional[str] The name of the pipeline parameter for which this step was passed as an argument. enable_cache A boolean indicating if caching is enabled for this step. requires_context A boolean indicating if this step requires a StepContext object during execution. Source code in zenml/steps/base_step.py class BaseStep ( metaclass = BaseStepMeta ): \"\"\"Abstract base class for all ZenML steps. Attributes: name: The name of this step. pipeline_parameter_name: The name of the pipeline parameter for which this step was passed as an argument. enable_cache: A boolean indicating if caching is enabled for this step. requires_context: A boolean indicating if this step requires a `StepContext` object during execution. \"\"\" # TODO [ENG-156]: Ensure these are ordered INPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa OUTPUT_SIGNATURE : ClassVar [ Dict [ str , Type [ Any ]]] = None # type: ignore[assignment] # noqa CONFIG_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None CONFIG_CLASS : ClassVar [ Optional [ Type [ BaseStepConfig ]]] = None CONTEXT_PARAMETER_NAME : ClassVar [ Optional [ str ]] = None PARAM_SPEC : Dict [ str , Any ] = {} INPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} OUTPUT_SPEC : Dict [ str , Type [ BaseArtifact ]] = {} INSTANCE_CONFIGURATION : Dict [ str , Any ] = {} def __init__ ( self , * args : Any , ** kwargs : Any ) -> None : self . name = self . __class__ . __name__ self . pipeline_parameter_name : Optional [ str ] = None kwargs . update ( getattr ( self , INSTANCE_CONFIGURATION )) self . requires_context = bool ( self . CONTEXT_PARAMETER_NAME ) self . _created_by_functional_api = kwargs . pop ( PARAM_CREATED_BY_FUNCTIONAL_API , False ) enable_cache = kwargs . pop ( PARAM_ENABLE_CACHE , None ) if enable_cache is None : if self . requires_context : # Using the StepContext inside a step provides access to # external resources which might influence the step execution. # We therefore disable caching unless it is explicitly enabled enable_cache = False logger . debug ( \"Step ' %s ': Step context required and caching not \" \"explicitly enabled.\" , self . name , ) else : # Default to cache enabled if not explicitly set enable_cache = True logger . debug ( \"Step ' %s ': Caching %s .\" , self . name , \"enabled\" if enable_cache else \"disabled\" , ) self . enable_cache = enable_cache self . _explicit_materializers : Dict [ str , Type [ BaseMaterializer ]] = {} self . _component : Optional [ _ZenMLSimpleComponent ] = None self . _verify_init_arguments ( * args , ** kwargs ) self . _verify_output_spec () @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\" def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers @property def _internal_execution_parameters ( self ) -> Dict [ str , Any ]: \"\"\"ZenML internal execution parameters for this step.\"\"\" parameters = { PARAM_PIPELINE_PARAMETER_NAME : self . pipeline_parameter_name } if self . enable_cache : # Caching is enabled so we compute a hash of the step function code # and materializers to catch changes in the step behavior def _get_hashed_source ( value : Any ) -> str : \"\"\"Returns a hash of the objects source code.\"\"\" source_code = inspect . getsource ( value ) return hashlib . sha256 ( source_code . encode ( \"utf-8\" )) . hexdigest () # If the step was defined using the functional api, only track # changes to the entrypoint function. Otherwise track changes to # the entire step class. source_object = ( self . entrypoint if self . _created_by_functional_api else self . __class__ ) parameters [ \"step_source\" ] = _get_hashed_source ( source_object ) for name , materializer in self . get_materializers () . items (): key = f \" { name } _materializer_source\" parameters [ key ] = _get_hashed_source ( materializer ) else : # Add a random string to the execution properties to disable caching random_string = f \" { random . getrandbits ( 128 ) : 032x } \" parameters [ \"disable_cache\" ] = random_string return { INTERNAL_EXECUTION_PARAMETER_PREFIX + key : value for key , value in parameters . items () } def _verify_init_arguments ( self , * args : Any , ** kwargs : Any ) -> None : \"\"\"Verifies the initialization args and kwargs of this step. This method makes sure that there is only a config object passed at initialization and that it was passed using the correct name and type specified in the step declaration. If the correct config object was found, additionally saves the config parameters to `self.PARAM_SPEC`. Args: *args: The args passed to the init method of this step. **kwargs: The kwargs passed to the init method of this step. Raises: StepInterfaceError: If there are too many arguments or arguments with a wrong name/type. \"\"\" maximum_arg_count = 1 if self . CONFIG_CLASS else 0 arg_count = len ( args ) + len ( kwargs ) if arg_count > maximum_arg_count : raise StepInterfaceError ( f \"Too many arguments ( { arg_count } , expected: \" f \" { maximum_arg_count } ) passed when creating a \" f \"' { self . name } ' step.\" ) if self . CONFIG_PARAMETER_NAME and self . CONFIG_CLASS : if args : config = args [ 0 ] elif kwargs : key , config = kwargs . popitem () if key != self . CONFIG_PARAMETER_NAME : raise StepInterfaceError ( f \"Unknown keyword argument ' { key } ' when creating a \" f \"' { self . name } ' step, only expected a single \" f \"argument with key ' { self . CONFIG_PARAMETER_NAME } '.\" ) else : # This step requires configuration parameters but no config # object was passed as an argument. The parameters might be # set via default values in the config class or in a # configuration file, so we continue for now and verify # that all parameters are set before running the step return if not isinstance ( config , self . CONFIG_CLASS ): raise StepInterfaceError ( f \"` { config } ` object passed when creating a \" f \"' { self . name } ' step is not a \" f \"` { self . CONFIG_CLASS . __name__ } ` instance.\" ) self . PARAM_SPEC = config . dict () def _verify_output_spec ( self ) -> None : \"\"\"Verifies the explicitly set output artifact types of this step. Raises: StepInterfaceError: If an output artifact type is specified for a non-existent step output or the artifact type is not allowed for the corresponding output type. \"\"\" for output_name , artifact_type in self . OUTPUT_SPEC . items (): if output_name not in self . OUTPUT_SIGNATURE : raise StepInterfaceError ( f \"Found explicit artifact type for unrecognized output \" f \"' { output_name } ' in step ' { self . name } '. Output \" f \"artifact types can only be specified for the outputs \" f \"of this step: { set ( self . OUTPUT_SIGNATURE ) } .\" ) if not issubclass ( artifact_type , BaseArtifact ): raise StepInterfaceError ( f \"Invalid artifact type ( { artifact_type } ) for output \" f \"' { output_name } ' of step ' { self . name } '. Only \" f \"`BaseArtifact` subclasses are allowed as artifact types.\" ) output_type = self . OUTPUT_SIGNATURE [ output_name ] allowed_artifact_types = set ( type_registry . get_artifact_type ( output_type ) ) if artifact_type not in allowed_artifact_types : raise StepInterfaceError ( f \"Artifact type ` { artifact_type } ` for output \" f \"' { output_name } ' of step ' { self . name } ' is not an \" f \"allowed artifact type for the defined output type \" f \"` { output_type } `. Allowed artifact types: \" f \" { allowed_artifact_types } . If you want to extend the \" f \"allowed artifact types, implement a custom \" f \"`BaseMaterializer` subclass and set its \" f \"`ASSOCIATED_ARTIFACT_TYPES` and `ASSOCIATED_TYPES` \" f \"accordingly.\" ) def _update_and_verify_parameter_spec ( self ) -> None : \"\"\"Verifies and prepares the config parameters for running this step. When the step requires config parameters, this method: - checks if config parameters were set via a config object or file - tries to set missing config parameters from default values of the config class Raises: MissingStepParameterError: If no value could be found for one or more config parameters. StepInterfaceError: If a config parameter value couldn't be serialized to json. \"\"\" if self . CONFIG_CLASS : # we need to store a value for all config keys inside the # metadata store to make sure caching works as expected missing_keys = [] for name , field in self . CONFIG_CLASS . __fields__ . items (): if name in self . PARAM_SPEC : # a value for this parameter has been set already continue if field . required : # this field has no default value set and therefore needs # to be passed via an initialized config object missing_keys . append ( name ) else : # use default value from the pydantic config class self . PARAM_SPEC [ name ] = field . default if missing_keys : raise MissingStepParameterError ( self . name , missing_keys , self . CONFIG_CLASS ) def _prepare_input_artifacts ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Dict [ str , Channel ]: \"\"\"Verifies and prepares the input artifacts for running this step. Args: *artifacts: Positional input artifacts passed to the __call__ method. **kw_artifacts: Keyword input artifacts passed to the __call__ method. Returns: Dictionary containing both the positional and keyword input artifacts. Raises: StepInterfaceError: If there are too many or too few artifacts. \"\"\" input_artifact_keys = list ( self . INPUT_SIGNATURE . keys ()) if len ( artifacts ) > len ( input_artifact_keys ): raise StepInterfaceError ( f \"Too many input artifacts for step ' { self . name } '. \" f \"This step expects { len ( input_artifact_keys ) } artifact(s) \" f \"but got { len ( artifacts ) + len ( kw_artifacts ) } .\" ) combined_artifacts = {} for i , artifact in enumerate ( artifacts ): if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for positional \" f \"argument { i } of step ' { self . name } '. Only outputs \" f \"from previous steps can be used as arguments when \" f \"connecting steps.\" ) key = input_artifact_keys [ i ] combined_artifacts [ key ] = artifact for key , artifact in kw_artifacts . items (): if key in combined_artifacts : # an artifact for this key was already set by # the positional input artifacts raise StepInterfaceError ( f \"Unexpected keyword argument ' { key } ' for step \" f \"' { self . name } '. An artifact for this key was \" f \"already passed as a positional argument.\" ) if not isinstance ( artifact , Channel ): raise StepInterfaceError ( f \"Wrong argument type (` { type ( artifact ) } `) for argument \" f \"' { key } ' of step ' { self . name } '. Only outputs from \" f \"previous steps can be used as arguments when \" f \"connecting steps.\" ) combined_artifacts [ key ] = artifact # check if there are any missing or unexpected artifacts expected_artifacts = set ( self . INPUT_SIGNATURE . keys ()) actual_artifacts = set ( combined_artifacts . keys ()) missing_artifacts = expected_artifacts - actual_artifacts unexpected_artifacts = actual_artifacts - expected_artifacts if missing_artifacts : raise StepInterfaceError ( f \"Missing input artifact(s) for step \" f \"' { self . name } ': { missing_artifacts } .\" ) if unexpected_artifacts : raise StepInterfaceError ( f \"Unexpected input artifact(s) for step \" f \"' { self . name } ': { unexpected_artifacts } . This step \" f \"only requires the following artifacts: { expected_artifacts } .\" ) return combined_artifacts def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns @property def component ( self ) -> _ZenMLSimpleComponent : \"\"\"Returns a TFX component.\"\"\" if not self . _component : raise StepInterfaceError ( \"Trying to access the step component \" \"before creating it via calling the step.\" ) return self . _component def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self","title":"BaseStep"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.component","text":"Returns a TFX component.","title":"component"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.__call__","text":"Generates a component when called. Source code in zenml/steps/base_step.py def __call__ ( self , * artifacts : Channel , ** kw_artifacts : Channel ) -> Union [ Channel , List [ Channel ]]: \"\"\"Generates a component when called.\"\"\" # TODO [ENG-157]: replaces Channels with ZenML class (BaseArtifact?) self . _update_and_verify_parameter_spec () # Prepare the input artifacts and spec input_artifacts = self . _prepare_input_artifacts ( * artifacts , ** kw_artifacts ) self . INPUT_SPEC = { arg_name : artifact_type . type # type:ignore[misc] for arg_name , artifact_type in input_artifacts . items () } # make sure we have registered materializers for each output materializers = self . get_materializers ( ensure_complete = True ) # Prepare the output artifacts and spec for key , value in self . OUTPUT_SIGNATURE . items (): verified_types = type_registry . get_artifact_type ( value ) if key not in self . OUTPUT_SPEC : self . OUTPUT_SPEC [ key ] = verified_types [ 0 ] execution_parameters = { ** self . PARAM_SPEC , ** self . _internal_execution_parameters , } # Convert execution parameter values to strings try : execution_parameters = { k : json . dumps ( v ) for k , v in execution_parameters . items () } except TypeError as e : raise StepInterfaceError ( f \"Failed to serialize execution parameters for step \" f \"' { self . name } '. Please make sure to only use \" f \"json serializable parameter values.\" ) from e source_fn = getattr ( self , STEP_INNER_FUNC_NAME ) component_class = generate_component_class ( step_name = self . name , step_module = self . __module__ , input_spec = self . INPUT_SPEC , output_spec = self . OUTPUT_SPEC , execution_parameter_names = set ( execution_parameters ), step_function = source_fn , materializers = materializers , ) self . _component = component_class ( ** input_artifacts , ** execution_parameters ) # Resolve the returns in the right order. returns = [ self . component . outputs [ key ] for key in self . OUTPUT_SPEC ] # If its one return we just return the one channel not as a list if len ( returns ) == 1 : return returns [ 0 ] else : return returns","title":"__call__()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.entrypoint","text":"Abstract method for core step logic. Source code in zenml/steps/base_step.py @abstractmethod def entrypoint ( self , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Abstract method for core step logic.\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.get_materializers","text":"Returns available materializers for the outputs of this step. Parameters: Name Type Description Default ensure_complete bool If set to True , this method will raise a StepInterfaceError if no materializer can be found for an output. False Returns: Type Description Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] A dictionary mapping output names to BaseMaterializer subclasses. If no explicit materializer was set using step.with_return_materializers(...) , this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Exceptions: Type Description StepInterfaceError (Only if ensure_complete is set to True ) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. Source code in zenml/steps/base_step.py def get_materializers ( self , ensure_complete : bool = False ) -> Dict [ str , Type [ BaseMaterializer ]]: \"\"\"Returns available materializers for the outputs of this step. Args: ensure_complete: If set to `True`, this method will raise a `StepInterfaceError` if no materializer can be found for an output. Returns: A dictionary mapping output names to `BaseMaterializer` subclasses. If no explicit materializer was set using `step.with_return_materializers(...)`, this checks the default materializer registry to find a materializer for the type of the output. If no materializer is registered, the output of this method will not contain an entry for this output. Raises: StepInterfaceError: (Only if `ensure_complete` is set to `True`) If an output does not have an explicit materializer assigned to it and there is no default materializer registered for the output type. \"\"\" materializers = self . _explicit_materializers for output_name , output_type in self . OUTPUT_SIGNATURE . items (): if output_name in materializers : # Materializer for this output was set explicitly pass elif default_materializer_registry . is_registered ( output_type ): materializer = default_materializer_registry [ output_type ] materializers [ output_name ] = materializer else : if ensure_complete : raise StepInterfaceError ( f \"Unable to find materializer for output \" f \"' { output_name } ' of type ` { output_type } ` in step \" f \"' { self . name } '. Please make sure to either \" f \"explicitly set a materializer for step outputs \" f \"using `step.with_return_materializers(...)` or \" f \"registering a default materializer for specific \" f \"types by subclassing `BaseMaterializer` and setting \" f \"its `ASSOCIATED_TYPES` class variable.\" ) return materializers","title":"get_materializers()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStep.with_return_materializers","text":"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Parameters: Name Type Description Default materializers Union[Type[zenml.materializers.base_materializer.BaseMaterializer], Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]]] The materializers for the outputs of this step. required Returns: Type Description ~T The object that this method was called on. Exceptions: Type Description StepInterfaceError If a materializer is not a BaseMaterializer subclass or a materializer for a non-existent output is given. Source code in zenml/steps/base_step.py def with_return_materializers ( self : T , materializers : Union [ Type [ BaseMaterializer ], Dict [ str , Type [ BaseMaterializer ]] ], ) -> T : \"\"\"Register materializers for step outputs. If a single materializer is passed, it will be used for all step outputs. Otherwise, the dictionary keys specify the output names for which the materializers will be used. Args: materializers: The materializers for the outputs of this step. Returns: The object that this method was called on. Raises: StepInterfaceError: If a materializer is not a `BaseMaterializer` subclass or a materializer for a non-existent output is given. \"\"\" def _is_materializer_class ( value : Any ) -> bool : \"\"\"Checks whether the given object is a `BaseMaterializer` subclass.\"\"\" is_class = isinstance ( value , type ) return is_class and issubclass ( value , BaseMaterializer ) if isinstance ( materializers , dict ): allowed_output_names = set ( self . OUTPUT_SIGNATURE ) for output_name , materializer in materializers . items (): if output_name not in allowed_output_names : raise StepInterfaceError ( f \"Got unexpected materializers for non-existent \" f \"output ' { output_name } ' in step ' { self . name } '. \" f \"Only materializers for the outputs \" f \" { allowed_output_names } of this step can\" f \" be registered.\" ) if not _is_materializer_class ( materializer ): raise StepInterfaceError ( f \"Got unexpected object ` { materializer } ` as \" f \"materializer for output ' { output_name } ' of step \" f \"' { self . name } '. Only `BaseMaterializer` \" f \"subclasses are allowed.\" ) self . _explicit_materializers [ output_name ] = materializer elif _is_materializer_class ( materializers ): # Set the materializer for all outputs of this step self . _explicit_materializers = { key : materializers for key in self . OUTPUT_SIGNATURE } else : raise StepInterfaceError ( f \"Got unexpected object ` { materializers } ` as output \" f \"materializer for step ' { self . name } '. Only \" f \"`BaseMaterializer` subclasses or dictionaries mapping \" f \"output names to `BaseMaterializer` subclasses are allowed \" f \"as input when specifying return materializers.\" ) return self","title":"with_return_materializers()"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStepMeta","text":"Metaclass for BaseStep . Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class Source code in zenml/steps/base_step.py class BaseStepMeta ( type ): \"\"\"Metaclass for `BaseStep`. Checks whether everything passed in: * Has a matching materializer. * Is a subclass of the Config class \"\"\" def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls","title":"BaseStepMeta"},{"location":"api_docs/steps/#zenml.steps.base_step.BaseStepMeta.__new__","text":"Set up a new class with a qualified spec. Source code in zenml/steps/base_step.py def __new__ ( mcs , name : str , bases : Tuple [ Type [ Any ], ... ], dct : Dict [ str , Any ] ) -> \"BaseStepMeta\" : \"\"\"Set up a new class with a qualified spec.\"\"\" dct . setdefault ( \"PARAM_SPEC\" , {}) dct . setdefault ( \"INPUT_SPEC\" , {}) dct . setdefault ( \"OUTPUT_SPEC\" , {}) cls = cast ( Type [ \"BaseStep\" ], super () . __new__ ( mcs , name , bases , dct )) cls . INPUT_SIGNATURE = {} cls . OUTPUT_SIGNATURE = {} cls . CONFIG_PARAMETER_NAME = None cls . CONFIG_CLASS = None cls . CONTEXT_PARAMETER_NAME = None # Get the signature of the step function step_function_signature = inspect . getfullargspec ( getattr ( cls , STEP_INNER_FUNC_NAME ) ) if bases : # We're not creating the abstract `BaseStep` class # but a concrete implementation. Make sure the step function # signature does not contain variable *args or **kwargs variable_arguments = None if step_function_signature . varargs : variable_arguments = f \"* { step_function_signature . varargs } \" elif step_function_signature . varkw : variable_arguments = f \"** { step_function_signature . varkw } \" if variable_arguments : raise StepInterfaceError ( f \"Unable to create step ' { name } ' with variable arguments \" f \"' { variable_arguments } '. Please make sure your step \" f \"functions are defined with a fixed amount of arguments.\" ) step_function_args = ( step_function_signature . args + step_function_signature . kwonlyargs ) # Remove 'self' from the signature if it exists if step_function_args and step_function_args [ 0 ] == \"self\" : step_function_args . pop ( 0 ) # Verify the input arguments of the step function for arg in step_function_args : arg_type = step_function_signature . annotations . get ( arg , None ) if not arg_type : raise StepInterfaceError ( f \"Missing type annotation for argument ' { arg } ' when \" f \"trying to create step ' { name } '. Please make sure to \" f \"include type annotations for all your step inputs \" f \"and outputs.\" ) if issubclass ( arg_type , BaseStepConfig ): # Raise an error if we already found a config in the signature if cls . CONFIG_CLASS is not None : raise StepInterfaceError ( f \"Found multiple configuration arguments \" f \"(' { cls . CONFIG_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `BaseStepConfig` subclass as input \" f \"argument for a step.\" ) cls . CONFIG_PARAMETER_NAME = arg cls . CONFIG_CLASS = arg_type elif issubclass ( arg_type , StepContext ): if cls . CONTEXT_PARAMETER_NAME is not None : raise StepInterfaceError ( f \"Found multiple context arguments \" f \"(' { cls . CONTEXT_PARAMETER_NAME } ' and ' { arg } ') when \" f \"trying to create step ' { name } '. Please make sure to \" f \"only have one `StepContext` as input \" f \"argument for a step.\" ) cls . CONTEXT_PARAMETER_NAME = arg else : # Can't do any check for existing materializers right now # as they might get be defined later, so we simply store the # argument name and type for later use. cls . INPUT_SIGNATURE . update ({ arg : arg_type }) # Parse the returns of the step function return_type = step_function_signature . annotations . get ( \"return\" , None ) if return_type is not None : if isinstance ( return_type , Output ): cls . OUTPUT_SIGNATURE = dict ( return_type . items ()) else : cls . OUTPUT_SIGNATURE [ SINGLE_RETURN_OUT_NAME ] = return_type # Raise an exception if input and output names of a step overlap as # tfx requires them to be unique # TODO [ENG-155]: Can we prefix inputs and outputs to avoid this # restriction? counter : Counter [ str ] = collections . Counter () counter . update ( list ( cls . INPUT_SIGNATURE )) counter . update ( list ( cls . OUTPUT_SIGNATURE )) if cls . CONFIG_CLASS : counter . update ( list ( cls . CONFIG_CLASS . __fields__ . keys ())) shared_keys = { k for k in counter . elements () if counter [ k ] > 1 } if shared_keys : raise StepInterfaceError ( f \"The following keys are overlapping in the input, output and \" f \"config parameter names of step ' { name } ': { shared_keys } . \" f \"Please make sure that your input, output and config \" f \"parameter names are unique.\" ) return cls","title":"__new__()"},{"location":"api_docs/steps/#zenml.steps.base_step_config","text":"","title":"base_step_config"},{"location":"api_docs/steps/#zenml.steps.base_step_config.BaseStepConfig","text":"Base configuration class to pass execution params into a step. Source code in zenml/steps/base_step_config.py class BaseStepConfig ( BaseModel ): \"\"\"Base configuration class to pass execution params into a step.\"\"\"","title":"BaseStepConfig"},{"location":"api_docs/steps/#zenml.steps.builtin_steps","text":"","title":"builtin_steps"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer","text":"","title":"pandas_analyzer"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer","text":"Simple step implementation which analyzes a given pd.DataFrame Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzer ( BaseAnalyzerStep ): \"\"\"Simple step implementation which analyzes a given pd.DataFrame\"\"\" # Manually defining the type of the output artifacts OUTPUT_SPEC = { \"statistics\" : StatisticsArtifact , \"schema\" : SchemaArtifact } def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema","title":"PandasAnalyzer"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer.CONFIG_CLASS","text":"Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzer.entrypoint","text":"Main entrypoint function for the pandas analyzer Parameters: Name Type Description Default dataset DataFrame pd.DataFrame, the given dataset required config PandasAnalyzerConfig the configuration of the step required Returns: Type Description <zenml.steps.step_output.Output object at 0x7fefc4168b50> the statistics and the schema of the given dataframe Source code in zenml/steps/builtin_steps/pandas_analyzer.py def entrypoint ( # type: ignore[override] self , dataset : pd . DataFrame , config : PandasAnalyzerConfig , ) -> Output ( # type:ignore[valid-type] statistics = pd . DataFrame , schema = pd . DataFrame ): \"\"\"Main entrypoint function for the pandas analyzer Args: dataset: pd.DataFrame, the given dataset config: the configuration of the step Returns: the statistics and the schema of the given dataframe \"\"\" statistics = dataset . describe ( percentiles = config . percentiles , include = config . include , exclude = config . exclude , ) . T schema = dataset . dtypes . to_frame () . T . astype ( str ) return statistics , schema","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_analyzer.PandasAnalyzerConfig","text":"Config class for the PandasAnalyzer Config Source code in zenml/steps/builtin_steps/pandas_analyzer.py class PandasAnalyzerConfig ( BaseAnalyzerConfig ): \"\"\"Config class for the PandasAnalyzer Config\"\"\" percentiles : List [ float ] = [ 0.25 , 0.5 , 0.75 ] include : Optional [ Union [ str , List [ Type [ Any ]]]] = None exclude : Optional [ Union [ str , List [ Type [ Any ]]]] = None","title":"PandasAnalyzerConfig"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource","text":"","title":"pandas_datasource"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource","text":"Simple step implementation to ingest from a csv file using pandas Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasource ( BaseDatasourceStep ): \"\"\"Simple step implementation to ingest from a csv file using pandas\"\"\" def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , )","title":"PandasDatasource"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource.CONFIG_CLASS","text":"Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasource.entrypoint","text":"Main entrypoint method for the PandasDatasource Parameters: Name Type Description Default config PandasDatasourceConfig the configuration of the step required Returns: Type Description DataFrame the resulting dataframe Source code in zenml/steps/builtin_steps/pandas_datasource.py def entrypoint ( # type: ignore[override] self , config : PandasDatasourceConfig , ) -> pd . DataFrame : \"\"\"Main entrypoint method for the PandasDatasource Args: config: the configuration of the step Returns: the resulting dataframe \"\"\" return pd . read_csv ( filepath_or_buffer = config . path , sep = config . sep , header = config . header , names = config . names , index_col = config . index_col , )","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.builtin_steps.pandas_datasource.PandasDatasourceConfig","text":"Config class for the pandas csv datasource Source code in zenml/steps/builtin_steps/pandas_datasource.py class PandasDatasourceConfig ( BaseDatasourceConfig ): \"\"\"Config class for the pandas csv datasource\"\"\" path : str sep : str = \",\" header : Union [ int , List [ int ], str ] = \"infer\" names : Optional [ List [ str ]] = None index_col : Optional [ Union [ int , str , List [ Union [ int , str ]], bool ]] = None","title":"PandasDatasourceConfig"},{"location":"api_docs/steps/#zenml.steps.step_context","text":"","title":"step_context"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext","text":"Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a StepContext object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a StepContext object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the StepContext and automatically pass it when executing your step. Note : When using a StepContext inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the @step decorator or when initializing your custom step class. Source code in zenml/steps/step_context.py class StepContext : \"\"\"Provides additional context inside a step function. This class is used to access materializers and artifact URIs inside a step function. To use it, add a `StepContext` object to the signature of your step function like this: @step def my_step(context: StepContext, ...) context.get_output_materializer(...) You do not need to create a `StepContext` object yourself and pass it when creating the step, as long as you specify it in the signature ZenML will create the `StepContext` and automatically pass it when executing your step. **Note**: When using a `StepContext` inside a step, ZenML disables caching for this step by default as the context provides access to external resources which might influence the result of your step execution. To enable caching anyway, explicitly enable it in the `@step` decorator or when initializing your custom step class. \"\"\" def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepInterfaceError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () } def _get_output ( self , output_name : Optional [ str ] = None ) -> StepContextOutput : \"\"\"Returns the materializer and artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the materializer and URI. Returns: Tuple containing the materializer and artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" output_count = len ( self . _outputs ) if output_count == 0 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step does not have any outputs.\" ) if not output_name and output_count > 1 : raise StepContextError ( f \"Unable to get step output for step ' { self . step_name } ': \" f \"This step has multiple outputs ( { set ( self . _outputs ) } ), \" f \"please specify which output to return.\" ) if output_name : if output_name not in self . _outputs : raise StepContextError ( f \"Unable to get step output ' { output_name } ' for \" f \"step ' { self . step_name } '. This step does not have an \" f \"output with the given name, please specify one of the \" f \"available outputs: { set ( self . _outputs ) } .\" ) return self . _outputs [ output_name ] else : return next ( iter ( self . _outputs . values ())) def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact ) def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri )","title":"StepContext"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.__init__","text":"Initializes a StepContext instance. Parameters: Name Type Description Default step_name str The name of the step that this context is used in. required output_materializers Dict[str, Type[BaseMaterializer]] The output materializers of the step that this context is used in. required output_artifacts Dict[str, BaseArtifact] The output artifacts of the step that this context is used in. required Exceptions: Type Description StepInterfaceError If the keys of the output materializers and Source code in zenml/steps/step_context.py def __init__ ( self , step_name : str , output_materializers : Dict [ str , Type [ \"BaseMaterializer\" ]], output_artifacts : Dict [ str , \"BaseArtifact\" ], ): \"\"\"Initializes a StepContext instance. Args: step_name: The name of the step that this context is used in. output_materializers: The output materializers of the step that this context is used in. output_artifacts: The output artifacts of the step that this context is used in. Raises: StepInterfaceError: If the keys of the output materializers and output artifacts do not match. \"\"\" if output_materializers . keys () != output_artifacts . keys (): raise StepContextError ( f \"Mismatched keys in output materializers and output \" f \"artifacts for step ' { step_name } '. Output materializer \" f \"keys: { set ( output_materializers ) } , output artifact \" f \"keys: { set ( output_artifacts ) } \" ) self . step_name = step_name self . _outputs = { key : StepContextOutput ( output_materializers [ key ], output_artifacts [ key ] ) for key in output_materializers . keys () }","title":"__init__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.get_output_artifact_uri","text":"Returns the artifact URI for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. None Returns: Type Description str Artifact URI for the given output. Exceptions: Type Description StepInterfaceError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_artifact_uri ( self , output_name : Optional [ str ] = None ) -> str : \"\"\"Returns the artifact URI for a given step output. Args: output_name: Optional name of the output for which to get the URI. If no name is given and the step only has a single output, the URI of this output will be returned. If the step has multiple outputs, an exception will be raised. Returns: Artifact URI for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" return cast ( str , self . _get_output ( output_name ) . artifact . uri )","title":"get_output_artifact_uri()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContext.get_output_materializer","text":"Returns a materializer for a given step output. Parameters: Name Type Description Default output_name Optional[str] Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. None custom_materializer_class Optional[Type[BaseMaterializer]] If given, this BaseMaterializer subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. None Returns: Type Description BaseMaterializer A materializer initialized with the output artifact for the given output. Exceptions: Type Description StepInterfaceError If the step has no outputs, no output for the given output_name or if no output_name was given but the step has multiple outputs. Source code in zenml/steps/step_context.py def get_output_materializer ( self , output_name : Optional [ str ] = None , custom_materializer_class : Optional [ Type [ \"BaseMaterializer\" ]] = None , ) -> \"BaseMaterializer\" : \"\"\"Returns a materializer for a given step output. Args: output_name: Optional name of the output for which to get the materializer. If no name is given and the step only has a single output, the materializer of this output will be returned. If the step has multiple outputs, an exception will be raised. custom_materializer_class: If given, this `BaseMaterializer` subclass will be initialized with the output artifact instead of the materializer that was registered for this step output. Returns: A materializer initialized with the output artifact for the given output. Raises: StepInterfaceError: If the step has no outputs, no output for the given `output_name` or if no `output_name` was given but the step has multiple outputs. \"\"\" materializer_class , artifact = self . _get_output ( output_name ) # use custom materializer class if provided or fallback to default # materializer for output materializer_class = custom_materializer_class or materializer_class return materializer_class ( artifact )","title":"get_output_materializer()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput","text":"Tuple containing materializer class and artifact for a step output. Source code in zenml/steps/step_context.py class StepContextOutput ( NamedTuple ): \"\"\"Tuple containing materializer class and artifact for a step output.\"\"\" materializer_class : Type [ \"BaseMaterializer\" ] artifact : \"BaseArtifact\"","title":"StepContextOutput"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__getnewargs__","text":"Return self as a plain tuple. Used by copy and pickle. Source code in zenml/steps/step_context.py def __getnewargs__ ( self ): 'Return self as a plain tuple. Used by copy and pickle.' return _tuple ( self )","title":"__getnewargs__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__new__","text":"Create new instance of StepContextOutput(materializer_class, artifact)","title":"__new__()"},{"location":"api_docs/steps/#zenml.steps.step_context.StepContextOutput.__repr__","text":"Return a nicely formatted representation string Source code in zenml/steps/step_context.py def __repr__ ( self ): 'Return a nicely formatted representation string' return self . __class__ . __name__ + repr_fmt % self","title":"__repr__()"},{"location":"api_docs/steps/#zenml.steps.step_decorator","text":"","title":"step_decorator"},{"location":"api_docs/steps/#zenml.steps.step_decorator.step","text":"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as name , it features a nested decorator structure. Parameters: Name Type Description Default _func Optional[~F] The decorated function. None name Optional[str] The name of the step. If left empty, the name of the decorated function will be used as a fallback. None enable_cache Optional[bool] Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a StepContext (see :class: zenml.steps.step_context.StepContext for more information). None output_types Optional[Dict[str, Type[BaseArtifact]]] A dictionary which sets different outputs to non-default artifact types None Returns: Type Description Union[Type[zenml.steps.base_step.BaseStep], Callable[[~F], Type[zenml.steps.base_step.BaseStep]]] the inner decorator which creates the step class based on the ZenML BaseStep Source code in zenml/steps/step_decorator.py def step ( _func : Optional [ F ] = None , * , name : Optional [ str ] = None , enable_cache : Optional [ bool ] = None , output_types : Optional [ Dict [ str , Type [ \"BaseArtifact\" ]]] = None , ) -> Union [ Type [ BaseStep ], Callable [[ F ], Type [ BaseStep ]]]: \"\"\"Outer decorator function for the creation of a ZenML step In order to be able to work with parameters such as `name`, it features a nested decorator structure. Args: _func: The decorated function. name: The name of the step. If left empty, the name of the decorated function will be used as a fallback. enable_cache: Specify whether caching is enabled for this step. If no value is passed, caching is enabled by default unless the step requires a `StepContext` (see :class:`zenml.steps.step_context.StepContext` for more information). output_types: A dictionary which sets different outputs to non-default artifact types Returns: the inner decorator which creates the step class based on the ZenML BaseStep \"\"\" def inner_decorator ( func : F ) -> Type [ BaseStep ]: \"\"\"Inner decorator function for the creation of a ZenML Step Args: func: types.FunctionType, this function will be used as the \"process\" method of the generated Step Returns: The class of a newly generated ZenML Step. \"\"\" step_name = name or func . __name__ output_spec = output_types or {} return type ( # noqa step_name , ( BaseStep ,), { STEP_INNER_FUNC_NAME : staticmethod ( func ), INSTANCE_CONFIGURATION : { PARAM_ENABLE_CACHE : enable_cache , PARAM_CREATED_BY_FUNCTIONAL_API : True , }, OUTPUT_SPEC : output_spec , \"__module__\" : func . __module__ , }, ) if _func is None : return inner_decorator else : return inner_decorator ( _func )","title":"step()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces","text":"","title":"step_interfaces"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step","text":"","title":"base_analyzer_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerConfig","text":"Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\"","title":"BaseAnalyzerConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep","text":"Base step implementation for any analyzer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerStep ( BaseStep ): \"\"\"Base step implementation for any analyzer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\"","title":"BaseAnalyzerStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep.CONFIG_CLASS","text":"Base class for analyzer step configurations Source code in zenml/steps/step_interfaces/base_analyzer_step.py class BaseAnalyzerConfig ( BaseStepConfig ): \"\"\"Base class for analyzer step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_analyzer_step.BaseAnalyzerStep.entrypoint","text":"Base entrypoint for any analyzer implementation Source code in zenml/steps/step_interfaces/base_analyzer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseAnalyzerConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] statistics = StatisticsArtifact , schema = SchemaArtifact ): \"\"\"Base entrypoint for any analyzer implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step","text":"","title":"base_datasource_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceConfig","text":"Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\"","title":"BaseDatasourceConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep","text":"Base step implementation for any datasource step implementation on ZenML Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceStep ( BaseStep ): \"\"\"Base step implementation for any datasource step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\"","title":"BaseDatasourceStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep.CONFIG_CLASS","text":"Base class for datasource configs to inherit from Source code in zenml/steps/step_interfaces/base_datasource_step.py class BaseDatasourceConfig ( BaseStepConfig ): \"\"\"Base class for datasource configs to inherit from\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_datasource_step.BaseDatasourceStep.entrypoint","text":"Base entrypoint for any datasource implementation Source code in zenml/steps/step_interfaces/base_datasource_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , config : BaseDatasourceConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any datasource implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step","text":"","title":"base_drift_detection_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionConfig","text":"Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\"","title":"BaseDriftDetectionConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep","text":"Base step implementation for any drift detection step implementation on ZenML Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionStep ( BaseStep ): \"\"\"Base step implementation for any drift detection step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\"","title":"BaseDriftDetectionStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep.CONFIG_CLASS","text":"Base class for drift detection step configurations Source code in zenml/steps/step_interfaces/base_drift_detection_step.py class BaseDriftDetectionConfig ( BaseStepConfig ): \"\"\"Base class for drift detection step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_drift_detection_step.BaseDriftDetectionStep.entrypoint","text":"Base entrypoint for any drift detection implementation Source code in zenml/steps/step_interfaces/base_drift_detection_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , reference_dataset : DataArtifact , comparison_dataset : DataArtifact , config : BaseDriftDetectionConfig , context : StepContext , ) -> Any : \"\"\"Base entrypoint for any drift detection implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step","text":"","title":"base_evaluator_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorConfig","text":"Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\"","title":"BaseEvaluatorConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep","text":"Base step implementation for any evaluator step implementation on ZenML Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorStep ( BaseStep ): \"\"\"Base step implementation for any evaluator step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\"","title":"BaseEvaluatorStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep.CONFIG_CLASS","text":"Base class for evaluator step configurations Source code in zenml/steps/step_interfaces/base_evaluator_step.py class BaseEvaluatorConfig ( BaseStepConfig ): \"\"\"Base class for evaluator step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_evaluator_step.BaseEvaluatorStep.entrypoint","text":"Base entrypoint for any evaluator implementation Source code in zenml/steps/step_interfaces/base_evaluator_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , model : ModelArtifact , config : BaseEvaluatorConfig , context : StepContext , ) -> DataArtifact : \"\"\"Base entrypoint for any evaluator implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step","text":"","title":"base_preprocessor_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorConfig","text":"Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\"","title":"BasePreprocessorConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep","text":"Base step implementation for any Preprocessor step implementation on ZenML Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorStep ( BaseStep ): \"\"\"Base step implementation for any Preprocessor step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\"","title":"BasePreprocessorStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep.CONFIG_CLASS","text":"Base class for Preprocessor step configurations Source code in zenml/steps/step_interfaces/base_preprocessor_step.py class BasePreprocessorConfig ( BaseStepConfig ): \"\"\"Base class for Preprocessor step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_preprocessor_step.BasePreprocessorStep.entrypoint","text":"Base entrypoint for any Preprocessor implementation Source code in zenml/steps/step_interfaces/base_preprocessor_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , test_dataset : DataArtifact , validation_dataset : DataArtifact , statistics : StatisticsArtifact , schema : SchemaArtifact , config : BasePreprocessorConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train_transformed = DataArtifact , test_transformed = DataArtifact , validation_transformed = DataArtifact , ): \"\"\"Base entrypoint for any Preprocessor implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step","text":"","title":"base_split_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep","text":"Base step implementation for any split step implementation on ZenML Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStep ( BaseStep ): \"\"\"Base step implementation for any split step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\"","title":"BaseSplitStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep.CONFIG_CLASS","text":"Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStep.entrypoint","text":"Entrypoint for a function for the split steps to run Source code in zenml/steps/step_interfaces/base_split_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , dataset : DataArtifact , config : BaseSplitStepConfig , context : StepContext , ) -> Output ( # type:ignore[valid-type] train = DataArtifact , test = DataArtifact , validation = DataArtifact ): \"\"\"Entrypoint for a function for the split steps to run\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_split_step.BaseSplitStepConfig","text":"Base class for split configs to inherit from Source code in zenml/steps/step_interfaces/base_split_step.py class BaseSplitStepConfig ( BaseStepConfig ): \"\"\"Base class for split configs to inherit from\"\"\"","title":"BaseSplitStepConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step","text":"","title":"base_trainer_step"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerConfig","text":"Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\"","title":"BaseTrainerConfig"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep","text":"Base step implementation for any Trainer step implementation on ZenML Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerStep ( BaseStep ): \"\"\"Base step implementation for any Trainer step implementation on ZenML\"\"\" STEP_INNER_FUNC_NAME = \"entrypoint\" @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\"","title":"BaseTrainerStep"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep.CONFIG_CLASS","text":"Base class for Trainer step configurations Source code in zenml/steps/step_interfaces/base_trainer_step.py class BaseTrainerConfig ( BaseStepConfig ): \"\"\"Base class for Trainer step configurations\"\"\"","title":"CONFIG_CLASS"},{"location":"api_docs/steps/#zenml.steps.step_interfaces.base_trainer_step.BaseTrainerStep.entrypoint","text":"Base entrypoint for any Trainer implementation Source code in zenml/steps/step_interfaces/base_trainer_step.py @abstractmethod def entrypoint ( # type: ignore[override] self , train_dataset : DataArtifact , validation_dataset : DataArtifact , config : BaseTrainerConfig , context : StepContext , ) -> ModelArtifact : \"\"\"Base entrypoint for any Trainer implementation\"\"\"","title":"entrypoint()"},{"location":"api_docs/steps/#zenml.steps.step_output","text":"","title":"step_output"},{"location":"api_docs/steps/#zenml.steps.step_output.Output","text":"A named tuple with a default name that cannot be overridden. Source code in zenml/steps/step_output.py class Output ( object ): \"\"\"A named tuple with a default name that cannot be overridden.\"\"\" def __init__ ( self , ** kwargs : Type [ Any ]): # TODO [ENG-161]: do we even need the named tuple here or is # a list of tuples (name, Type) sufficient? self . outputs = NamedTuple ( \"ZenOutput\" , ** kwargs ) # type: ignore[misc] def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items ()","title":"Output"},{"location":"api_docs/steps/#zenml.steps.step_output.Output.items","text":"Yields a tuple of type (output_name, output_type). Source code in zenml/steps/step_output.py def items ( self ) -> Iterator [ Tuple [ str , Type [ Any ]]]: \"\"\"Yields a tuple of type (output_name, output_type).\"\"\" yield from self . outputs . __annotations__ . items ()","title":"items()"},{"location":"api_docs/steps/#zenml.steps.utils","text":"The collection of utility functions/classes are inspired by their original implementation of the Tensorflow Extended team, which can be found here: https://github.com/tensorflow/tfx/blob/master/tfx/dsl/component/experimental /decorators.py This version is heavily adjusted to work with the Pipeline-Step paradigm which is proposed by ZenML.","title":"utils"},{"location":"api_docs/steps/#zenml.steps.utils.do_types_match","text":"Check whether type_a and type_b match. Parameters: Name Type Description Default type_a Type[Any] First Type to check. required type_b Type[Any] Second Type to check. required Returns: Type Description bool True if types match, otherwise False. Source code in zenml/steps/utils.py def do_types_match ( type_a : Type [ Any ], type_b : Type [ Any ]) -> bool : \"\"\"Check whether type_a and type_b match. Args: type_a: First Type to check. type_b: Second Type to check. Returns: True if types match, otherwise False. \"\"\" # TODO [ENG-158]: Check more complicated cases where type_a can be a sub-type # of type_b return type_a == type_b","title":"do_types_match()"},{"location":"api_docs/steps/#zenml.steps.utils.generate_component_class","text":"Generates a TFX component class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required step_module str Module in which the step class is defined. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required step_function Callable[..., Any] The actual function to execute when running the step. required materializers Dict[str, Type[zenml.materializers.base_materializer.BaseMaterializer]] Materializer classes for all outputs of the step. required Returns: Type Description Type[_ZenMLSimpleComponent] A TFX component class. Source code in zenml/steps/utils.py def generate_component_class ( step_name : str , step_module : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], step_function : Callable [ ... , Any ], materializers : Dict [ str , Type [ BaseMaterializer ]], ) -> Type [ \"_ZenMLSimpleComponent\" ]: \"\"\"Generates a TFX component class for a ZenML step. Args: step_name: Name of the step for which the component will be created. step_module: Module in which the step class is defined. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. step_function: The actual function to execute when running the step. materializers: Materializer classes for all outputs of the step. Returns: A TFX component class. \"\"\" component_spec_class = generate_component_spec_class ( step_name = step_name , input_spec = input_spec , output_spec = output_spec , execution_parameter_names = execution_parameter_names , ) # Create executor class executor_class_name = f \" { step_name } _Executor\" executor_class = type ( executor_class_name , ( _FunctionExecutor ,), { \"_FUNCTION\" : staticmethod ( step_function ), \"__module__\" : step_module , \"materializers\" : materializers , PARAM_STEP_NAME : step_name , }, ) # Add the executor class to the module in which the step was defined module = sys . modules [ step_module ] setattr ( module , executor_class_name , executor_class ) return type ( step_name , ( _ZenMLSimpleComponent ,), { \"SPEC_CLASS\" : component_spec_class , \"EXECUTOR_SPEC\" : ExecutorClassSpec ( executor_class = executor_class ), \"__module__\" : step_module , }, )","title":"generate_component_class()"},{"location":"api_docs/steps/#zenml.steps.utils.generate_component_spec_class","text":"Generates a TFX component spec class for a ZenML step. Parameters: Name Type Description Default step_name str Name of the step for which the component will be created. required input_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Input artifacts of the step. required output_spec Dict[str, Type[zenml.artifacts.base_artifact.BaseArtifact]] Output artifacts of the step required execution_parameter_names Set[str] Execution parameter names of the step. required Returns: Type Description Type[tfx.types.component_spec.ComponentSpec] A TFX component spec class. Source code in zenml/steps/utils.py def generate_component_spec_class ( step_name : str , input_spec : Dict [ str , Type [ BaseArtifact ]], output_spec : Dict [ str , Type [ BaseArtifact ]], execution_parameter_names : Set [ str ], ) -> Type [ component_spec . ComponentSpec ]: \"\"\"Generates a TFX component spec class for a ZenML step. Args: step_name: Name of the step for which the component will be created. input_spec: Input artifacts of the step. output_spec: Output artifacts of the step execution_parameter_names: Execution parameter names of the step. Returns: A TFX component spec class. \"\"\" inputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in input_spec . items () } outputs = { key : component_spec . ChannelParameter ( type = artifact_type ) for key , artifact_type in output_spec . items () } parameters = { key : component_spec . ExecutionParameter ( type = str ) # type: ignore[no-untyped-call] # noqa for key in execution_parameter_names } return type ( f \" { step_name } _Spec\" , ( component_spec . ComponentSpec ,), { \"INPUTS\" : inputs , \"OUTPUTS\" : outputs , \"PARAMETERS\" : parameters , }, )","title":"generate_component_spec_class()"},{"location":"api_docs/utils/","text":"Utils zenml.utils special The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions. analytics_utils Analytics code for ZenML get_environment () Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native Source code in zenml/utils/analytics_utils.py def get_environment () -> str : \"\"\"Returns a string representing the execution environment of the pipeline. Currently, one of `docker`, `paperspace`, 'colab', or `native`\"\"\" if in_docker (): return \"docker\" elif in_google_colab (): return \"colab\" elif in_paperspace_gradient (): return \"paperspace\" else : return \"native\" get_segment_key () Get key for authorizing to Segment backend. Returns: Type Description str Segment key as a string. Source code in zenml/utils/analytics_utils.py def get_segment_key () -> str : \"\"\"Get key for authorizing to Segment backend. Returns: Segment key as a string. \"\"\" if IS_DEBUG_ENV : return SEGMENT_KEY_DEV else : return SEGMENT_KEY_PROD get_system_info () Returns system info as a dict. Returns: Type Description Dict[str, Any] A dict of system information. Source code in zenml/utils/analytics_utils.py def get_system_info () -> Dict [ str , Any ]: \"\"\"Returns system info as a dict. Returns: A dict of system information. \"\"\" system = platform . system () if system == \"Windows\" : release , version , csd , ptype = platform . win32_ver () return { \"os\" : \"windows\" , \"windows_version_release\" : release , \"windows_version\" : version , \"windows_version_service_pack\" : csd , \"windows_version_os_type\" : ptype , } if system == \"Darwin\" : return { \"os\" : \"mac\" , \"mac_version\" : platform . mac_ver ()[ 0 ]} if system == \"Linux\" : return { \"os\" : \"linux\" , \"linux_distro\" : distro . id (), \"linux_distro_like\" : distro . like (), \"linux_distro_version\" : distro . version (), } # We don't collect data for any other system. return { \"os\" : \"unknown\" } in_docker () Returns: True if running in a Docker container, else False Source code in zenml/utils/analytics_utils.py def in_docker () -> bool : \"\"\"Returns: True if running in a Docker container, else False\"\"\" # TODO [ENG-167]: Make this more reliable and add test. try : with open ( \"/proc/1/cgroup\" , \"rt\" ) as ifh : info = ifh . read () return \"docker\" in info or \"kubepod\" in info except ( FileNotFoundError , Exception ): return False in_google_colab () Returns: True if running in a Google Colab env, else False Source code in zenml/utils/analytics_utils.py def in_google_colab () -> bool : \"\"\"Returns: True if running in a Google Colab env, else False\"\"\" if \"COLAB_GPU\" in os . environ : return True return False in_paperspace_gradient () Returns: True if running in a Paperspace Gradient env, else False Source code in zenml/utils/analytics_utils.py def in_paperspace_gradient () -> bool : \"\"\"Returns: True if running in a Paperspace Gradient env, else False\"\"\" if \"PAPERSPACE_NOTEBOOK_REPO_ID\" in os . environ : return True return False parametrized ( dec ) This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: Source code in zenml/utils/analytics_utils.py def parametrized ( dec : Callable [ ... , Callable [ ... , Any ]] ) -> Callable [ ... , Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]]: \"\"\"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:\"\"\" def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl return layer track ( * args , ** kwargs ) Internal layer Source code in zenml/utils/analytics_utils.py def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl track_event ( event , metadata = None ) Track segment event if user opted-in. Parameters: Name Type Description Default event str Name of event to track in segment. required metadata Optional[Dict[str, Any]] Dict of metadata to track. None Returns: Type Description bool True if event is sent successfully, False is not. Source code in zenml/utils/analytics_utils.py def track_event ( event : str , metadata : Optional [ Dict [ str , Any ]] = None ) -> bool : \"\"\" Track segment event if user opted-in. Args: event: Name of event to track in segment. metadata: Dict of metadata to track. Returns: True if event is sent successfully, False is not. \"\"\" try : import analytics from zenml.config.global_config import GlobalConfig if analytics . write_key is None : analytics . write_key = get_segment_key () assert ( analytics . write_key is not None ), \"Analytics key not set but trying to make telemetry call.\" # Set this to 1 to avoid backoff loop analytics . max_retries = 1 gc = GlobalConfig () logger . debug ( f \"Attempting analytics: User: { gc . user_id } , \" f \"Event: { event } ,\" f \"Metadata: { metadata } \" ) if not gc . analytics_opt_in and event not in [ OPT_OUT_ANALYTICS , OPT_IN_ANALYTICS , ]: return False if metadata is None : metadata = {} # add basics metadata . update ( get_system_info ()) metadata . update ( { \"environment\" : get_environment (), \"version\" : __version__ , } ) analytics . track ( str ( gc . user_id ), event , metadata ) logger . debug ( f \"Analytics sent: User: { gc . user_id } , Event: { event } , Metadata: \" f \" { metadata } \" ) return True except Exception as e : # We should never fail main thread logger . debug ( f \"Analytics failed due to: { e } \" ) return False daemon Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/ check_if_daemon_is_running ( pid_file ) Checks whether a daemon process indicated by the PID file is running. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to check. required Source code in zenml/utils/daemon.py def check_if_daemon_is_running ( pid_file : str ) -> bool : \"\"\"Checks whether a daemon process indicated by the PID file is running. Args: pid_file: Path to file containing the PID of the daemon process to check. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): return False return psutil . pid_exists ( pid ) run_as_daemon ( daemon_function , pid_file , log_file = None , working_directory = '/' ) Runs a function as a daemon process. Parameters: Name Type Description Default daemon_function Callable[..., Any] The function to run as a daemon. required pid_file str Path to file in which to store the PID of the daemon process. required log_file Optional[str] Optional file to which the daemons stdout/stderr will be redirected to. None working_directory str Working directory for the daemon process, defaults to the root directory. '/' Exceptions: Type Description FileExistsError If the PID file already exists. Source code in zenml/utils/daemon.py def run_as_daemon ( daemon_function : Callable [ ... , Any ], pid_file : str , log_file : Optional [ str ] = None , working_directory : str = \"/\" , ) -> None : \"\"\"Runs a function as a daemon process. Args: daemon_function: The function to run as a daemon. pid_file: Path to file in which to store the PID of the daemon process. log_file: Optional file to which the daemons stdout/stderr will be redirected to. working_directory: Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError: If the PID file already exists. \"\"\" # convert to absolute path as we will change working directory later pid_file = os . path . abspath ( pid_file ) if log_file : log_file = os . path . abspath ( log_file ) # check if PID file exists if os . path . exists ( pid_file ): raise FileExistsError ( f \"The PID file ' { pid_file } ' already exists, either the daemon \" f \"process is already running or something went wrong.\" ) # first fork try : pid = os . fork () if pid > 0 : # this is the process that called `run_as_daemon` so we # simply return so it can keep running return except OSError as e : logger . error ( \"Unable to fork (error code: %d )\" , e . errno ) sys . exit ( 1 ) # decouple from parent environment os . chdir ( working_directory ) os . setsid () os . umask ( 0 ) # second fork try : pid = os . fork () if pid > 0 : # this is the parent of the future daemon process, kill it # so the daemon gets adopted by the init process sys . exit ( 0 ) except OSError as e : sys . stderr . write ( f \"Unable to fork (error code: { e . errno } )\" ) sys . exit ( 1 ) # redirect standard file descriptors to devnull (or the given logfile) devnull = \"/dev/null\" if hasattr ( os , \"devnull\" ): devnull = os . devnull devnull_fd = os . open ( devnull , os . O_RDWR ) log_fd = os . open ( log_file , os . O_CREAT | os . O_RDWR ) if log_file else None out_fd = log_fd or devnull_fd os . dup2 ( devnull_fd , sys . stdin . fileno ()) os . dup2 ( out_fd , sys . stdout . fileno ()) os . dup2 ( out_fd , sys . stderr . fileno ()) # write the PID file with open ( pid_file , \"w+\" ) as f : f . write ( f \" { os . getpid () } \\n \" ) # register actions in case this process exits/gets killed def sigterm ( signum : int , frame : Optional [ types . FrameType ]) -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) def cleanup () -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) signal . signal ( signal . SIGTERM , sigterm ) atexit . register ( cleanup ) # finally run the actual daemon code daemon_function () stop_daemon ( pid_file , kill_children = True ) Stops a daemon process. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to kill. required kill_children bool If True , all child processes of the daemon process will be killed as well. True Source code in zenml/utils/daemon.py def stop_daemon ( pid_file : str , kill_children : bool = True ) -> None : \"\"\"Stops a daemon process. Args: pid_file: Path to file containing the PID of the daemon process to kill. kill_children: If `True`, all child processes of the daemon process will be killed as well. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): logger . warning ( \"Daemon PID file ' %s ' does not exist.\" , pid_file ) return if psutil . pid_exists ( pid ): process = psutil . Process ( pid ) if kill_children : for child in process . children ( recursive = True ): child . kill () process . kill () else : logger . warning ( \"PID from ' %s ' does not exist.\" , pid_file ) networking_utils find_available_port () Finds a local unoccupied port. Source code in zenml/utils/networking_utils.py def find_available_port () -> int : \"\"\"Finds a local unoccupied port.\"\"\" with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , 0 )) _ , port = s . getsockname () return cast ( int , port ) port_available ( port ) Checks if a local port is available. Source code in zenml/utils/networking_utils.py def port_available ( port : int ) -> bool : \"\"\"Checks if a local port is available.\"\"\" try : with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , port )) except socket . error as e : logger . debug ( \"Port %d unavailable: %s \" , port , e ) return False return True source_utils These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class module_source: This is a python-import type path to a module, e.g. some.mod file_path, relative_path, absolute_path: These are file system paths. source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string. create_zenml_pin () Creates a ZenML pin for source pinning from release version. Source code in zenml/utils/source_utils.py def create_zenml_pin () -> str : \"\"\"Creates a ZenML pin for source pinning from release version.\"\"\" return f \" { APP_NAME } _ { __version__ } \" get_absolute_path_from_module_source ( module ) Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Parameters: Name Type Description Default module str A module e.g. zenml.core.step . required Source code in zenml/utils/source_utils.py def get_absolute_path_from_module_source ( module : str ) -> str : \"\"\"Get a directory path from module source. E.g. `zenml.core.step` will return `full/path/to/zenml/core/step`. Args: module: A module e.g. `zenml.core.step`. \"\"\" mod = importlib . import_module ( module ) return mod . __path__ [ 0 ] # type: ignore[no-any-return, attr-defined] get_class_source_from_source ( source ) Gets class source from source, i.e. module.path@version, returns version. Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_class_source_from_source ( source : str ) -> str : \"\"\"Gets class source from source, i.e. module.path@version, returns version. Args: source: source pointing to potentially pinned sha. \"\"\" # source need not even be pinned return source . split ( \"@\" )[ 0 ] get_module_source_from_class ( class_ ) Takes class input and returns module_source. If class is already string then returns the same. Parameters: Name Type Description Default class_ Union[Type[Any], str] object of type class. required Source code in zenml/utils/source_utils.py def get_module_source_from_class ( class_ : Union [ Type [ Any ], str ] ) -> Optional [ str ]: \"\"\"Takes class input and returns module_source. If class is already string then returns the same. Args: class_: object of type class. \"\"\" if isinstance ( class_ , str ): module_source = class_ else : # Infer it from the class provided if not inspect . isclass ( class_ ): raise AssertionError ( \"step_type is neither string nor class.\" ) module_source = class_ . __module__ + \".\" + class_ . __name__ return module_source get_module_source_from_file_path ( file_path ) Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Parameters: Name Type Description Default file_path str Absolute file path to a file within the module. required Source code in zenml/utils/source_utils.py def get_module_source_from_file_path ( file_path : str ) -> str : \"\"\"Gets module_source from a file_path. E.g. `/home/myrepo/step/trainer.py` returns `myrepo.step.trainer` if `myrepo` is the root of the repo. Args: file_path: Absolute file path to a file within the module. \"\"\" from zenml.core.repo import Repository repo_path = Repository () . path # Replace repo_path with file_path to get relative path left over relative_file_path = file_path . replace ( repo_path , \"\" )[ 1 :] # Kick out the .py and replace `/` with `.` to get the module source relative_file_path = relative_file_path . replace ( \".py\" , \"\" ) module_source = relative_file_path . replace ( \"/\" , \".\" ) return module_source get_module_source_from_source ( source ) Gets module source from source. E.g. some.module.file.class@version , returns some.module . Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_module_source_from_source ( source : str ) -> str : \"\"\"Gets module source from source. E.g. `some.module.file.class@version`, returns `some.module`. Args: source: source pointing to potentially pinned sha. \"\"\" class_source = get_class_source_from_source ( source ) return \".\" . join ( class_source . split ( \".\" )[: - 2 ]) get_relative_path_from_module_source ( module_source ) Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Parameters: Name Type Description Default module_source str A module e.g. zenml.core.step required Source code in zenml/utils/source_utils.py def get_relative_path_from_module_source ( module_source : str ) -> str : \"\"\"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source: A module e.g. zenml.core.step \"\"\" return module_source . replace ( \".\" , \"/\" ) import_class_by_path ( class_path ) Imports a class based on a given path Parameters: Name Type Description Default class_path str str, class_source e.g. this.module.Class required Returns: the given class Source code in zenml/utils/source_utils.py def import_class_by_path ( class_path : str ) -> Type [ Any ]: \"\"\"Imports a class based on a given path Args: class_path: str, class_source e.g. this.module.Class Returns: the given class \"\"\" classname = class_path . split ( \".\" )[ - 1 ] modulename = \".\" . join ( class_path . split ( \".\" )[ 0 : - 1 ]) mod = importlib . import_module ( modulename ) return getattr ( mod , classname ) # type: ignore[no-any-return] import_python_file ( file_path ) Imports a python file. Parameters: Name Type Description Default file_path str Path to python file that should be imported. required Returns: Type Description module The imported module. Source code in zenml/utils/source_utils.py def import_python_file ( file_path : str ) -> types . ModuleType : \"\"\"Imports a python file. Args: file_path: Path to python file that should be imported. Returns: The imported module. \"\"\" # Add directory of python file to PYTHONPATH so we can import it file_path = os . path . abspath ( file_path ) sys . path . append ( os . path . dirname ( file_path )) module_name = os . path . splitext ( os . path . basename ( file_path ))[ 0 ] return importlib . import_module ( module_name ) is_inside_repository ( file_path ) Returns whether a file is inside a zenml repository. Source code in zenml/utils/source_utils.py def is_inside_repository ( file_path : str ) -> bool : \"\"\"Returns whether a file is inside a zenml repository.\"\"\" from zenml.core.repo import Repository repo_path = pathlib . Path ( Repository () . path ) . resolve () absolute_file_path = pathlib . Path ( file_path ) . resolve () return repo_path in absolute_file_path . parents is_standard_pin ( pin ) Returns True if pin is valid ZenML pin, else False. Parameters: Name Type Description Default pin str potential ZenML pin like 'zenml_0.1.1' required Source code in zenml/utils/source_utils.py def is_standard_pin ( pin : str ) -> bool : \"\"\"Returns `True` if pin is valid ZenML pin, else False. Args: pin: potential ZenML pin like 'zenml_0.1.1' \"\"\" if pin . startswith ( f \" { APP_NAME } _\" ): return True return False is_standard_source ( source ) Returns True if source is a standard ZenML source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/utils/source_utils.py def is_standard_source ( source : str ) -> bool : \"\"\"Returns `True` if source is a standard ZenML source. Args: source: class_source e.g. this.module.Class[@pin]. \"\"\" if source . split ( \".\" )[ 0 ] == \"zenml\" : return True return False is_third_party_module ( file_path ) Returns whether a file belongs to a third party package. Source code in zenml/utils/source_utils.py def is_third_party_module ( file_path : str ) -> bool : \"\"\"Returns whether a file belongs to a third party package.\"\"\" absolute_file_path = pathlib . Path ( file_path ) . resolve () for path in site . getsitepackages () + [ site . getusersitepackages ()]: if pathlib . Path ( path ) . resolve () in absolute_file_path . parents : return True return False load_source_path_class ( source ) Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/utils/source_utils.py def load_source_path_class ( source : str ) -> Type [ Any ]: \"\"\"Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" if \"@\" in source : source = source . split ( \"@\" )[ 0 ] logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = import_class_by_path ( source ) return class_ resolve_class ( class_ ) Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class. Source code in zenml/utils/source_utils.py def resolve_class ( class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class. \"\"\" initial_source = class_ . __module__ + \".\" + class_ . __name__ if is_standard_source ( initial_source ): return resolve_standard_source ( initial_source ) try : file_path = inspect . getfile ( class_ ) except TypeError : # builtin file return initial_source if ( initial_source . startswith ( \"__main__\" ) or not is_inside_repository ( file_path ) or is_third_party_module ( file_path ) ): return initial_source # Regular user file inside the repository -> get the full module # path relative to the repository module_source = get_module_source_from_file_path ( file_path ) # ENG-123 Sanitize for Windows OS # module_source = module_source.replace(\"\\\\\", \".\") return module_source + \".\" + class_ . __name__ resolve_standard_source ( source ) Creates a ZenML pin for source pinning from release version. Parameters: Name Type Description Default source str class_source e.g. this.module.Class. required Source code in zenml/utils/source_utils.py def resolve_standard_source ( source : str ) -> str : \"\"\"Creates a ZenML pin for source pinning from release version. Args: source: class_source e.g. this.module.Class. \"\"\" if \"@\" in source : raise AssertionError ( f \"source { source } is already pinned.\" ) pin = create_zenml_pin () return f \" { source } @ { pin } \" string_utils get_human_readable_filesize ( bytes_ ) Convert a file size in bytes into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_filesize ( bytes_ : int ) -> str : \"\"\"Convert a file size in bytes into a human-readable string.\"\"\" size = abs ( float ( bytes_ )) for unit in [ \"B\" , \"KiB\" , \"MiB\" , \"GiB\" ]: if size < 1024.0 or unit == \"GiB\" : break size /= 1024.0 return f \" { size : .2f } { unit } \" get_human_readable_time ( seconds ) Convert seconds into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_time ( seconds : float ) -> str : \"\"\"Convert seconds into a human-readable string.\"\"\" prefix = \"-\" if seconds < 0 else \"\" seconds = abs ( seconds ) int_seconds = int ( seconds ) days , int_seconds = divmod ( int_seconds , 86400 ) hours , int_seconds = divmod ( int_seconds , 3600 ) minutes , int_seconds = divmod ( int_seconds , 60 ) if days > 0 : time_string = f \" { days } d { hours } h { minutes } m { int_seconds } s\" elif hours > 0 : time_string = f \" { hours } h { minutes } m { int_seconds } s\" elif minutes > 0 : time_string = f \" { minutes } m { int_seconds } s\" else : time_string = f \" { seconds : .3f } s\" return prefix + time_string yaml_utils is_yaml ( file_path ) Returns True if file_path is YAML, else False Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description bool True if is yaml, else False. Source code in zenml/utils/yaml_utils.py def is_yaml ( file_path : str ) -> bool : \"\"\"Returns True if file_path is YAML, else False Args: file_path: Path to YAML file. Returns: True if is yaml, else False. \"\"\" if file_path . endswith ( \"yaml\" ) or file_path . endswith ( \"yml\" ): return True return False read_json ( file_path ) Read JSON on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to JSON file. required Source code in zenml/utils/yaml_utils.py def read_json ( file_path : str ) -> Any : \"\"\"Read JSON on file path and returns contents as dict. Args: file_path: Path to JSON file. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return json . loads ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" ) read_yaml ( file_path ) Read YAML on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description Any Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def read_yaml ( file_path : str ) -> Any : \"\"\"Read YAML on file path and returns contents as dict. Args: file_path: Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return yaml . load ( contents , Loader = yaml . FullLoader ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" ) write_json ( file_path , contents ) Write contents as JSON format to file_path. Parameters: Name Type Description Default file_path str Path to JSON file. required contents Dict[str, Any] Contents of JSON file as dict. required Returns: Type Description None Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def write_json ( file_path : str , contents : Dict [ str , Any ]) -> None : \"\"\"Write contents as JSON format to file_path. Args: file_path: Path to JSON file. contents: Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): # If it is a local path and it doesn't exist, raise Exception. raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , json . dumps ( contents ) ) write_yaml ( file_path , contents ) Write contents as YAML format to file_path. Parameters: Name Type Description Default file_path str Path to YAML file. required contents Dict[Any, Any] Contents of YAML file as dict. required Source code in zenml/utils/yaml_utils.py def write_yaml ( file_path : str , contents : Dict [ Any , Any ]) -> None : \"\"\"Write contents as YAML format to file_path. Args: file_path: Path to YAML file. contents: Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , yaml . dump ( contents ))","title":"Utils"},{"location":"api_docs/utils/#utils","text":"","title":"Utils"},{"location":"api_docs/utils/#zenml.utils","text":"The utils module contains utility functions handling analytics, reading and writing YAML data as well as other general purpose functions.","title":"utils"},{"location":"api_docs/utils/#zenml.utils.analytics_utils","text":"Analytics code for ZenML","title":"analytics_utils"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_environment","text":"Returns a string representing the execution environment of the pipeline. Currently, one of docker , paperspace , 'colab', or native Source code in zenml/utils/analytics_utils.py def get_environment () -> str : \"\"\"Returns a string representing the execution environment of the pipeline. Currently, one of `docker`, `paperspace`, 'colab', or `native`\"\"\" if in_docker (): return \"docker\" elif in_google_colab (): return \"colab\" elif in_paperspace_gradient (): return \"paperspace\" else : return \"native\"","title":"get_environment()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_segment_key","text":"Get key for authorizing to Segment backend. Returns: Type Description str Segment key as a string. Source code in zenml/utils/analytics_utils.py def get_segment_key () -> str : \"\"\"Get key for authorizing to Segment backend. Returns: Segment key as a string. \"\"\" if IS_DEBUG_ENV : return SEGMENT_KEY_DEV else : return SEGMENT_KEY_PROD","title":"get_segment_key()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.get_system_info","text":"Returns system info as a dict. Returns: Type Description Dict[str, Any] A dict of system information. Source code in zenml/utils/analytics_utils.py def get_system_info () -> Dict [ str , Any ]: \"\"\"Returns system info as a dict. Returns: A dict of system information. \"\"\" system = platform . system () if system == \"Windows\" : release , version , csd , ptype = platform . win32_ver () return { \"os\" : \"windows\" , \"windows_version_release\" : release , \"windows_version\" : version , \"windows_version_service_pack\" : csd , \"windows_version_os_type\" : ptype , } if system == \"Darwin\" : return { \"os\" : \"mac\" , \"mac_version\" : platform . mac_ver ()[ 0 ]} if system == \"Linux\" : return { \"os\" : \"linux\" , \"linux_distro\" : distro . id (), \"linux_distro_like\" : distro . like (), \"linux_distro_version\" : distro . version (), } # We don't collect data for any other system. return { \"os\" : \"unknown\" }","title":"get_system_info()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_docker","text":"Returns: True if running in a Docker container, else False Source code in zenml/utils/analytics_utils.py def in_docker () -> bool : \"\"\"Returns: True if running in a Docker container, else False\"\"\" # TODO [ENG-167]: Make this more reliable and add test. try : with open ( \"/proc/1/cgroup\" , \"rt\" ) as ifh : info = ifh . read () return \"docker\" in info or \"kubepod\" in info except ( FileNotFoundError , Exception ): return False","title":"in_docker()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_google_colab","text":"Returns: True if running in a Google Colab env, else False Source code in zenml/utils/analytics_utils.py def in_google_colab () -> bool : \"\"\"Returns: True if running in a Google Colab env, else False\"\"\" if \"COLAB_GPU\" in os . environ : return True return False","title":"in_google_colab()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.in_paperspace_gradient","text":"Returns: True if running in a Paperspace Gradient env, else False Source code in zenml/utils/analytics_utils.py def in_paperspace_gradient () -> bool : \"\"\"Returns: True if running in a Paperspace Gradient env, else False\"\"\" if \"PAPERSPACE_NOTEBOOK_REPO_ID\" in os . environ : return True return False","title":"in_paperspace_gradient()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.parametrized","text":"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments: Source code in zenml/utils/analytics_utils.py def parametrized ( dec : Callable [ ... , Callable [ ... , Any ]] ) -> Callable [ ... , Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]]: \"\"\"This is a meta-decorator, that is, a decorator for decorators. As a decorator is a function, it actually works as a regular decorator with arguments:\"\"\" def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl return layer","title":"parametrized()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.track","text":"Internal layer Source code in zenml/utils/analytics_utils.py def layer ( * args : Any , ** kwargs : Any ) -> Callable [[ Callable [ ... , Any ]], Callable [ ... , Any ]]: \"\"\"Internal layer\"\"\" def repl ( f : Callable [ ... , Any ]) -> Callable [ ... , Any ]: \"\"\"Internal repl\"\"\" return dec ( f , * args , ** kwargs ) return repl","title":"track()"},{"location":"api_docs/utils/#zenml.utils.analytics_utils.track_event","text":"Track segment event if user opted-in. Parameters: Name Type Description Default event str Name of event to track in segment. required metadata Optional[Dict[str, Any]] Dict of metadata to track. None Returns: Type Description bool True if event is sent successfully, False is not. Source code in zenml/utils/analytics_utils.py def track_event ( event : str , metadata : Optional [ Dict [ str , Any ]] = None ) -> bool : \"\"\" Track segment event if user opted-in. Args: event: Name of event to track in segment. metadata: Dict of metadata to track. Returns: True if event is sent successfully, False is not. \"\"\" try : import analytics from zenml.config.global_config import GlobalConfig if analytics . write_key is None : analytics . write_key = get_segment_key () assert ( analytics . write_key is not None ), \"Analytics key not set but trying to make telemetry call.\" # Set this to 1 to avoid backoff loop analytics . max_retries = 1 gc = GlobalConfig () logger . debug ( f \"Attempting analytics: User: { gc . user_id } , \" f \"Event: { event } ,\" f \"Metadata: { metadata } \" ) if not gc . analytics_opt_in and event not in [ OPT_OUT_ANALYTICS , OPT_IN_ANALYTICS , ]: return False if metadata is None : metadata = {} # add basics metadata . update ( get_system_info ()) metadata . update ( { \"environment\" : get_environment (), \"version\" : __version__ , } ) analytics . track ( str ( gc . user_id ), event , metadata ) logger . debug ( f \"Analytics sent: User: { gc . user_id } , Event: { event } , Metadata: \" f \" { metadata } \" ) return True except Exception as e : # We should never fail main thread logger . debug ( f \"Analytics failed due to: { e } \" ) return False","title":"track_event()"},{"location":"api_docs/utils/#zenml.utils.daemon","text":"Utility functions to start/stop daemon processes. This is only implemented for UNIX systems and therefore doesn't work on Windows. Based on https://www.jejik.com/articles/2007/02/a_simple_unix_linux_daemon_in_python/","title":"daemon"},{"location":"api_docs/utils/#zenml.utils.daemon.check_if_daemon_is_running","text":"Checks whether a daemon process indicated by the PID file is running. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to check. required Source code in zenml/utils/daemon.py def check_if_daemon_is_running ( pid_file : str ) -> bool : \"\"\"Checks whether a daemon process indicated by the PID file is running. Args: pid_file: Path to file containing the PID of the daemon process to check. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): return False return psutil . pid_exists ( pid )","title":"check_if_daemon_is_running()"},{"location":"api_docs/utils/#zenml.utils.daemon.run_as_daemon","text":"Runs a function as a daemon process. Parameters: Name Type Description Default daemon_function Callable[..., Any] The function to run as a daemon. required pid_file str Path to file in which to store the PID of the daemon process. required log_file Optional[str] Optional file to which the daemons stdout/stderr will be redirected to. None working_directory str Working directory for the daemon process, defaults to the root directory. '/' Exceptions: Type Description FileExistsError If the PID file already exists. Source code in zenml/utils/daemon.py def run_as_daemon ( daemon_function : Callable [ ... , Any ], pid_file : str , log_file : Optional [ str ] = None , working_directory : str = \"/\" , ) -> None : \"\"\"Runs a function as a daemon process. Args: daemon_function: The function to run as a daemon. pid_file: Path to file in which to store the PID of the daemon process. log_file: Optional file to which the daemons stdout/stderr will be redirected to. working_directory: Working directory for the daemon process, defaults to the root directory. Raises: FileExistsError: If the PID file already exists. \"\"\" # convert to absolute path as we will change working directory later pid_file = os . path . abspath ( pid_file ) if log_file : log_file = os . path . abspath ( log_file ) # check if PID file exists if os . path . exists ( pid_file ): raise FileExistsError ( f \"The PID file ' { pid_file } ' already exists, either the daemon \" f \"process is already running or something went wrong.\" ) # first fork try : pid = os . fork () if pid > 0 : # this is the process that called `run_as_daemon` so we # simply return so it can keep running return except OSError as e : logger . error ( \"Unable to fork (error code: %d )\" , e . errno ) sys . exit ( 1 ) # decouple from parent environment os . chdir ( working_directory ) os . setsid () os . umask ( 0 ) # second fork try : pid = os . fork () if pid > 0 : # this is the parent of the future daemon process, kill it # so the daemon gets adopted by the init process sys . exit ( 0 ) except OSError as e : sys . stderr . write ( f \"Unable to fork (error code: { e . errno } )\" ) sys . exit ( 1 ) # redirect standard file descriptors to devnull (or the given logfile) devnull = \"/dev/null\" if hasattr ( os , \"devnull\" ): devnull = os . devnull devnull_fd = os . open ( devnull , os . O_RDWR ) log_fd = os . open ( log_file , os . O_CREAT | os . O_RDWR ) if log_file else None out_fd = log_fd or devnull_fd os . dup2 ( devnull_fd , sys . stdin . fileno ()) os . dup2 ( out_fd , sys . stdout . fileno ()) os . dup2 ( out_fd , sys . stderr . fileno ()) # write the PID file with open ( pid_file , \"w+\" ) as f : f . write ( f \" { os . getpid () } \\n \" ) # register actions in case this process exits/gets killed def sigterm ( signum : int , frame : Optional [ types . FrameType ]) -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) def cleanup () -> None : \"\"\"Removes the PID file.\"\"\" os . remove ( pid_file ) signal . signal ( signal . SIGTERM , sigterm ) atexit . register ( cleanup ) # finally run the actual daemon code daemon_function ()","title":"run_as_daemon()"},{"location":"api_docs/utils/#zenml.utils.daemon.stop_daemon","text":"Stops a daemon process. Parameters: Name Type Description Default pid_file str Path to file containing the PID of the daemon process to kill. required kill_children bool If True , all child processes of the daemon process will be killed as well. True Source code in zenml/utils/daemon.py def stop_daemon ( pid_file : str , kill_children : bool = True ) -> None : \"\"\"Stops a daemon process. Args: pid_file: Path to file containing the PID of the daemon process to kill. kill_children: If `True`, all child processes of the daemon process will be killed as well. \"\"\" try : with open ( pid_file , \"r\" ) as f : pid = int ( f . read () . strip ()) except ( IOError , FileNotFoundError ): logger . warning ( \"Daemon PID file ' %s ' does not exist.\" , pid_file ) return if psutil . pid_exists ( pid ): process = psutil . Process ( pid ) if kill_children : for child in process . children ( recursive = True ): child . kill () process . kill () else : logger . warning ( \"PID from ' %s ' does not exist.\" , pid_file )","title":"stop_daemon()"},{"location":"api_docs/utils/#zenml.utils.networking_utils","text":"","title":"networking_utils"},{"location":"api_docs/utils/#zenml.utils.networking_utils.find_available_port","text":"Finds a local unoccupied port. Source code in zenml/utils/networking_utils.py def find_available_port () -> int : \"\"\"Finds a local unoccupied port.\"\"\" with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , 0 )) _ , port = s . getsockname () return cast ( int , port )","title":"find_available_port()"},{"location":"api_docs/utils/#zenml.utils.networking_utils.port_available","text":"Checks if a local port is available. Source code in zenml/utils/networking_utils.py def port_available ( port : int ) -> bool : \"\"\"Checks if a local port is available.\"\"\" try : with socket . socket ( socket . AF_INET , socket . SOCK_STREAM ) as s : s . bind (( \"127.0.0.1\" , port )) except socket . error as e : logger . debug ( \"Port %d unavailable: %s \" , port , e ) return False return True","title":"port_available()"},{"location":"api_docs/utils/#zenml.utils.source_utils","text":"These utils are predicated on the following definitions: class_source: This is a python-import type path to a class, e.g. some.mod.class module_source: This is a python-import type path to a module, e.g. some.mod file_path, relative_path, absolute_path: These are file system paths. source: This is a class_source or module_source. If it is a class_source, it can also be optionally pinned. pin: Whatever comes after the @ symbol from a source, usually the git sha or the version of zenml as a string.","title":"source_utils"},{"location":"api_docs/utils/#zenml.utils.source_utils.create_zenml_pin","text":"Creates a ZenML pin for source pinning from release version. Source code in zenml/utils/source_utils.py def create_zenml_pin () -> str : \"\"\"Creates a ZenML pin for source pinning from release version.\"\"\" return f \" { APP_NAME } _ { __version__ } \"","title":"create_zenml_pin()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_absolute_path_from_module_source","text":"Get a directory path from module source. E.g. zenml.core.step will return full/path/to/zenml/core/step . Parameters: Name Type Description Default module str A module e.g. zenml.core.step . required Source code in zenml/utils/source_utils.py def get_absolute_path_from_module_source ( module : str ) -> str : \"\"\"Get a directory path from module source. E.g. `zenml.core.step` will return `full/path/to/zenml/core/step`. Args: module: A module e.g. `zenml.core.step`. \"\"\" mod = importlib . import_module ( module ) return mod . __path__ [ 0 ] # type: ignore[no-any-return, attr-defined]","title":"get_absolute_path_from_module_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_class_source_from_source","text":"Gets class source from source, i.e. module.path@version, returns version. Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_class_source_from_source ( source : str ) -> str : \"\"\"Gets class source from source, i.e. module.path@version, returns version. Args: source: source pointing to potentially pinned sha. \"\"\" # source need not even be pinned return source . split ( \"@\" )[ 0 ]","title":"get_class_source_from_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_class","text":"Takes class input and returns module_source. If class is already string then returns the same. Parameters: Name Type Description Default class_ Union[Type[Any], str] object of type class. required Source code in zenml/utils/source_utils.py def get_module_source_from_class ( class_ : Union [ Type [ Any ], str ] ) -> Optional [ str ]: \"\"\"Takes class input and returns module_source. If class is already string then returns the same. Args: class_: object of type class. \"\"\" if isinstance ( class_ , str ): module_source = class_ else : # Infer it from the class provided if not inspect . isclass ( class_ ): raise AssertionError ( \"step_type is neither string nor class.\" ) module_source = class_ . __module__ + \".\" + class_ . __name__ return module_source","title":"get_module_source_from_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_file_path","text":"Gets module_source from a file_path. E.g. /home/myrepo/step/trainer.py returns myrepo.step.trainer if myrepo is the root of the repo. Parameters: Name Type Description Default file_path str Absolute file path to a file within the module. required Source code in zenml/utils/source_utils.py def get_module_source_from_file_path ( file_path : str ) -> str : \"\"\"Gets module_source from a file_path. E.g. `/home/myrepo/step/trainer.py` returns `myrepo.step.trainer` if `myrepo` is the root of the repo. Args: file_path: Absolute file path to a file within the module. \"\"\" from zenml.core.repo import Repository repo_path = Repository () . path # Replace repo_path with file_path to get relative path left over relative_file_path = file_path . replace ( repo_path , \"\" )[ 1 :] # Kick out the .py and replace `/` with `.` to get the module source relative_file_path = relative_file_path . replace ( \".py\" , \"\" ) module_source = relative_file_path . replace ( \"/\" , \".\" ) return module_source","title":"get_module_source_from_file_path()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_module_source_from_source","text":"Gets module source from source. E.g. some.module.file.class@version , returns some.module . Parameters: Name Type Description Default source str source pointing to potentially pinned sha. required Source code in zenml/utils/source_utils.py def get_module_source_from_source ( source : str ) -> str : \"\"\"Gets module source from source. E.g. `some.module.file.class@version`, returns `some.module`. Args: source: source pointing to potentially pinned sha. \"\"\" class_source = get_class_source_from_source ( source ) return \".\" . join ( class_source . split ( \".\" )[: - 2 ])","title":"get_module_source_from_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.get_relative_path_from_module_source","text":"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Parameters: Name Type Description Default module_source str A module e.g. zenml.core.step required Source code in zenml/utils/source_utils.py def get_relative_path_from_module_source ( module_source : str ) -> str : \"\"\"Get a directory path from module, relative to root of repository. E.g. zenml.core.step will return zenml/core/step. Args: module_source: A module e.g. zenml.core.step \"\"\" return module_source . replace ( \".\" , \"/\" )","title":"get_relative_path_from_module_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.import_class_by_path","text":"Imports a class based on a given path Parameters: Name Type Description Default class_path str str, class_source e.g. this.module.Class required Returns: the given class Source code in zenml/utils/source_utils.py def import_class_by_path ( class_path : str ) -> Type [ Any ]: \"\"\"Imports a class based on a given path Args: class_path: str, class_source e.g. this.module.Class Returns: the given class \"\"\" classname = class_path . split ( \".\" )[ - 1 ] modulename = \".\" . join ( class_path . split ( \".\" )[ 0 : - 1 ]) mod = importlib . import_module ( modulename ) return getattr ( mod , classname ) # type: ignore[no-any-return]","title":"import_class_by_path()"},{"location":"api_docs/utils/#zenml.utils.source_utils.import_python_file","text":"Imports a python file. Parameters: Name Type Description Default file_path str Path to python file that should be imported. required Returns: Type Description module The imported module. Source code in zenml/utils/source_utils.py def import_python_file ( file_path : str ) -> types . ModuleType : \"\"\"Imports a python file. Args: file_path: Path to python file that should be imported. Returns: The imported module. \"\"\" # Add directory of python file to PYTHONPATH so we can import it file_path = os . path . abspath ( file_path ) sys . path . append ( os . path . dirname ( file_path )) module_name = os . path . splitext ( os . path . basename ( file_path ))[ 0 ] return importlib . import_module ( module_name )","title":"import_python_file()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_inside_repository","text":"Returns whether a file is inside a zenml repository. Source code in zenml/utils/source_utils.py def is_inside_repository ( file_path : str ) -> bool : \"\"\"Returns whether a file is inside a zenml repository.\"\"\" from zenml.core.repo import Repository repo_path = pathlib . Path ( Repository () . path ) . resolve () absolute_file_path = pathlib . Path ( file_path ) . resolve () return repo_path in absolute_file_path . parents","title":"is_inside_repository()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_standard_pin","text":"Returns True if pin is valid ZenML pin, else False. Parameters: Name Type Description Default pin str potential ZenML pin like 'zenml_0.1.1' required Source code in zenml/utils/source_utils.py def is_standard_pin ( pin : str ) -> bool : \"\"\"Returns `True` if pin is valid ZenML pin, else False. Args: pin: potential ZenML pin like 'zenml_0.1.1' \"\"\" if pin . startswith ( f \" { APP_NAME } _\" ): return True return False","title":"is_standard_pin()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_standard_source","text":"Returns True if source is a standard ZenML source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@pin]. required Source code in zenml/utils/source_utils.py def is_standard_source ( source : str ) -> bool : \"\"\"Returns `True` if source is a standard ZenML source. Args: source: class_source e.g. this.module.Class[@pin]. \"\"\" if source . split ( \".\" )[ 0 ] == \"zenml\" : return True return False","title":"is_standard_source()"},{"location":"api_docs/utils/#zenml.utils.source_utils.is_third_party_module","text":"Returns whether a file belongs to a third party package. Source code in zenml/utils/source_utils.py def is_third_party_module ( file_path : str ) -> bool : \"\"\"Returns whether a file belongs to a third party package.\"\"\" absolute_file_path = pathlib . Path ( file_path ) . resolve () for path in site . getsitepackages () + [ site . getusersitepackages ()]: if pathlib . Path ( path ) . resolve () in absolute_file_path . parents : return True return False","title":"is_third_party_module()"},{"location":"api_docs/utils/#zenml.utils.source_utils.load_source_path_class","text":"Loads a Python class from the source. Parameters: Name Type Description Default source str class_source e.g. this.module.Class[@sha] required Source code in zenml/utils/source_utils.py def load_source_path_class ( source : str ) -> Type [ Any ]: \"\"\"Loads a Python class from the source. Args: source: class_source e.g. this.module.Class[@sha] \"\"\" if \"@\" in source : source = source . split ( \"@\" )[ 0 ] logger . debug ( \"Unpinned step found with no git sha. Attempting to \" \"load class from current repository state.\" ) class_ = import_class_by_path ( source ) return class_","title":"load_source_path_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.resolve_class","text":"Resolves a class into a serializable source string. Parameters: Name Type Description Default class_ Type[Any] A Python Class reference. required Returns: source_path e.g. this.module.Class. Source code in zenml/utils/source_utils.py def resolve_class ( class_ : Type [ Any ]) -> str : \"\"\"Resolves a class into a serializable source string. Args: class_: A Python Class reference. Returns: source_path e.g. this.module.Class. \"\"\" initial_source = class_ . __module__ + \".\" + class_ . __name__ if is_standard_source ( initial_source ): return resolve_standard_source ( initial_source ) try : file_path = inspect . getfile ( class_ ) except TypeError : # builtin file return initial_source if ( initial_source . startswith ( \"__main__\" ) or not is_inside_repository ( file_path ) or is_third_party_module ( file_path ) ): return initial_source # Regular user file inside the repository -> get the full module # path relative to the repository module_source = get_module_source_from_file_path ( file_path ) # ENG-123 Sanitize for Windows OS # module_source = module_source.replace(\"\\\\\", \".\") return module_source + \".\" + class_ . __name__","title":"resolve_class()"},{"location":"api_docs/utils/#zenml.utils.source_utils.resolve_standard_source","text":"Creates a ZenML pin for source pinning from release version. Parameters: Name Type Description Default source str class_source e.g. this.module.Class. required Source code in zenml/utils/source_utils.py def resolve_standard_source ( source : str ) -> str : \"\"\"Creates a ZenML pin for source pinning from release version. Args: source: class_source e.g. this.module.Class. \"\"\" if \"@\" in source : raise AssertionError ( f \"source { source } is already pinned.\" ) pin = create_zenml_pin () return f \" { source } @ { pin } \"","title":"resolve_standard_source()"},{"location":"api_docs/utils/#zenml.utils.string_utils","text":"","title":"string_utils"},{"location":"api_docs/utils/#zenml.utils.string_utils.get_human_readable_filesize","text":"Convert a file size in bytes into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_filesize ( bytes_ : int ) -> str : \"\"\"Convert a file size in bytes into a human-readable string.\"\"\" size = abs ( float ( bytes_ )) for unit in [ \"B\" , \"KiB\" , \"MiB\" , \"GiB\" ]: if size < 1024.0 or unit == \"GiB\" : break size /= 1024.0 return f \" { size : .2f } { unit } \"","title":"get_human_readable_filesize()"},{"location":"api_docs/utils/#zenml.utils.string_utils.get_human_readable_time","text":"Convert seconds into a human-readable string. Source code in zenml/utils/string_utils.py def get_human_readable_time ( seconds : float ) -> str : \"\"\"Convert seconds into a human-readable string.\"\"\" prefix = \"-\" if seconds < 0 else \"\" seconds = abs ( seconds ) int_seconds = int ( seconds ) days , int_seconds = divmod ( int_seconds , 86400 ) hours , int_seconds = divmod ( int_seconds , 3600 ) minutes , int_seconds = divmod ( int_seconds , 60 ) if days > 0 : time_string = f \" { days } d { hours } h { minutes } m { int_seconds } s\" elif hours > 0 : time_string = f \" { hours } h { minutes } m { int_seconds } s\" elif minutes > 0 : time_string = f \" { minutes } m { int_seconds } s\" else : time_string = f \" { seconds : .3f } s\" return prefix + time_string","title":"get_human_readable_time()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils","text":"","title":"yaml_utils"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.is_yaml","text":"Returns True if file_path is YAML, else False Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description bool True if is yaml, else False. Source code in zenml/utils/yaml_utils.py def is_yaml ( file_path : str ) -> bool : \"\"\"Returns True if file_path is YAML, else False Args: file_path: Path to YAML file. Returns: True if is yaml, else False. \"\"\" if file_path . endswith ( \"yaml\" ) or file_path . endswith ( \"yml\" ): return True return False","title":"is_yaml()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.read_json","text":"Read JSON on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to JSON file. required Source code in zenml/utils/yaml_utils.py def read_json ( file_path : str ) -> Any : \"\"\"Read JSON on file path and returns contents as dict. Args: file_path: Path to JSON file. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return json . loads ( contents ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" )","title":"read_json()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.read_yaml","text":"Read YAML on file path and returns contents as dict. Parameters: Name Type Description Default file_path str Path to YAML file. required Returns: Type Description Any Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def read_yaml ( file_path : str ) -> Any : \"\"\"Read YAML on file path and returns contents as dict. Args: file_path: Path to YAML file. Returns: Contents of the file in a dict. Raises: FileNotFoundError if file does not exist. \"\"\" if fileio . file_exists ( file_path ): contents = zenml . io . utils . read_file_contents_as_string ( file_path ) return yaml . load ( contents , Loader = yaml . FullLoader ) else : raise FileNotFoundError ( f \" { file_path } does not exist.\" )","title":"read_yaml()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.write_json","text":"Write contents as JSON format to file_path. Parameters: Name Type Description Default file_path str Path to JSON file. required contents Dict[str, Any] Contents of JSON file as dict. required Returns: Type Description None Contents of the file in a dict. Source code in zenml/utils/yaml_utils.py def write_json ( file_path : str , contents : Dict [ str , Any ]) -> None : \"\"\"Write contents as JSON format to file_path. Args: file_path: Path to JSON file. contents: Contents of JSON file as dict. Returns: Contents of the file in a dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): # If it is a local path and it doesn't exist, raise Exception. raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , json . dumps ( contents ) )","title":"write_json()"},{"location":"api_docs/utils/#zenml.utils.yaml_utils.write_yaml","text":"Write contents as YAML format to file_path. Parameters: Name Type Description Default file_path str Path to YAML file. required contents Dict[Any, Any] Contents of YAML file as dict. required Source code in zenml/utils/yaml_utils.py def write_yaml ( file_path : str , contents : Dict [ Any , Any ]) -> None : \"\"\"Write contents as YAML format to file_path. Args: file_path: Path to YAML file. contents: Contents of YAML file as dict. Raises: FileNotFoundError if directory does not exist. \"\"\" if not fileio . is_remote ( file_path ): dir_ = str ( Path ( file_path ) . parent ) if not fileio . is_dir ( dir_ ): raise FileNotFoundError ( f \"Directory { dir_ } does not exist.\" ) zenml . io . utils . write_file_contents_as_string ( file_path , yaml . dump ( contents ))","title":"write_yaml()"},{"location":"api_docs/visualizers/","text":"Visualizers zenml.visualizers special The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves. base_pipeline_run_visualizer BasePipelineRunVisualizer ( BaseVisualizer ) The base implementation of a ZenML Pipeline Run Visualizer. Source code in zenml/visualizers/base_pipeline_run_visualizer.py class BasePipelineRunVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Run Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize pipeline runs. Source code in zenml/visualizers/base_pipeline_run_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\" base_pipeline_visualizer BasePipelineVisualizer ( BaseVisualizer ) The base implementation of a ZenML Pipeline Visualizer. Source code in zenml/visualizers/base_pipeline_visualizer.py class BasePipelineVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize pipelines. Source code in zenml/visualizers/base_pipeline_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\" base_step_visualizer BaseStepVisualizer ( BaseVisualizer ) The base implementation of a ZenML Step Visualizer. Source code in zenml/visualizers/base_step_visualizer.py class BaseStepVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Step Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False running_in_notebook () staticmethod Detect whether we're running in a Jupyter notebook or not Source code in zenml/visualizers/base_step_visualizer.py @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False visualize ( self , object , * args , ** kwargs ) Method to visualize steps. Source code in zenml/visualizers/base_step_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" base_visualizer BaseVisualizer Base class for all ZenML Visualizers. Source code in zenml/visualizers/base_visualizer.py class BaseVisualizer : \"\"\"Base class for all ZenML Visualizers.\"\"\" @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\" visualize ( self , object , * args , ** kwargs ) Method to visualize objects. Source code in zenml/visualizers/base_visualizer.py @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"Visualizers"},{"location":"api_docs/visualizers/#visualizers","text":"","title":"Visualizers"},{"location":"api_docs/visualizers/#zenml.visualizers","text":"The visualizers module offers a way of constructing and displaying visualizations of steps and pipeline results. The BaseVisualizer class is at the root of all the other visualizers, including options to view the results of pipeline runs, steps and pipelines themselves.","title":"visualizers"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer","text":"","title":"base_pipeline_run_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer.BasePipelineRunVisualizer","text":"The base implementation of a ZenML Pipeline Run Visualizer. Source code in zenml/visualizers/base_pipeline_run_visualizer.py class BasePipelineRunVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Run Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\"","title":"BasePipelineRunVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_run_visualizer.BasePipelineRunVisualizer.visualize","text":"Method to visualize pipeline runs. Source code in zenml/visualizers/base_pipeline_run_visualizer.py @abstractmethod def visualize ( self , object : PipelineRunView , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize pipeline runs.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer","text":"","title":"base_pipeline_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer.BasePipelineVisualizer","text":"The base implementation of a ZenML Pipeline Visualizer. Source code in zenml/visualizers/base_pipeline_visualizer.py class BasePipelineVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Pipeline Visualizer.\"\"\" @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\"","title":"BasePipelineVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_pipeline_visualizer.BasePipelineVisualizer.visualize","text":"Method to visualize pipelines. Source code in zenml/visualizers/base_pipeline_visualizer.py @abstractmethod def visualize ( self , object : PipelineView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize pipelines.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer","text":"","title":"base_step_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer","text":"The base implementation of a ZenML Step Visualizer. Source code in zenml/visualizers/base_step_visualizer.py class BaseStepVisualizer ( BaseVisualizer ): \"\"\"The base implementation of a ZenML Step Visualizer.\"\"\" @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\" @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False","title":"BaseStepVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer.running_in_notebook","text":"Detect whether we're running in a Jupyter notebook or not Source code in zenml/visualizers/base_step_visualizer.py @staticmethod def running_in_notebook () -> bool : \"\"\"Detect whether we're running in a Jupyter notebook or not\"\"\" try : from IPython import get_ipython # type: ignore if get_ipython () is None : # IPython is installed but not running from a notebook return False else : return True except ImportError : # We do not even have IPython installed return False","title":"running_in_notebook()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_step_visualizer.BaseStepVisualizer.visualize","text":"Method to visualize steps. Source code in zenml/visualizers/base_step_visualizer.py @abstractmethod def visualize ( self , object : StepView , * args : Any , ** kwargs : Any ) -> Any : \"\"\"Method to visualize steps.\"\"\"","title":"visualize()"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer","text":"","title":"base_visualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer.BaseVisualizer","text":"Base class for all ZenML Visualizers. Source code in zenml/visualizers/base_visualizer.py class BaseVisualizer : \"\"\"Base class for all ZenML Visualizers.\"\"\" @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"BaseVisualizer"},{"location":"api_docs/visualizers/#zenml.visualizers.base_visualizer.BaseVisualizer.visualize","text":"Method to visualize objects. Source code in zenml/visualizers/base_visualizer.py @abstractmethod def visualize ( self , object : Any , * args : Any , ** kwargs : Any ) -> None : \"\"\"Method to visualize objects.\"\"\"","title":"visualize()"},{"location":"api_docs/cli/artifact_store/","text":"Artifact Store zenml.cli.artifact_store","title":"Artifact Store"},{"location":"api_docs/cli/artifact_store/#artifact-store","text":"","title":"Artifact Store"},{"location":"api_docs/cli/artifact_store/#zenml.cli.artifact_store","text":"","title":"artifact_store"},{"location":"api_docs/cli/base/","text":"Base zenml.cli.base","title":"Base"},{"location":"api_docs/cli/base/#base","text":"","title":"Base"},{"location":"api_docs/cli/base/#zenml.cli.base","text":"","title":"base"},{"location":"api_docs/cli/cli/","text":"Cli zenml.cli.cli .. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io","title":"Cli"},{"location":"api_docs/cli/cli/#cli","text":"","title":"Cli"},{"location":"api_docs/cli/cli/#zenml.cli.cli","text":".. currentmodule:: ce_cli.cli .. moduleauthor:: ZenML GmbH support@zenml.io","title":"cli"},{"location":"api_docs/cli/config/","text":"Config zenml.cli.config CLI for manipulating ZenML local and global config file.","title":"Config"},{"location":"api_docs/cli/config/#config","text":"","title":"Config"},{"location":"api_docs/cli/config/#zenml.cli.config","text":"CLI for manipulating ZenML local and global config file.","title":"config"},{"location":"api_docs/cli/container_registry/","text":"Container Registry zenml.cli.container_registry","title":"Container Registry"},{"location":"api_docs/cli/container_registry/#container-registry","text":"","title":"Container Registry"},{"location":"api_docs/cli/container_registry/#zenml.cli.container_registry","text":"","title":"container_registry"},{"location":"api_docs/cli/example/","text":"Example zenml.cli.example Example Class for all example objects. Source code in zenml/cli/example.py class Example : \"\"\"Class for all example objects.\"\"\" def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo @property def readme_content ( self ) -> str : \"\"\"Returns the readme content associated with a particular example.\"\"\" readme_file = os . path . join ( self . path_in_repo , \"README.md\" ) try : with open ( readme_file ) as readme : readme_content = readme . read () return readme_content except FileNotFoundError : if fileio . file_exists ( str ( self . path_in_repo )) and fileio . is_dir ( str ( self . path_in_repo ) ): raise ValueError ( f \"No README.md file found in \" f \" { self . path_in_repo } \" ) else : raise FileNotFoundError ( f \"Example { self . name } is not one of the available options.\" f \" \\n \" f \"To list all available examples, type: `zenml example \" f \"list`\" ) readme_content : str property readonly Returns the readme content associated with a particular example. __init__ ( self , name , path_in_repo ) special Create a new Example instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path_in_repo Path Path to the local example within the global zenml folder. required Source code in zenml/cli/example.py def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo ExamplesRepo Class for the examples repository object. Source code in zenml/cli/example.py class ExamplesRepo : \"\"\"Class for the examples repository object.\"\"\" def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout ( branch = self . latest_release ) @property def active_version ( self ) -> Optional [ str ]: \"\"\"In case a tagged version is checked out, this property returns that version, else None is returned\"\"\" return next ( ( tag for tag in self . repo . tags if tag . commit == self . repo . head . commit ), None , ) @property def latest_release ( self ) -> str : \"\"\"Returns the latest release for the examples repository.\"\"\" tags = sorted ( self . repo . tags , key = lambda t : t . commit . committed_datetime , # type: ignore ) latest_tag = parse ( tags [ - 1 ] . name ) if type ( latest_tag ) is not Version : return \"main\" return tags [ - 1 ] . name # type: ignore @property def is_cloned ( self ) -> bool : \"\"\"Returns whether we have already cloned the examples repository.\"\"\" return self . cloning_path . exists () @property def examples_dir ( self ) -> str : \"\"\"Returns the path for the examples directory.\"\"\" return os . path . join ( self . cloning_path , \"examples\" ) @property def examples_run_bash_script ( self ) -> str : return os . path . join ( self . examples_dir , EXAMPLES_RUN_SCRIPT ) def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( self . latest_release ) active_version : Optional [ str ] property readonly In case a tagged version is checked out, this property returns that version, else None is returned examples_dir : str property readonly Returns the path for the examples directory. is_cloned : bool property readonly Returns whether we have already cloned the examples repository. latest_release : str property readonly Returns the latest release for the examples repository. __init__ ( self , cloning_path ) special Create a new ExamplesRepo instance. Source code in zenml/cli/example.py def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout ( branch = self . latest_release ) checkout ( self , branch ) Checks out a specific branch or tag of the examples repository Exceptions: Type Description GitCommandError if branch doesn't exist. Source code in zenml/cli/example.py def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) checkout_latest_release ( self ) Checks out the latest release of the examples repository. Source code in zenml/cli/example.py def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( self . latest_release ) clone ( self ) Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. Source code in zenml/cli/example.py def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) delete ( self ) Delete cloning_path if it exists. Source code in zenml/cli/example.py def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) GitExamplesHandler Class for the GitExamplesHandler that interfaces with the CLI tool. Source code in zenml/cli/example.py class GitExamplesHandler ( object ): \"\"\"Class for the GitExamplesHandler that interfaces with the CLI tool.\"\"\" def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) @property def examples ( self ) -> List [ Example ]: \"\"\"Property that contains a list of examples\"\"\" return [ Example ( name , Path ( os . path . join ( self . examples_repo . examples_dir , name )) ) for name in sorted ( os . listdir ( self . examples_repo . examples_dir )) if ( not name . startswith ( \".\" ) and not name . startswith ( \"__\" ) and not name . startswith ( \"README\" ) and not name . endswith ( \".sh\" ) ) ] @property def is_matching_versions ( self ) -> bool : \"\"\"Returns a boolean whether the checked out examples are on the same code version as zenml\"\"\" return zenml_version_installed == str ( self . examples_repo . active_version ) def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples def pull ( self , version : str = \"\" , force : bool = False , branch : str = \"main\" ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if version == \"\" : version = zenml_version_installed if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : if branch not in self . examples_repo . repo . references : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to use main.\" ) branch = \"main\" if branch != \"main\" : self . examples_repo . checkout ( branch = branch ) else : self . examples_repo . checkout ( version ) except GitCommandError : logger . warning ( f \"Version { version } does not exist in remote repository. \" f \"Reverting to `main`.\" ) self . examples_repo . checkout ( \"main\" ) def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( version = self . examples_repo . latest_release , force = True ) def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory ) examples : List [ zenml . cli . example . Example ] property readonly Property that contains a list of examples is_matching_versions : bool property readonly Returns a boolean whether the checked out examples are on the same code version as zenml __init__ ( self ) special Create a new GitExamplesHandler instance. Source code in zenml/cli/example.py def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) clean_current_examples ( self ) Deletes the ZenML examples directory from your current working directory. Source code in zenml/cli/example.py def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory ) copy_example ( self , example , destination_dir ) Copies an example to the destination_dir. Source code in zenml/cli/example.py def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) get_examples ( self , example_name = None ) Method that allows you to get an example by name. If no example is supplied, all examples are returned Parameters: Name Type Description Default example_name Optional[str] Name of an example. None Source code in zenml/cli/example.py def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples is_example ( self , example_name = None ) Checks if the supplied example_name corresponds to an example Source code in zenml/cli/example.py def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False pull ( self , version = '' , force = False , branch = 'main' ) Pulls the examples from the main git examples repository. Source code in zenml/cli/example.py def pull ( self , version : str = \"\" , force : bool = False , branch : str = \"main\" ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if version == \"\" : version = zenml_version_installed if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : if branch not in self . examples_repo . repo . references : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to use main.\" ) branch = \"main\" if branch != \"main\" : self . examples_repo . checkout ( branch = branch ) else : self . examples_repo . checkout ( version ) except GitCommandError : logger . warning ( f \"Version { version } does not exist in remote repository. \" f \"Reverting to `main`.\" ) self . examples_repo . checkout ( \"main\" ) pull_latest_examples ( self ) Pulls the latest examples from the examples repository. Source code in zenml/cli/example.py def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( version = self . examples_repo . latest_release , force = True ) LocalExample Class to encapsulate all properties and methods of the local example that can be run from the CLI Source code in zenml/cli/example.py class LocalExample : \"\"\"Class to encapsulate all properties and methods of the local example that can be run from the CLI\"\"\" def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path @property def python_files_in_dir ( self ) -> List [ str ]: \"\"\"List of all python files in the drectl in local example directory the __init__.py file is excluded from this list\"\"\" py_in_dir = fileio . find_files ( str ( self . path ), \"*.py\" ) py_files = [] for file in py_in_dir : # Make sure only files directly in dir are considered, not files # in sub dirs if self . path == Path ( file ) . parent : if Path ( file ) . name != \"__init__.py\" : py_files . append ( file ) return py_files @property def has_single_python_file ( self ) -> bool : \"\"\"Boolean that states if only one python file is present\"\"\" return len ( self . python_files_in_dir ) == 1 @property def has_any_python_file ( self ) -> bool : \"\"\"Boolean that states if any python file is present\"\"\" return len ( self . python_files_in_dir ) > 0 @property def executable_python_example ( self ) -> str : \"\"\"Return the python file for the example\"\"\" if self . has_single_python_file : return self . python_files_in_dir [ 0 ] elif self . has_any_python_file : logger . warning ( \"This example has multiple executable python files. \" \"The last one in alphanumerical order is taken.\" ) return sorted ( self . python_files_in_dir )[ - 1 ] else : raise RuntimeError ( \"No pipeline runner script found in example. \" f \"Files found: { self . python_files_in_dir } \" ) def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name }) executable_python_example : str property readonly Return the python file for the example has_any_python_file : bool property readonly Boolean that states if any python file is present has_single_python_file : bool property readonly Boolean that states if only one python file is present python_files_in_dir : List [ str ] property readonly List of all python files in the drectl in local example directory the init .py file is excluded from this list __init__ ( self , path , name ) special Create a new LocalExample instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path Path Path at which the example is installed required Source code in zenml/cli/example.py def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path is_present ( self ) Checks if the example is installed at the given path. Source code in zenml/cli/example.py def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) run_example ( self , example_runner , force ) Run the local example using the bash script at the supplied location Parameters: Name Type Description Default example_runner List[str] Sequence of locations of executable file(s) to run the example required force bool Whether to force the install required Source code in zenml/cli/example.py def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name })","title":"Example"},{"location":"api_docs/cli/example/#example","text":"","title":"Example"},{"location":"api_docs/cli/example/#zenml.cli.example","text":"","title":"example"},{"location":"api_docs/cli/example/#zenml.cli.example.Example","text":"Class for all example objects. Source code in zenml/cli/example.py class Example : \"\"\"Class for all example objects.\"\"\" def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo @property def readme_content ( self ) -> str : \"\"\"Returns the readme content associated with a particular example.\"\"\" readme_file = os . path . join ( self . path_in_repo , \"README.md\" ) try : with open ( readme_file ) as readme : readme_content = readme . read () return readme_content except FileNotFoundError : if fileio . file_exists ( str ( self . path_in_repo )) and fileio . is_dir ( str ( self . path_in_repo ) ): raise ValueError ( f \"No README.md file found in \" f \" { self . path_in_repo } \" ) else : raise FileNotFoundError ( f \"Example { self . name } is not one of the available options.\" f \" \\n \" f \"To list all available examples, type: `zenml example \" f \"list`\" )","title":"Example"},{"location":"api_docs/cli/example/#zenml.cli.example.Example.readme_content","text":"Returns the readme content associated with a particular example.","title":"readme_content"},{"location":"api_docs/cli/example/#zenml.cli.example.Example.__init__","text":"Create a new Example instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path_in_repo Path Path to the local example within the global zenml folder. required Source code in zenml/cli/example.py def __init__ ( self , name : str , path_in_repo : Path ) -> None : \"\"\"Create a new Example instance. Args: name: The name of the example, specifically the name of the folder on git path_in_repo: Path to the local example within the global zenml folder. \"\"\" self . name = name self . path_in_repo = path_in_repo","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo","text":"Class for the examples repository object. Source code in zenml/cli/example.py class ExamplesRepo : \"\"\"Class for the examples repository object.\"\"\" def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout ( branch = self . latest_release ) @property def active_version ( self ) -> Optional [ str ]: \"\"\"In case a tagged version is checked out, this property returns that version, else None is returned\"\"\" return next ( ( tag for tag in self . repo . tags if tag . commit == self . repo . head . commit ), None , ) @property def latest_release ( self ) -> str : \"\"\"Returns the latest release for the examples repository.\"\"\" tags = sorted ( self . repo . tags , key = lambda t : t . commit . committed_datetime , # type: ignore ) latest_tag = parse ( tags [ - 1 ] . name ) if type ( latest_tag ) is not Version : return \"main\" return tags [ - 1 ] . name # type: ignore @property def is_cloned ( self ) -> bool : \"\"\"Returns whether we have already cloned the examples repository.\"\"\" return self . cloning_path . exists () @property def examples_dir ( self ) -> str : \"\"\"Returns the path for the examples directory.\"\"\" return os . path . join ( self . cloning_path , \"examples\" ) @property def examples_run_bash_script ( self ) -> str : return os . path . join ( self . examples_dir , EXAMPLES_RUN_SCRIPT ) def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" ) def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" ) def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch ) def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( self . latest_release )","title":"ExamplesRepo"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.active_version","text":"In case a tagged version is checked out, this property returns that version, else None is returned","title":"active_version"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.examples_dir","text":"Returns the path for the examples directory.","title":"examples_dir"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.is_cloned","text":"Returns whether we have already cloned the examples repository.","title":"is_cloned"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.latest_release","text":"Returns the latest release for the examples repository.","title":"latest_release"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.__init__","text":"Create a new ExamplesRepo instance. Source code in zenml/cli/example.py def __init__ ( self , cloning_path : Path ) -> None : \"\"\"Create a new ExamplesRepo instance.\"\"\" self . cloning_path = cloning_path try : self . repo = Repo ( self . cloning_path ) except NoSuchPathError or InvalidGitRepositoryError : self . repo = None # type: ignore logger . debug ( f \"`Cloning_path`: { self . cloning_path } was empty, \" \"Automatically cloning the examples.\" ) self . clone () self . checkout ( branch = self . latest_release )","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.checkout","text":"Checks out a specific branch or tag of the examples repository Exceptions: Type Description GitCommandError if branch doesn't exist. Source code in zenml/cli/example.py def checkout ( self , branch : str ) -> None : \"\"\"Checks out a specific branch or tag of the examples repository Raises: GitCommandError: if branch doesn't exist. \"\"\" logger . info ( f \"Checking out branch: { branch } \" ) self . repo . git . checkout ( branch )","title":"checkout()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.checkout_latest_release","text":"Checks out the latest release of the examples repository. Source code in zenml/cli/example.py def checkout_latest_release ( self ) -> None : \"\"\"Checks out the latest release of the examples repository.\"\"\" self . checkout ( self . latest_release )","title":"checkout_latest_release()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.clone","text":"Clones repo to cloning_path. If you break off the operation with a KeyBoardInterrupt before the cloning is completed, this method will delete whatever was partially downloaded from your system. Source code in zenml/cli/example.py def clone ( self ) -> None : \"\"\"Clones repo to cloning_path. If you break off the operation with a `KeyBoardInterrupt` before the cloning is completed, this method will delete whatever was partially downloaded from your system.\"\"\" self . cloning_path . mkdir ( parents = True , exist_ok = False ) try : logger . info ( f \"Cloning repo { GIT_REPO_URL } to { self . cloning_path } \" ) self . repo = Repo . clone_from ( GIT_REPO_URL , self . cloning_path , branch = \"main\" ) except KeyboardInterrupt : self . delete () logger . error ( \"Canceled download of repository.. Rolled back.\" )","title":"clone()"},{"location":"api_docs/cli/example/#zenml.cli.example.ExamplesRepo.delete","text":"Delete cloning_path if it exists. Source code in zenml/cli/example.py def delete ( self ) -> None : \"\"\"Delete `cloning_path` if it exists.\"\"\" if self . cloning_path . exists (): shutil . rmtree ( self . cloning_path ) else : raise AssertionError ( f \"Cannot delete the examples repository from \" f \" { self . cloning_path } as it does not exist.\" )","title":"delete()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler","text":"Class for the GitExamplesHandler that interfaces with the CLI tool. Source code in zenml/cli/example.py class GitExamplesHandler ( object ): \"\"\"Class for the GitExamplesHandler that interfaces with the CLI tool.\"\"\" def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir ) @property def examples ( self ) -> List [ Example ]: \"\"\"Property that contains a list of examples\"\"\" return [ Example ( name , Path ( os . path . join ( self . examples_repo . examples_dir , name )) ) for name in sorted ( os . listdir ( self . examples_repo . examples_dir )) if ( not name . startswith ( \".\" ) and not name . startswith ( \"__\" ) and not name . startswith ( \"README\" ) and not name . endswith ( \".sh\" ) ) ] @property def is_matching_versions ( self ) -> bool : \"\"\"Returns a boolean whether the checked out examples are on the same code version as zenml\"\"\" return zenml_version_installed == str ( self . examples_repo . active_version ) def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples def pull ( self , version : str = \"\" , force : bool = False , branch : str = \"main\" ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if version == \"\" : version = zenml_version_installed if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : if branch not in self . examples_repo . repo . references : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to use main.\" ) branch = \"main\" if branch != \"main\" : self . examples_repo . checkout ( branch = branch ) else : self . examples_repo . checkout ( version ) except GitCommandError : logger . warning ( f \"Version { version } does not exist in remote repository. \" f \"Reverting to `main`.\" ) self . examples_repo . checkout ( \"main\" ) def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( version = self . examples_repo . latest_release , force = True ) def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True ) def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory )","title":"GitExamplesHandler"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.examples","text":"Property that contains a list of examples","title":"examples"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.is_matching_versions","text":"Returns a boolean whether the checked out examples are on the same code version as zenml","title":"is_matching_versions"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.__init__","text":"Create a new GitExamplesHandler instance. Source code in zenml/cli/example.py def __init__ ( self ) -> None : \"\"\"Create a new GitExamplesHandler instance.\"\"\" self . repo_dir = zenml . io . utils . get_global_config_directory () self . examples_dir = Path ( os . path . join ( self . repo_dir , EXAMPLES_GITHUB_REPO ) ) self . examples_repo = ExamplesRepo ( self . examples_dir )","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.clean_current_examples","text":"Deletes the ZenML examples directory from your current working directory. Source code in zenml/cli/example.py def clean_current_examples ( self ) -> None : \"\"\"Deletes the ZenML examples directory from your current working directory.\"\"\" examples_directory = os . path . join ( os . getcwd (), \"zenml_examples\" ) shutil . rmtree ( examples_directory )","title":"clean_current_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.copy_example","text":"Copies an example to the destination_dir. Source code in zenml/cli/example.py def copy_example ( self , example : Example , destination_dir : str ) -> None : \"\"\"Copies an example to the destination_dir.\"\"\" fileio . create_dir_if_not_exists ( destination_dir ) fileio . copy_dir ( str ( example . path_in_repo ), destination_dir , overwrite = True )","title":"copy_example()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.get_examples","text":"Method that allows you to get an example by name. If no example is supplied, all examples are returned Parameters: Name Type Description Default example_name Optional[str] Name of an example. None Source code in zenml/cli/example.py def get_examples ( self , example_name : Optional [ str ] = None ) -> List [ Example ]: \"\"\"Method that allows you to get an example by name. If no example is supplied, all examples are returned Args: example_name: Name of an example. \"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return [ example_dict [ example_name ]] else : raise KeyError ( f \"Example { example_name } does not exist! \" f \"Available examples: { [ example_dict . keys ()] } \" ) else : return self . examples","title":"get_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.is_example","text":"Checks if the supplied example_name corresponds to an example Source code in zenml/cli/example.py def is_example ( self , example_name : Optional [ str ] = None ) -> bool : \"\"\"Checks if the supplied example_name corresponds to an example\"\"\" example_dict = { e . name : e for e in self . examples } if example_name : if example_name in example_dict . keys (): return True return False","title":"is_example()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.pull","text":"Pulls the examples from the main git examples repository. Source code in zenml/cli/example.py def pull ( self , version : str = \"\" , force : bool = False , branch : str = \"main\" ) -> None : \"\"\"Pulls the examples from the main git examples repository.\"\"\" if version == \"\" : version = zenml_version_installed if not self . examples_repo . is_cloned : self . examples_repo . clone () elif force : self . examples_repo . delete () self . examples_repo . clone () try : if branch not in self . examples_repo . repo . references : warning ( f \"The specified branch { branch } not found in \" \"repo, falling back to use main.\" ) branch = \"main\" if branch != \"main\" : self . examples_repo . checkout ( branch = branch ) else : self . examples_repo . checkout ( version ) except GitCommandError : logger . warning ( f \"Version { version } does not exist in remote repository. \" f \"Reverting to `main`.\" ) self . examples_repo . checkout ( \"main\" )","title":"pull()"},{"location":"api_docs/cli/example/#zenml.cli.example.GitExamplesHandler.pull_latest_examples","text":"Pulls the latest examples from the examples repository. Source code in zenml/cli/example.py def pull_latest_examples ( self ) -> None : \"\"\"Pulls the latest examples from the examples repository.\"\"\" self . pull ( version = self . examples_repo . latest_release , force = True )","title":"pull_latest_examples()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample","text":"Class to encapsulate all properties and methods of the local example that can be run from the CLI Source code in zenml/cli/example.py class LocalExample : \"\"\"Class to encapsulate all properties and methods of the local example that can be run from the CLI\"\"\" def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path @property def python_files_in_dir ( self ) -> List [ str ]: \"\"\"List of all python files in the drectl in local example directory the __init__.py file is excluded from this list\"\"\" py_in_dir = fileio . find_files ( str ( self . path ), \"*.py\" ) py_files = [] for file in py_in_dir : # Make sure only files directly in dir are considered, not files # in sub dirs if self . path == Path ( file ) . parent : if Path ( file ) . name != \"__init__.py\" : py_files . append ( file ) return py_files @property def has_single_python_file ( self ) -> bool : \"\"\"Boolean that states if only one python file is present\"\"\" return len ( self . python_files_in_dir ) == 1 @property def has_any_python_file ( self ) -> bool : \"\"\"Boolean that states if any python file is present\"\"\" return len ( self . python_files_in_dir ) > 0 @property def executable_python_example ( self ) -> str : \"\"\"Return the python file for the example\"\"\" if self . has_single_python_file : return self . python_files_in_dir [ 0 ] elif self . has_any_python_file : logger . warning ( \"This example has multiple executable python files. \" \"The last one in alphanumerical order is taken.\" ) return sorted ( self . python_files_in_dir )[ - 1 ] else : raise RuntimeError ( \"No pipeline runner script found in example. \" f \"Files found: { self . python_files_in_dir } \" ) def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) ) def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name })","title":"LocalExample"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.executable_python_example","text":"Return the python file for the example","title":"executable_python_example"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.has_any_python_file","text":"Boolean that states if any python file is present","title":"has_any_python_file"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.has_single_python_file","text":"Boolean that states if only one python file is present","title":"has_single_python_file"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.python_files_in_dir","text":"List of all python files in the drectl in local example directory the init .py file is excluded from this list","title":"python_files_in_dir"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.__init__","text":"Create a new LocalExample instance. Parameters: Name Type Description Default name str The name of the example, specifically the name of the folder on git required path Path Path at which the example is installed required Source code in zenml/cli/example.py def __init__ ( self , path : Path , name : str ) -> None : \"\"\"Create a new LocalExample instance. Args: name: The name of the example, specifically the name of the folder on git path: Path at which the example is installed \"\"\" self . name = name self . path = path","title":"__init__()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.is_present","text":"Checks if the example is installed at the given path. Source code in zenml/cli/example.py def is_present ( self ) -> bool : \"\"\"Checks if the example is installed at the given path.\"\"\" return fileio . file_exists ( str ( self . path )) and fileio . is_dir ( str ( self . path ) )","title":"is_present()"},{"location":"api_docs/cli/example/#zenml.cli.example.LocalExample.run_example","text":"Run the local example using the bash script at the supplied location Parameters: Name Type Description Default example_runner List[str] Sequence of locations of executable file(s) to run the example required force bool Whether to force the install required Source code in zenml/cli/example.py def run_example ( self , example_runner : List [ str ], force : bool ) -> None : \"\"\"Run the local example using the bash script at the supplied location Args: example_runner: Sequence of locations of executable file(s) to run the example force: Whether to force the install \"\"\" if all ( map ( fileio . file_exists , example_runner )): call = ( example_runner + [ \"--executable\" , self . executable_python_example ] + [ \"-f\" ] * force ) try : # TODO [ENG-271]: Catch errors that might be thrown # in subprocess subprocess . check_call ( call , cwd = str ( self . path ), shell = click . _compat . WIN ) except RuntimeError : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) except subprocess . CalledProcessError as e : if e . returncode == 38 : raise NotImplementedError ( f \"Currently the example { self . name } \" \"has no implementation for the \" \"run method\" ) else : raise FileNotFoundError ( \"Bash File(s) to run Examples not found at\" f \" { example_runner } \" ) # Telemetry track_event ( RUN_EXAMPLE , { \"name\" : self . name })","title":"run_example()"},{"location":"api_docs/cli/integration/","text":"Integration zenml.cli.integration","title":"Integration"},{"location":"api_docs/cli/integration/#integration","text":"","title":"Integration"},{"location":"api_docs/cli/integration/#zenml.cli.integration","text":"","title":"integration"},{"location":"api_docs/cli/metadata_store/","text":"Metadata Store zenml.cli.metadata_store","title":"Metadata Store"},{"location":"api_docs/cli/metadata_store/#metadata-store","text":"","title":"Metadata Store"},{"location":"api_docs/cli/metadata_store/#zenml.cli.metadata_store","text":"","title":"metadata_store"},{"location":"api_docs/cli/orchestrator/","text":"Orchestrator zenml.cli.orchestrator","title":"Orchestrator"},{"location":"api_docs/cli/orchestrator/#orchestrator","text":"","title":"Orchestrator"},{"location":"api_docs/cli/orchestrator/#zenml.cli.orchestrator","text":"","title":"orchestrator"},{"location":"api_docs/cli/pipeline/","text":"Pipeline zenml.cli.pipeline CLI to interact with pipelines.","title":"Pipeline"},{"location":"api_docs/cli/pipeline/#pipeline","text":"","title":"Pipeline"},{"location":"api_docs/cli/pipeline/#zenml.cli.pipeline","text":"CLI to interact with pipelines.","title":"pipeline"},{"location":"api_docs/cli/stack/","text":"Stack zenml.cli.stack CLI for manipulating ZenML local and global config file.","title":"Stack"},{"location":"api_docs/cli/stack/#stack","text":"","title":"Stack"},{"location":"api_docs/cli/stack/#zenml.cli.stack","text":"CLI for manipulating ZenML local and global config file.","title":"stack"},{"location":"api_docs/cli/utils/","text":"Utils zenml.cli.utils activate_integrations ( func ) Decorator that activates all ZenML integrations. Source code in zenml/cli/utils.py def activate_integrations ( func : F ) -> F : \"\"\"Decorator that activates all ZenML integrations.\"\"\" @functools . wraps ( func ) def _wrapper ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function\"\"\" from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () return func ( * args , ** kwargs ) return cast ( F , _wrapper ) confirmation ( text , * args , ** kwargs ) Echo a confirmation string on the CLI. Parameters: Name Type Description Default text str Input text string. required *args Any Args to be passed to click.confirm(). () **kwargs Any Kwargs to be passed to click.confirm(). {} Returns: Type Description bool Boolean based on user response. Source code in zenml/cli/utils.py def confirmation ( text : str , * args : Any , ** kwargs : Any ) -> bool : \"\"\"Echo a confirmation string on the CLI. Args: text: Input text string. *args: Args to be passed to click.confirm(). **kwargs: Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. \"\"\" return click . confirm ( click . style ( text , fg = \"yellow\" ), * args , ** kwargs ) declare ( text ) Echo a declaration on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def declare ( text : str ) -> None : \"\"\"Echo a declaration on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"green\" )) error ( text ) Echo an error string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def error ( text : str ) -> None : \"\"\"Echo an error string on the CLI. Args: text: Input text string. Raises: click.ClickException when called. \"\"\" raise click . ClickException ( message = click . style ( text , fg = \"red\" , bold = True )) format_component_list ( component_list , active_component ) Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Parameters: Name Type Description Default component_list Mapping[str, BaseComponent] The component_list is a mapping of component key to component class with its relevant attributes required active_component str The component that is currently active required Returns: Type Description list_of_dicts A list of all components with each component as a dict Source code in zenml/cli/utils.py def format_component_list ( component_list : Mapping [ str , \"BaseComponent\" ], active_component : str ) -> List [ Dict [ str , str ]]: \"\"\"Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list: The component_list is a mapping of component key to component class with its relevant attributes active_component: The component that is currently active Returns: list_of_dicts: A list of all components with each component as a dict \"\"\" list_of_dicts = [] for key , c in component_list . items (): # Make sure that the `name` key is not taken in the component dict # In case `name` exists, it is replaced inplace with `component_name` component_dict = { \"COMPONENT_NAME\" if k == \"name\" else k . upper (): v for k , v in c . dict ( exclude = { \"_superfluous_options\" }) . items () } data = { \"ACTIVE\" : \"*\" if key == active_component else \"\" , \"NAME\" : key } data . update ( component_dict ) list_of_dicts . append ( data ) return list_of_dicts format_date ( dt , format = '%Y-%m- %d %H:%M:%S' ) Format a date into a string. Parameters: Name Type Description Default dt datetime Datetime object to be formatted. required format str The format in string you want the datetime formatted to. '%Y-%m-%d %H:%M:%S' Returns: Type Description str Formatted string according to specification. Source code in zenml/cli/utils.py def format_date ( dt : datetime . datetime , format : str = \"%Y-%m- %d %H:%M:%S\" ) -> str : \"\"\"Format a date into a string. Args: dt: Datetime object to be formatted. format: The format in string you want the datetime formatted to. Returns: Formatted string according to specification. \"\"\" if dt is None : return \"\" # make sure this is UTC dt = dt . replace ( tzinfo = tz . tzutc ()) if sys . platform != \"win32\" : # On non-windows get local time zone. local_zone = tz . tzlocal () dt = dt . astimezone ( local_zone ) else : logger . warning ( \"On Windows, all times are displayed in UTC timezone.\" ) return dt . strftime ( format ) install_package ( package ) Installs pypi package into the current environment with pip Source code in zenml/cli/utils.py def install_package ( package : str ) -> None : \"\"\"Installs pypi package into the current environment with pip\"\"\" subprocess . check_call ([ sys . executable , \"-m\" , \"pip\" , \"install\" , package ]) parse_unknown_options ( args ) Parse unknown options from the CLI. Parameters: Name Type Description Default args List[str] A list of strings from the CLI. required Returns: Type Description Dict[str, Any] Dict of parsed args. Source code in zenml/cli/utils.py def parse_unknown_options ( args : List [ str ]) -> Dict [ str , Any ]: \"\"\"Parse unknown options from the CLI. Args: args: A list of strings from the CLI. Returns: Dict of parsed args. \"\"\" warning_message = ( \"Please provide args with a proper \" \"identifier as the key and the following structure: \" '--custom_argument=\"value\"' ) assert all ( a . startswith ( \"--\" ) for a in args ), warning_message assert all ( len ( a . split ( \"=\" )) == 2 for a in args ), warning_message p_args = [ a . lstrip ( \"--\" ) . split ( \"=\" ) for a in args ] assert all ( k . isidentifier () for k , _ in p_args ), warning_message r_args = { k : v for k , v in p_args } assert len ( p_args ) == len ( r_args ), \"Replicated arguments!\" return r_args pretty_print ( obj ) Pretty print an object on the CLI. Parameters: Name Type Description Default obj Any Any object with a str method defined. required Source code in zenml/cli/utils.py def pretty_print ( obj : Any ) -> None : \"\"\"Pretty print an object on the CLI. Args: obj: Any object with a __str__ method defined. \"\"\" click . echo ( str ( obj )) print_component_properties ( properties ) Prints the properties of a component. Parameters: Name Type Description Default properties Dict[str, str] A dictionary of properties. required Source code in zenml/cli/utils.py def print_component_properties ( properties : Dict [ str , str ]) -> None : \"\"\"Prints the properties of a component. Args: properties: A dictionary of properties. \"\"\" for key , value in properties . items (): cli_utils . declare ( f \" { key . upper () } : { value } \" ) print_table ( obj ) Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Parameters: Name Type Description Default obj List[Dict[str, Any]] A List containing dictionaries. required Source code in zenml/cli/utils.py def print_table ( obj : List [ Dict [ str , Any ]]) -> None : \"\"\"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj: A List containing dictionaries. \"\"\" click . echo ( tabulate ( obj , headers = \"keys\" )) title ( text ) Echo a title formatted string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def title ( text : str ) -> None : \"\"\"Echo a title formatted string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text . upper (), fg = \"cyan\" , bold = True , underline = True )) uninstall_package ( package ) Uninstalls pypi package from the current environment with pip Source code in zenml/cli/utils.py def uninstall_package ( package : str ) -> None : \"\"\"Uninstalls pypi package from the current environment with pip\"\"\" subprocess . check_call ( [ sys . executable , \"-m\" , \"pip\" , \"uninstall\" , \"-y\" , package ] ) warning ( text ) Echo a warning string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def warning ( text : str ) -> None : \"\"\"Echo a warning string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"yellow\" , bold = True ))","title":"Utils"},{"location":"api_docs/cli/utils/#utils","text":"","title":"Utils"},{"location":"api_docs/cli/utils/#zenml.cli.utils","text":"","title":"utils"},{"location":"api_docs/cli/utils/#zenml.cli.utils.activate_integrations","text":"Decorator that activates all ZenML integrations. Source code in zenml/cli/utils.py def activate_integrations ( func : F ) -> F : \"\"\"Decorator that activates all ZenML integrations.\"\"\" @functools . wraps ( func ) def _wrapper ( * args : Any , ** kwargs : Any ) -> Any : \"\"\"Inner decorator function\"\"\" from zenml.integrations.registry import integration_registry integration_registry . activate_integrations () return func ( * args , ** kwargs ) return cast ( F , _wrapper )","title":"activate_integrations()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.confirmation","text":"Echo a confirmation string on the CLI. Parameters: Name Type Description Default text str Input text string. required *args Any Args to be passed to click.confirm(). () **kwargs Any Kwargs to be passed to click.confirm(). {} Returns: Type Description bool Boolean based on user response. Source code in zenml/cli/utils.py def confirmation ( text : str , * args : Any , ** kwargs : Any ) -> bool : \"\"\"Echo a confirmation string on the CLI. Args: text: Input text string. *args: Args to be passed to click.confirm(). **kwargs: Kwargs to be passed to click.confirm(). Returns: Boolean based on user response. \"\"\" return click . confirm ( click . style ( text , fg = \"yellow\" ), * args , ** kwargs )","title":"confirmation()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.declare","text":"Echo a declaration on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def declare ( text : str ) -> None : \"\"\"Echo a declaration on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"green\" ))","title":"declare()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.error","text":"Echo an error string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def error ( text : str ) -> None : \"\"\"Echo an error string on the CLI. Args: text: Input text string. Raises: click.ClickException when called. \"\"\" raise click . ClickException ( message = click . style ( text , fg = \"red\" , bold = True ))","title":"error()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.format_component_list","text":"Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Parameters: Name Type Description Default component_list Mapping[str, BaseComponent] The component_list is a mapping of component key to component class with its relevant attributes required active_component str The component that is currently active required Returns: Type Description list_of_dicts A list of all components with each component as a dict Source code in zenml/cli/utils.py def format_component_list ( component_list : Mapping [ str , \"BaseComponent\" ], active_component : str ) -> List [ Dict [ str , str ]]: \"\"\"Formats a list of components into a List of Dicts. This list of dicts can then be printed in a table style using cli_utils.print_table. Args: component_list: The component_list is a mapping of component key to component class with its relevant attributes active_component: The component that is currently active Returns: list_of_dicts: A list of all components with each component as a dict \"\"\" list_of_dicts = [] for key , c in component_list . items (): # Make sure that the `name` key is not taken in the component dict # In case `name` exists, it is replaced inplace with `component_name` component_dict = { \"COMPONENT_NAME\" if k == \"name\" else k . upper (): v for k , v in c . dict ( exclude = { \"_superfluous_options\" }) . items () } data = { \"ACTIVE\" : \"*\" if key == active_component else \"\" , \"NAME\" : key } data . update ( component_dict ) list_of_dicts . append ( data ) return list_of_dicts","title":"format_component_list()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.format_date","text":"Format a date into a string. Parameters: Name Type Description Default dt datetime Datetime object to be formatted. required format str The format in string you want the datetime formatted to. '%Y-%m-%d %H:%M:%S' Returns: Type Description str Formatted string according to specification. Source code in zenml/cli/utils.py def format_date ( dt : datetime . datetime , format : str = \"%Y-%m- %d %H:%M:%S\" ) -> str : \"\"\"Format a date into a string. Args: dt: Datetime object to be formatted. format: The format in string you want the datetime formatted to. Returns: Formatted string according to specification. \"\"\" if dt is None : return \"\" # make sure this is UTC dt = dt . replace ( tzinfo = tz . tzutc ()) if sys . platform != \"win32\" : # On non-windows get local time zone. local_zone = tz . tzlocal () dt = dt . astimezone ( local_zone ) else : logger . warning ( \"On Windows, all times are displayed in UTC timezone.\" ) return dt . strftime ( format )","title":"format_date()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.install_package","text":"Installs pypi package into the current environment with pip Source code in zenml/cli/utils.py def install_package ( package : str ) -> None : \"\"\"Installs pypi package into the current environment with pip\"\"\" subprocess . check_call ([ sys . executable , \"-m\" , \"pip\" , \"install\" , package ])","title":"install_package()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.parse_unknown_options","text":"Parse unknown options from the CLI. Parameters: Name Type Description Default args List[str] A list of strings from the CLI. required Returns: Type Description Dict[str, Any] Dict of parsed args. Source code in zenml/cli/utils.py def parse_unknown_options ( args : List [ str ]) -> Dict [ str , Any ]: \"\"\"Parse unknown options from the CLI. Args: args: A list of strings from the CLI. Returns: Dict of parsed args. \"\"\" warning_message = ( \"Please provide args with a proper \" \"identifier as the key and the following structure: \" '--custom_argument=\"value\"' ) assert all ( a . startswith ( \"--\" ) for a in args ), warning_message assert all ( len ( a . split ( \"=\" )) == 2 for a in args ), warning_message p_args = [ a . lstrip ( \"--\" ) . split ( \"=\" ) for a in args ] assert all ( k . isidentifier () for k , _ in p_args ), warning_message r_args = { k : v for k , v in p_args } assert len ( p_args ) == len ( r_args ), \"Replicated arguments!\" return r_args","title":"parse_unknown_options()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.pretty_print","text":"Pretty print an object on the CLI. Parameters: Name Type Description Default obj Any Any object with a str method defined. required Source code in zenml/cli/utils.py def pretty_print ( obj : Any ) -> None : \"\"\"Pretty print an object on the CLI. Args: obj: Any object with a __str__ method defined. \"\"\" click . echo ( str ( obj ))","title":"pretty_print()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.print_component_properties","text":"Prints the properties of a component. Parameters: Name Type Description Default properties Dict[str, str] A dictionary of properties. required Source code in zenml/cli/utils.py def print_component_properties ( properties : Dict [ str , str ]) -> None : \"\"\"Prints the properties of a component. Args: properties: A dictionary of properties. \"\"\" for key , value in properties . items (): cli_utils . declare ( f \" { key . upper () } : { value } \" )","title":"print_component_properties()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.print_table","text":"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Parameters: Name Type Description Default obj List[Dict[str, Any]] A List containing dictionaries. required Source code in zenml/cli/utils.py def print_table ( obj : List [ Dict [ str , Any ]]) -> None : \"\"\"Echoes the list of dicts in a table format. The input object should be a List of Dicts. Each item in that list represent a line in the Table. Each dict should have the same keys. The keys of the dict will be used as headers of the resulting table. Args: obj: A List containing dictionaries. \"\"\" click . echo ( tabulate ( obj , headers = \"keys\" ))","title":"print_table()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.title","text":"Echo a title formatted string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def title ( text : str ) -> None : \"\"\"Echo a title formatted string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text . upper (), fg = \"cyan\" , bold = True , underline = True ))","title":"title()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.uninstall_package","text":"Uninstalls pypi package from the current environment with pip Source code in zenml/cli/utils.py def uninstall_package ( package : str ) -> None : \"\"\"Uninstalls pypi package from the current environment with pip\"\"\" subprocess . check_call ( [ sys . executable , \"-m\" , \"pip\" , \"uninstall\" , \"-y\" , package ] )","title":"uninstall_package()"},{"location":"api_docs/cli/utils/#zenml.cli.utils.warning","text":"Echo a warning string on the CLI. Parameters: Name Type Description Default text str Input text string. required Source code in zenml/cli/utils.py def warning ( text : str ) -> None : \"\"\"Echo a warning string on the CLI. Args: text: Input text string. \"\"\" click . echo ( click . style ( text , fg = \"yellow\" , bold = True ))","title":"warning()"},{"location":"api_docs/cli/version/","text":"Version zenml.cli.version","title":"Version"},{"location":"api_docs/cli/version/#version","text":"","title":"Version"},{"location":"api_docs/cli/version/#zenml.cli.version","text":"","title":"version"}]}