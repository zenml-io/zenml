# ğŸ¥— S3 and RDS MLOps Stack Recipe 

There can be many motivations behind taking your ML application setup to a cloud environment, from neeeding specialized compute ğŸ’ª for training jobs to having a 24x7 load-balanced deployment of your trained model serving user requests ğŸš€.

We know that the process to set up an MLOps stack can be daunting. There are many components (ever increasing) and each have their own requirements. To make your life easier, we already have a [documentation page](https://docs.zenml.io/cloud-guide/overview) that takes you step-by-step through the entire journey in a cloud platform of your choice (AWS, GCP and Azure supported for now). This recipe, however, goes one step further. 

You can have a simple MLOps stack ready for running your pipelines after you execute this recipe ğŸ˜. It sets up the following resources: 
- A local [orchestrator](https://docs.zenml.io/mlops-stacks/orchestrators) for your workloads.
- An S3 bucket as an [artifact store](https://docs.zenml.io/mlops-stacks/artifact-stores), which can be used to store all your ML artifacts like the model, checkpoints, etc. 
- An AWS RDS MySQL instance as a [metadata store](https://docs.zenml.io/mlops-stacks/metadata-stores) that is essential to track all your metadata and its location in your artifact store.  
- A [secrets manager](https://docs.zenml.io/mlops-stacks/secrets-managers) enabled for storing secrets for the metadata store. 

Keep in mind, this is a basic setup to get you up and running on AWS with a minimal MLOps stack and more configuration options are coming in the form of new recipes! ğŸ‘€

## Prerequisites

* You must have an AWS account where you have sufficient permissions to create and destroy resources that will be created as part of this recipe.
* Have [Terraform](https://learn.hashicorp.com/tutorials/terraform/install-cli#install-terraform) and [Helm](https://helm.sh/docs/intro/install/#from-script) installed on your system.


## ğŸ… Inputs

Before starting, you should know the values that you have to keep ready for use in the script. 
- Check out the `locals.tf` file to configure basic information about your deployments.
- Take a look at the `values.tfvars.json` file to know what values have to be supplied during the execution of the script. These are mostly sensitive values like MLflow passwords, AWS access keys, etc. Make sure you don't commit them!

> **Warning** 
> The `prefix` local variable you assign should have a unique value for each stack. This ensures that the stack you create doesn't interfere with the stacks somebody else in your organization has created with this script.

> **Warning**
> The CIDR block used for the VPC (inside the vpc.tf file) needs to be unique too, preferably. For example, if `10.10.0.0/16` is already under use by some VPC in your account, you can use `10.11.0.0/16` instead. However, this is not required.

## ğŸ§‘â€ğŸ³ Cooking the recipe

1. Pull this recipe to your local system.

    ```shell
    zenml stack recipe pull aws-stores-minimal
    ```
2. ğŸ¨ Customize your deployment by editing the default values in the `locals.tf` file.

3. ğŸ” Add your secret information like keys and passwords into the `values.tfvars.json` file which is not committed and only exists locally.

5. ğŸš€ Deploy the recipe with this simple command.

    ```
    zenml stack recipe deploy aws-stores-minimal
    ```

    > **Note**
    > If you want to allow ZenML to automatically import the created resources as a ZenML stack, pass the `--import` flag to the command above. By default, the imported stack will have the same name as the stack recipe and you can provide your own with the `--stack-name` option.
    

6. You'll notice that a ZenML stack configuration file gets created after the previous command executes ğŸ¤¯! This YAML file can be imported as a ZenML stack manually by running the following command.

    ```
    zenml stack import <stack-name> <path-to-the-created-stack-config-yaml>
    ```

> **Note**
>
>  You need to have your AWS credentials saved locally under ~/.aws/credentials

## ğŸœ Outputs 

The script, after running, outputs the following.
| Output | Description |
--- | ---

s3-bucket-path | The path of the S3 bucket. Useful while registering the artifact store|
metadata-db-host | The host endpoint of the deployed metadata store |
metadata-db-username | The username for the database user |
metadata-db-password | The master password for the database |
container-registry-URI | The URI for your ECR |

For outputs that are sensitive, you'll see that they are not shown directly on the logs. To view the full list of outputs, run the following command.

```bash
terraform output
```

To view individual sensitive outputs, use the following format. Here, the metadata password is being obtained. 

```bash
terraform output metadata-db-password
```
## Deleting Resources

Using the ZenML stack recipe CLI commands, you can run the following commands to delete your resources and optionally clean up the recipe files that you had downloaded to your local system.

1. ğŸ—‘ï¸ Run the destroy command which removes all resources and their dependencies from the cloud.

    ```shell
    zenml stack recipe destroy aws-stores-minimal
    ```

2. (Optional) ğŸ§¹ Clean up all stack recipe files that you had pulled to your local system.

    ```shell
    zenml stack recipe clean
    ```


In the case of this recipe, due to existing problems with Kubernetes and Terraform, there might be some resources that get stuck in the `Terminating` state forever. 

To combat this, either delete the Kubernetes nodes from the cloud console directly (recommended) or run the following command after double-checking that your local `kubectl` client is configured to talk to the created cluster. 

```bash
# WARNING: only run this when you are sure that kubectl points to the right cluster.
kubectl delete node --all
```

## Using the recipes without the ZenML CLI

As mentioned above, you can still use the recipe without having using the `zenml stack recipe` CLI commands or even without installing ZenML. Since each recipe is a group of Terraform modules, you can simply employ the terraform CLI to perform `apply` and `destroy` operations.

### Create the resources

1. ğŸ¨ Customize your deployment by editing the default values in the `locals.tf` file.

2. ğŸ” Add your secret information like keys and passwords into the `values.tfvars.json` file which is not committed and only exists locally.

3. Initiliaze Terraform modules and download provider definitions.
    ```bash
    terraform init
    ```

4. Apply the recipe.
    ```bash
    terraform apply
    ```

### Deleting resources

1. ğŸ—‘ï¸ Run the destroy function to clean up all resources.

    ```
    terraform destroy
    ```


## Troubleshoot Known Problems

These are some known problems that might arise out of running this recipe. Some of these 
are terraform commands but running `zenml stack recipe apply` would also achieve similar results as `terraform init` and `terraform apply`.

* Running the script for the first time might result in an error with one of the resources - the Istio Ingressway. This is because of a limitation with the resource `kubectl_manifest` that needs the cluster to be set up before it installs its own resources.
\
    ğŸ’¡ Fix - Run `terraform apply` again in a few minutes and this should get resolved.    



*  When executing terraform commands, an error like this one: `timeout while waiting for plugin to start` 
\
    ğŸ’¡ Fix - If you encounter this error with `apply`, `plan` or `destroy`, do `terraform init` and run your command again.

* While running `terraform apply`, an error which says `Failed to construct REST client` 
\
    ğŸ’¡ Fix - Run the `aws eks --region REGION update-kubeconfig --name <eks-cluster-name> --alias terraform` command and do `apply` again.

* While running a terraform command, this error might appear too: `context deadline exceeded`
\
    ğŸ’¡ Fix - This problem could arise due to strained system resources. Try running the command again after some time.
    