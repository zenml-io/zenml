# ZenML Pipeline Serving Examples

This directory contains examples demonstrating how to serve ZenML pipelines as FastAPI endpoints with real-time streaming capabilities.

## ğŸ“ Files

1. **`weather_pipeline.py`** - Simple weather analysis agent with LLM integration
2. **`chat_agent_pipeline.py`** - Streaming conversational AI chat agent 
3. **`test_serving.py`** - Test script to verify serving endpoints
4. **`README.md`** - This comprehensive guide

## ğŸ¯ Examples Overview

### 1. Weather Agent Pipeline
- **Purpose**: Analyze weather for any city with AI recommendations
- **Features**: LLM integration, rule-based fallback, parameter injection
- **API Mode**: Standard HTTP POST requests

### 2. Streaming Chat Agent Pipeline  
- **Purpose**: Real-time conversational AI with streaming responses
- **Features**: Token-by-token streaming, WebSocket support, Server-Sent Events
- **API Modes**: HTTP, WebSocket streaming, async jobs with SSE streaming

## Setup (Optional: For LLM Analysis)

To use real LLM analysis instead of rule-based fallback:

```bash
# Set your OpenAI API key
export OPENAI_API_KEY=your_openai_api_key_here

# Install OpenAI package
pip install openai
```

If no API key is provided, the pipeline will use an enhanced rule-based analysis as fallback.

# ğŸš€ Quick Start Guide

## ğŸ”§ Starting the Serving Service

ZenML serving supports multiple ways to start the service:

### Option 1: Modern Command-Line Arguments (Recommended)
```bash
# Basic usage with deployment ID
python -m zenml.serving --deployment_id <your-deployment-id>

# With custom configuration
python -m zenml.serving \
  --deployment_id <your-deployment-id> \
  --host 0.0.0.0 \
  --port 8080 \
  --workers 2 \
  --log_level debug
```

### Option 2: Legacy Environment Variables
```bash
export ZENML_PIPELINE_DEPLOYMENT_ID=<your-deployment-id>
export ZENML_SERVICE_HOST=0.0.0.0      # Optional
export ZENML_SERVICE_PORT=8080          # Optional  
export ZENML_SERVICE_WORKERS=2          # Optional
export ZENML_LOG_LEVEL=debug            # Optional
python -m zenml.serving
```

### Option 3: Advanced Entrypoint Configuration (For Integration)
```bash
# Using the serving entrypoint configuration class directly
python -m zenml.serving \
  --entrypoint_config_source zenml.serving.entrypoint_configuration.ServingEntrypointConfiguration \
  --deployment_id <your-deployment-id> \
  --host 0.0.0.0 \
  --port 8080
```

---

## Example 1: Weather Agent Pipeline

### Step 1: Create Pipeline Deployment

```bash
python weather_pipeline.py
```

This outputs a deployment ID like: `12345678-1234-5678-9abc-123456789abc`

### Step 2: Start Serving Service  

**Modern Command-Line Arguments (Recommended):**
```bash
python -m zenml.serving --deployment_id your_deployment_id_from_step_1
```

**Legacy Environment Variable Method:**
```bash
export ZENML_PIPELINE_DEPLOYMENT_ID=your_deployment_id_from_step_1
python -m zenml.serving
```

**Custom Configuration:**
```bash
python -m zenml.serving --deployment_id your_id --host 0.0.0.0 --port 8080 --workers 2 --log_level debug
```

Service starts on `http://localhost:8000` (or your custom port)

### Step 3: Test Weather Analysis

```bash
# Test with curl
curl -X POST "http://localhost:8000/invoke" \
  -H "Content-Type: application/json" \
  -d '{"parameters": {"city": "Paris"}}'

# Or use test script
python test_serving.py
```

---

## Example 2: Streaming Chat Agent Pipeline

### Step 1: Create Chat Pipeline Deployment

```bash
python chat_agent_pipeline.py
```

**Expected Output:**
```
ğŸ¤– Creating Chat Agent Pipeline Deployment...

ğŸ’¡ Note: Skipping local test due to ZenML integration loading issues
ğŸ“¦ Creating deployment for serving...

âœ… Deployment ID: f770327d-4ce0-4a6c-8033-955c2e990736
```

### Step 2: Start Serving Service

**Modern Command-Line Arguments (Recommended):**
```bash
python -m zenml.serving --deployment_id f770327d-4ce0-4a6c-8033-955c2e990736
```

**Legacy Environment Variable Method:**
```bash
export ZENML_PIPELINE_DEPLOYMENT_ID=f770327d-4ce0-4a6c-8033-955c2e990736  
python -m zenml.serving
```

### Step 3: Test Streaming Chat (Multiple Methods)

#### Method A: Simple HTTP Request
```bash
curl -X POST "http://localhost:8000/invoke" \
  -H "Content-Type: application/json" \
  -d '{"parameters": {"message": "Hello!", "user_name": "Alice", "personality": "helpful"}}'
```

#### Method B: Async Job + SSE Streaming (Recommended)
```bash
# Step 1: Create async job
curl -X POST 'http://localhost:8000/invoke?mode=async' \
  -H 'Content-Type: application/json' \
  -d '{"parameters": {"message": "Tell me about AI", "user_name": "Alice"}}'

# Response: {"job_id": "job-123", ...}

# Step 2: Stream real-time results
curl http://localhost:8000/stream/job-123
```

#### Method C: WebSocket Streaming (Real-time bidirectional)
```bash
# Install wscat if needed: npm install -g wscat
wscat -c ws://localhost:8000/stream

# Send message:
{"parameters": {"message": "Hi there!", "user_name": "Alice", "enable_streaming": true}}
```

### Step 4: Monitor Job Status
```bash
# Check specific job
curl http://localhost:8000/jobs/job-123

# List all jobs  
curl http://localhost:8000/jobs

# Cancel a job
curl -X POST http://localhost:8000/jobs/job-123/cancel

# View metrics
curl http://localhost:8000/concurrency/stats
```

# ğŸ“š API Reference

## Core Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | Service overview with documentation |
| `/health` | GET | Health check and uptime |
| `/info` | GET | Pipeline schema and configuration |
| `/invoke` | POST | Execute pipeline (sync/async modes) |
| `/metrics` | GET | Execution statistics |

## Streaming & Job Management

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/stream` | WebSocket | Real-time bidirectional streaming |
| `/jobs/{job_id}` | GET | Get job status and results |
| `/jobs/{job_id}/cancel` | POST | Cancel running job |
| `/jobs` | GET | List jobs with filtering |
| `/stream/{job_id}` | GET | Server-Sent Events stream |
| `/concurrency/stats` | GET | Concurrency and performance metrics |

## Parameters

### Weather Pipeline
```json
{
  "parameters": {
    "city": "string"
  }
}
```

### Chat Agent Pipeline  
```json
{
  "parameters": {
    "message": "string",
    "user_name": "string (optional)",
    "personality": "helpful|creative|professional|casual (optional)",
    "enable_streaming": "boolean (optional)"
  }
}
```

# ğŸ—ï¸ Architecture Overview

## How ZenML Serving Works

1. **ğŸ“¦ Pipeline Deployment**: Create deployment without execution
2. **ğŸš€ Serving Service**: FastAPI loads deployment and exposes endpoints  
3. **âš¡ Runtime Execution**: Each API call executes with different parameters
4. **ğŸ”„ Streaming Layer**: Real-time events via WebSocket/SSE for streaming pipelines

## Key Features

- **ğŸ¯ Parameter Injection**: Runtime parameter customization per request
- **ğŸ”„ Streaming Support**: Token-by-token streaming for conversational AI
- **âš–ï¸ Load Management**: Concurrency limits and request queuing
- **ğŸ“Š Job Tracking**: Async job lifecycle management with cancellation
- **ğŸ›¡ï¸ Thread Safety**: Cross-thread event publishing and state management
- **ğŸ“ˆ Observability**: Comprehensive metrics and health monitoring

## Streaming Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Client        â”‚    â”‚   FastAPI        â”‚    â”‚   Pipeline      â”‚
â”‚                 â”‚    â”‚   Serving        â”‚    â”‚   Execution     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HTTP POST       â”‚â”€â”€â”€â–¶â”‚ /invoke?mode=    â”‚â”€â”€â”€â–¶â”‚ DirectExecution â”‚
â”‚ mode=async      â”‚    â”‚ async            â”‚    â”‚ Engine          â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ Response:       â”‚â—€â”€â”€â”€â”‚ {"job_id": ...}  â”‚    â”‚ Background      â”‚
â”‚ {"job_id":...}  â”‚    â”‚                  â”‚    â”‚ Thread          â”‚
â”‚                 â”‚    â”‚                  â”‚    â”‚                 â”‚
â”‚ SSE Stream:     â”‚    â”‚ /stream/{job_id} â”‚    â”‚ Event Callback  â”‚
â”‚ curl /stream/   â”‚â”€â”€â”€â–¶â”‚                  â”‚â—€â”€â”€â”€â”‚ (Thread-Safe)   â”‚
â”‚ {job_id}        â”‚    â”‚ Server-Sent      â”‚    â”‚                 â”‚
â”‚                 â”‚â—€â”€â”€â”€â”‚ Events           â”‚    â”‚ StreamManager   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Production Considerations

- **ğŸ”’ Security**: Add authentication and rate limiting
- **ğŸ“ˆ Scaling**: Use multiple workers with shared job registry  
- **ğŸ—„ï¸ Persistence**: Consider Redis for job state in multi-instance deployments
- **ğŸ“Š Monitoring**: Integrate with observability tools (Prometheus, Grafana)
- **ğŸš¨ Error Handling**: Implement retry logic and circuit breakers