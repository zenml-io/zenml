{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zenml.repo import Repository\n",
    "from zenml.datasources import CSVDatasource\n",
    "from zenml.pipelines import TrainingPipeline\n",
    "from zenml.steps.evaluator import TFMAEvaluator\n",
    "from zenml.steps.preprocesser import StandardPreprocesser\n",
    "from zenml.steps.split import RandomSplit\n",
    "from zenml.steps.trainer import TFFeedForwardTrainer\n",
    "from zenml.repo import Repository, ArtifactStore\n",
    "from zenml.utils.naming_utils import transformed_label_name\n",
    "from zenml.steps.deployer import GCAIPDeployer\n",
    "from zenml.steps.deployer import CortexDeployer\n",
    "from examples.cortex.predictor.tf import TensorFlowPredictor\n",
    "from zenml.backends.orchestrator import OrchestratorGCPBackend\n",
    "from zenml.metadata import MySQLMetadataStore\n",
    "from zenml.backends.processing import ProcessingDataFlowBackend\n",
    "from zenml.backends.training import SingleGPUTrainingGCAIPBackend\n",
    "from zenml.backends.processing import ProcessingDataFlowBackend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be creating a ZenML training pipeline and showcasing the modularity of ZenML backends in this example. On a high level, here is what a ZenML training pipeline looks like: \n",
    "\n",
    "<img src=\"graphics/architecture.png\" width=\"600\" height=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GCP_BUCKET=os.getenv('GCP_BUCKET')\n",
    "GCP_PROJECT=os.getenv('GCP_PROJECT')\n",
    "GCP_REGION=os.getenv('GCP_REGION')\n",
    "GCP_CLOUD_SQL_INSTANCE_NAME=os.getenv('GCP_CLOUD_SQL_INSTANCE_NAME')\n",
    "MODEL_NAME=os.getenv('MODEL_NAME')\n",
    "CORTEX_ENV=os.getenv('CORTEX_ENV')\n",
    "MYSQL_DB=os.getenv('MYSQL_DB')\n",
    "MYSQL_USER=os.getenv('MYSQL_USER')\n",
    "MYSQL_PWD=os.getenv('MYSQL_PWD')\n",
    "MYSQL_PORT=os.getenv('MYSQL_PORT')\n",
    "MYSQL_HOST=os.getenv('MYSQL_HOST')\n",
    "CONNECTION_NAME = f'{GCP_PROJECT}:{GCP_REGION}:{GCP_CLOUD_SQL_INSTANCE_NAME}'\n",
    "TRAINING_JOB_DIR = os.path.join(GCP_BUCKET, 'gcp_gcaip_training/staging')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo: Repository = Repository.get_instance()\n",
    "    \n",
    "# Define artifact store in the cloud\n",
    "cloud_artifact_store = ArtifactStore(os.path.join(GCP_BUCKET, 'all_feature_demo'))\n",
    "\n",
    "# Define metadata store in the cloud\n",
    "cloud_metadata_store = MySQLMetadataStore(\n",
    "    host=MYSQL_HOST,\n",
    "    port=int(MYSQL_PORT),\n",
    "    database=MYSQL_DB,\n",
    "    username=MYSQL_USER,\n",
    "    password=MYSQL_PWD,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create first pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline = TrainingPipeline(name='Experiment 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a datasource. This will automatically track and version it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ds = CSVDatasource(name='Pima Indians Diabetes', path='gs://zenml_quickstart/diabetes.csv')\n",
    "except:\n",
    "    repo: Repository = Repository.get_instance()\n",
    "    ds = repo.get_datasource_by_name('Pima Indians Diabetes')\n",
    "training_pipeline.add_datasource(ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a split step to partition data into train and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_split(RandomSplit(split_map={'train': 0.7, 'eval': 0.2, 'test':0.1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a preprocessing step to transform data to be ML-capable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_preprocesser(\n",
    "    StandardPreprocesser(\n",
    "        features=['times_pregnant', 'pgc', 'dbp', 'tst', 'insulin', 'bmi',\n",
    "                  'pedigree', 'age'],\n",
    "        labels=['has_diabetes'],\n",
    "        overwrite={'has_diabetes': {\n",
    "            'transform': [{'method': 'no_transform', 'parameters': {}}]}}\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add a trainer which defines model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add an evaluator to calculate slicing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.add_evaluator(\n",
    "    TFMAEvaluator(slices=[['has_diabetes']],\n",
    "                  metrics={transformed_label_name('has_diabetes'):\n",
    "                     ['binary_crossentropy', 'binary_accuracy']}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline.view_statistics(magic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_pipeline.evaluate(magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = repo.get_datasources()\n",
    "datasource = datasources[0]\n",
    "print(datasource)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datasource.sample_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skip preprocessing with your next (warm-starting) pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clone first experiment and only change one hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_2 = training_pipeline.copy('Experiment 2')\n",
    "training_pipeline_2.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_2.evaluate(magic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify theres still only one datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasources = repo.get_datasources()\n",
    "print(f\"We have {len(datasources)} datasources\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.compare_training_runs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribute splitting/preprocessing easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_3 = repo.get_pipeline_by_name('Experiment 1').copy('Experiment 3')\n",
    "\n",
    "# Define the processing backend\n",
    "processing_backend = ProcessingDataFlowBackend(\n",
    "    project=GCP_PROJECT,\n",
    "    staging_location=os.path.join(GCP_BUCKET, 'dataflow_processing/staging'),\n",
    ")\n",
    "\n",
    "\n",
    "# Run processing step with that backend\n",
    "training_pipeline_3.add_split(\n",
    "    RandomSplit(split_map={'train': 0.7, 'eval': 0.2, 'test': 0.1}).with_backend(\n",
    "        processing_backend)\n",
    ")\n",
    "\n",
    "training_pipeline_3.run(artifact_store=cloud_artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Easily train on the cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_pipeline_4 = training_pipeline.copy('Experiment 4')\n",
    "\n",
    "# Add a trainer with a GCAIP backend\n",
    "training_backend = SingleGPUTrainingGCAIPBackend(\n",
    "    project=GCP_PROJECT,\n",
    "    job_dir=TRAINING_JOB_DIR\n",
    ")\n",
    "\n",
    "training_pipeline_4.add_trainer(TFFeedForwardTrainer(\n",
    "    loss='binary_crossentropy',\n",
    "    last_activation='sigmoid',\n",
    "    output_units=1,\n",
    "    metrics=['accuracy'],\n",
    "    epochs=20).with_backend(training_backend))\n",
    "\n",
    "training_pipeline_4.run(artifact_store=cloud_artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestrate pipeline whereever you like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_5 = training_pipeline.copy('Experiment 5')\n",
    "\n",
    "# Define the orchestrator backend\n",
    "cloud_orchestrator_backend = OrchestratorGCPBackend(\n",
    "    cloudsql_connection_name=CONNECTION_NAME,\n",
    "    project=GCP_PROJECT,\n",
    "    preemptible=True,  # reduce costs by using preemptible instances\n",
    "    machine_type='n1-standard-4',\n",
    "    gpu='nvidia-tesla-k80',\n",
    "    gpu_count=1,\n",
    ")\n",
    "\n",
    "# Run the pipeline\n",
    "training_pipeline_5.run(\n",
    "    backend=cloud_orchestrator_backend,\n",
    "    metadata_store=cloud_metadata_store,\n",
    "    artifact_store=cloud_artifact_store,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add a deployer step with different integrations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 1: Deploy to Google Cloud AI Platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_6 = training_pipeline.copy('Experiment 6')\n",
    "training_pipeline_6.add_deployment(\n",
    "    GCAIPDeployer(\n",
    "        project_id=GCP_PROJECT,\n",
    "        model_name=MODEL_NAME,\n",
    "    )\n",
    ")\n",
    "\n",
    "training_pipeline_6.run(artifact_store=cloud_artifact_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option 2: Deploy to Kubernetes via Cortex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "training_pipeline_7 = training_pipeline.copy('Experiment 7')\n",
    "\n",
    "# Add cortex deployer\n",
    "api_config = {\n",
    "    \"name\": MODEL_NAME,\n",
    "    \"kind\": \"RealtimeAPI\",\n",
    "    \"predictor\": {\n",
    "        \"type\": \"tensorflow\",\n",
    "        \"models\": {\"signature_key\": \"serving_default\"}}\n",
    "}\n",
    "training_pipeline_7.add_deployment(\n",
    "    CortexDeployer(\n",
    "        env=CORTEX_ENV,\n",
    "        api_config=api_config,\n",
    "        predictor=TensorFlowPredictor,\n",
    "    )\n",
    ")\n",
    "\n",
    "training_pipeline_7.run(artifact_store=cloud_artifact_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
