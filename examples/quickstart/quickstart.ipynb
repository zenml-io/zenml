{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quickstart-header"
   },
   "source": [
    "# ZenML Quickstart: From Agent-Only to Agent+Classifier\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)\n",
    "\n",
    "**The modern AI development story in 5 minutes.**\n",
    "\n",
    "This quickstart demonstrates how ZenML unifies ML and Agent workflows, showing the natural progression from a generic agent to a specialized one powered by your own trained models.\n",
    "\n",
    "## ğŸ¯ The Story\n",
    "\n",
    "**Phase 1**: Deploy a support agent â†’ Generic responses  \n",
    "**Phase 2**: Train an intent classifier â†’ Tag as \"production\"  \n",
    "**Phase 3**: Same agent automatically upgrades â†’ Specialized responses  \n",
    "\n",
    "**The \"aha\" moment**: ZenML connects batch training and real-time serving with the same primitives. Train offline, promote artifacts to production, and agents automatically consume them online."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-section"
   },
   "source": [
    "## ğŸ› ï¸ Setup\n",
    "\n",
    "First, let's install ZenML and required dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-dependencies"
   },
   "outputs": [],
   "source": [
    "# Install ZenML and dependencies\n",
    "!pip install \"zenml[server]\" scikit-learn requests -q\n",
    "\n",
    "# If running in Colab, we need to restart runtime after installation\n",
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    print(\n",
    "        \"âš ï¸ Please restart runtime after installation (Runtime â†’ Restart runtime)\"\n",
    "    )\n",
    "    print(\"Then continue with the next cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "init-zenml"
   },
   "source": [
    "Initialize ZenML and set up the deployer stack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zenml-init"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Initialize ZenML repository\n",
    "!zenml init\n",
    "\n",
    "# Set up deployer stack (required for serving)\n",
    "!zenml deployer register docker -f docker\n",
    "!zenml stack register docker-deployer -o default -a default -D docker --set\n",
    "\n",
    "print(\"âœ… ZenML setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-files"
   },
   "source": [
    "Download the quickstart files if not already present:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "get-quickstart-files"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from pathlib import Path\n",
    "\n",
    "# Base URL for quickstart files\n",
    "base_url = (\n",
    "    \"https://raw.githubusercontent.com/zenml-io/zenml/main/examples/quickstart\"\n",
    ")\n",
    "\n",
    "# Files to download\n",
    "files = {\n",
    "    \"run.py\": \"run.py\",\n",
    "    \"configs/agent.yaml\": \"configs/agent.yaml\",\n",
    "    \"pipelines/intent_training_pipeline.py\": \"pipelines/intent_training_pipeline.py\",\n",
    "    \"pipelines/agent_serving_pipeline.py\": \"pipelines/agent_serving_pipeline.py\",\n",
    "    \"steps/data.py\": \"steps/data.py\",\n",
    "    \"steps/train.py\": \"steps/train.py\",\n",
    "    \"steps/infer.py\": \"steps/infer.py\",\n",
    "    \"pipelines/__init__.py\": \"pipelines/__init__.py\",\n",
    "    \"steps/__init__.py\": \"steps/__init__.py\",\n",
    "}\n",
    "\n",
    "# Create directories and download files\n",
    "for file_path, local_path in files.items():\n",
    "    local_dir = Path(local_path).parent\n",
    "    local_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not Path(local_path).exists():\n",
    "        try:\n",
    "            url = f\"{base_url}/{file_path}\"\n",
    "            urllib.request.urlretrieve(url, local_path)\n",
    "            print(f\"âœ… Downloaded {local_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to download {local_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"ğŸ“ {local_path} already exists\")\n",
    "\n",
    "print(\"\\nğŸ‰ All files ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase1-header"
   },
   "source": [
    "## ğŸ“‹ Phase 1: Deploy Agent-Only\n",
    "\n",
    "Let's deploy the agent without any classifier - it will use generic responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deploy-agent"
   },
   "outputs": [],
   "source": [
    "# Deploy the agent serving pipeline\n",
    "!zenml pipeline deploy pipelines.agent_serving_pipeline.agent_serving_pipeline \\\n",
    "  -n support-agent -c configs/agent.yaml\n",
    "\n",
    "print(\"\\nâœ… Agent deployed! It's now running with generic responses only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-phase1"
   },
   "source": [
    "Test the agent with a sample banking question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-agent-generic"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import requests\n",
    "\n",
    "# Wait a moment for deployment to be ready\n",
    "print(\"â³ Waiting for agent to be ready...\")\n",
    "time.sleep(10)\n",
    "\n",
    "# Test the agent\n",
    "url = \"http://localhost:8000/invoke\"\n",
    "payload = {\"parameters\": {\"text\": \"my card is lost and i need a replacement\"}}\n",
    "\n",
    "try:\n",
    "    response = requests.post(\n",
    "        url, json=payload, headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    result = response.json()\n",
    "\n",
    "    print(\"ğŸ¤– Agent Response (Phase 1 - Generic):\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    # Parse the nested JSON response\n",
    "    if \"output\" in result:\n",
    "        agent_output = json.loads(result[\"output\"])\n",
    "        print(\n",
    "            f\"\\nğŸ“Š Intent Source: {agent_output.get('intent_source', 'unknown')}\"\n",
    "        )\n",
    "        print(f\"ğŸ¯ Detected Intent: {agent_output.get('intent', 'unknown')}\")\n",
    "        print(f\"ğŸ“ˆ Confidence: {agent_output.get('confidence', 0):.2f}\")\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\n",
    "        \"âŒ Could not connect to agent. Make sure Docker is running and the deployment succeeded.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase2-header"
   },
   "source": [
    "## ğŸ§  Phase 2: Train Intent Classifier\n",
    "\n",
    "Now let's train an intent classifier and automatically tag it as \"production\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-classifier"
   },
   "outputs": [],
   "source": [
    "# Run the training pipeline\n",
    "!python run.py --train\n",
    "\n",
    "print(\"\\nâœ… Training complete! The classifier is now tagged as 'production'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check-artifacts"
   },
   "source": [
    "Let's verify the artifact was created and tagged:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify-artifacts"
   },
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "# Check if our production artifact exists\n",
    "client = Client()\n",
    "try:\n",
    "    versions = client.list_artifact_versions(name=\"intent-classifier\")\n",
    "\n",
    "    print(\"ğŸ“¦ Intent Classifier Artifacts:\")\n",
    "    for version in versions:\n",
    "        tags = [tag.name for tag in version.tags] if version.tags else []\n",
    "        print(f\"  Version {version.version}: {tags}\")\n",
    "\n",
    "        if \"production\" in tags:\n",
    "            print(f\"  âœ… Found production version: {version.version}\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"  âŒ No production-tagged version found\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error checking artifacts: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phase3-header"
   },
   "source": [
    "## ğŸš€ Phase 3: Agent Automatically Upgrades\n",
    "\n",
    "Update the same deployment - the agent will now load the production classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "update-deployment"
   },
   "outputs": [],
   "source": [
    "# Update the deployment with the -u flag\n",
    "!zenml pipeline deploy pipelines.agent_serving_pipeline.agent_serving_pipeline \\\n",
    "  -n support-agent -c configs/agent.yaml -u\n",
    "\n",
    "print(\"\\nâœ… Deployment updated! Agent will now use the production classifier.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-phase3"
   },
   "source": [
    "Test with the same request - now it should use the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-agent-specialized"
   },
   "outputs": [],
   "source": [
    "# Wait for updated deployment to be ready\n",
    "print(\"â³ Waiting for updated agent to be ready...\")\n",
    "time.sleep(15)\n",
    "\n",
    "# Test the upgraded agent\n",
    "try:\n",
    "    response = requests.post(\n",
    "        url, json=payload, headers={\"Content-Type\": \"application/json\"}\n",
    "    )\n",
    "    result = response.json()\n",
    "\n",
    "    print(\"ğŸ¤– Agent Response (Phase 3 - Specialized):\")\n",
    "    print(json.dumps(result, indent=2))\n",
    "\n",
    "    # Parse the nested JSON response\n",
    "    if \"output\" in result:\n",
    "        agent_output = json.loads(result[\"output\"])\n",
    "        print(\n",
    "            f\"\\nğŸ“Š Intent Source: {agent_output.get('intent_source', 'unknown')}\"\n",
    "        )\n",
    "        print(f\"ğŸ¯ Detected Intent: {agent_output.get('intent', 'unknown')}\")\n",
    "        print(f\"ğŸ“ˆ Confidence: {agent_output.get('confidence', 0):.2f}\")\n",
    "\n",
    "        # Highlight the upgrade\n",
    "        if agent_output.get(\"intent_source\") == \"classifier\":\n",
    "            print(\"\\nğŸ‰ SUCCESS! Agent is now using the trained classifier!\")\n",
    "        else:\n",
    "            print(\n",
    "                \"\\nâš ï¸ Agent is still using generic responses. Try again in a moment.\"\n",
    "            )\n",
    "\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\n",
    "        \"âŒ Could not connect to agent. Make sure the deployment is running.\"\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error testing agent: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "comparison"
   },
   "source": [
    "## ğŸ” The Magic Explained\n",
    "\n",
    "Let's look at how the automatic upgrade works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "show-magic"
   },
   "outputs": [],
   "source": [
    "# Show the key code that makes this work\n",
    "print(\"ğŸª„ The Automatic Upgrade Logic:\")\n",
    "print(\"\"\"\n",
    "# At deployment startup (on_init_hook)\n",
    "def _load_production_classifier_if_any():\n",
    "    versions = client.list_artifact_versions(name=\"intent-classifier\")\n",
    "\n",
    "    for version in versions:\n",
    "        if \"production\" in [tag.name for tag in version.tags]:\n",
    "            global _router\n",
    "            _router = version.load()  # ğŸ¯ Agent upgraded!\n",
    "            break\n",
    "\n",
    "# The Decision Logic:\n",
    "if _router is not None:\n",
    "    # âœ… Use classifier for specific responses\n",
    "    intent_source = \"classifier\"\n",
    "else:\n",
    "    # âŒ Use generic LLM responses  \n",
    "    intent_source = \"llm\"\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nğŸ¯ Key Points:\")\n",
    "print(\"1. ğŸš€ Deploy once â†’ Agent works immediately with generic responses\")\n",
    "print(\"2. ğŸ§  Train model â†’ Automatically tagged as 'production'\")\n",
    "print(\"3. ğŸ”„ Update deployment â†’ Agent finds and loads the production model\")\n",
    "print(\"4. âœ¨ Same endpoint, better responses â†’ No code changes needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "test-different-intents"
   },
   "source": [
    "## ğŸ§ª Test Different Banking Intents\n",
    "\n",
    "Let's test the classifier with different banking questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-intents"
   },
   "outputs": [],
   "source": [
    "# Test different banking intents\n",
    "test_cases = [\n",
    "    \"what is my current balance\",\n",
    "    \"i need to make a payment\",\n",
    "    \"i want to dispute a charge\",\n",
    "    \"can you increase my credit limit\",\n",
    "    \"hello can you help me\",\n",
    "]\n",
    "\n",
    "print(\"ğŸ§ª Testing Different Banking Intents:\\n\")\n",
    "\n",
    "for i, test_text in enumerate(test_cases, 1):\n",
    "    print(f\"Test {i}: '{test_text}'\")\n",
    "\n",
    "    payload = {\"parameters\": {\"text\": test_text}}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            url, json=payload, headers={\"Content-Type\": \"application/json\"}\n",
    "        )\n",
    "        result = response.json()\n",
    "\n",
    "        if \"output\" in result:\n",
    "            agent_output = json.loads(result[\"output\"])\n",
    "            intent = agent_output.get(\"intent\", \"unknown\")\n",
    "            confidence = agent_output.get(\"confidence\", 0)\n",
    "            source = agent_output.get(\"intent_source\", \"unknown\")\n",
    "\n",
    "            print(\n",
    "                f\"  â†’ Intent: {intent} (confidence: {confidence:.2f}, source: {source})\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"  â†’ Error: No output in response\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  â†’ Error: {e}\")\n",
    "\n",
    "    print()\n",
    "    time.sleep(1)  # Brief pause between requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup"
   },
   "source": [
    "## ğŸ§¹ Cleanup\n",
    "\n",
    "Stop the deployment when you're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "stop-deployment"
   },
   "outputs": [],
   "source": [
    "# Stop the deployment\n",
    "try:\n",
    "    !zenml deployment delete support-agent\n",
    "    print(\"âœ… Deployment stopped and cleaned up.\")\n",
    "except:\n",
    "    print(\"â„¹ï¸ Deployment may have already been stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "conclusion"
   },
   "source": [
    "## ğŸ¯ What You Just Learned\n",
    "\n",
    "Congratulations! You've experienced ZenML's unified ML and Agent workflow:\n",
    "\n",
    "### ğŸ”„ **The Progression**\n",
    "1. **Generic Agent** â†’ Deployed instantly, works with basic responses\n",
    "2. **Trained Classifier** â†’ Added specialized intelligence with production tagging\n",
    "3. **Automatic Upgrade** â†’ Same endpoint, enhanced capabilities\n",
    "\n",
    "### ğŸ—ï¸ **ZenML Features Demonstrated**\n",
    "- âœ… **Unified Workflows**: Same primitives for batch training and real-time serving\n",
    "- âœ… **Production Tagging**: Automatic artifact promotion with `add_tags()`\n",
    "- âœ… **Warm Container Serving**: Models load once at startup for fast responses\n",
    "- âœ… **Artifact Lineage**: Full tracking from training to deployment\n",
    "- âœ… **Stack Portability**: Deploy anywhere with consistent APIs\n",
    "\n",
    "### ğŸŒŸ **Key Takeaway**\n",
    "*One framework for ML and Agents. Train offline, promote artifacts into production, and serve online with the same developer experience.*\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸš€ **Next Steps**\n",
    "- ğŸ“– [Full ZenML Documentation](https://docs.zenml.io/)\n",
    "- ğŸ’¬ [Join our Community](https://zenml.io/slack)\n",
    "- ğŸ¢ [ZenML Pro](https://zenml.io/pro) for teams\n",
    "- ğŸŒ [More Examples](https://github.com/zenml-io/zenml/tree/main/examples)\n",
    "\n",
    "**Ready to build your own AI workflows?** ZenML provides the infrastructure to make them reliable, reproducible, and production-ready."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}