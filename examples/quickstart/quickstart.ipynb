{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## üåç Overview\n",
    "\n",
    "Welcome to this minimalistic MLOps project, designed as an ideal starting point for anyone looking to master the art of\n",
    "deploying Machine Learning workflows in production environments. This repository serves as a practical guide to bridge\n",
    "the gap between local development and cloud-scale operations.\n",
    "\n",
    "\n",
    "In today's fast-paced AI landscape, the ability to seamlessly transition from experimentation to deployment is not just\n",
    "a convenience‚Äîit's a necessity. ZenML empowers you to break free from the constraints of local computing and\n",
    "harness the power of cloud infrastructure with minimal friction.\n",
    "\n",
    "\n",
    "Imagine developing your ML models on your laptop, then with just a few tweaks, watching them come to life on\n",
    "cutting-edge cloud platforms. This isn't just about scaling resources; it's about scaling your impact. You're not only\n",
    "optimizing your workflow but also setting your team up for the realities of modern ML engineering.\n",
    "\n",
    "\n",
    "Our notebook demonstrates how simple this can be. You'll witness firsthand how code written for local execution\n",
    "can be effortlessly adapted to run on major cloud providers. This isn't just a technical exercise ‚Äî it's a paradigm \n",
    "shift in how you approach ML development.\n",
    "\n",
    "\n",
    "However, ZenML is not just about deployment into compute. The ZenML Dashboard opens up a world of insights, allowing you\n",
    "to dive deep into the metadata of your runs. This level of observability is crucial for maintaining, debugging, \n",
    "and optimizing your ML pipelines in production environments with full reproducibility and auditability. The diagram \n",
    "below shows the steps that we will take to demonstrate this.\n",
    "\n",
    "<img src=\".assets/Overview.png\" width=\"50%\" alt=\"Quickstart Overview\">\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466b16",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to run this notebook, no local installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# üë∂ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML and all requirement to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install uv\n",
    "!pip install zenml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "# In case we are in a google colab, clone all additional relevant files\n",
    "if Environment.in_google_colab():\n",
    "    # Pull required modules from this example\n",
    "    !git clone -b main https://github.com/zenml-io/zenml\n",
    "    !cp -r zenml/examples/quickstart/* .\n",
    "    !rm -rf zenml\n",
    "\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Kernel to ensure all libraries are properly loaded\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044374",
   "metadata": {},
   "source": [
    "\n",
    "Please wait for the installation to complete before running subsequent cells. At\n",
    "the end of the installation, the notebook kernel will restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce581",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è Step 1: Connect to your ZenML Server\n",
    "To run this quickstart you need to connect to a ZenML Server. You can deploy it [yourself on your own infrastructure](https://docs.zenml.io/getting-started/deploying-zenml) or try it out for free, no credit-card required in our [ZenML Pro managed service](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587315",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_server_url = \"INSERT_YOUR_SERVER_URL_HERE\"  # in the form \"https://URL_TO_SERVER\"\n",
    "\n",
    "!zenml connect --url $zenml_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and define the root for imports and docker builds\n",
    "!zenml init"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48460",
   "metadata": {},
   "source": [
    "## ü•á Step 2: Build and run your first pipeline\n",
    "\n",
    "In this quickstart we'll be working with a small dataset of sentences in old english paired with more modern formulations. The task is a text-to-text transformation.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to break down your code into distinct functions that load your data, bring it into the correct shape and finally produce a model. This is the experimentation phase where we try to massage our data into the right format and feed it into our model training.\n",
    "\n",
    "<img src=\".assets/Experiment.png\" width=\"20%\" alt=\"Experimentation phase and pipeline construction\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from datasets import Dataset\n",
    "from typing import Tuple\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from zenml import step\n",
    "PROMPT = \"\"  # In case you want to also use a prompt you can set it here\n",
    "\n",
    "def read_data_from_url(url):\n",
    "    inputs = []\n",
    "    targets = []\n",
    "\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for bad responses\n",
    "\n",
    "    for line in response.text.splitlines():\n",
    "        old, modern = line.strip().split(\"|\")\n",
    "        inputs.append(f\"{PROMPT}{old}\")\n",
    "        targets.append(modern)\n",
    "\n",
    "    return {\"input\": inputs, \"target\": targets}\n",
    "\n",
    "\n",
    "@step\n",
    "def load_data() -> Tuple[\n",
    "    Annotated[Dataset, \"dataset\"],\n",
    "    Annotated[Dataset, \"test_dataset\"],\n",
    "]:\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "\n",
    "    # URLs for the data files\n",
    "    train_url = \"https://storage.googleapis.com/zenml-public-bucket/quickstart-files/translations.txt\"\n",
    "    test_url = \"https://storage.googleapis.com/zenml-public-bucket/quickstart-files/test-translations.txt\"\n",
    "\n",
    "    # Fetch and process the data\n",
    "    data = read_data_from_url(train_url)\n",
    "    test_data = read_data_from_url(test_url)\n",
    "\n",
    "    return Dataset.from_dict(data), Dataset.from_dict(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286b67",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines one step at a time.  If you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = load_data()\n",
    "print(f\"Input: {train_dataset['input'][0]} - Target: {train_dataset['target'][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05291",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the input/output pair looks to be in the right format ü•≥.\n",
    "\n",
    "For the sake of this quickstart we have prepared a few steps in the steps-directory. We'll now connect these together into a pipeline. To do this simply plug multiple steps together through their inputs and outputs. Then just add the `@pipeline` decorator to the function that connects the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline, Model\n",
    "from zenml.client import Client\n",
    "from zenml.model.model import Model\n",
    "\n",
    "from steps import load_data, tokenize_data, train_model, evaluate_model, model_tester\n",
    "from steps.model_trainer import T5_Model\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()\n",
    "\n",
    "Client().activate_stack(\"default\") # We will start by using the default stack which is local\n",
    "\n",
    "model_name = \"YeOldeEnglishTranslator\"\n",
    "model = Model(\n",
    "  name = \"YeOldeEnglishTranslator\",\n",
    "  description = \"Model to translate from old to modern english\",\n",
    "  tags = [\"quickstart\", \"llm\", \"t5\"]\n",
    ")\n",
    "\n",
    "@pipeline(enable_cache=True, model=model)\n",
    "def english_translation_pipeline(\n",
    "    model_type: T5_Model,\n",
    "    per_device_train_batch_size: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "    dataloader_num_workers: int,\n",
    "    num_train_epochs: int = 5,\n",
    "):\n",
    "    \"\"\"Define a pipeline that connects the steps.\"\"\"\n",
    "    dataset, test_dataset = load_data()\n",
    "    tokenized_dataset, tokenizer = tokenize_data(dataset, model_type)\n",
    "    model = train_model(\n",
    "        tokenized_dataset,\n",
    "        model_type,\n",
    "        num_train_epochs,\n",
    "        per_device_train_batch_size,\n",
    "        gradient_accumulation_steps,\n",
    "        dataloader_num_workers,\n",
    "    )\n",
    "    evaluate_model(model, tokenized_dataset)\n",
    "    model_tester(model, tokenizer, test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73c23",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just as with the step - by calling the\n",
    "pipeline function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the pipeline and configure some parameters at runtime\n",
    "pipeline_run = english_translation_pipeline(\n",
    "    model_type=\"t5-small\",\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42078a",
   "metadata": {},
   "source": [
    "As you can see the pipeline has run successfully. It also printed out some examples - however it seems the model is not yet able to solve the task well. But we validated that the pipeline works.\n",
    "\n",
    "<img src=\".assets/DAG.png\" width=\"50%\" alt=\"Dashboard view\">\n",
    "\n",
    "Above you can see what the dashboard view of the pipeline in the ZenML Dashboard. You can find the URL for this in the logs above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f09d",
   "metadata": {},
   "source": [
    "We can now access the trained model and it's tokenizer from the ZenML Model Control Plane. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model object\n",
    "model = client.get_model_version(model_name).get_model_artifact('model').load()\n",
    "tokenizer = client.get_model_version(model_name).get_artifact('tokenizer').load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd8592-2295-4421-a6e6-f619ed389e8c",
   "metadata": {},
   "source": [
    "With this in hand we can now play around with the model directly and try out some examples ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e514ac-1a0a-49a0-b8a4-e33cee12c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "test_text = \"I do desire we may be better strangers\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e653c7a-4073-424e-8a59-c69f49526b96",
   "metadata": {},
   "source": [
    "## Lets recap what we've done so far\n",
    "\n",
    "We created a modular pipeline, this pipeline is modularly constructed from different steps. We have shown that this pipeline runs locally.\n",
    "\n",
    "As expected, the modcel does not yet solve its task. To train a model that can solve our task well, we would have to train a larger model for longer. For this, we'll need to move away from our local environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# ‚åö Step 3: Scale it up in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a791b32b-f6be-4ae2-867c-5e628f363858",
   "metadata": {},
   "source": [
    "Our last section confirmed to us, that the pipeline works. Let's now run the pipeline in the environment of your choice.\n",
    "\n",
    "For you to be able to try this step, you will need to have access to a cloud environment (AWS, GCP, AZURE). ZenML wraps around all the major cloud providers and orchestration tools and lets you easily deploy your code onto them.\n",
    "\n",
    "To do this lets head over to the `Stack` section of your ZenML Dashboard. Here you'll be able to either connect to an existing or deploy a new environment. Choose on of the options presented to you there and come back when you have a stack ready to go. Then proceed to the appropirate section below. **Do not** run all three. Also be sure that you are running with a remote ZenML server (see Step 1 above).\n",
    "\n",
    "<img src=\".assets/StackCreate.png\" width=\"20%\" alt=\"Stack creation in the ZenML Dashboard\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e02652-34ae-4b79-948e-1d80f559fdf5",
   "metadata": {},
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a95e2a-2c55-4068-8111-5ea5559203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install gcp -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"zenmldocker/zenml-public-pipelines:quickstart-0.64.0-py3.11\",\n",
    "            requirements=[\"\"], # Add any additional requirements here\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa58b33-712c-4926-b12b-feceda3384c8",
   "metadata": {},
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95af245-a7fd-4d64-b0af-d5a96a788846",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sagemaker s3 s3fs\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings, ResourceSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"zenmldocker/zenml-public-pipelines:quickstart-0.64.0-py3.11\",\n",
    "            requirements=[\"\"], # Add any additional requirements here\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07454193-8bc8-4adf-bcca-db598b891ccf",
   "metadata": {},
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63c109-0ba5-4a62-adaa-47aa9612373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install azure -y\n",
    "\n",
    "from zenml.client import Client\n",
    "from zenml.config import DockerSettings\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"INSERT_STACK_NAME_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"zenmldocker/zenml-public-pipelines:quickstart-0.64.0-py3.11\",\n",
    "            requirements=[\"\"], # Add any additional requirements here\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f17b7a-5a82-4975-b9bd-6a63fbb97a68",
   "metadata": {},
   "source": [
    "## üöÄ Ready to launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f30c-9a8e-46ca-ba44-cf16ea715dac",
   "metadata": {},
   "source": [
    "We now have configured zenml to use your very own cloud infrastructure.\n",
    "\n",
    "<img src=\".assets/SwitchStack.png\" width=\"20%\" alt=\"Stack switching with ZenML\">\n",
    "\n",
    "For the next pipeline run, we'll be training the same t5 model (`t5_small`) on your own infrastrucutre.\n",
    "\n",
    "Note: The whole process may take a bit longer the first time around, as your pipeline code needs to be built into docker containers to be run in the orchestration environment of your stack. Any consecutive run of the pipeline, even with different parameters set, will not take as long again thanks to docker caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e758fe-6ea3-42ff-bea8-33953135bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = configured_english_translation_pipeline(\n",
    "    model_type=\"t5-small\", \n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    dataloader_num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eef2c6-6dfb-4b67-9883-594a0df20173",
   "metadata": {},
   "source": [
    "You did it! You build a pipeline locally, verified that all its parts work well together and now are running it on a production environment\n",
    "\n",
    "<img src=\".assets/Production.png\" width=\"20%\" alt=\"Pipeline running on your infrastructure.\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231677c-1fd6-4ec3-8c8c-47fd9406072e",
   "metadata": {},
   "source": [
    "## Now its Up to you"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c31e0d-dfab-4692-a406-0dc439f25443",
   "metadata": {},
   "source": [
    "You can now start worrying about making the model actually work well, as the model results are still not acceptable.\n",
    "\n",
    "Here are some things that you could do:\n",
    "\n",
    "* Iterate on the training data and its tokenization\n",
    "* You can switch out the model itself. Instead of `model_type=\"t5_small\"` you could use `model_type=\"t5_large\"` for example\n",
    "* You can train for longer by increasing the `num_train_epochs=xxx`. In order to speed this up you can also add accelerators to your orchestrators. Learn more about this in the section below.\n",
    "\n",
    "No matter what avenue you choose to actually make the model work, we would love to see how you did it, so please reach out and share your solution with us either on [**Slack Community**](https://zenml.io/slack) or through our email hello@zenml.io."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a03054e-8b3e-4edb-9d87-82ae51693d2d",
   "metadata": {},
   "source": [
    "## Adding Accelerators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec09c1c-4f6e-4f59-a99e-87500004a174",
   "metadata": {},
   "source": [
    "Each of the cloud providers allows users to add accelerators to their serverless offerings. Here's what you need to add to the pipeline settings in order to unlock gpus. Keep in mind, that you might have to increase your quotas within the cloud providers.\n",
    "\n",
    "### GCP\n",
    "\n",
    "For GCP Vertex you can use the the VertexOrchestratorSettings to specify configuration options, click [here](https://docs.zenml.io/stack-components/orchestrators/vertex#additional-configuration) to learn more. Additionally you can use the Resource Settings for more general infrastructure configuration.\n",
    "\n",
    "```python\n",
    "from zenml.config import ResourceSettings\n",
    "from zenml.integrations.gcp.flavors.vertex_orchestrator_flavor import VertexOrchestratorSettings\n",
    "\n",
    "english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"orchestrator.vertex\": VertexOrchestratorSettings(\n",
    "            node_selector_constraint=(\"cloud.google.com/gke-accelerator\", \"NVIDIA_TESLA_P4\")\n",
    "        )\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\", gpu=1),\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### AWS\n",
    "\n",
    "For AWS Sagemaker you can use the the SagemakerOrchestratorSettings to specify configuration options, click [here](https://docs.zenml.io/stack-components/orchestrators/sagemaker#configuration-at-pipeline-or-step-level) to learn more.\n",
    "\n",
    "```python\n",
    "from zenml.integrations.aws.flavors.sagemaker_orchestrator_flavor import SagemakerOrchestratorSettings\n",
    "\n",
    "english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"orchestrator.sagemaker\": SagemakerOrchestratorSettings(instance_type=\"ml.p2.xlarge\")\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "### Azure\n",
    "\n",
    "For Azure Skypilot you can use the the SkypilotAzureOrchestratorSettings to specify configuration options, click [here](https://docs.zenml.io/stack-components/orchestrators/skypilot-vm#additional-configuration) to learn more.\n",
    "\n",
    "```python\n",
    "from zenml.integrations.skypilot_azure.flavors.skypilot_orchestrator_azure_vm_flavor import SkypilotAzureOrchestratorSettings\n",
    "\n",
    "english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"orchestrator.vm_azure\": SkypilotAzureOrchestratorSettings(instance_type=\"Standard_NC6\")\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out [ZenML Pro](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560354d-9e78-4061-aaff-2e6213229911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
