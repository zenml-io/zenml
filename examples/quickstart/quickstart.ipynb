{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ab391a",
   "metadata": {},
   "source": [
    "# Intro to MLOps using ZenML\n",
    "\n",
    "## ðŸŒ Overview\n",
    "\n",
    "This repository is a minimalistic MLOps project intended as a starting point to learn how to put ML workflows in production. It features: \n",
    "\n",
    "- A very simple training pipeline that loads the a dataset and trains a model.\n",
    "\n",
    "Within this notebook we will show you how simple it is to switch where your code runs and where your data is stored. You will also learn how all the metadata of your run is stored and accessible through ZenML.\n",
    "\n",
    "Follow along this notebook to understand how you can use ZenML to productionalize your ML workflows!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f466b16",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to see ZenML in action, no installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/quickstart.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b2977c",
   "metadata": {},
   "source": [
    "# ðŸ‘¶ Step 0. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f40eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"zenml[server]\" pyarrow datasets transformers transformers[torch] torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aad397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "# In case we are in a google colab, clone all additional relevant files\n",
    "if Environment.in_google_colab():\n",
    "    # Pull required modules from this example\n",
    "    !git clone -b main https://github.com/zenml-io/zenml\n",
    "    !cp -r zenml/examples/quickstart/* .\n",
    "    !rm -rf zenml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76f562e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart Kernel to ensure all libraries are properly loaded\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b044374",
   "metadata": {},
   "source": [
    "\n",
    "Please wait for the installation to complete before running subsequent cells. At\n",
    "the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ce581",
   "metadata": {},
   "source": [
    "## â˜ï¸ Step 1: Connect to your ZenML Server\n",
    "To run this quickstart you need to connect to a ZenML Server. You can deploy it [yourself](https://docs.zenml.io/getting-started/deploying-zenml) or try it out for free, no credit-card required in our [ZenML Pro managed service](https://zenml.io/pro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2587315",
   "metadata": {},
   "outputs": [],
   "source": [
    "zenml_server_url = \"INSERT_YOUR_SERVER_URL_HERE\"  # in the form \"https://URL_TO_SERVER\"\n",
    "\n",
    "!zenml connect --url $zenml_server_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081d5616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ZenML and set the default stack\n",
    "!zenml init\n",
    "\n",
    "!zenml stack set default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f6b4c8-09a1-48ba-b971-662cba06745b",
   "metadata": {},
   "source": [
    "Default stack in this case means the code will run on the machine that is running this notebook and all output data will be stored there as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f775f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the imports at the top\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import pandas as pd\n",
    "from zenml import step, pipeline, Model, get_step_context\n",
    "from zenml.client import Client\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from steps.model_trainer import T5_Model\n",
    "from zenml.config import ResourceSettings, DockerSettings\n",
    "\n",
    "from typing import Optional, List\n",
    "\n",
    "from zenml import pipeline\n",
    "\n",
    "from steps import load_data, tokenize_data, train_model, evaluate_model, test_random_sentences\n",
    "\n",
    "from zenml.logger import get_logger\n",
    "\n",
    "logger = get_logger(__name__)\n",
    "\n",
    "# Initialize the ZenML client to fetch objects from the ZenML Server\n",
    "client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e48460",
   "metadata": {},
   "source": [
    "## ðŸ¥‡ Step 2: Run your first pipeline\n",
    "\n",
    "We'll start off by importing our data and training a simple nlp model. In this quickstart we'll be working with a small dataset of sentences in old english paired with more modern formulations. The task is a text-to-text transformation.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to break down your code into distinct functions that load your data, bring it into the correct shape and finally produce a model. H#\n",
    "Here is our first function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd974d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def load_data() -> Annotated[Dataset, \"raw_dataset\"]:\n",
    "    \"\"\"Load and prepare the dataset.\"\"\"\n",
    "\n",
    "    def read_data(file_path):\n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            for line in file:\n",
    "                old, modern = line.strip().split(\"|\")\n",
    "                inputs.append(f\"Translate Old English to Modern English: {old}\")\n",
    "                targets.append(modern)\n",
    "\n",
    "        return {\"input\": inputs, \"target\": targets}\n",
    "\n",
    "    # Assuming your file is named 'translations.txt'\n",
    "    data = read_data(\"translations.txt\")\n",
    "    return Dataset.from_dict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8ba4c6",
   "metadata": {},
   "source": [
    "The whole function is decorated with the ZenML-`@step` decorator. Once this step is added to a pipeline, ZenML will automatically version, track, and cache the data that is produced by this function as an `artifact`. This enables you to \n",
    "reproduce your data at any point in the future, even if the original data source\n",
    "changes or disappears. \n",
    "\n",
    "Note the typing of the function outputs. These are not only good practice, but also\n",
    "help ZenML store and load your data appropriately. By using `Annotated` type hint in the output of the\n",
    "step, we are also naming our outputs. This will make\n",
    "it possible to access it by name later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6286b67",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines one step at a time.  If you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d838e2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_data()\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c05291",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the values are all in the right format ðŸ¥³.\n",
    "\n",
    "We're now at the point where can bring this step (and some others) together into a single\n",
    "pipeline. To do this simply plug multiple steps together through their inputs and outputs.\n",
    "Then just add the `@pipeline` decorator to the function that connects the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a9537",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline()\n",
    "def english_translation_pipeline(\n",
    "    model_type: T5_Model,\n",
    "    num_train_epochs: int,\n",
    "    per_device_train_batch_size: int,\n",
    "    gradient_accumulation_steps: int,\n",
    "):\n",
    "    \"\"\"Define a pipeline that connects the steps.\"\"\"\n",
    "    dataset = load_data()\n",
    "    tokenized_dataset = tokenize_data(dataset)\n",
    "    model, tokenizer = train_model(tokenized_dataset, model_type, num_train_epochs, per_device_train_batch_size, gradient_accumulation_steps)\n",
    "    evaluate_model(model, tokenized_dataset)\n",
    "    test_random_sentences(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd73c23",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just as with the step - by calling the\n",
    "pipeline function itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0aa9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = configured_english_translation_pipeline(model_type=\"t5-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c42078a",
   "metadata": {},
   "source": [
    "As you can see the pipeline has run succesfully. Lets check this out by following the Dashboard URL that you can find in the logs above. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8471f93",
   "metadata": {},
   "source": [
    "We can also fetch the pipeline from the server and view the results directly in the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f208b200",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()\n",
    "run = client.get_pipeline(\"english_translation_pipeline\").last_run\n",
    "print(run.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a037f09d",
   "metadata": {},
   "source": [
    "We can also access the trained model directly by accessing the run. The cool thing here is, this direct access is gonna be available not just now, within this notebook session, but at any later point when you or your colleaugues might need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb0312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model object\n",
    "model = run.steps[\"train_model\"].outputs[\"model\"].load()\n",
    "tokenizer = run.steps[\"train_model\"].outputs[\"tokenizer\"].load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fd8592-2295-4421-a6e6-f619ed389e8c",
   "metadata": {},
   "source": [
    "With these in hand we can now play around with the model directly and try out some examples ourselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e514ac-1a0a-49a0-b8a4-e33cee12c765",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"I do desire we may be better strangers\"\n",
    "\n",
    "input_ids = tokenizer(\n",
    "    test_text,\n",
    "    return_tensors=\"pt\",\n",
    "    max_length=128,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    ").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        max_length=128,\n",
    "        num_return_sequences=1,\n",
    "        no_repeat_ngram_size=2,\n",
    "        top_k=50,\n",
    "        top_p=0.95,\n",
    "        temperature=0.7,\n",
    "    )\n",
    "\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e653c7a-4073-424e-8a59-c69f49526b96",
   "metadata": {},
   "source": [
    "## Lets recap what we've done so far\n",
    "\n",
    "1) We have created a pipeline that takes in a dataset and trains a small Model on it\n",
    "2) This pipeline is broken down in such a way that we can easily iterate on the individual parts without breaking the whole\n",
    "\n",
    "As expected, the performance of this model is not good. To train a model that can solve our task well, we wopuld have to train a larger model. For this, we'll need to move onto the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c28b474",
   "metadata": {},
   "source": [
    "# âŒš Step 3: Scale it up in the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5abe597-dc26-4687-885b-9315049fffdc",
   "metadata": {},
   "source": [
    "The model we have trained on our local machine is proof that our pipeline works. However, we want to train a much more powerful model. For this we'll need to scale onto more powerful machines.\n",
    "\n",
    "So lets take this to the next level and run the pipeline in the environment of your choice. In ZenML we use the word \"stack\" to describe the environment for a pipline run. A stack consists of different components, however you'll only really need to care about the compute and data storage for this example.\n",
    "\n",
    "For you to be able to try this step, you will need to have acess to some cloud compute and cloud storage somewhere (aws, gcp, azure, etc...). ZenML wrapps around all the major cloud providers and orchestration tools and lets you easily plug your code into them.\n",
    "\n",
    "To do this lets head over to the Stack section of your ZenML Dashboard. Here you'll be able to either connect to an existing or deploy a new environment. Choose on of the options presented to you there and come back when you have a stack ready to go. \n",
    "\n",
    "<img src=\".assets/stack_creation.png\" width=\"50%\" alt=\"Pipelines Overview\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafa9eec-c679-476a-a49a-9ce9f3aef37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "# Set the name of your stack here\n",
    "stack_name = \"YOU_STACK_NAME_GOES_HERE\"\n",
    "\n",
    "Client().activate_stack(stack_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d9fc9-b0ce-4bf8-a13b-3338f94ba13d",
   "metadata": {},
   "source": [
    "ZenML runs your pipeline on cloud compute by dockerizing the pipeline. In order to do so we need to install the appropriate requirements for tyour cloud provider and set the prepared parent image for quicker run times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e02652-34ae-4b79-948e-1d80f559fdf5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a95e2a-2c55-4068-8111-5ea5559203da",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install gcp -y\n",
    "\n",
    "from zenml.integrations.gcp.flavors.vertex_orchestrator_flavor import VertexOrchestratorSettings\n",
    "\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\")\n",
    "        \"orchestrator.vertex\": VertexOrchestratorSettings(node_selector_constraint=(\"cloud.google.com/gke-accelerator\", \"NVIDIA_TESLA_P4\"))\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa58b33-712c-4926-b12b-feceda3384c8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95af245-a7fd-4d64-b0af-d5a96a788846",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install aws s3 -y\n",
    "\n",
    "from zenml.integrations.aws.flavors.sagemaker_orchestrator_flavor import SagemakerOrchestratorSettings\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"resources\": ResourceSettings(memory=\"32GB\"),\n",
    "        \"orchestrator.sagemaker\": SagemakerOrchestratorSettings(instance_type=\"ml.p2.xlarge\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07454193-8bc8-4adf-bcca-db598b891ccf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b63c109-0ba5-4a62-adaa-47aa9612373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install azure -y\n",
    "\n",
    "from zenml.integrations.skypilot.flavors.skypilot_orchestrator_base_vm_config import SkypilotBaseOrchestratorSettings\n",
    "\n",
    "configured_english_translation_pipeline = english_translation_pipeline.with_options(\n",
    "    settings={\n",
    "        \"docker\": DockerSettings(\n",
    "            parent_image=\"pytorch/pytorch:2.4.0-cuda11.8-cudnn9-runtime\",\n",
    "            requirements=[\"zenml==0.63.0\",\"pyarrow\",\"datasets\",\"transformers[torch]\",\"sentencepiece\"],\n",
    "            environment={\"ZENML_DISABLE_STEP_LOGS_STORAGE\": True}\n",
    "        ),\n",
    "        \"orchestrator.sagemaker\": SkypilotBaseOrchestratorSettings(accelerators='V100', memory=\"32+\", cpus=\"8+\")\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f17b7a-5a82-4975-b9bd-6a63fbb97a68",
   "metadata": {},
   "source": [
    "## Ready to launch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df14f30c-9a8e-46ca-ba44-cf16ea715dac",
   "metadata": {},
   "source": [
    "We now have configured zenml to use your very own cloud infrastructure for the next pipeline run, lets see this in action by running the pipeline again on the smaller t5 model (`t5_small`).\n",
    "\n",
    "Note: The whole process may take a bit longer the first time around, as your pipeline code needs to be built into docker containers to be run in the orchestration environment of your stack. Any consecutive run of the pipeline, even with different parameters set, will not take as long again thanks to docker caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e758fe-6ea3-42ff-bea8-33953135bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run = configured_english_translation_pipeline(model_type=\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3480bd9c-8430-4a9b-bfa1-e0f3457024c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673467e4-cf2f-455d-a83e-12b059510b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07fa6f-2563-42c3-a203-021abd9900e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e14bc4-46a6-4135-980f-3856d3a73835",
   "metadata": {},
   "outputs": [],
   "source": [
    "... summary ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ee4fc-f102-4b99-bdc3-2f1670c87679",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You're a legit MLOps engineer now! You have created a training pipeline and you\n",
    "have deployed it into a production-ready environment with the compute of your \n",
    "choice. You also have gotten a hang of the ZenML Dashboard.\n",
    "\n",
    "## Further exploration\n",
    "\n",
    "This was just the tip of the iceberg of what ZenML can do; check out the [**docs**](https://docs.zenml.io/) to learn more\n",
    "about the capabilities of ZenML. For example, you might want to:\n",
    "\n",
    "- [Deploy ZenML](https://docs.zenml.io/user-guide/production-guide/connect-deployed-zenml) to collaborate with your colleagues.\n",
    "- Run the same pipeline on a [cloud MLOps stack in production](https://docs.zenml.io/user-guide/production-guide/cloud-stack).\n",
    "- Track your metrics in an experiment tracker like [MLflow](https://docs.zenml.io/stacks-and-components/component-guide/experiment-trackers/mlflow).\n",
    "\n",
    "## What next?\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack) and become part of the ZenML family!\n",
    "* If you want to quickly get started with ZenML, check out [ZenML Pro](https://zenml.io/pro)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
