{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Up and Running Quickly\n",
    "\n",
    "## ðŸŒ Overview\n",
    "\n",
    "This quickstart aims to give you a small illustration of what ZenML can do. We will:\n",
    "\n",
    "- Import some data from a public dataset (Adult Census Income), then train two models (SGD and Random Forest)\n",
    "- Compare and evaluate which model performs better, and deploy the best one.\n",
    "- Run a prediction on the deployed model.\n",
    "\n",
    "Along the way we will also show you how to:\n",
    "\n",
    "- Automatically version, track, and cache data, models, and other artifacts,\n",
    "- Track model hyperparameters and metrics in an experiment tracking tool,\n",
    "\n",
    "This will give you enough to get started building your own ZenML Pipelines.\n",
    "Let's dive in!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAGRAM SHOWING THE FLOW"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- introduction to what the quickstart is about\n",
    "- what will be covered here / what we'll do\n",
    "\n",
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to see ZenML in action, no signup / installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/new_quickstart/new_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started. First we'll install the latest version of\n",
    "ZenML as well as the two integrations we'll need for this quickstart: `sklearn`\n",
    "and `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add things relating to cloudflare pipelines etc and zenml installation\n",
    "# !pip install -q -e \"../../../.[dev]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !zenml integration install sklearn mlflow -y\n",
    "!zenml init\n",
    "# %pip install pyngrok pyparsing==2.4.2  # required for Colab #TODO: STILL NEEDED?\n",
    "\n",
    "# # automatically restart kernel\n",
    "# import IPython\n",
    "# IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data\n",
    "\n",
    "We'll start off by importing our data. In this quickstart we'll be working with\n",
    "[the Adult Census Income](https://archive.ics.uci.edu/dataset/2/adult) dataset\n",
    "which is publicly available on the UCI Machine Learning Repository. The task is\n",
    "to predict whether a person makes over $50k a year based on a number of\n",
    "features. These features are things like age, work class, education level,\n",
    "marital status, occupation, relationship, race, sex, capital gain, capital loss,\n",
    "hours per week, and native country.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to do\n",
    "something similar to this: import your data and get it in the right shape for\n",
    "your training. ZenML mostly gets out of your way when you're writing your Python\n",
    "code, as you'll see from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from zenml import step\n",
    "\n",
    "\n",
    "@step\n",
    "def training_data_loader() -> (\n",
    "    Tuple[\n",
    "        Annotated[pd.DataFrame, \"X_train\"],\n",
    "        Annotated[pd.DataFrame, \"X_test\"],\n",
    "        Annotated[pd.Series, \"y_train\"],\n",
    "        Annotated[pd.Series, \"y_test\"],\n",
    "    ]\n",
    "):\n",
    "    \"\"\"Load the Census Income dataset as tuple of Pandas DataFrame / Series.\"\"\"\n",
    "    # Load the dataset\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    column_names = [\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"income\",\n",
    "    ]\n",
    "    data = pd.read_csv(\n",
    "        url, names=column_names, na_values=\"?\", skipinitialspace=True\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode categorical features and drop original columns\n",
    "    categorical_cols = [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"native-country\",\n",
    "    ]\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Encode target feature\n",
    "    data[\"income\"] = data[\"income\"].apply(\n",
    "        lambda x: 1 if x.strip() == \">50K\" else 0\n",
    "    )\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(\"income\", axis=1)\n",
    "    y = data[\"income\"]\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the data, dropping some columns and then splitting it up into train\n",
    "and test sets. The whole function is decorated with the `@step` decorator, which\n",
    "tells ZenML to track this function as a step in the pipeline. This means that\n",
    "ZenML will automatically version, track, and cache the data that is produced by\n",
    "this function. This is a very powerful feature, as it means that you can\n",
    "reproduce your data at any point in the future, even if the original data source\n",
    "changes or disappears.\n",
    "\n",
    "You'll also notice that we have included type hints for the outputs\n",
    "to the function. These are not only useful for anyone reading your code, but\n",
    "help ZenML process your data in a way appropriate to the specific data types.\n",
    "We're using the `Annotated` type hint here, which is a special type hint that\n",
    "allows us to give a name to our output. This is useful for when you have\n",
    "multiple outputs from a single step and you might want to access a specific\n",
    "output by key later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines as you work, so if you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19863</th>\n",
       "      <td>53</td>\n",
       "      <td>168539</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24342</th>\n",
       "      <td>49</td>\n",
       "      <td>56841</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>28</td>\n",
       "      <td>154571</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25710</th>\n",
       "      <td>60</td>\n",
       "      <td>188236</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13824</th>\n",
       "      <td>53</td>\n",
       "      <td>87158</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "19863   53  168539              5             0             0              70   \n",
       "24342   49   56841             13             0             0              70   \n",
       "10027   28  154571             10             0             0              40   \n",
       "25710   60  188236              6             0             0              40   \n",
       "13824   53   87158              9             0             0              40   \n",
       "\n",
       "       workclass_Local-gov  workclass_Private  workclass_Self-emp-inc  \\\n",
       "19863                False              False                   False   \n",
       "24342                False              False                   False   \n",
       "10027                False               True                   False   \n",
       "25710                False               True                   False   \n",
       "13824                False               True                   False   \n",
       "\n",
       "       workclass_Self-emp-not-inc  ...  native-country_Portugal  \\\n",
       "19863                        True  ...                    False   \n",
       "24342                        True  ...                    False   \n",
       "10027                       False  ...                    False   \n",
       "25710                       False  ...                    False   \n",
       "13824                       False  ...                    False   \n",
       "\n",
       "       native-country_Puerto-Rico  native-country_Scotland  \\\n",
       "19863                       False                    False   \n",
       "24342                       False                    False   \n",
       "10027                       False                    False   \n",
       "25710                       False                    False   \n",
       "13824                       False                    False   \n",
       "\n",
       "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
       "19863                 False                  False                    False   \n",
       "24342                 False                  False                    False   \n",
       "10027                 False                  False                    False   \n",
       "25710                 False                  False                    False   \n",
       "13824                 False                  False                    False   \n",
       "\n",
       "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "19863                           False                          True   \n",
       "24342                           False                          True   \n",
       "10027                           False                         False   \n",
       "25710                           False                          True   \n",
       "13824                           False                          True   \n",
       "\n",
       "       native-country_Vietnam  native-country_Yugoslavia  \n",
       "19863                   False                      False  \n",
       "24342                   False                      False  \n",
       "10027                   False                      False  \n",
       "25710                   False                      False  \n",
       "13824                   False                      False  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = training_data_loader()\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the values are all in the right format. We\n",
    "can shift to training some models now! ðŸ¥³"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Models\n",
    "\n",
    "Now that we have our data it makes sense to train some models to get a sense of\n",
    "how difficult the task is. The Census Income\n",
    "dataset is sufficiently large and complex that it's unlikely we'll be able to\n",
    "train a model that behaves perfectly since the problem is inherently complex,\n",
    "but we can get a sense of what a reasonable baseline looks like.\n",
    "\n",
    "We'll start with two simple models, a SGD Classifier and a Random Forest\n",
    "Classifier, both batteries-included from `sklearn`. We'll train them both on the\n",
    "same data and then compare their performance.\n",
    "\n",
    "Since we're starting our work properly, it makes sense to start tracking the\n",
    "experimentation that we're doing. ZenML integrates with MLflow to make this\n",
    "easy. This happens out of the box when using our experiment tracker integration\n",
    "and stack components. We'll see how this works below, but first let's set up\n",
    "ZenML to know that it should use the MLFlow experiment tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow --flavor=mlflow\n",
    "\n",
    "# Register a new stack with our experiment tracker\n",
    "!zenml stack register quickstart -a default\\\n",
    "                                 -o default\\\n",
    "                                 -e mlflow\n",
    "\n",
    "!zenml stack set quickstart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point our stack looks like this. We can now write the steps where we'll\n",
    "train our models, making sure to specify the name of our experiment tracker in\n",
    "the `@step` decorator. We could specify this manually using a string, but\n",
    "instead we'll use the ZenML `Client` to access the name of our active stack's\n",
    "experiment tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "\n",
    "@step(experiment_tracker=experiment_tracker.name)\n",
    "def random_forest_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn Random Forest classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hyperparams and metrics to MLflow\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "@step(experiment_tracker=experiment_tracker.name)\n",
    "def sgd_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a SGD classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hyperparams and metrics to MLflow\n",
    "    model = SGDClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two training steps both return different kinds of `sklearn` classifier\n",
    "models, so we use the generic `ClassifierMixin` type hint for the return type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end goal of this quick baseline evaluation is to understand which of the two\n",
    "models performs better. We'll use the `evaluator` step to compare the two\n",
    "models. This step takes in the two models we trained above, and compares them on\n",
    "the test data we created earlier. It returns whichever model performs best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@step\n",
    "def evaluator(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    model1: ClassifierMixin,\n",
    "    model2: ClassifierMixin,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Calculate the accuracy on the test set and return the best model of two.\"\"\"\n",
    "    test_acc1 = model1.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    test_acc2 = model2.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    print(f\"Test accuracy ({model1.__class__.__name__}): {test_acc1}\")\n",
    "    print(f\"Test accuracy ({model2.__class__.__name__}): {test_acc2}\")\n",
    "    return model1 if test_acc1 > test_acc2 else model2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll likely want to use our model in the future so instead of simply outputting\n",
    "the model we'll use the MLflow model registry to store it. This allows us to\n",
    "version the model for retrieval and use later on as well as to use other\n",
    "functionality made possible within the MLflow dashboard. This step is a bit\n",
    "different from the ones listed above in that we're using a pre-built ZenML step\n",
    "instead of just writing our own. You'll often come across these pre-built steps\n",
    "for common workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_registry import (\n",
    "    mlflow_register_model_step,\n",
    ")\n",
    "\n",
    "model_name = \"zenml-quickstart-model\"\n",
    "\n",
    "register_model = mlflow_register_model_step.with_options(\n",
    "    parameters=dict(\n",
    "        name=model_name,\n",
    "        description=\"The first run of the Quickstart pipeline.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now at the point where can bring all these steps together into a single\n",
    "pipeline, the top-level organising entity for code in ZenML. Creating such a pipeline is\n",
    "as simple as adding a `@pipeline` decorator to a function. This specific\n",
    "pipeline doesn't return a value, but that option is available to you if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "\n",
    "\n",
    "@pipeline\n",
    "def train_and_register_model_pipeline() -> None:\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    register_model.after(evaluator)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = training_data_loader()\n",
    "    model1 = random_forest_trainer_mlflow(X_train=X_train, y_train=y_train)\n",
    "    model2 = sgd_trainer_mlflow(X_train=X_train, y_train=y_train)\n",
    "    best_model = evaluator(\n",
    "        X_test=X_test, y_test=y_test, model1=model1, model2=model2\n",
    "    )\n",
    "    register_model(best_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we specify that we want the model registration to run *after* the\n",
    "evaluation step. This is because we won't have a model ready for registration\n",
    "until the evaluation has taken place. ZenML automatically tries to run steps in\n",
    "parallel, so sometimes if you have this kind of sequencing you need to do then\n",
    "you'll need to specify it explicitly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used the built-in MLflow registry to store our model, but ZenML doesn't\n",
    "yet know that we want to use the MLflow flavor of the model registry stack\n",
    "component in our stack. Let's add that now and update our stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow --flavor=mlflow\n",
    "\n",
    "# Update our stack to include the model registry\n",
    "!zenml stack update quickstart -r mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DIAGRAM SHOWING THE NEW STACK"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just -- as with the step -- by calling the\n",
    "pipeline function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered pipeline \u001b[0m\u001b[33mtrain_and_register_model_pipeline\u001b[1;35m (version 2).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33mtrain_and_register_model_pipeline\u001b[1;35m on stack \u001b[0m\u001b[33mquickstart\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has finished in 9.887s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrandom_forest_trainer_mlflow\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/05 10:30:18 INFO mlflow.tracking.fluent: Experiment with name 'train_and_register_model_pipeline' does not exist. Creating a new experiment.\n",
      "2023/07/05 10:30:19 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2023/07/05 10:31:02 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/strickvl/.pyenv/versions/3.10.11/envs/quickstart/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.9999171121886526\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrandom_forest_trainer_mlflow\u001b[1;35m has finished in 47.507s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33msgd_trainer_mlflow\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/05 10:31:07 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7788967632309669\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33msgd_trainer_mlflow\u001b[1;35m has finished in 21.729s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mevaluator\u001b[1;35m has started.\u001b[0m\n",
      "Test accuracy (RandomForestClassifier): 0.8508204873197415\n",
      "Test accuracy (SGDClassifier): 0.7821979114868225\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mevaluator\u001b[1;35m has finished in 1.650s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_register_model_step\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mNo registered model with name zenml-quickstart-model found. Creating a newregistered model.\u001b[0m\n",
      "\u001b[1;35mMLflow model registry does not take a version as an argument. Registering a new version for the model \u001b[0m\u001b[33m'zenml-quickstart-model'\u001b[1;35m a version will be assigned automatically.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/05 10:31:33 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: zenml-quickstart-model, version 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered model zenml-quickstart-model with version 1 from source file:///Users/strickvl/Library/Application Support/zenml/local_stores/d32e1983-2602-4334-968a-6ee66c2e1783/mlruns/289096806505521610/a816b9b0d8ed47eea4b78634fc246393/artifacts/model.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_register_model_step\u001b[1;35m has finished in 1.015s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mtrain_and_register_model_pipeline-2023_07_05-08_30_07_764991\u001b[1;35m has finished in 1m26s.\u001b[0m\n",
      "\u001b[1;35mPipeline visualization can be seen in the ZenML Dashboard. Run \u001b[0m\u001b[33mzenml up\u001b[1;35m to see your pipeline!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_and_register_model_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the logs already how our model training went: the\n",
    "`RandomForestClassifier` performed considerably better than the `SGDClassifier`,\n",
    "so that will have been the model that was returned from the evaluation step and\n",
    "then registered with the MLflow model registry."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAYBE SHOW THE ZENML DASHBOARD HERE\n",
    "\n",
    "ALSO MAYBE SHOW THE MLFLOW UI + HOW TO ACCESS IT HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Cloudflare tunnel\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "!zenml up --port 8237 & cloudflared tunnel --url http://localhost:8237"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[[And you can use `wget -qO- http://localhost:55555/quicktunnel` to get the\n",
    "magic hostname.]]]\n",
    "\n",
    "TODO: Reimplement this natively under the hood"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using MLflow for our experiment tracking. If you'd like to inspect the\n",
    "MLflow dashboard to see your experiments and what's been logged so far, run the\n",
    "following cell. This cell will spin up a local server that you can access via\n",
    "the link mentioned after the \"Listening at:\" `INFO` log statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add cloudflare tunnel for MLflow UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-07-05 10:33:20 +0200] [11690] [INFO] Starting gunicorn 20.1.0\n",
      "[2023-07-05 10:33:20 +0200] [11690] [INFO] Listening at: http://127.0.0.1:5000 (11690)\n",
      "[2023-07-05 10:33:20 +0200] [11690] [INFO] Using worker: sync\n",
      "[2023-07-05 10:33:20 +0200] [11691] [INFO] Booting worker with pid: 11691\n",
      "[2023-07-05 10:33:20 +0200] [11692] [INFO] Booting worker with pid: 11692\n",
      "[2023-07-05 10:33:20 +0200] [11693] [INFO] Booting worker with pid: 11693\n",
      "[2023-07-05 10:33:20 +0200] [11694] [INFO] Booting worker with pid: 11694\n",
      "^C\n",
      "[2023-07-05 10:33:37 +0200] [11690] [INFO] Handling signal: int\n",
      "[2023-07-05 10:33:38 +0200] [11693] [INFO] Worker exiting (pid: 11693)\n",
      "[2023-07-05 10:33:38 +0200] [11692] [INFO] Worker exiting (pid: 11692)\n",
      "[2023-07-05 10:33:38 +0200] [11691] [INFO] Worker exiting (pid: 11691)\n",
      "[2023-07-05 10:33:38 +0200] [11694] [INFO] Worker exiting (pid: 11694)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = get_tracking_uri()\n",
    "!mlflow ui --backend-store-uri $MLFLOW_TRACKING_URI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline above registered the best model with the MLflow model registry.\n",
    "Whenever you register a model it also versions the model since it's likely that\n",
    "you'll be iterating and improving your model over time.\n",
    "\n",
    "We'll now turn to actually deploying our model and serving some predictions, for\n",
    "which we'll need to specify the model version we want to use. You can specify\n",
    "the version number manually but below we'll use the ZenML `Client` to get the\n",
    "latest version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "most_recentmodel_version_number = int(\n",
    "    Client()\n",
    "    .active_stack.model_registry.list_model_versions(metadata={})[0]\n",
    "    .version\n",
    ")\n",
    "most_recentmodel_version_number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, and we've found the best one, we want to deploy it\n",
    "and run some inference on the deployed model. We'll use the local MLflow model\n",
    "deployer which once again comes with some pre-built ZenML steps to save you reinventing the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_deployer import (\n",
    "    mlflow_model_registry_deployer_step,\n",
    ")\n",
    "from zenml.integrations.mlflow.steps.mlflow_registry import (\n",
    "    mlflow_register_model_step,\n",
    ")\n",
    "from zenml.model_registries.base_model_registry import (\n",
    "    ModelRegistryModelMetadata,\n",
    ")\n",
    "\n",
    "model_deployer = mlflow_model_registry_deployer_step.with_options(\n",
    "    parameters=dict(\n",
    "        registry_model_name=model_name,\n",
    "        registry_model_version=most_recentmodel_version_number,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you deploy a model this is usually something you want to remain available\n",
    "and running for a long time, so ZenML automatically creates a background service\n",
    "for your deployed model. We load the service (already created by the\n",
    "`model_deployer` step) and then use it to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.client import Client\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_and_register_model_pipeline.\"\"\"\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"train_and_register_model_pipeline\",\n",
    "        running=True,\n",
    "    )\n",
    "    return services[0]\n",
    "\n",
    "\n",
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: pd.DataFrame,\n",
    ") -> Annotated[list, \"predictions\"]:\n",
    "    \"\"\"Run a inference request against a prediction service.\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    print(f\"Running predictions on data (single individual): {data.to_numpy()[0]}\")\n",
    "    prediction = service.predict(data.to_numpy())\n",
    "    print(f\"Prediction (for single example slice) is: {bool(prediction.tolist()[0])}\")\n",
    "    return prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2;36mRunning with active stack: \u001b[0m\u001b[2;32m'quickstart'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Registering model deployer 'mlflow'...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ™\u001b[0m Registering model deployer 'mlflow'...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Registering model deployer 'mlflow'...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mSuccessfully registered model_deployer `mlflow`.\u001b[0m\n",
      "\u001b[2;32mâ ¹\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;36mRegistering model deployer 'mlflow'...\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Registering model deployer 'mlflow'...\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[?25l\u001b[32mâ ‹\u001b[0m Updating stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ™\u001b[0m Updating stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¹\u001b[0m Updating stack...\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[2;36mStack `quickstart` successfully updated!\u001b[0m\n",
      "\u001b[2;32mâ ¸\u001b[0m\u001b[2;36m \u001b[0m\u001b[2;36mUpdating stack...\u001b[0m\n",
      "\u001b[2K\u001b[1A\u001b[2K\u001b[32mâ ¸\u001b[0m Updating stack...\n",
      "\n",
      "\u001b[1A\u001b[2K\u001b[1A\u001b[2K"
     ]
    }
   ],
   "source": [
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow --flavor=mlflow\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack update quickstart -d mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there are some dependencies in terms of how the step needs to run, so\n",
    "we specify those upfront. For example, the prediction service needs to be loaded\n",
    "before we try to make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def deploy_and_predict() -> None:\n",
    "    \"\"\"Deploy the best model and run some predictions.\"\"\"\n",
    "    prediction_service_loader.after(model_deployer)\n",
    "    predictor.after(prediction_service_loader)\n",
    "    model_deployer()\n",
    "    _, inference_data, _, _ = training_data_loader()\n",
    "    model_deployment_service = prediction_service_loader()\n",
    "    predictor(service=model_deployment_service, data=inference_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered pipeline \u001b[0m\u001b[33mdeploy_and_predict\u001b[1;35m (version 1).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33mdeploy_and_predict\u001b[1;35m on stack \u001b[0m\u001b[33mquickstart\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_model_registry_deployer_step\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUpdating an existing MLflow deployment service: MLFlowDeploymentService[599fd11b-d4c9-46c6-8119-764b4f5ea707] (type: model-serving, flavor: mlflow)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a6de6a34c246f9bb1871248f189a63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mMLflow deployment service started and reachable at:\n",
      "    http://127.0.0.1:8002/invocations\n",
      "\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_model_registry_deployer_step\u001b[1;35m has finished in 46.857s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has finished in 0.383s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on data (example slice): [28 76714 15 0 0 55 False True False False False False False False False\n",
      " False False False False False False False False False False True False\n",
      " False False False True False False False False False False False False\n",
      " False False True False False False False True False False False False\n",
      " False False False True True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False True False False]\n",
      "Prediction (example slice) is: False\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has finished in 0.976s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mdeploy_and_predict-2023_07_05-08_50_06_948298\u001b[1;35m has finished in 51.279s.\u001b[0m\n",
      "\u001b[1;35mPipeline visualization can be seen in the ZenML Dashboard. Run \u001b[0m\u001b[33mzenml up\u001b[1;35m to see your pipeline!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "deploy_and_predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, the purpose of this model was to predict whether or not someone\n",
    "earns more than $50k per year. You can see a single example in the output above.\n",
    "Given the features of a particular individual, the model predicts that they do\n",
    "not earn more than $50k per year.\n",
    "\n",
    "If we were interested in learning more about the model's predictions, we could\n",
    "separately load the predictor service and use it to pass in some other data or\n",
    "try things out. To load the predictor we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLFlowDeploymentService[599fd11b-d4c9-46c6-8119-764b4f5ea707] (type: model-serving, flavor: mlflow)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_service = deploy_and_predict.model.last_successful_run.steps[\n",
    "    \"prediction_service_loader\"\n",
    "].output.load()\n",
    "\n",
    "predictor_service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, passing in some data is as simple as calling the `predict` method\n",
    "on the predictor service. We can try this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions: [0 0 0 0 0 0 0 1 0 0]\n",
      "Ground truth:      [1 1 1 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Model predictions: {predictor_service.predict(X_test.to_numpy()[25:35])}\"\n",
    ")\n",
    "print(f\"Ground truth:      {y_test.to_numpy()[25:35]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're passing in some of our test data into the model and getting back the\n",
    "predictions. You can already start to see some of the places where our\n",
    "predictions are not matching the ground truth labels. This is to be expected but\n",
    "we could potentially use this to now iterate on our models by adding more steps.\n",
    "\n",
    "To get an overview of the models and model versions that we have registered and\n",
    "deployed so\n",
    "far, we can use the CLI to list these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2;36mRunning with active stack: \u001b[0m\u001b[2;32m'quickstart'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1m         NAME         \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mDESCRIPTION\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mMETADATA\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¨\n",
      "â”ƒ zenml-quickstart-model â”‚             â”‚          â”ƒ\n",
      "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”›\n"
     ]
    }
   ],
   "source": [
    "!zenml model-registry models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2;36mRunning with active stack: \u001b[0m\u001b[2;32m'quickstart'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m                    \u001b[0mâ”‚\u001b[1m               \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mVERSION_DESCRIPTIO\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m                    \u001b[0mâ”ƒ\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1m       NAME       \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mMODEL_VERSION\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mN                 \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mMETADATA          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¨\n",
      "â”ƒ zenml-quickstart-m â”‚ 1             â”‚ The first run of   â”‚ {'zenml_version':  â”ƒ\n",
      "â”ƒ        odel        â”‚               â”‚ the Quickstart     â”‚ '0.41.0',          â”ƒ\n",
      "â”ƒ                    â”‚               â”‚ pipeline.          â”‚ 'zenml_run_name':  â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 'train_and_registe â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ r_model_pipeline-2 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 023_07_05-08_30_07 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ _764991',          â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 'zenml_pipeline_na â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ me':               â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 'train_and_registe â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ r_model_pipeline', â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚                    â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 'zenml_pipeline_ru â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ n_uuid':           â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ '7f83b0f3-3fab-4d4 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ e-8b70-8bd9d9342b6 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ e',                â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 'zenml_workspace': â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ '55f342ac-05a6-465 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 9-b851-8bf5047d340 â”ƒ\n",
      "â”ƒ                    â”‚               â”‚                    â”‚ 6'}                â”ƒ\n",
      "â”—â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”›\n"
     ]
    }
   ],
   "source": [
    "!zenml model-registry models list-versions zenml-quickstart-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36mUsing the default local database.\u001b[0m\n",
      "\u001b[2;36mRunning with active workspace: \u001b[0m\u001b[2;32m'default'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "\u001b[2;36mRunning with active stack: \u001b[0m\u001b[2;32m'quickstart'\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;2;36m(\u001b[0m\u001b[2;36mrepository\u001b[0m\u001b[1;2;36m)\u001b[0m\n",
      "â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”¯â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
      "â”ƒ\u001b[1m        \u001b[0mâ”‚\u001b[1m                  \u001b[0mâ”‚\u001b[1m                  \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mPIPELINE_STEP_NA\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m            \u001b[0mâ”ƒ\n",
      "â”ƒ\u001b[1m \u001b[0m\u001b[1mSTATUS\u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mUUID            \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mPIPELINE_NAME   \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mME              \u001b[0m\u001b[1m \u001b[0mâ”‚\u001b[1m \u001b[0m\u001b[1mMODEL_NAME\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
      "â” â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¨\n",
      "â”ƒ   âœ…   â”‚ 599fd11b-d4c9-46 â”‚ train_and_regist â”‚                  â”‚ model      â”ƒ\n",
      "â”ƒ        â”‚ c6-8119-764b4f5e â”‚ er_model_pipelin â”‚                  â”‚            â”ƒ\n",
      "â”ƒ        â”‚ a707             â”‚ e                â”‚                  â”‚            â”ƒ\n",
      "â”—â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”·â”â”â”â”â”â”â”â”â”â”â”â”â”›\n"
     ]
    }
   ],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all this on the ZenML Dashboard, simply spin up the server again and\n",
    "view the steps via the DAG visualiser and also browse the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Cloudflare tunnel\n",
    "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n",
    "\n",
    "!zenml up --port 8237 & cloudflared tunnel --url http://localhost:8237"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built two ML pipelines! You trained two models, evaluated them against\n",
    "a test set, registered the best one with the MLflow model registry, deployed it\n",
    "and served some predictions. You also learned how to iterate on your models and\n",
    "data by using some of the ZenML utility abstractions. You saw how to view your\n",
    "artifacts and stacks via the CLI as well as the ZenML Dashboard.\n",
    "\n",
    "And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://zenml.io/integrations) page for a list of all the cool MLOps tools that ZenML supports!\n",
    "\n",
    "To improve upon the ML workflows we built in this quickstart, you could, for instance:\n",
    "- [Deploy ZenML on the Cloud](https://docs.zenml.io/platform-guide/set-up-your-mlops-platform/deploy-zenml) to collaborate with your teammates,\n",
    "- Experiment with more sophisticated models, such as [XGBoost](https://zenml.io/integrations/xgboost),\n",
    "- Monitor your model's ongoing performance using [Evidently](https://zenml.io/integrations/evidently),\n",
    "- Set up automated [Slack alerts](https://zenml.io/integrations/zen-ml-slack-integration) to get notified when data drift happens,\n",
    "- Run the pipelines on scalable, distributed stacks like [Kubeflow](https://zenml.io/integrations/kubeflow)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go next\n",
    "\n",
    "* If you have questions or feedback... \n",
    "  * Join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If this quickstart was a bit too quick for you...\n",
    "  * Check out the [**Starter\n",
    "    Guide**](https://docs.zenml.io/user-guide/starter-guide) which guides you\n",
    "    through ZenML as well as some practical ways to get going on your MLOps journey.\n",
    "* If you want to learn more about using or extending ZenML...\n",
    "  * Check out our [**Docs**](https://docs.zenml.io/) or read through our code on [**Github**](https://github.com/zenml-io/zenml).\n",
    "* If you want to quickly learn how to use a specific tool with ZenML...\n",
    "  * Check out our collection of [**Examples**](https://github.com/zenml-io/zenml/tree/doc/hamza-misc-updates/examples).\n",
    "* If you want to see some advanced ZenML use cases... \n",
    "  * Check out [**ZenML Projects**](https://github.com/zenml-io/zenml-projects), our collection of production-grade ML use-cases."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
