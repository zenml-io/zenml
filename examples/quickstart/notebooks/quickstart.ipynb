{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Up and Running Quickly\n",
    "\n",
    "## ðŸŒ Overview\n",
    "\n",
    "This quickstart demonstrates some of ZenML's features. We will:\n",
    "\n",
    "- Import some data from a public dataset (Adult Census Income), then train two models (SGD and Random Forest)\n",
    "- Compare and evaluate which model performs better, and deploy the best one.\n",
    "- Run a prediction on the deployed model.\n",
    "\n",
    "Along the way we will also show you how to:\n",
    "\n",
    "- Automatically version, track, and cache data, models, and other artifacts,\n",
    "- Track model hyperparameters and metrics in an experiment tracking tool\n",
    "\n",
    "This will give you enough to get started building your own ZenML Pipelines.\n",
    "Let's dive in!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on Colab\n",
    "\n",
    "You can use Google Colab to see ZenML in action, no signup / installation\n",
    "required!\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](\n",
    "https://colab.research.google.com/github/zenml-io/zenml/blob/main/examples/quickstart/new_quickstart/new_quickstart.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Install Requirements\n",
    "\n",
    "Let's install ZenML to get started. First we'll install the latest version of\n",
    "ZenML as well as the two integrations we'll need for this quickstart: `sklearn`\n",
    "and `mlflow`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"zenml[server]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.environment import Environment\n",
    "\n",
    "if Environment.in_google_colab():\n",
    "    # Install Cloudflare Tunnel binary\n",
    "    !wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb && dpkg -i cloudflared-linux-amd64.deb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml integration install sklearn mlflow -y\n",
    "\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(restart=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml init"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please wait for the installation to complete before running subsequent cells. At the end of the installation, the notebook kernel will automatically restart."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Data\n",
    "\n",
    "We'll start off by importing our data. In this quickstart we'll be working with\n",
    "[the Adult Census Income](https://archive.ics.uci.edu/dataset/2/adult) dataset\n",
    "which is publicly available on the UCI Machine Learning Repository. The task is\n",
    "to predict whether a person makes over $50k a year based on a number of\n",
    "features. These features are things like age, work class, education level,\n",
    "marital status, occupation, relationship, race, sex, capital gain, capital loss,\n",
    "hours per week, and native country.\n",
    "\n",
    "When you're getting started with a machine learning problem you'll want to do\n",
    "something similar to this: import your data and get it in the right shape for\n",
    "your training. ZenML mostly gets out of your way when you're writing your Python\n",
    "code, as you'll see from the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from zenml import step\n",
    "\n",
    "\n",
    "@step\n",
    "def training_data_loader() -> (\n",
    "    Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]\n",
    "):\n",
    "    \"\"\"Load the Census Income dataset as tuple of Pandas DataFrame / Series.\"\"\"\n",
    "    # Load the dataset\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "    column_names = [\n",
    "        \"age\",\n",
    "        \"workclass\",\n",
    "        \"fnlwgt\",\n",
    "        \"education\",\n",
    "        \"education-num\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"capital-gain\",\n",
    "        \"capital-loss\",\n",
    "        \"hours-per-week\",\n",
    "        \"native-country\",\n",
    "        \"income\",\n",
    "    ]\n",
    "    data = pd.read_csv(\n",
    "        url, names=column_names, na_values=\"?\", skipinitialspace=True\n",
    "    )\n",
    "\n",
    "    # Drop rows with missing values\n",
    "    data = data.dropna()\n",
    "\n",
    "    # Encode categorical features and drop original columns\n",
    "    categorical_cols = [\n",
    "        \"workclass\",\n",
    "        \"education\",\n",
    "        \"marital-status\",\n",
    "        \"occupation\",\n",
    "        \"relationship\",\n",
    "        \"race\",\n",
    "        \"sex\",\n",
    "        \"native-country\",\n",
    "    ]\n",
    "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "    # Encode target feature\n",
    "    data[\"income\"] = data[\"income\"].apply(\n",
    "        lambda x: 1 if x.strip() == \">50K\" else 0\n",
    "    )\n",
    "\n",
    "    # Separate features and target\n",
    "    X = data.drop(\"income\", axis=1)\n",
    "    y = data[\"income\"]\n",
    "\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    return (X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We download the data, dropping some columns and then splitting it up into train\n",
    "and test sets. The whole function is decorated with the `@step` decorator, which\n",
    "tells ZenML to track this function as a step in the pipeline. This means that\n",
    "ZenML will automatically version, track, and cache the data that is produced by\n",
    "this function. This is a very powerful feature, as it means that you can\n",
    "reproduce your data at any point in the future, even if the original data source\n",
    "changes or disappears.\n",
    "\n",
    "You'll also notice that we have included type hints for the outputs\n",
    "to the function. These are not only useful for anyone reading your code, but\n",
    "help ZenML process your data in a way appropriate to the specific data types.\n",
    "We're using the `Annotated` type hint here, which is a special type hint that\n",
    "allows us to give a name to our output. This is useful for when you have\n",
    "multiple outputs from a single step and you might want to access a specific\n",
    "output by key later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ZenML is built in a way that allows you to experiment with your data and build\n",
    "your pipelines as you work, so if you want to call this function to see how it\n",
    "works, you can just call it directly. Here we take a look at the first few rows\n",
    "of your training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education-num</th>\n",
       "      <th>capital-gain</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>hours-per-week</th>\n",
       "      <th>workclass_Local-gov</th>\n",
       "      <th>workclass_Private</th>\n",
       "      <th>workclass_Self-emp-inc</th>\n",
       "      <th>workclass_Self-emp-not-inc</th>\n",
       "      <th>...</th>\n",
       "      <th>native-country_Portugal</th>\n",
       "      <th>native-country_Puerto-Rico</th>\n",
       "      <th>native-country_Scotland</th>\n",
       "      <th>native-country_South</th>\n",
       "      <th>native-country_Taiwan</th>\n",
       "      <th>native-country_Thailand</th>\n",
       "      <th>native-country_Trinadad&amp;Tobago</th>\n",
       "      <th>native-country_United-States</th>\n",
       "      <th>native-country_Vietnam</th>\n",
       "      <th>native-country_Yugoslavia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19863</th>\n",
       "      <td>53</td>\n",
       "      <td>168539</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24342</th>\n",
       "      <td>49</td>\n",
       "      <td>56841</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10027</th>\n",
       "      <td>28</td>\n",
       "      <td>154571</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25710</th>\n",
       "      <td>60</td>\n",
       "      <td>188236</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13824</th>\n",
       "      <td>53</td>\n",
       "      <td>87158</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       age  fnlwgt  education-num  capital-gain  capital-loss  hours-per-week  \\\n",
       "19863   53  168539              5             0             0              70   \n",
       "24342   49   56841             13             0             0              70   \n",
       "10027   28  154571             10             0             0              40   \n",
       "25710   60  188236              6             0             0              40   \n",
       "13824   53   87158              9             0             0              40   \n",
       "\n",
       "       workclass_Local-gov  workclass_Private  workclass_Self-emp-inc  \\\n",
       "19863                False              False                   False   \n",
       "24342                False              False                   False   \n",
       "10027                False               True                   False   \n",
       "25710                False               True                   False   \n",
       "13824                False               True                   False   \n",
       "\n",
       "       workclass_Self-emp-not-inc  ...  native-country_Portugal  \\\n",
       "19863                        True  ...                    False   \n",
       "24342                        True  ...                    False   \n",
       "10027                       False  ...                    False   \n",
       "25710                       False  ...                    False   \n",
       "13824                       False  ...                    False   \n",
       "\n",
       "       native-country_Puerto-Rico  native-country_Scotland  \\\n",
       "19863                       False                    False   \n",
       "24342                       False                    False   \n",
       "10027                       False                    False   \n",
       "25710                       False                    False   \n",
       "13824                       False                    False   \n",
       "\n",
       "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
       "19863                 False                  False                    False   \n",
       "24342                 False                  False                    False   \n",
       "10027                 False                  False                    False   \n",
       "25710                 False                  False                    False   \n",
       "13824                 False                  False                    False   \n",
       "\n",
       "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
       "19863                           False                          True   \n",
       "24342                           False                          True   \n",
       "10027                           False                         False   \n",
       "25710                           False                          True   \n",
       "13824                           False                          True   \n",
       "\n",
       "       native-country_Vietnam  native-country_Yugoslavia  \n",
       "19863                   False                      False  \n",
       "24342                   False                      False  \n",
       "10027                   False                      False  \n",
       "25710                   False                      False  \n",
       "13824                   False                      False  \n",
       "\n",
       "[5 rows x 96 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = training_data_loader()\n",
    "X_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks as we'd expect and the values are all in the right format. We\n",
    "can shift to training some models now! ðŸ¥³"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train Models\n",
    "\n",
    "Now that we have our data it makes sense to train some models to get a sense of\n",
    "how difficult the task is. The Census Income\n",
    "dataset is sufficiently large and complex that it's unlikely we'll be able to\n",
    "train a model that behaves perfectly since the problem is inherently complex,\n",
    "but we can get a sense of what a reasonable baseline looks like.\n",
    "\n",
    "We'll start with two simple models, a SGD Classifier and a Random Forest\n",
    "Classifier, both batteries-included from `sklearn`. We'll train them both on the\n",
    "same data and then compare their performance.\n",
    "\n",
    "Since we're starting our work properly, it makes sense to start tracking the\n",
    "experimentation that we're doing. ZenML integrates with MLflow to make this\n",
    "easy. This happens out of the box when using our experiment tracker integration\n",
    "and stack components. We'll see how this works below, but first let's set up\n",
    "ZenML to know that it should use the MLFlow experiment tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow experiment tracker\n",
    "!zenml experiment-tracker register mlflow --flavor=mlflow\n",
    "\n",
    "# Register a new stack with our experiment tracker\n",
    "!zenml stack register quickstart -a default\\\n",
    "                                 -o default\\\n",
    "                                 -e mlflow\n",
    "\n",
    "!zenml stack set quickstart"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now write the steps where we'll\n",
    "train our models, making sure to specify the name of our experiment tracker in\n",
    "the `@step` decorator. We could specify this manually using a string, but\n",
    "instead we'll use the ZenML `Client` to access the name of our active stack's\n",
    "experiment tracker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from zenml.client import Client\n",
    "\n",
    "experiment_tracker = Client().active_stack.experiment_tracker\n",
    "\n",
    "\n",
    "@step(experiment_tracker=experiment_tracker.name)\n",
    "def random_forest_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a sklearn Random Forest classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hyperparams and metrics to MLflow\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "@step(experiment_tracker=experiment_tracker.name)\n",
    "def sgd_trainer_mlflow(\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    ") -> ClassifierMixin:\n",
    "    \"\"\"Train a SGD classifier and log to MLflow.\"\"\"\n",
    "    mlflow.sklearn.autolog()  # log all model hyperparams and metrics to MLflow\n",
    "    model = SGDClassifier()\n",
    "    model.fit(X_train.to_numpy(), y_train.to_numpy())\n",
    "    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())\n",
    "    print(f\"Train accuracy: {train_acc}\")\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our two training steps both return different kinds of `sklearn` classifier\n",
    "models, so we use the generic `ClassifierMixin` type hint for the return type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end goal of this quick baseline evaluation is to understand which of the two\n",
    "models performs better. We'll use the `evaluator` step to compare the two\n",
    "models. This step takes in the two models we trained above, and compares them on\n",
    "the test data we created earlier. It returns whichever model performs best along\n",
    "with the accuracy score for that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "@step\n",
    "def best_model_selector(\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    model1: ClassifierMixin,\n",
    "    model2: ClassifierMixin,\n",
    ") -> Tuple[\n",
    "    Annotated[ClassifierMixin, \"best_model\"],\n",
    "    Annotated[float, \"best_model_test_acc\"],\n",
    "]:\n",
    "    \"\"\"Calculate the accuracy on the test set and return the best model and its accuracy.\"\"\"\n",
    "    test_acc1 = model1.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    test_acc2 = model2.score(X_test.to_numpy(), y_test.to_numpy())\n",
    "    print(f\"Test accuracy ({model1.__class__.__name__}): {test_acc1}\")\n",
    "    print(f\"Test accuracy ({model2.__class__.__name__}): {test_acc2}\")\n",
    "    if test_acc1 > test_acc2:\n",
    "        best_model = model1\n",
    "        best_model_test_acc = test_acc1\n",
    "    else:\n",
    "        best_model = model2\n",
    "        best_model_test_acc = test_acc2\n",
    "    return best_model, best_model_test_acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the use of the `typing` module's `Annotated` type hint in the output of the\n",
    "step. We're using this to give a name to the output of the step, which will make\n",
    "it possible to access it via a keyword later on."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll likely want to use our model in the future so instead of simply outputting\n",
    "the model we'll use the MLflow model registry to store it. This allows us to\n",
    "version the model for retrieval and use later on as well as to use other\n",
    "functionality made possible within the MLflow dashboard. This step is a bit\n",
    "different from the ones listed above in that we're using a pre-built ZenML step\n",
    "instead of just writing our own. You'll often come across these pre-built steps\n",
    "for common workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_registry import (\n",
    "    mlflow_register_model_step,\n",
    ")\n",
    "\n",
    "model_name = \"zenml-quickstart-model\"\n",
    "\n",
    "register_model = mlflow_register_model_step.with_options(\n",
    "    parameters=dict(\n",
    "        name=model_name,\n",
    "        description=\"The first run of the Quickstart pipeline.\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now at the point where can bring all these steps together into a single\n",
    "pipeline, the top-level organising entity for code in ZenML. Creating such a pipeline is\n",
    "as simple as adding a `@pipeline` decorator to a function. This specific\n",
    "pipeline doesn't return a value, but that option is available to you if you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml import pipeline\n",
    "\n",
    "\n",
    "@pipeline(enable_cache=True)\n",
    "def train_and_register_model_pipeline() -> None:\n",
    "    \"\"\"Train a model.\"\"\"\n",
    "    X_train, X_test, y_train, y_test = training_data_loader()\n",
    "    model1 = random_forest_trainer_mlflow(X_train=X_train, y_train=y_train)\n",
    "    model2 = sgd_trainer_mlflow(X_train=X_train, y_train=y_train)\n",
    "    best_model, _ = best_model_selector(\n",
    "        X_test=X_test, y_test=y_test, model1=model1, model2=model2\n",
    "    )\n",
    "    register_model(best_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've used the built-in MLflow registry to store our model, but ZenML doesn't\n",
    "yet know that we want to use the MLflow flavor of the model registry stack\n",
    "component in our stack. Let's add that now and update our stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow model registry\n",
    "!zenml model-registry register mlflow --flavor=mlflow\n",
    "\n",
    "# Update our stack to include the model registry\n",
    "!zenml stack update quickstart -r mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../_assets/local_stack_with_local_mlflow_tracker_and_registry.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're ready to run the pipeline now, which we can do just -- as with the step -- by calling the\n",
    "pipeline function itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered pipeline \u001b[0m\u001b[33mtrain_and_register_model_pipeline\u001b[1;35m (version 2).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33mtrain_and_register_model_pipeline\u001b[1;35m on stack \u001b[0m\u001b[33mquickstart\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has finished in 3.983s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrandom_forest_trainer_mlflow\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/19 11:21:05 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "2023/07/19 11:21:19 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/Users/strickvl/.pyenv/versions/3.10.11/envs/quickstart/lib/python3.10/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mrandom_forest_trainer_mlflow\u001b[1;35m has finished in 16.762s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33msgd_trainer_mlflow\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/19 11:21:22 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7857350076671226\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33msgd_trainer_mlflow\u001b[1;35m has finished in 8.787s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mbest_model_selector\u001b[1;35m has started.\u001b[0m\n",
      "Test accuracy (RandomForestClassifier): 0.8498259572352064\n",
      "Test accuracy (SGDClassifier): 0.7818664014586442\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mbest_model_selector\u001b[1;35m has finished in 0.889s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_register_model_step\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mMLflow model registry does not take a version as an argument. Registering a new version for the model \u001b[0m\u001b[33m'zenml-quickstart-model'\u001b[1;35m a version will be assigned automatically.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/07/19 11:21:34 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: zenml-quickstart-model, version 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered model zenml-quickstart-model with version 7 from source file:///Users/strickvl/Library/Application Support/zenml/local_stores/ad4aa15a-e692-4775-8b91-7a8a0c786061/mlruns/722610537823825016/c1acea16f5544bc0a434a1a49ba955d0/artifacts/model.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_register_model_step\u001b[1;35m has finished in 0.868s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mtrain_and_register_model_pipeline-2023_07_19-09_20_59_508727\u001b[1;35m has finished in 35.310s.\u001b[0m\n",
      "\u001b[1;35mDashboard URL: http://127.0.0.1:8237/workspaces/default/pipelines/3000e19f-aa8d-4075-ba35-ffd9d1773a1d/runs/8232cfe6-23eb-4557-98a8-fe0e5cf0f86f/dag\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "train_and_register_model_pipeline()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see from the logs already how our model training went: the\n",
    "`RandomForestClassifier` performed considerably better than the `SGDClassifier`,\n",
    "so that will have been the model that was returned from the evaluation step and\n",
    "then registered with the MLflow model registry."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you might be interested to view your pipeline in the ZenML\n",
    "Dashboard. You can spin this up by executing the next cell. This will start a\n",
    "server which you can access by clicking on the link that appears in the output\n",
    "of the cell.\n",
    "\n",
    "Log into the Dashboard using default credentials (username 'default' and\n",
    "password left blank). From there you can inspect the pipeline or the specific\n",
    "pipeline run. You can also examine the stack and components that we've\n",
    "registered to run everything.\n",
    "\n",
    "![](../llm_quickstart/_assets/zenml-up.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Environment.in_google_colab():\n",
    "    # run ZenML through a cloudflare tunnel to get a public endpoint\n",
    "    !zenml up --port 8237 & cloudflared tunnel --url http://localhost:8237\n",
    "else:\n",
    "    !zenml up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using MLflow for our experiment tracking. If you'd like to inspect the\n",
    "MLflow dashboard to see your experiments and what's been logged so far, run the\n",
    "following cell. This cell will spin up a local server that you can access via\n",
    "the link mentioned after the \"Listening at:\" `INFO` log statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zenml.integrations.mlflow.mlflow_utils import get_tracking_uri\n",
    "\n",
    "os.environ[\"MLFLOW_TRACKING_URI\"] = get_tracking_uri()\n",
    "\n",
    "if Environment.in_google_colab():\n",
    "    # run mlflow through a cloudflare tunnel to get a public endpoint\n",
    "    !mlflow ui --backend-store-uri $MLFLOW_TRACKING_URI & cloudflared tunnel --url http://localhost:5000\n",
    "else:\n",
    "    !mlflow ui --backend-store-uri $MLFLOW_TRACKING_URI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our pipeline above registered the best model with the MLflow model registry.\n",
    "Whenever you register a model it also versions the model since it's likely that\n",
    "you'll be iterating and improving your model over time.\n",
    "\n",
    "We'll now turn to actually deploying our model and serving some predictions, for\n",
    "which we'll need to specify the model version we want to use. You can specify\n",
    "the version number manually but below we'll use the ZenML `Client` to get the\n",
    "latest version number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from zenml.client import Client\n",
    "\n",
    "most_recent_model_version_number = int(\n",
    "    Client()\n",
    "    .active_stack.model_registry.list_model_versions(metadata={})[0]\n",
    "    .version\n",
    ")\n",
    "most_recent_model_version_number"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've trained our model, and we've found the best one, we want to deploy it\n",
    "and run some inference on the deployed model. We'll use the local MLflow model\n",
    "deployer which once again comes with some pre-built ZenML steps to save you reinventing the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.integrations.mlflow.steps.mlflow_deployer import (\n",
    "    mlflow_model_registry_deployer_step,\n",
    ")\n",
    "\n",
    "model_deployer = mlflow_model_registry_deployer_step.with_options(\n",
    "    parameters=dict(\n",
    "        registry_model_name=model_name,\n",
    "        registry_model_version=most_recent_model_version_number,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you deploy a model this is usually something you want to remain available\n",
    "and running for a long time, so ZenML automatically creates a background service\n",
    "for your deployed model. We load the service (already created by the\n",
    "`model_deployer` step) and then use it to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zenml.services import BaseService\n",
    "from zenml.client import Client\n",
    "\n",
    "\n",
    "@step(enable_cache=False)\n",
    "def prediction_service_loader() -> BaseService:\n",
    "    \"\"\"Load the model service of our train_and_register_model_pipeline.\"\"\"\n",
    "    client = Client()\n",
    "    model_deployer = client.active_stack.model_deployer\n",
    "    services = model_deployer.find_model_server(\n",
    "        pipeline_name=\"train_and_register_model_pipeline\",\n",
    "        running=True,\n",
    "    )\n",
    "    return services[0]\n",
    "\n",
    "\n",
    "@step\n",
    "def predictor(\n",
    "    service: BaseService,\n",
    "    data: pd.DataFrame,\n",
    ") -> Annotated[list, \"predictions\"]:\n",
    "    \"\"\"Run a inference request against a prediction service.\"\"\"\n",
    "    service.start(timeout=10)  # should be a NOP if already started\n",
    "    print(f\"Running predictions on data (single individual): {data.to_numpy()[0]}\")\n",
    "    prediction = service.predict(data.to_numpy())\n",
    "    print(f\"Prediction (for single example slice) is: {bool(prediction.tolist()[0])}\")\n",
    "    return prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the MLflow model deployer\n",
    "!zenml model-deployer register mlflow --flavor=mlflow\n",
    "\n",
    "# Register a new stack with the new stack components\n",
    "!zenml stack update quickstart -d mlflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again there are some dependencies in terms of how the step needs to run, so\n",
    "we specify those upfront. For example, the prediction service needs to be loaded\n",
    "before we try to make predictions with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline\n",
    "def deploy_and_predict() -> None:\n",
    "    \"\"\"Deploy the best model and run some predictions.\"\"\"\n",
    "    prediction_service_loader.after(model_deployer)\n",
    "\n",
    "    model_deployer()\n",
    "    _, inference_data, _, _ = training_data_loader()\n",
    "    model_deployment_service = prediction_service_loader()\n",
    "    predictor(service=model_deployment_service, data=inference_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we specify that we want the `prediction_service_loader` step to run *after* the\n",
    "model_deployer step. This is because we won't have a model ready for prediction\n",
    "until the deployment has taken place. ZenML automatically tries to run steps in\n",
    "parallel, so sometimes if you have this kind of sequencing you need to do then\n",
    "you'll need to specify it explicitly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../_assets/local_stack_with_local_mlflow_tracker_and_registry_and_deployer.png)\n",
    "\n",
    "Unlike in the previous case where we just ran the pipeline directly, we might\n",
    "not want to deploy the model every time. Consider the case where our models are\n",
    "returning values under 50% accuracy on the test data. In that case we might want\n",
    "to address the issues with accuracy and not spin up a deployment at all. We can\n",
    "access the artifacts associated with the previous pipeline run and check the\n",
    "test accuracy metric to see if it's above a certain threshold. Adding this to\n",
    "our workflow is as simple as adding a conditional step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mRegistered pipeline \u001b[0m\u001b[33mdeploy_and_predict\u001b[1;35m (version 2).\u001b[0m\n",
      "\u001b[1;35mRunning pipeline \u001b[0m\u001b[33mdeploy_and_predict\u001b[1;35m on stack \u001b[0m\u001b[33mquickstart\u001b[1;35m (caching enabled)\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_model_registry_deployer_step\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUpdating an existing MLflow deployment service: MLFlowDeploymentService[030f4bbf-8fdc-4827-a660-ca4fc6b325e1] (type: model-serving, flavor: mlflow)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e3e28723d24286b064f450a0e1a257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h\r\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[1A\u001b[2K"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mMLflow deployment service started and reachable at:\n",
      "    http://127.0.0.1:8006/invocations\n",
      "\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mmlflow_model_registry_deployer_step\u001b[1;35m has finished in 11.802s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mUsing cached version of \u001b[0m\u001b[33mtraining_data_loader\u001b[1;35m.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has started.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mprediction_service_loader\u001b[1;35m has finished in 0.308s.\u001b[0m\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has started.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": [
       "\u001b[?25l"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "\u001b[?25h</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[?25h"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running predictions on data (single individual): [28 76714 15 0 0 55 False True False False False False False False False\n",
      " False False False False False False False False False False True False\n",
      " False False False True False False False False False False False False\n",
      " False False True False False False False True False False False False\n",
      " False False False True True False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False True False False]\n",
      "Prediction (for single example slice) is: False\n",
      "\u001b[1;35mStep \u001b[0m\u001b[33mpredictor\u001b[1;35m has finished in 0.453s.\u001b[0m\n",
      "\u001b[1;35mPipeline run \u001b[0m\u001b[33mdeploy_and_predict-2023_07_19-09_22_08_384753\u001b[1;35m has finished in 14.505s.\u001b[0m\n",
      "\u001b[1;35mDashboard URL: http://127.0.0.1:8237/workspaces/default/pipelines/abd070b3-5a2c-457e-8555-d796f26a0db7/runs/059954ac-9c23-4204-9680-c1f2621dbaab/dag\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "best_model_test_accuracy = (\n",
    "    Client().get_pipeline(\"train_and_register_model_pipeline\")\n",
    "    .last_successful_run.steps[\"best_model_selector\"]\n",
    "    .outputs[\"best_model_test_acc\"].load()\n",
    ")\n",
    "\n",
    "if best_model_test_accuracy > 0.7:\n",
    "    deploy_and_predict()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall, the purpose of this model was to predict whether or not someone\n",
    "earns more than $50,000 USD per year. You can see a single example in the output above.\n",
    "Given the features of a particular individual, the model predicts that they do\n",
    "not earn more than $50k per year.\n",
    "\n",
    "If we were interested in learning more about the model's predictions, we could\n",
    "separately load the predictor service and use it to pass in some other data or\n",
    "try things out. To load the predictor we can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLFlowDeploymentService[030f4bbf-8fdc-4827-a660-ca4fc6b325e1] (type: model-serving, flavor: mlflow)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_service = deploy_and_predict.model.last_successful_run.steps[\n",
    "    \"prediction_service_loader\"\n",
    "].output.load()\n",
    "\n",
    "predictor_service"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, passing in some data is as simple as calling the `predict` method\n",
    "on the predictor service. We can try this here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model predictions: [0 0 0 0 0 0 0 1 0 0]\n",
      "Ground truth:      [1 1 1 0 0 1 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    f\"Model predictions: {predictor_service.predict(X_test.to_numpy()[25:35])}\"\n",
    ")\n",
    "print(f\"Ground truth:      {y_test.to_numpy()[25:35]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're passing in some of our test data into the model and getting back the\n",
    "predictions. You can already start to see some of the places where our\n",
    "predictions are not matching the ground truth labels. This is to be expected but\n",
    "we could potentially use this to now iterate on our models by adding more steps.\n",
    "\n",
    "To get an overview of the models and model versions that we have registered and\n",
    "deployed so\n",
    "far, we can use the CLI to list these out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-registry models list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-registry models list-versions zenml-quickstart-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zenml model-deployer models list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view all this on the ZenML Dashboard, simply spin up the server again and\n",
    "view the steps via the DAG visualiser and also browse the artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if Environment.in_google_colab():\n",
    "    # run ZenML through a cloudflare tunnel to get a public endpoint\n",
    "    !zenml up --port 8237 & cloudflared tunnel --url http://localhost:8237\n",
    "else:\n",
    "    !zenml up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations!\n",
    "\n",
    "You just built two ML pipelines! You trained two models, evaluated them against\n",
    "a test set, registered the best one with the MLflow model registry, deployed it\n",
    "and served some predictions. You also learned how to iterate on your models and\n",
    "data by using some of the ZenML utility abstractions. You saw how to view your\n",
    "artifacts and stacks via the CLI as well as the ZenML Dashboard.\n",
    "\n",
    "And that is just the tip of the iceberg of what ZenML can do; check out the [**Integrations**](https://zenml.io/integrations) page for a list of all the cool MLOps tools that ZenML supports!\n",
    "\n",
    "## What to do now\n",
    "\n",
    "* If you have questions or feedback... join our [**Slack Community**](https://zenml.io/slack-invite) and become part of the ZenML family!\n",
    "* If you want to try ZenML in a real-world setting... check out the [ZenML Cloud](https://cloud.zenml.io/), a free trial of\n",
    "    ZenML's managed offering that runs on your Cloud platform. [**Sign up here**](https://sandbox.zenml.io/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
