File: docs/book/how-to/debug-and-solve-issues.md

# Debugging Guide for ZenML

This guide provides best practices for debugging common issues with ZenML and obtaining help.

## When to Get Help
Before seeking assistance, follow this checklist:
- Search Slack using the built-in search function.
- Look for answers in [GitHub issues](https://github.com/zenml-io/zenml/issues).
- Use the search bar in the [ZenML documentation](https://docs.zenml.io).
- Review the [common errors](debug-and-solve-issues.md#most-common-errors) section.
- Analyze [additional logs](debug-and-solve-issues.md#41-additional-logs) and [client/server logs](debug-and-solve-issues.md#client-and-server-logs).

If unresolved, post your question on [Slack](https://zenml.io/slack).

## How to Post on Slack
Include the following information in your post:

### 1. System Information
Run the command below and attach the output:
```shell
zenml info -a -s
```
For specific package issues, use:
```shell
zenml info -p <package_name>
```

### 2. What Happened?
Briefly describe:
- Your goal
- Expected outcome
- Actual outcome

### 3. How to Reproduce the Error?
Provide step-by-step instructions or a video to reproduce the issue.

### 4. Relevant Log Output
Attach relevant logs and the full error traceback. If lengthy, use services like [Pastebin](https://pastebin.com/) or [GitHub's Gist](https://gist.github.com/). Always include outputs from:
- `zenml status`
- `zenml stack describe`

For orchestrator logs, include the relevant pod logs if applicable.

### 4.1 Additional Logs
If default logs are insufficient, change the verbosity level:
```shell
export ZENML_LOGGING_VERBOSITY=DEBUG
```
Refer to documentation for setting environment variables on [Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/), [macOS](https://youngstone89.medium.com/setting-up-environment-variables-in-mac-os-28e5941c771c), and [Windows](https://www.computerhope.com/issues/ch000549.htm).

### Client and Server Logs
For server-related issues, view logs with:
```shell
zenml logs
```

## Most Common Errors
### Error initializing rest store
Occurs as:
```bash
RuntimeError: Error initializing rest store with URL 'http://127.0.0.1:8237': HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded...
```
**Solution:** Re-run `zenml login --local` after restarting your machine.

### Column 'step_configuration' cannot be null
Error message:
```bash
sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1048, "Column 'step_configuration' cannot be null")
```
**Solution:** Ensure step configurations are within the character limit.

### 'NoneType' object has no attribute 'name'
Example error:
```shell
AttributeError: 'NoneType' object has no attribute 'name'
```
**Solution:** Register the required stack components, e.g.:
```shell
zenml experiment-tracker register mlflow_tracker --flavor=mlflow
zenml stack update -e mlflow_tracker
```

This guide aims to streamline the debugging process for ZenML users by providing essential troubleshooting steps and common error resolutions.

================================================================================

File: docs/book/how-to/pipeline-development/README.md

# Pipeline Development in ZenML

This section details the key components and processes involved in developing pipelines using ZenML.

## Key Concepts

1. **Pipelines**: A pipeline is a sequence of steps that define the workflow for data processing and model training.

2. **Steps**: Individual tasks within a pipeline, such as data ingestion, preprocessing, model training, and evaluation.

3. **Components**: Reusable building blocks for steps, which can include custom code or existing libraries.

## Development Process

1. **Define Pipeline**: Use the `@pipeline` decorator to create a pipeline function.
   ```python
   from zenml.pipelines import pipeline

   @pipeline
   def my_pipeline():
       step1()
       step2()
   ```

2. **Create Steps**: Define steps using the `@step` decorator.
   ```python
   from zenml.steps import step

   @step
   def step1():
       # Step 1 logic

   @step
   def step2():
       # Step 2 logic
   ```

3. **Run Pipeline**: Execute the pipeline using the `run` method.
   ```python
   my_pipeline.run()
   ```

## Configuration

- **Parameters**: Pass parameters to steps for customization.
- **Artifacts**: Manage input and output data between steps using artifacts.

## Best Practices

- Modularize steps for reusability.
- Use version control for pipeline code.
- Test individual steps before integrating into the pipeline.

This summary encapsulates the essential aspects of pipeline development in ZenML, focusing on the structure, creation, and execution of pipelines while highlighting best practices.

================================================================================

File: docs/book/how-to/pipeline-development/run-remote-notebooks/limitations-of-defining-steps-in-notebook-cells.md

# Limitations of Defining Steps in Notebook Cells

To run ZenML steps defined in notebook cells remotely (with a remote orchestrator or step operator), the following conditions must be met:

- The cell can only contain Python code; Jupyter magic commands or shell commands (starting with `%` or `!`) are not allowed.
- The cell **must not** call code from other notebook cells. However, functions or classes imported from Python files are permitted.
- The cell **must not** rely on imports from previous cells; it must perform all necessary imports itself, including ZenML imports (e.g., `from zenml import step`).

================================================================================

File: docs/book/how-to/pipeline-development/run-remote-notebooks/README.md

### Summary: Running Remote Pipelines from Jupyter Notebooks

ZenML allows the definition and execution of steps and pipelines within Jupyter Notebooks, running them remotely. The code from notebook cells is extracted and executed as Python modules in Docker containers.

#### Key Points:
- **Execution Environment**: Steps defined in notebooks are executed remotely in Docker containers.
- **Cell Requirements**: Specific conditions must be met for notebook cells containing step definitions.
  
#### Additional Resources:
- **Limitations**: Refer to [Limitations of Defining Steps in Notebook Cells](limitations-of-defining-steps-in-notebook-cells.md).
- **Single Step Execution**: See [Run a Single Step from a Notebook](run-a-single-step-from-a-notebook.md).

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/pipeline-development/run-remote-notebooks/run-a-single-step-from-a-notebook.md

### Summary of Running a Single Step in ZenML

To run a single step from a notebook using ZenML, you can invoke the step like a regular Python function. ZenML will create a pipeline with that step and execute it on the active stack. Be mindful of the [limitations](limitations-of-defining-steps-in-notebook-cells.md) when defining steps in notebook cells.

#### Example Code

```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from typing import Tuple
from typing_extensions import Annotated

@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    gamma: float = 0.001,
) -> Tuple[
    Annotated[ClassifierMixin, "trained_model"],
    Annotated[float, "training_acc"],
]:
    """Train a sklearn SVC classifier."""
    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())
    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
    print(f"Train accuracy: {train_acc}")
    return model, train_acc

X_train = pd.DataFrame(...)  # Define your training data
y_train = pd.Series(...)      # Define your training labels

# Execute the step
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

### Key Points
- Use the `@step` decorator to define a step.
- The step can be executed directly in a notebook, creating a pipeline automatically.
- Ensure to handle limitations specific to notebook environments.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/what-can-be-configured.md

# Configuration Overview

This documentation provides a sample YAML configuration file for a ZenML pipeline, highlighting key settings and parameters. For a comprehensive list of all possible keys, refer to the linked page.

## Sample YAML Configuration

```yaml
build: dcd6fafb-c200-4e85-8328-428bef98d804

enable_artifact_metadata: True
enable_artifact_visualization: False
enable_cache: False
enable_step_logs: True

extra: 
  any_param: 1
  another_random_key: "some_string"

model:
  name: "classification_model"
  version: production
  audience: "Data scientists"
  description: "This classifies hotdogs and not hotdogs"
  ethics: "No ethical implications"
  license: "Apache 2.0"
  limitations: "Only works for hotdogs"
  tags: ["sklearn", "hotdog", "classification"]

parameters: 
  dataset_name: "another_dataset"

run_name: "my_great_run"

schedule:
  catchup: true
  cron_expression: "* * * * *"

settings:
  docker:
    apt_packages: ["curl"]
    copy_files: True
    dockerfile: "Dockerfile"
    dockerignore: ".dockerignore"
    environment:
      ZENML_LOGGING_VERBOSITY: DEBUG
    parent_image: "zenml-io/zenml-cuda"
    requirements: ["torch"]
    skip_build: False
  
  resources:
    cpu_count: 2
    gpu_count: 1
    memory: "4Gb"

steps:
  train_model:
    parameters:
      data_source: "best_dataset"
    experiment_tracker: "mlflow_production"
    step_operator: "vertex_gpu"
    outputs: {}
    failure_hook_source: {}
    success_hook_source: {}
    enable_artifact_metadata: True
    enable_artifact_visualization: True
    enable_cache: False
    enable_step_logs: True
    extra: {}
    model: {}
    settings:
      docker: {}
      resources: {}
      step_operator.sagemaker:
        estimator_args:
          instance_type: m7g.medium
```

## Key Configuration Sections

### `enable_XXX` Parameters
Boolean flags control various behaviors:
- `enable_artifact_metadata`: Attach metadata to artifacts.
- `enable_artifact_visualization`: Attach visualizations of artifacts.
- `enable_cache`: Use caching.
- `enable_step_logs`: Enable step logs.

### `build` ID
Specifies the UUID of the Docker image to use. If provided, Docker image building is skipped for remote orchestrators.

### Configuring the `model`
Defines the ZenML model for the pipeline:

```yaml
model:
  name: "ModelName"
  version: "production"
  description: An example model
  tags: ["classifier"]
```

### Pipeline and Step `parameters`
Parameters are JSON-serializable values defined at the pipeline or step level:

```yaml
parameters:
    gamma: 0.01

steps:
    trainer:
        parameters:
            gamma: 0.001
```

### Setting the `run_name`
To change the run name, use:

```yaml
run_name: <INSERT_RUN_NAME_HERE>
```
*Note: Avoid static names for scheduled runs to prevent conflicts.*

### Stack Component Runtime Settings
Settings for Docker and resource configurations:

```yaml
settings:
  docker:
    requirements:
      - pandas
  resources:
    cpu_count: 2
    gpu_count: 1
    memory: "4Gb"
```

### Step-Specific Configuration
Certain configurations apply only at the step level, such as:
- `experiment_tracker`: Name of the experiment tracker.
- `step_operator`: Name of the step operator.
- `outputs`: Configuration of output artifacts.

### Hooks
Specify `failure_hook_source` and `success_hook_source` for handling step outcomes.

This summary encapsulates the essential configuration details needed for understanding and implementing a ZenML pipeline.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/README.md

ZenML allows for easy configuration and execution of pipelines using YAML files. These files enable runtime configuration of parameters, caching behavior, and stack components. Key topics include:

- **What can be configured**: Details on configurable elements.
- **Configuration hierarchy**: Structure of configuration settings.
- **Autogenerate a template YAML file**: Instructions for generating a template.

For further details, refer to the linked sections:
- [What can be configured](what-can-be-configured.md)
- [Configuration hierarchy](configuration-hierarchy.md)
- [Autogenerate a template YAML file](autogenerate-a-template-yaml-file.md)

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/autogenerate-a-template-yaml-file.md

### Summary of Documentation on Autogenerating a YAML Configuration Template

To create a YAML configuration template for a specific pipeline, use the `.write_run_configuration_template()` method. This method generates a YAML file with all options commented out, allowing you to select the relevant settings.

#### Code Example
```python
from zenml import pipeline

@pipeline(enable_cache=True)
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter=parameter)
    train_model(dataset)

simple_ml_pipeline.write_run_configuration_template(path="<Insert_path_here>")
```

#### Generated YAML Configuration Template Structure
The generated YAML configuration template includes the following key sections:

- **build**: Configuration for the pipeline build.
- **enable_artifact_metadata**: Optional boolean for artifact metadata.
- **model**: Contains model attributes such as `name`, `description`, and `version`.
- **parameters**: Optional mapping for parameters.
- **schedule**: Configuration for scheduling the pipeline runs.
- **settings**: Includes Docker settings and resource specifications (CPU, GPU, memory).
- **steps**: Configuration for each step in the pipeline (e.g., `load_data`, `train_model`), including settings, parameters, and outputs.

#### Example of Step Configuration
Each step can have settings for:
- **enable_artifact_metadata**
- **model**: Similar attributes as in the model section.
- **settings**: Docker and resource configurations.
- **outputs**: Defines the outputs of the step.

#### Additional Configuration
You can also specify a stack while generating the template using:
```python
simple_ml_pipeline.write_run_configuration_template(stack=<Insert_stack_here>)
```

This concise overview captures the essential details of the documentation while maintaining clarity and technical accuracy.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/runtime-configuration.md

### Summary of ZenML Settings Configuration

**Overview**: ZenML allows runtime configuration of stack components and pipelines through `Settings`, which are managed via the `BaseSettings` concept. 

**Key Configuration Areas**:
- **Resource Requirements**: Define resources needed for pipeline steps.
- **Containerization**: Customize Docker image requirements.
- **Component-Specific Configurations**: Pass runtime parameters, such as experiment names for trackers.

### Types of Settings
1. **General Settings**: Applicable to all pipelines.
   - `DockerSettings`: Docker configuration.
   - `ResourceSettings`: Resource specifications.

2. **Stack-Component-Specific Settings**: Tailored for specific stack components, identified by keys like `<COMPONENT_CATEGORY>` or `<COMPONENT_CATEGORY>.<COMPONENT_FLAVOR>`. Examples include:
   - `SkypilotAWSOrchestratorSettings`
   - `KubeflowOrchestratorSettings`
   - `MLflowExperimentTrackerSettings`
   - `WandbExperimentTrackerSettings`
   - `WhylogsDataValidatorSettings`
   - `SagemakerStepOperatorSettings`
   - `VertexStepOperatorSettings`
   - `AzureMLStepOperatorSettings`

### Registration vs. Runtime Settings
- **Registration-Time Settings**: Static configurations that remain constant across pipeline runs (e.g., `tracking_url` for MLflow).
- **Runtime Settings**: Dynamic configurations that can change with each pipeline execution (e.g., `experiment_name`).

Default values can be set during registration, which can be overridden at runtime.

### Specifying Settings
When defining stack-component-specific settings, use the correct key format:
- `<COMPONENT_CATEGORY>` (e.g., `step_operator`)
- `<COMPONENT_CATEGORY>.<COMPONENT_FLAVOR>`

If the specified settings do not match the active component flavor, they will be ignored.

### Example Code Snippets

**Python Code**:
```python
@step(step_operator="nameofstepoperator", settings={"step_operator": {"estimator_args": {"instance_type": "m7g.medium"}}})
def my_step():
    ...

@step(step_operator="nameofstepoperator", settings={"step_operator": SagemakerStepOperatorSettings(instance_type="m7g.medium")})
def my_step():
    ...
```

**YAML Configuration**:
```yaml
steps:
  my_step:
    step_operator: "nameofstepoperator"
    settings:
      step_operator:
        estimator_args:
          instance_type: m7g.medium
```

This summary encapsulates the essential information regarding ZenML settings configuration, providing a clear understanding of its structure and usage.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/retrieve-used-configuration-of-a-run.md

To extract the configuration used for a completed pipeline run, you can access the `config` attribute of the pipeline run or a specific step within it. 

### Code Example:
```python
from zenml.client import Client

pipeline_run = Client().get_pipeline_run(<PIPELINE_RUN_NAME>)

# Access general pipeline configuration
pipeline_run.config

# Access configuration for a specific step
pipeline_run.steps[<STEP_NAME>].config
```

This allows you to retrieve both the overall configuration and the configuration for individual steps in the pipeline.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/how-to-use-config.md

### Configuration Files in ZenML

**Overview**: 
Using a YAML configuration file is recommended for separating configuration from code in ZenML. Configuration can also be specified directly in code, but YAML files enhance clarity and maintainability.

**Configuration Example**:
A minimal YAML configuration file might look like this:

```yaml
enable_cache: False

parameters:
  dataset_name: "best_dataset"  
  
steps:
  load_data:
    enable_cache: False
```

**Python Code Example**:
To apply the configuration in a pipeline, use the following Python code:

```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def simple_ml_pipeline(dataset_name: str):
    load_data(dataset_name)
    
if __name__ == "__main__":
    simple_ml_pipeline.with_options(config_path=<INSERT_PATH_TO_CONFIG_YAML>)()
```

**Functionality**:
This setup runs `simple_ml_pipeline` with caching disabled for the `load_data` step and sets the `dataset_name` parameter to `best_dataset`.

================================================================================

File: docs/book/how-to/pipeline-development/use-configuration-files/configuration-hierarchy.md

### Configuration Hierarchy in ZenML

In ZenML, configuration settings follow a specific hierarchy:

- **Code Configurations**: Override YAML file configurations.
- **Step-Level Configurations**: Override pipeline-level configurations.
- **Attribute Merging**: Dictionaries are merged for attributes.

### Example Code

```python
from zenml import pipeline, step
from zenml.config import ResourceSettings

@step
def load_data(parameter: int) -> dict:
    ...

@step(settings={"resources": ResourceSettings(gpu_count=1, memory="2GB")})
def train_model(data: dict) -> None:
    ...

@pipeline(settings={"resources": ResourceSettings(cpu_count=2, memory="1GB")}) 
def simple_ml_pipeline(parameter: int):
    ...

# Merged configurations
train_model.configuration.settings["resources"]
# -> cpu_count: 2, gpu_count=1, memory="2GB"

simple_ml_pipeline.configuration.settings["resources"]
# -> cpu_count: 2, memory="1GB"
```

### Key Points
- Step configurations take precedence over pipeline configurations.
- Resource settings can be defined at both the step and pipeline levels, with step settings overriding pipeline settings when applicable.

================================================================================

File: docs/book/how-to/pipeline-development/develop-locally/local-prod-pipeline-variants.md

### Summary: Creating Pipeline Variants for Local Development and Production in ZenML

When developing ZenML pipelines, it's useful to create different variants for local development and production. This allows for rapid iteration during development while maintaining a robust setup for production. Variants can be created using:

1. **Configuration Files**
2. **Code Implementation**
3. **Environment Variables**

#### 1. Using Configuration Files
ZenML supports YAML configuration files for pipeline and step settings. Example configuration for development:

```yaml
enable_cache: False
parameters:
    dataset_name: "small_dataset"
steps:
    load_data:
        enable_cache: False
```

To apply this configuration:

```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def ml_pipeline(dataset_name: str):
    load_data(dataset_name)

if __name__ == "__main__":
    ml_pipeline.with_options(config_path="path/to/config.yaml")()
```

You can maintain separate files like `config_dev.yaml` for development and `config_prod.yaml` for production.

#### 2. Implementing Variants in Code
You can define pipeline variants directly in your code:

```python
import os
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "small_dataset" if is_dev else "full_dataset"
    load_data(dataset)

if __name__ == "__main__":
    is_dev = os.environ.get("ZENML_ENVIRONMENT") == "dev"
    ml_pipeline(is_dev=is_dev)
```

This allows toggling between variants using a boolean flag.

#### 3. Using Environment Variables
Environment variables can dictate which variant to run:

```python
import os

config_path = "config_dev.yaml" if os.environ.get("ZENML_ENVIRONMENT") == "dev" else "config_prod.yaml"
ml_pipeline.with_options(config_path=config_path)()
```

Run the pipeline with:
```bash
ZENML_ENVIRONMENT=dev python run.py
```
or
```bash
ZENML_ENVIRONMENT=prod python run.py
```

### Development Variant Considerations
For development, optimize for faster iteration by:
- Using smaller datasets
- Specifying a local execution stack
- Reducing training epochs and batch size
- Using smaller base models

Example configuration for development:

```yaml
parameters:
    dataset_path: "data/small_dataset.csv"
epochs: 1
batch_size: 16
stack: local_stack
```

Or in code:

```python
@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "data/small_dataset.csv" if is_dev else "data/full_dataset.csv"
    epochs = 1 if is_dev else 100
    batch_size = 16 if is_dev else 64
    
    load_data(dataset)
    train_model(epochs=epochs, batch_size=batch_size)
```

Creating different pipeline variants enables efficient local testing and debugging while maintaining a comprehensive setup for production, enhancing the development workflow.

================================================================================

File: docs/book/how-to/pipeline-development/develop-locally/README.md

# Develop Locally

This section outlines best practices for developing pipelines locally, enabling faster iteration and cost-effective execution. It is common to use a smaller subset of data or synthetic data for local development. ZenML supports this workflow, allowing users to develop locally and then transition to running pipelines on more powerful remote hardware when necessary. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/pipeline-development/develop-locally/keep-your-dashboard-server-clean.md

### Summary of ZenML Pipeline Cleanliness Documentation

#### Overview
This documentation provides guidance on maintaining a clean development environment for ZenML pipelines, minimizing clutter in the dashboard and server during iterative runs.

#### Key Options for Cleanliness

1. **Run Locally**: 
   - To avoid server clutter, disconnect from the remote server and run a local server:
     ```bash
     zenml login --local
     ```
   - Reconnect with:
     ```bash
     zenml login <remote-url>
     ```

2. **Unlisted Runs**: 
   - Create pipeline runs without associating them with a pipeline:
     ```python
     pipeline_instance.run(unlisted=True)
     ```
   - These runs won't appear on the pipeline's dashboard page.

3. **Deleting Pipeline Runs**:
   - Delete a specific run:
     ```bash
     zenml pipeline runs delete <PIPELINE_RUN_NAME_OR_ID>
     ```
   - Delete all runs from the last 24 hours:
     ```python
     #!/usr/bin/env python3
     import datetime
     from zenml.client import Client

     def delete_recent_pipeline_runs():
         zc = Client()
         time_filter = (datetime.datetime.utcnow() - datetime.timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")
         recent_runs = zc.list_pipeline_runs(created=f"gt:{time_filter}")
         for run in recent_runs:
             zc.delete_pipeline_run(run.id)
         print(f"Deleted {len(recent_runs)} pipeline runs.")

     if __name__ == "__main__":
         delete_recent_pipeline_runs()
     ```

4. **Deleting Pipelines**:
   - Remove unnecessary pipelines:
     ```bash
     zenml pipeline delete <PIPELINE_ID_OR_NAME>
     ```

5. **Unique Pipeline Names**:
   - Assign custom names to runs for differentiation:
     ```python
     training_pipeline = training_pipeline.with_options(run_name="custom_pipeline_run_name")
     training_pipeline()
     ```

6. **Model Management**:
   - Delete a model:
     ```bash
     zenml model delete <MODEL_NAME>
     ```

7. **Artifact Management**:
   - Prune unreferenced artifacts:
     ```bash
     zenml artifact prune
     ```

8. **Cleaning Environment**:
   - Use `zenml clean` to remove all local pipelines, runs, and artifacts:
     ```bash
     zenml clean --local
     ```

By following these practices, users can maintain an organized pipeline dashboard, focusing on relevant runs for their projects.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/schedule-a-pipeline.md

### Summary: Scheduling Pipelines in ZenML

#### Supported Orchestrators
Not all orchestrators support scheduling. The following orchestrators do support it:
- **Supported**: Airflow, AzureML, Databricks, HyperAI, Kubeflow, Kubernetes, Vertex.
- **Not Supported**: Local, LocalDocker, Sagemaker, Skypilot (all variants), Tekton.

#### Setting a Schedule
To set a schedule for a pipeline, you can use either cron expressions or human-readable notations. 

**Example Code:**
```python
from zenml.config.schedule import Schedule
from zenml import pipeline
from datetime import datetime

@pipeline()
def my_pipeline(...):
    ...

# Using cron expression
schedule = Schedule(cron_expression="5 14 * * 3")
# Using human-readable notation
schedule = Schedule(start_time=datetime.now(), interval_second=1800)

my_pipeline = my_pipeline.with_options(schedule=schedule)
my_pipeline()
```

For more scheduling options, refer to the [SDK documentation](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.schedule.Schedule).

#### Pausing/Stopping a Schedule
The method to pause or stop a scheduled pipeline varies by orchestrator. For instance, in Kubeflow, you can use the UI for this purpose. Users must consult their orchestrator's documentation for specific instructions. 

**Important Note**: ZenML schedules the run, but users are responsible for managing the lifecycle of the schedule. Running a pipeline with a schedule multiple times creates separate scheduled pipelines with unique names. 

#### Additional Resources
For more information on orchestrators, see [orchestrators.md](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/delete-a-pipeline.md

### Summary of Pipeline Deletion Documentation

#### Deleting a Pipeline
You can delete a pipeline using either the CLI or the Python SDK.

**CLI Command:**
```shell
zenml pipeline delete <PIPELINE_NAME>
```

**Python SDK:**
```python
from zenml.client import Client

Client().delete_pipeline(<PIPELINE_NAME>)
```

**Note:** Deleting a pipeline does not remove associated runs or artifacts.

To delete multiple pipelines, especially those with the same prefix, use the following script:

```python
from zenml.client import Client

client = Client()
pipelines_list = client.list_pipelines(name="startswith:test_pipeline", size=100)
target_pipeline_ids = [p.id for p in pipelines_list.items]

if input("Do you really want to delete these pipelines? (y/n): ").lower() == 'y':
    for pid in target_pipeline_ids:
        client.delete_pipeline(pid)
```

#### Deleting a Pipeline Run
You can delete a pipeline run using the CLI or the Python SDK.

**CLI Command:**
```shell
zenml pipeline runs delete <RUN_NAME_OR_ID>
```

**Python SDK:**
```python
from zenml.client import Client

Client().delete_pipeline_run(<RUN_NAME_OR_ID>)
```

This documentation provides the necessary commands and scripts for effectively deleting pipelines and their runs using ZenML.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/configuring-a-pipeline-at-runtime.md

### Runtime Configuration of a Pipeline

To run a pipeline with a different configuration, use the `pipeline.with_options` method. You can configure options in two ways:

1. Explicitly, e.g., `with_options(steps={"trainer": {"parameters": {"param1": 1}}})`
2. By passing a YAML file: `with_options(config_file="path_to_yaml_file")`

For more details on these options, refer to the [configuration files documentation](../../pipeline-development/use-configuration-files/README.md).

**Note:** To trigger a pipeline from a client or another pipeline, use the `PipelineRunConfiguration` object. More information can be found [here](../../pipeline-development/trigger-pipelines/use-templates-python.md#advanced-usage-run-a-template-from-another-pipeline).

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/compose-pipelines.md

### Summary of ZenML Pipeline Composition

ZenML enables the reuse of steps between pipelines by allowing the composition of pipelines. This helps avoid code duplication by extracting common functionality into separate functions.

#### Example Code

```python
from zenml import pipeline

@pipeline
def data_loading_pipeline(mode: str):
    data = training_data_loader_step() if mode == "train" else test_data_loader_step()
    return preprocessing_step(data)

@pipeline
def training_pipeline():
    training_data = data_loading_pipeline(mode="train")
    model = training_step(data=training_data)
    test_data = data_loading_pipeline(mode="test")
    evaluation_step(model=model, data=test_data)
```

In this example, `data_loading_pipeline` is invoked within `training_pipeline`, effectively treating it as a step. Only the parent pipeline is visible in the dashboard. For triggering a pipeline from another, refer to the advanced usage documentation.

#### Additional Resources
- Learn more about orchestrators [here](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/README.md

### Summary of ZenML Pipeline Documentation

**Overview**: Building pipelines in ZenML involves using the `@step` and `@pipeline` decorators.

#### Example Code

```python
from zenml import pipeline, step

@step
def load_data() -> dict:
    return {'features': [[1, 2], [3, 4], [5, 6]], 'labels': [0, 1, 0]}

@step
def train_model(data: dict) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}")

@pipeline
def simple_ml_pipeline():
    dataset = load_data()
    train_model(dataset)

# Run the pipeline
simple_ml_pipeline()
```

#### Execution and Logging
When executed, the pipeline's run is logged to the ZenML dashboard, which requires a ZenML server running locally or remotely. The dashboard displays the Directed Acyclic Graph (DAG) and associated metadata.

#### Additional Features
For more advanced pipeline functionalities, refer to the following topics:
- Configure pipeline/step parameters
- Name and annotate step outputs
- Control caching behavior
- Run pipeline from another pipeline
- Control execution order of steps
- Customize step invocation IDs
- Name your pipeline runs
- Use failure/success hooks
- Hyperparameter tuning
- Attach and fetch metadata within steps
- Enable/disable log storing
- Access secrets in a step

For detailed documentation on these features, please refer to the respective links provided in the original documentation.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/use-pipeline-step-parameters.md

### Summary of Parameterization in ZenML Pipelines

**Overview**: Steps and pipelines in ZenML can be parameterized like standard Python functions. Parameters can be either **artifacts** (outputs from other steps) or **parameters** (explicitly provided values).

#### Key Points:

1. **Parameters for Steps**:
   - **Artifacts**: Outputs from previous steps.
   - **Parameters**: Explicit values that configure step behavior.
   - Only JSON-serializable values (via Pydantic) can be passed as parameters. For non-JSON-serializable objects (e.g., NumPy arrays), use **External Artifacts**.

2. **Example Step and Pipeline**:
   ```python
   from zenml import step, pipeline

   @step
   def my_step(input_1: int, input_2: int) -> None:
       pass

   @pipeline
   def my_pipeline():
       int_artifact = some_other_step()
       my_step(input_1=int_artifact, input_2=42)
   ```

3. **Using YAML Configuration**:
   - Parameters can be defined in a YAML file, allowing for easy updates without modifying code.
   ```yaml
   # config.yaml
   parameters:
     environment: production
   steps:
     my_step:
       parameters:
         input_2: 42
   ```

   ```python
   from zenml import step, pipeline

   @step
   def my_step(input_1: int, input_2: int) -> None:
       ...

   @pipeline
   def my_pipeline(environment: str):
       ...

   if __name__=="__main__":
       my_pipeline.with_options(config_paths="config.yaml")()
   ```

4. **Conflicts in Configuration**:
   - Conflicts may arise if parameters in the YAML file are overridden in code. ZenML will notify the user of such conflicts.
   ```yaml
   # config.yaml
   parameters:
       some_param: 24
   steps:
     my_step:
       parameters:
         input_2: 42
   ```

   ```python
   @pipeline
   def my_pipeline(some_param: int):
       my_step(input_1=42, input_2=43)  # Conflict here
   ```

5. **Caching Behavior**:
   - Steps are cached only if parameter values or artifact inputs match exactly with previous executions. If upstream steps are not cached, the step will execute again.

#### Additional Resources:
- For more on configuration files: [Use Configuration Files](use-pipeline-step-parameters.md)
- For caching control: [Control Caching Behavior](control-caching-behavior.md)

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/reference-environment-variables-in-configurations.md

# Reference Environment Variables in ZenML Configurations

ZenML enables referencing environment variables in both code and configuration files using the syntax `${ENV_VARIABLE_NAME}`.

## In-code Example

```python
from zenml import step

@step(extra={"value_from_environment": "${ENV_VAR}"})
def my_step() -> None:
    ...
```

## Configuration File Example

```yaml
extra:
  value_from_environment: ${ENV_VAR}
  combined_value: prefix_${ENV_VAR}_suffix
```

This approach enhances the flexibility of configurations by allowing dynamic values based on the environment.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/name-your-pipeline-runs.md

### Summary of Pipeline Run Naming in ZenML

Pipeline run names are automatically generated based on the current date and time, as shown in the example:

```bash
Pipeline run training_pipeline-2023_05_24-12_41_04_576473 has finished in 3.742s.
```

To customize a run name, use the `run_name` parameter in the `with_options()` method:

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name"
)
training_pipeline()
```

Run names must be unique. For multiple or scheduled runs, compute the name dynamically or use placeholders. Placeholders can be set in the `@pipeline` decorator or `pipeline.with_options` function. Standard placeholders include:

- `{date}`: current date (e.g., `2024_11_27`)
- `{time}`: current UTC time (e.g., `11_07_09_326492`)

Example with placeholders:

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name_{experiment_name}_{date}_{time}"
)
training_pipeline()
```

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/run-pipelines-asynchronously.md

### Summary: Running Pipelines Asynchronously

Pipelines in ZenML run synchronously by default, meaning the terminal displays logs during execution. To run pipelines asynchronously, you can configure the orchestrator by setting `synchronous=False`. This can be done either at the pipeline level or in a YAML configuration file.

**Python Code Example:**
```python
from zenml import pipeline

@pipeline(settings={"orchestrator": {"synchronous": False}})
def my_pipeline():
    ...
```

**YAML Configuration Example:**
```yaml
settings:
  orchestrator.<STACK_NAME>:
    synchronous: false
```

For more information about orchestrators, refer to the [orchestrators documentation](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/hyper-parameter-tuning.md

### Hyperparameter Tuning with ZenML

**Overview**: Hyperparameter tuning is in development for ZenML. Currently, it can be implemented using a simple pipeline structure.

**Basic Pipeline Example**:
This example demonstrates a grid search for hyperparameters, specifically varying the learning rate:

```python
@pipeline
def my_pipeline(step_count: int) -> None:
    data = load_data_step()
    after = []
    for i in range(step_count):
        train_step(data, learning_rate=i * 0.0001, name=f"train_step_{i}")
        after.append(f"train_step_{i}")
    model = select_model_step(..., after=after)
```

**E2E Example**:
In the E2E example, the `Hyperparameter tuning stage` uses a loop to perform searches over model configurations:

```python
after = []
search_steps_prefix = "hp_tuning_search_"
for i, model_search_configuration in enumerate(MetaConfig.model_search_space):
    step_name = f"{search_steps_prefix}{i}"
    hp_tuning_single_search(
        model_metadata=ExternalArtifact(value=model_search_configuration),
        id=step_name,
        dataset_trn=dataset_trn,
        dataset_tst=dataset_tst,
        target=target,
    )
    after.append(step_name)

best_model_config = hp_tuning_select_best_model(
    search_steps_prefix=search_steps_prefix, after=after
)
```

**Challenges**: Currently, ZenML does not support passing a variable number of artifacts into a step programmatically. Instead, the `select_model_step` queries artifacts using the ZenML Client:

```python
from zenml import step, get_step_context
from zenml.client import Client

@step
def select_model_step():
    run_name = get_step_context().pipeline_run.name
    run = Client().get_pipeline_run(run_name)

    trained_models_by_lr = {}
    for step_name, step in run.steps.items():
        if step_name.startswith("train_step"):
            for output_name, output in step.outputs.items():
                if output_name == "<NAME_OF_MODEL_OUTPUT_IN_TRAIN_STEP>":
                    model = output.load()
                    lr = step.config.parameters["learning_rate"]
                    trained_models_by_lr[lr] = model
    
    # Evaluate models to find the best one
    for lr, model in trained_models_by_lr.items():
        ...
```

**Resources**: For further implementation details, refer to the step files in the `steps/hp_tuning` folder:
- `hp_tuning_single_search(...)`: Performs randomized hyperparameter search.
- `hp_tuning_select_best_model(...)`: Identifies the best model based on previous searches and defined metrics. 

This documentation provides a concise overview of hyperparameter tuning in ZenML, outlining the current implementation method and challenges while preserving essential technical details.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/control-caching-behavior.md

### ZenML Caching Behavior Summary

By default, ZenML caches steps in pipelines when the code and parameters remain unchanged. 

#### Caching Control

- **Step Level Caching**:
  - Use `@step(enable_cache=True)` to enable caching.
  - Use `@step(enable_cache=False)` to disable caching, which overrides pipeline-level settings.

- **Pipeline Level Caching**:
  - Use `@pipeline(enable_cache=True)` to enable caching for the entire pipeline.

#### Example Code
```python
@step(enable_cache=True)
def load_data(parameter: int) -> dict:
    ...

@step(enable_cache=False)
def train_model(data: dict) -> None:
    ...

@pipeline(enable_cache=True)
def simple_ml_pipeline(parameter: int):
    ...
```

#### Dynamic Configuration
Caching settings can be modified after initial setup:
```python
my_step.configure(enable_cache=...)
my_pipeline.configure(enable_cache=...)
```

#### Additional Information
For YAML configuration, refer to the [configuration files documentation](../../pipeline-development/use-configuration-files/).

**Note**: Caching occurs only when code and parameters are unchanged.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/run-an-individual-step.md

### Summary of ZenML Step Execution Documentation

To run an individual step in ZenML, invoke the step like a standard Python function. ZenML will create a temporary pipeline for the step, which is `unlisted` and can be viewed in the "Runs" tab.

#### Step Definition Example
```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from typing import Tuple, Annotated

@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    gamma: float = 0.001,
) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
    """Train a sklearn SVC classifier."""
    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())
    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
    print(f"Train accuracy: {train_acc}")
    return model, train_acc

X_train = pd.DataFrame(...)
y_train = pd.Series(...)

# Execute the step
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

#### Direct Step Execution
To run the step without ZenML's involvement, use the `entrypoint(...)` method:
```python
model, train_acc = svc_trainer.entrypoint(X_train=X_train, y_train=y_train)
```

#### Default Behavior
Set the environment variable `ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK` to `True` to make direct function calls the default behavior for steps, bypassing the ZenML stack.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/control-execution-order-of-steps.md

# Control Execution Order of Steps in ZenML

ZenML determines the execution order of pipeline steps based on data dependencies. For example, in the following pipeline, `step_3` depends on the outputs of `step_1` and `step_2`, allowing both to run in parallel before `step_3` starts:

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1()
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

To enforce specific execution order constraints, you can use non-data dependencies by specifying invocation IDs. For a single step, use `my_step(after="other_step")`. For multiple upstream steps, pass a list: `my_step(after=["other_step", "other_step_2"])`. For more details on invocation IDs, refer to the [documentation here](using-a-custom-step-invocation-id.md).

Here's an example where `step_1` will only start after `step_2` has completed:

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1(after="step_2")
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

In this setup, ZenML ensures `step_1` executes after `step_2`.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/fetching-pipelines.md

### Summary of Documentation on Inspecting Pipeline Runs and Outputs

#### Overview
This documentation explains how to inspect completed pipeline runs and their outputs in ZenML, covering how to fetch pipelines, runs, steps, and artifacts programmatically.

#### Pipeline Hierarchy
The hierarchy consists of:
- **Pipelines** (1:N) → **Runs** (1:N) → **Steps** (1:N) → **Artifacts**.

#### Fetching Pipelines
- **Get a Specific Pipeline:**
  ```python
  from zenml.client import Client
  pipeline_model = Client().get_pipeline("first_pipeline")
  ```

- **List All Pipelines:**
  - **Python:**
    ```python
    pipelines = Client().list_pipelines()
    ```
  - **CLI:**
    ```shell
    zenml pipeline list
    ```

#### Working with Runs
- **Get All Runs of a Pipeline:**
  ```python
  runs = pipeline_model.runs
  ```

- **Get the Last Run:**
  ```python
  last_run = pipeline_model.last_run  # or pipeline_model.runs[0]
  ```

- **Execute a Pipeline and Get the Latest Run:**
  ```python
  run = training_pipeline()
  ```

- **Get a Specific Run:**
  ```python
  pipeline_run = Client().get_pipeline_run("first_pipeline-2023_06_20-16_20_13_274466")
  ```

#### Run Information
- **Status:**
  ```python
  status = run.status  # Possible states: initialized, failed, completed, running, cached
  ```

- **Configuration:**
  ```python
  pipeline_config = run.config
  pipeline_settings = run.config.settings
  ```

- **Component-Specific Metadata:**
  ```python
  run_metadata = run.run_metadata
  orchestrator_url = run_metadata["orchestrator_url"].value
  ```

#### Steps and Artifacts
- **Access Steps:**
  ```python
  steps = run.steps
  step = run.steps["first_step"]
  ```

- **Output Artifacts:**
  ```python
  output = step.outputs["output_name"]  # or step.output for single output
  my_pytorch_model = output.load()
  ```

- **Fetch Artifacts Directly:**
  ```python
  artifact = Client().get_artifact('iris_dataset')
  output = artifact.versions['2022']  # Get specific version
  loaded_artifact = output.load()
  ```

#### Metadata and Visualizations
- **Access Metadata:**
  ```python
  output_metadata = output.run_metadata
  storage_size_in_bytes = output_metadata["storage_size"].value
  ```

- **Visualize Artifacts:**
  ```python
  output.visualize()
  ```

#### Fetching Information During Execution
You can fetch information about previous runs while a pipeline is executing:
```python
from zenml import get_step_context
from zenml.client import Client

@step
def my_step():
    current_run_name = get_step_context().pipeline_run.name
    current_run = Client().get_pipeline_run(current_run_name)
    previous_run = current_run.pipeline.runs[1]  # Index 0 is the current run
```

#### Code Example
A complete example demonstrating how to load a trained model from a pipeline:
```python
from typing_extensions import Tuple, Annotated
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from zenml import pipeline, step
from zenml.client import Client

@step
def training_data_loader() -> Tuple[Annotated[pd.DataFrame, "X_train"], Annotated[pd.DataFrame, "X_test"], Annotated[pd.Series, "y_train"], Annotated[pd.Series, "y_test"]]:
    iris = load_iris(as_frame=True)
    X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)
    return X_train, X_test, y_train, y_test

@step
def svc_trainer(X_train: pd.DataFrame, y_train: pd.Series, gamma: float = 0.001) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())
    return model, model.score(X_train.to_numpy(), y_train.to_numpy())

@pipeline
def training_pipeline(gamma: float = 0.002):
    X_train, X_test, y_train, y_test = training_data_loader()
    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)

if __name__ == "__main__":
    last_run = training_pipeline()
    model = last_run.steps["svc_trainer"].outputs["trained_model"].load()
```

This summary captures essential technical details and code snippets for understanding how to inspect and manage pipeline runs and their outputs in ZenML.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/access-secrets-in-a-step.md

# Accessing Secrets in ZenML Steps

ZenML secrets are secure groupings of **key-value pairs** stored in the ZenML secrets store, each identified by a **name** for easy reference in pipelines and stacks. To learn about configuring and creating secrets, refer to the [platform guide on secrets](../../../getting-started/deploying-zenml/secret-management.md).

You can access secrets in your steps using the ZenML `Client` API, allowing you to securely use secrets for API queries without hard-coding access keys.

## Example Code

```python
from zenml import step
from zenml.client import Client
from somewhere import authenticate_to_some_api

@step
def secret_loader() -> None:
    """Load a secret from the server."""
    secret = Client().get_secret("<SECRET_NAME>")
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

### Additional Resources
- [Creating and managing secrets](../../interact-with-secrets.md)
- [Secrets backend in ZenML](../../../getting-started/deploying-zenml/secret-management.md)

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/get-past-pipeline-step-runs.md

To retrieve past pipeline or step runs in ZenML, use the `get_pipeline` method with the `last_run` property or access runs by index. Here’s a concise example:

```python
from zenml.client import Client

client = Client()

# Retrieve a pipeline by name
p = client.get_pipeline("mlflow_train_deploy_pipeline")

# Get the latest run
latest_run = p.last_run

# Access the first run by index
first_run = p[0]
```

This code demonstrates how to obtain the latest and first runs of a specified pipeline.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/step-output-typing-and-annotation.md

### Summary of Step Output Typing and Annotation in ZenML

**Step Outputs Storage**: Outputs from steps are stored in an artifact store. Annotate and name them for clarity.

#### Type Annotations
- Type annotations are optional but beneficial:
  - **Type Validation**: Ensures correct input types from upstream steps.
  - **Better Serialization**: With annotations, ZenML selects the appropriate materializer for outputs. Custom materializers can be created if built-in options are inadequate.

**Warning**: The built-in `CloudpickleMaterializer` can serialize any object but is not production-ready due to compatibility issues across Python versions and potential security risks from arbitrary code execution.

#### Code Examples
```python
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> float:
    return number ** 0.5

@step
def divide(a: int, b: int) -> Tuple[int, int]:
    return a // b, a % b
```

To enforce type annotations, set the environment variable `ZENML_ENFORCE_TYPE_ANNOTATIONS` to `True`.

#### Tuple vs. Multiple Outputs
- ZenML differentiates single output artifacts of type `Tuple` from multiple outputs based on the return statement:
  - A return statement with a tuple literal indicates multiple outputs.
  
```python
@step
def my_step() -> Tuple[int, int]:
    return 0, 1  # Multiple outputs
```

#### Step Output Names
- Default naming:
  - Single output: `output`
  - Multiple outputs: `output_0`, `output_1`, etc.
- Custom names can be set using `Annotated`:

```python
from typing_extensions import Annotated
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> Annotated[float, "custom_output_name"]:
    return number ** 0.5

@step
def divide(a: int, b: int) -> Tuple[
    Annotated[int, "quotient"],
    Annotated[int, "remainder"]
]:
    return a // b, a % b
```

If no custom names are provided, artifacts are named `{pipeline_name}::{step_name}::output`.

### Additional Resources
- For more on output annotation: [Output Annotation Documentation](../../data-artifact-management/handle-data-artifacts/return-multiple-outputs-from-a-step.md)
- For custom data types: [Custom Data Types Documentation](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md)

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/use-failure-success-hooks.md

### Summary of ZenML Hooks Documentation

**Overview**: ZenML provides hooks to execute actions after step execution, useful for notifications, logging, or resource cleanup. There are two types of hooks: `on_failure` and `on_success`.

- **`on_failure`**: Triggers when a step fails.
- **`on_success`**: Triggers when a step succeeds.

**Defining Hooks**: Hooks are defined as callback functions accessible within the pipeline repository. The `on_failure` hook can accept a `BaseException` argument to access the specific exception.

**Example**:
```python
from zenml import step

def on_failure(exception: BaseException):
    print(f"Step failed: {str(exception)}")

def on_success():
    print("Step succeeded!")

@step(on_failure=on_failure)
def my_failing_step() -> int:
    raise ValueError("Error")

@step(on_success=on_success)
def my_successful_step() -> int:
    return 1
```

**Pipeline-Level Hooks**: Hooks can also be defined at the pipeline level, which apply to all steps unless overridden by step-level hooks.

**Example**:
```python
from zenml import pipeline

@pipeline(on_failure=on_failure, on_success=on_success)
def my_pipeline(...):
    ...
```

**Accessing Step Information**: Inside hooks, you can use `get_step_context()` to access information about the current pipeline run or step.

**Example**:
```python
from zenml import get_step_context

def on_failure(exception: BaseException):
    context = get_step_context()
    print(context.step_run.name)
    print("Step failed!")

@step(on_failure=on_failure)
def my_step(some_parameter: int = 1):
    raise ValueError("My exception")
```

**Using Alerter Component**: Hooks can utilize the Alerter component to send notifications.

**Example**:
```python
from zenml import get_step_context, Client

def on_failure():
    step_name = get_step_context().step_run.name
    Client().active_stack.alerter.post(f"{step_name} just failed!")
```

**Standard Alerter Hooks**:
```python
from zenml.hooks import alerter_success_hook, alerter_failure_hook

@step(on_failure=alerter_failure_hook, on_success=alerter_success_hook)
def my_step(...):
    ...
```

**OpenAI ChatGPT Hook**: This hook generates potential fixes for exceptions using OpenAI's API. Ensure you have the OpenAI integration installed and API key stored in a ZenML secret.

**Example**:
```python
from zenml.integration.openai.hooks import openai_chatgpt_alerter_failure_hook

@step(on_failure=openai_chatgpt_alerter_failure_hook)
def my_step(...):
    ...
```

**Setup for OpenAI**:
```shell
zenml integration install openai
zenml secret create openai --api_key=<YOUR_API_KEY>
```

This documentation provides a comprehensive overview of using failure and success hooks in ZenML, including their definitions, examples, and integration with Alerter and OpenAI.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/retry-steps.md

### ZenML Step Retry Configuration

ZenML offers a built-in retry mechanism to automatically retry steps upon failure, useful for handling intermittent issues, such as resource unavailability on GPU-backed hardware. 

#### Retry Parameters:
1. **max_retries:** Maximum retry attempts for a failed step.
2. **delay:** Initial delay (in seconds) before the first retry.
3. **backoff:** Multiplier for the delay after each retry.

#### Step Definition with Retry:
You can configure retries directly in your step definition using the `@step` decorator:

```python
from zenml.config.retry_config import StepRetryConfig

@step(
    retry=StepRetryConfig(
        max_retries=3, 
        delay=10, 
        backoff=2
    )
)
def my_step() -> None:
    raise Exception("This is a test exception")
```

#### Important Note:
Infinite retries are not supported. Setting `max_retries` to a high value or omitting it will still enforce an internal maximum to prevent infinite loops. Choose a reasonable value based on expected transient failures.

### Related Documentation:
- [Failure/Success Hooks](use-failure-success-hooks.md)
- [Configure Pipelines](../../pipeline-development/use-configuration-files/how-to-use-config.md)

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/tag-your-pipeline-runs.md

# Tagging Pipeline Runs

You can specify tags for your pipeline runs in the following ways:

1. **Configuration File**:
   ```yaml
   # config.yaml
   tags:
     - tag_in_config_file
   ```

2. **Code Decorator or with_options Method**:
   ```python
   @pipeline(tags=["tag_on_decorator"])
   def my_pipeline():
       ...

   my_pipeline = my_pipeline.with_options(tags=["tag_on_with_options"])
   ```

When the pipeline is executed, tags from all specified locations will be merged and applied to the run.

================================================================================

File: docs/book/how-to/pipeline-development/build-pipelines/using-a-custom-step-invocation-id.md

# Custom Step Invocation ID in ZenML

When invoking a ZenML step in a pipeline, it is assigned a unique **invocation ID**. This ID can be used to define the execution order of pipeline steps or to fetch information about the invocation post-execution.

## Key Points:
- The first invocation of a step uses the step's name as its ID (e.g., `my_step`).
- Subsequent invocations append a suffix (_2, _3, etc.) to the step name to ensure uniqueness (e.g., `my_step_2`).
- You can specify a custom invocation ID by passing it as an argument. This ID must be unique within the pipeline.

## Example Code:
```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def example_pipeline():
    my_step()  # ID: my_step
    my_step()  # ID: my_step_2
    my_step(id="my_custom_invocation_id")  # Custom ID
```

================================================================================

File: docs/book/how-to/pipeline-development/training-with-gpus/README.md

# Summary of GPU Resource Management in ZenML

## Overview
ZenML allows scaling machine learning pipelines to the cloud, utilizing GPU-backed hardware for enhanced performance. This involves specifying resource requirements and ensuring the environment is configured correctly.

## Specifying Resource Requirements
To allocate resources for steps in your pipeline, use `ResourceSettings`:

```python
from zenml.config import ResourceSettings
from zenml import step

@step(settings={"resources": ResourceSettings(cpu_count=8, gpu_count=2, memory="8GB")})
def training_step(...) -> ...:
    # train a model
```

For orchestrators like Skypilot that do not support `ResourceSettings`, use specific orchestrator settings:

```python
from zenml import step
from zenml.integrations.skypilot.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings

skypilot_settings = SkypilotAWSOrchestratorSettings(cpus="2", memory="16", accelerators="V100:2")

@step(settings={"orchestrator": skypilot_settings})
def training_step(...) -> ...:
    # train a model
```

Refer to orchestrator documentation for compatibility details.

## Ensuring CUDA-Enabled Containers
To effectively utilize GPUs, ensure your container is CUDA-enabled:

1. **Specify a CUDA-enabled parent image**:
   ```python
   from zenml import pipeline
   from zenml.config import DockerSettings

   docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

2. **Add ZenML as a pip requirement**:
   ```python
   docker_settings = DockerSettings(
       parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
       requirements=["zenml==0.39.1", "torchvision"]
   )
   ```

Choose images carefully to avoid compatibility issues between local and remote environments. Prebuilt images are available for AWS, GCP, and Azure.

## Resetting CUDA Cache
Resetting the CUDA cache can help prevent issues during intensive GPU tasks. Use the following function at the start of GPU-enabled steps:

```python
import gc
import torch

def cleanup_memory() -> None:
    while gc.collect():
        torch.cuda.empty_cache()

@step
def training_step(...):
    cleanup_memory()
    # train a model
```

## Training Across Multiple GPUs
ZenML supports multi-GPU training on a single node. To manage this:

- Create a script for model training that runs in parallel across GPUs.
- Call this script from within the ZenML step, ensuring no multiple instances of ZenML are spawned.

For further assistance, connect with the ZenML community on Slack.

================================================================================

File: docs/book/how-to/pipeline-development/training-with-gpus/accelerate-distributed-training.md

### Summary: Distributed Training with Hugging Face's Accelerate in ZenML

ZenML integrates with Hugging Face's Accelerate library to facilitate distributed training in machine learning pipelines, enabling the use of multiple GPUs or nodes.

#### Using 🤗 Accelerate in ZenML Steps

To enable distributed execution in training steps, use the `run_with_accelerate` decorator:

```python
from zenml import step, pipeline
from zenml.integrations.huggingface.steps import run_with_accelerate

@run_with_accelerate(num_processes=4, multi_gpu=True)
@step
def training_step(some_param: int, ...):
    ...

@pipeline
def training_pipeline(some_param: int, ...):
    training_step(some_param, ...)
```

The decorator accepts arguments similar to the `accelerate launch` CLI command. For a complete list, refer to the [Accelerate CLI documentation](https://huggingface.co/docs/accelerate/en/package_reference/cli#accelerate-launch).

#### Configuration Options

Key arguments for `run_with_accelerate` include:
- `num_processes`: Number of processes for training.
- `cpu`: Force training on CPU.
- `multi_gpu`: Enable distributed GPU training.
- `mixed_precision`: Set mixed precision mode ('no', 'fp16', 'bf16').

#### Important Usage Notes
1. Use the decorator directly on steps with the '@' syntax; it cannot be used as a function inside a pipeline.
2. Use keyword arguments when calling accelerated steps.
3. Misuse raises a `RuntimeError` with guidance.

For a full example, see the [llm-lora-finetuning](https://github.com/zenml-io/zenml-projects/blob/main/llm-lora-finetuning/README.md) project.

#### Container Configuration for Accelerate

To run steps with Accelerate, ensure the environment is properly configured:

1. **Specify a CUDA-enabled parent image**:
   ```python
   from zenml import pipeline
   from zenml.config import DockerSettings

   docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

2. **Add Accelerate as a pip requirement**:
   ```python
   docker_settings = DockerSettings(
       parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
       requirements=["accelerate", "torchvision"]
   )

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

#### Multi-GPU Training

ZenML's Accelerate integration supports training on multiple GPUs, either on a single node or across nodes. Key steps include:
- Wrapping the training step with `run_with_accelerate`.
- Configuring Accelerate arguments (e.g., `num_processes`, `multi_gpu`).
- Ensuring training code is compatible with distributed training.

For assistance with distributed training, connect via [Slack](https://zenml.io/slack).

By utilizing Accelerate in ZenML, you can efficiently scale training processes while maintaining pipeline structure.

================================================================================

File: docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-cli.md

### ZenML CLI: Creating a Run Template

**Feature Availability**: This feature is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

**Command**: Use the ZenML CLI to create a run template with the following command:

```bash
zenml pipeline create-run-template <PIPELINE_SOURCE_PATH> --name=<TEMPLATE_NAME>
```
- Replace `<PIPELINE_SOURCE_PATH>` with `run.my_pipeline` if your pipeline is named `my_pipeline` in `run.py`.

**Requirements**: Ensure you have an active **remote stack** when executing this command. Alternatively, specify a stack using the `--stack` option.

================================================================================

File: docs/book/how-to/pipeline-development/trigger-pipelines/README.md

### Triggering a Pipeline in ZenML

In ZenML, you can trigger a pipeline using the pipeline function. Here’s a concise example:

```python
from zenml import step, pipeline

@step
def load_data() -> dict:
    return {'features': [[1, 2], [3, 4], [5, 6]], 'labels': [0, 1, 0]}

@step
def train_model(data: dict) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}.")

@pipeline
def simple_ml_pipeline():
    train_model(load_data())

if __name__ == "__main__":
    simple_ml_pipeline()
```

### Run Templates

Run Templates are parameterized configurations for ZenML pipelines, allowing for easy execution from the ZenML dashboard or via the Client/REST API. They serve as customizable blueprints for pipeline runs. 

**Note:** This feature is available only in ZenML Pro. For access, sign up [here](https://cloud.zenml.io).

**Resources for Using Templates:**
- [Python SDK](use-templates-python.md)
- [CLI](use-templates-cli.md)
- [Dashboard](use-templates-dashboard.md)
- [REST API](use-templates-rest-api.md)

================================================================================

File: docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-python.md

### ZenML Python SDK: Creating and Running Templates

#### Overview
This documentation covers the creation and execution of run templates using the ZenML Python SDK, a feature exclusive to ZenML Pro users.

#### Create a Template
To create a run template, use the ZenML client to fetch a pipeline run and then create a template:

```python
from zenml.client import Client

run = Client().get_pipeline_run(<RUN_NAME_OR_ID>)
Client().create_run_template(name=<TEMPLATE_NAME>, deployment_id=run.deployment_id)
```

**Note:** The selected pipeline run must be executed on a remote stack (including a remote orchestrator, artifact store, and container registry).

Alternatively, create a template directly from a pipeline definition:

```python
from zenml import pipeline

@pipeline
def my_pipeline():
    ...

template = my_pipeline.create_run_template(name=<TEMPLATE_NAME>)
```

#### Run a Template
To run a created template:

```python
from zenml.client import Client

template = Client().get_run_template(<TEMPLATE_NAME>)
config = template.config_template

# [OPTIONAL] Modify the config here

Client().trigger_pipeline(template_id=template.id, run_configuration=config)
```

Executing the template triggers a new run on the same stack as the original.

#### Advanced Usage: Triggering a Template from Another Pipeline
You can trigger a pipeline from within another pipeline using the following structure:

```python
import pandas as pd
from zenml import pipeline, step
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml.artifacts.utils import load_artifact
from zenml.client import Client
from zenml.config.pipeline_run_configuration import PipelineRunConfiguration

@step
def trainer(data_artifact_id: str):
    df = load_artifact(data_artifact_id)

@pipeline
def training_pipeline():
    trainer()

@step
def load_data() -> pd.DataFrame:
    ...

@step
def trigger_pipeline(df: UnmaterializedArtifact):
    run_config = PipelineRunConfiguration(
        steps={"trainer": {"parameters": {"data_artifact_id": df.id}}}
    )
    Client().trigger_pipeline("training_pipeline", run_configuration=run_config)

@pipeline
def loads_data_and_triggers_training():
    df = load_data()
    trigger_pipeline(df)
```

#### Additional Resources
- Learn more about [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) and the [`trigger_pipeline`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client) function in the SDK Docs.
- Read about Unmaterialized Artifacts [here](../../data-artifact-management/complex-usecases/unmaterialized-artifacts.md).

================================================================================

File: docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-dashboard.md

### ZenML Dashboard Template Management

**Feature Availability**: This functionality is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Creating a Template
1. Navigate to a pipeline run executed on a remote stack (requires a remote orchestrator, artifact store, and container registry).
2. Click on `+ New Template`, enter a name, and select `Create`.

#### Running a Template
- To run a template:
  - Click `Run a Pipeline` on the main `Pipelines` page, or
  - Access a specific template page and select `Run Template`.
  
You will be directed to the `Run Details` page, where you can:
- Upload a `.yaml` configuration file or
- Modify the configuration using the editor.

After initiating the run, a new execution will occur on the same stack as the original run.

================================================================================

File: docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-rest-api.md

### ZenML REST API: Running a Pipeline Template

**Note:** This feature is available only in [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Prerequisites
To trigger a pipeline via the REST API, you must have at least one run template for that pipeline and know the pipeline name.

#### Steps to Trigger a Pipeline

1. **Get Pipeline ID**
   - Call: `GET /pipelines?name=<PIPELINE_NAME>`
   - Response: Contains `<PIPELINE_ID>`.

   ```shell
   curl -X 'GET' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/pipelines?hydrate=false&name=training' \
     -H 'accept: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>'
   ```

2. **Get Template ID**
   - Call: `GET /run_templates?pipeline_id=<PIPELINE_ID>`
   - Response: Contains `<TEMPLATE_ID>`.

   ```shell
   curl -X 'GET' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates?hydrate=false&pipeline_id=<PIPELINE_ID>' \
     -H 'accept: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>'
   ```

3. **Run the Pipeline**
   - Call: `POST /run_templates/<TEMPLATE_ID>/runs` with `PipelineRunConfiguration` in the body.

   ```shell
   curl -X 'POST' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates/<TEMPLATE_ID>/runs' \
     -H 'accept: application/json' \
     -H 'Content-Type: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>' \
     -d '{
       "steps": {"model_trainer": {"parameters": {"model_type": "rf"}}}
     }'
   ```

A successful response indicates that the pipeline has been re-triggered with the specified configuration. 

For more details on obtaining a bearer token, refer to the [API reference](../../../reference/api-reference.md#using-a-bearer-token-to-access-the-api-programmatically).

================================================================================

File: docs/book/how-to/pipeline-development/configure-python-environments/handling-dependencies.md

### Handling Dependency Conflicts in ZenML

This documentation addresses common issues with conflicting dependencies when using ZenML alongside other libraries. ZenML is designed to be stack- and integration-agnostic, which can lead to dependency conflicts.

#### Installing Dependencies
Use the command:
```bash
zenml integration install ...
```
to install dependencies for specific integrations. After installing additional dependencies, verify that ZenML requirements are met by running:
```bash
zenml integration list
```
Look for the green tick symbol indicating all requirements are satisfied.

#### Suggestions for Resolving Conflicts

1. **Use `pip-compile` for Reproducibility**:
   - Consider using `pip-compile` from the [pip-tools package](https://pip-tools.readthedocs.io/) to create a static `requirements.txt` file for consistent environments.
   - For examples, refer to the [gitflow repository](https://github.com/zenml-io/zenml-gitflow#-software-requirements-management).

2. **Run `pip check`**:
   - Use `pip check` to verify compatibility of your environment's dependencies. It will list any conflicts.

3. **Known Dependency Issues**:
   - ZenML requires `click~=8.0.3` for its CLI. Using a version greater than 8.0.3 may lead to issues.

#### Manual Dependency Installation
You can manually install dependencies instead of using ZenML's integration installation, though this is not recommended. The command:
```bash
zenml integration install gcp
```
internally runs a `pip install` for the required packages.

To manually install dependencies, use:
```bash
# Export requirements to a file
zenml integration export-requirements --output-file integration-requirements.txt INTEGRATION_NAME

# Print requirements to console
zenml integration export-requirements INTEGRATION_NAME
```
After modifying the requirements, if using a remote orchestrator, update the `DockerSettings` object accordingly for proper configuration.

================================================================================

File: docs/book/how-to/pipeline-development/configure-python-environments/README.md

# Summary of ZenML Environment Configuration

## Overview
ZenML deployments involve multiple environments, each serving a specific purpose in managing dependencies and configurations for pipelines.

### Environment Types
1. **Client Environment (Runner Environment)**: 
   - Where ZenML pipelines are compiled (e.g., in a `run.py` script).
   - Types include:
     - Local development
     - CI runner in production
     - ZenML Pro runner
     - `runner` image orchestrated by ZenML server
   - Key Steps:
     1. Compile pipeline using `@pipeline` function.
     2. Create/trigger pipeline and step build environments if running remotely.
     3. Trigger run in the orchestrator.
   - Note: `@pipeline` is only called in this environment, focusing on compile-time logic.

2. **ZenML Server Environment**: 
   - A FastAPI application that manages pipelines and metadata, accessed during ZenML deployment.
   - Install dependencies during deployment, especially for custom integrations.

3. **Execution Environments**: 
   - When running locally, the client, server, and execution environments are the same.
   - For remote pipelines, ZenML builds Docker images (execution environments) to transfer code and environment to the orchestrator.
   - Configuration starts with a base image containing ZenML and Python, with additional pipeline dependencies added as needed.

4. **Image Builder Environment**: 
   - Execution environments are created locally using the Docker client by default, requiring Docker installation.
   - ZenML provides image builders, a stack component for building and pushing Docker images in a specialized environment.
   - If no image builder is configured, the local image builder is used for consistency.

### Important Links
- [ZenML Pro](https://zenml.io/pro)
- [Deploy ZenML](../../../getting-started/deploying-zenml/README.md)
- [Configure Server Environment](./configure-the-server-environment.md)
- [Containerize Your Pipeline](../../infrastructure-deployment/customize-docker-builds/README.md)
- [Image Builders](../../../component-guide/image-builders/image-builders.md)

This summary captures the essential technical details and processes involved in configuring Python environments for ZenML deployments.

================================================================================

File: docs/book/how-to/pipeline-development/configure-python-environments/configure-the-server-environment.md

### Configuring the Server Environment

The ZenML server environment is configured using environment variables, which must be set prior to deploying your server instance. For a complete list of available environment variables, refer to the [environment variables documentation](../../../reference/environment-variables.md). 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/control-logging/disable-colorful-logging.md

### Disabling Colorful Logging in ZenML

ZenML enables colorful logging by default for better readability. To disable this feature, set the following environment variable:

```bash
ZENML_LOGGING_COLORS_DISABLED=true
```

Setting this variable in the client environment (e.g., local machine) will disable colorful logging for remote pipeline runs as well. To disable it only locally while keeping it enabled for remote runs, configure the variable in your pipeline's environment:

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_COLORS_DISABLED": "false"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Alternatively, configure pipeline options
my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

This allows for flexible logging configurations based on the execution environment.

================================================================================

File: docs/book/how-to/control-logging/disable-rich-traceback.md

### Disabling Rich Traceback Output in ZenML

ZenML uses the [`rich`](https://rich.readthedocs.io/en/stable/traceback.html) library for enhanced traceback output, beneficial for debugging. To disable this feature, set the following environment variable:

```bash
export ZENML_ENABLE_RICH_TRACEBACK=false
```

This change will only affect local pipeline runs. To disable rich tracebacks for remote pipeline runs, set the variable in the pipeline run environment:

```python
docker_settings = DockerSettings(environment={"ZENML_ENABLE_RICH_TRACEBACK": "false"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Alternatively, configure options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

This ensures that plain text traceback output is displayed in both local and remote runs.

================================================================================

File: docs/book/how-to/control-logging/view-logs-on-the-dasbhoard.md

# Viewing Logs on the Dashboard

ZenML captures logs during step execution using a logging handler. Users can utilize the Python logging module or print statements, which ZenML will capture and store.

```python
import logging
from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")
    print("World.")
```

Logs are stored in the artifact store of your stack, and viewing them on the dashboard requires the ZenML server to have access to this store. Access conditions include:

- **Local ZenML Server**: Both local and remote artifact stores may be accessible based on client configuration.
- **Deployed ZenML Server**: Logs from runs on a local artifact store are not accessible. Logs from a remote artifact store may be accessible if configured with a service connector.

For configuration details, refer to the production guide on setting up a remote artifact store with a service connector. Properly configured logs will be displayed on the dashboard.

**Note**: To disable log storage due to performance or storage concerns, follow the provided instructions.

================================================================================

File: docs/book/how-to/control-logging/README.md

# Configuring ZenML's Default Logging Behavior

## Control Logging

ZenML generates different types of logs across various environments:

- **ZenML Server**: Produces server logs like any FastAPI server.
- **Client or Runner Environment**: Logs are generated during pipeline runs, capturing steps before, after, and during execution.
- **Execution Environment**: Logs are created at the orchestrator level during the execution of each pipeline step, typically using Python's `logging` module.

This section outlines how users can manage logging behavior across these environments.

================================================================================

File: docs/book/how-to/control-logging/set-logging-verbosity.md

### Summary: Setting Logging Verbosity in ZenML

ZenML defaults to `INFO` logging verbosity. To change this, set the environment variable:

```bash
export ZENML_LOGGING_VERBOSITY=INFO
```

Available options are `INFO`, `WARN`, `ERROR`, `CRITICAL`, and `DEBUG`. Note that changes made in the client environment (e.g., local machine) do not affect remote pipeline runs. To set logging verbosity for remote runs, configure the environment variable in the pipeline's environment:

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_VERBOSITY": "DEBUG"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

This ensures the specified logging level is applied to remote executions.

================================================================================

File: docs/book/how-to/control-logging/enable-or-disable-logs-storing.md

# ZenML Logging Configuration

ZenML captures logs during step execution using a logging handler. Users can utilize the Python logging module or print statements, which ZenML will log and store.

## Example Code
```python
import logging
from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")
    print("World.")
```

Logs are stored in the artifact store of your stack and can be displayed on the dashboard. Note: Logs are not viewable if not connected to a cloud artifact store with a service connector. For more details, refer to the [log viewing documentation](./view-logs-on-the-dasbhoard.md).

## Disabling Log Storage
To disable log storage, you can:

1. Use the `enable_step_logs` parameter in the `@step` or `@pipeline` decorator:
```python
from zenml import pipeline, step

@step(enable_step_logs=False)
def my_step() -> None:
    ...

@pipeline(enable_step_logs=False)
def my_pipeline():
    ...
```

2. Set the environmental variable `ZENML_DISABLE_STEP_LOGS_STORAGE` to `true`, which takes precedence over the above parameters. This variable must be set at the orchestrator level:
```python
docker_settings = DockerSettings(environment={"ZENML_DISABLE_STEP_LOGS_STORAGE": "true"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

This configuration allows users to control log storage effectively within their ZenML pipelines.

================================================================================

File: docs/book/how-to/configuring-zenml/configuring-zenml.md

### Configuring ZenML

This guide outlines methods to customize ZenML's behavior. Users can adapt various aspects of ZenML's functionality to suit their needs.

**Key Points:**
- ZenML allows configuration to modify its default behavior.
- Users can adjust settings based on specific requirements.

For detailed configuration options, refer to the ZenML documentation.

================================================================================

File: docs/book/how-to/model-management-metrics/README.md

# Model Management and Metrics in ZenML

This section addresses the management of machine learning models and the tracking of performance metrics within ZenML.

## Key Components:

1. **Model Management**:
   - ZenML facilitates versioning, storage, and retrieval of models.
   - Models can be registered and organized for easy access.

2. **Metrics Tracking**:
   - Metrics can be logged and monitored throughout the model lifecycle.
   - Supports integration with various tracking tools for visualization and analysis.

3. **Model Registry**:
   - Centralized repository for storing model metadata.
   - Enables easy comparison and selection of models based on performance.

4. **Performance Metrics**:
   - Common metrics include accuracy, precision, recall, and F1-score.
   - Custom metrics can also be defined and tracked.

5. **Integration**:
   - ZenML integrates with popular ML frameworks and tools for seamless model management.
   - Supports cloud storage solutions for model artifacts.

## Example Code Snippet:

```python
from zenml.model import Model
from zenml.metrics import log_metric

# Register a model
model = Model(name="my_model", version="1.0")
model.register()

# Log a metric
log_metric("accuracy", 0.95)
```

This summary encapsulates the essential aspects of model management and metrics tracking in ZenML, ensuring that critical information is retained for further inquiries.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/README.md

# Track Metrics and Metadata

ZenML provides the `log_metadata` function for logging and managing metrics and metadata across models, artifacts, steps, and runs. This function enables unified metadata logging and allows for automatic logging of the same metadata for related entities.

### Basic Usage
To log metadata within a step, use the following code:

```python
from zenml import step, log_metadata

@step
def my_step() -> ...:
    log_metadata(metadata={"accuracy": 0.91})
```

This logs the `accuracy` for the step, its pipeline run, and the model version if provided.

### Additional Use-Cases
The `log_metadata` function supports various targets (model, artifact, step, run) with flexible parameters. For more details, refer to:
- [Log metadata to a step](attach-metadata-to-a-step.md)
- [Log metadata to a run](attach-metadata-to-a-run.md)
- [Log metadata to an artifact](attach-metadata-to-an-artifact.md)
- [Log metadata to a model](attach-metadata-to-a-model.md)

### Important Note
Older methods for logging metadata (e.g., `log_model_metadata`, `log_artifact_metadata`, `log_step_metadata`) are deprecated. Use `log_metadata` for all future implementations.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/grouping-metadata.md

### Grouping Metadata in the Dashboard

To organize metadata in the ZenML dashboard, pass a dictionary of dictionaries to the `metadata` parameter. This groups metadata into cards, enhancing visualization and comprehension. 

**Example:**

```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize

log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="my_artifact_version",
)
```

In the ZenML dashboard, "model_metrics" and "data_details" will display as separate cards, each showing their respective key-value pairs.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/fetch-metadata-within-pipeline.md

### Fetch Metadata During Pipeline Composition

#### Pipeline Configuration using `PipelineContext`

To access pipeline configuration during composition, use the `zenml.get_pipeline_context()` function to retrieve the `PipelineContext`.

**Example Code:**
```python
from zenml import get_pipeline_context, pipeline

@pipeline(
    extra={
        "complex_parameter": [
            ("sklearn.tree", "DecisionTreeClassifier"),
            ("sklearn.ensemble", "RandomForestClassifier"),
        ]
    }
)
def my_pipeline():
    context = get_pipeline_context()
    after = []
    search_steps_prefix = "hp_tuning_search_"
    
    for i, model_search_configuration in enumerate(context.extra["complex_parameter"]):
        step_name = f"{search_steps_prefix}{i}"
        cross_validation(
            model_package=model_search_configuration[0],
            model_class=model_search_configuration[1],
            id=step_name
        )
        after.append(step_name)
    
    select_best_model(search_steps_prefix=search_steps_prefix, after=after)
```

For more details on the attributes and methods available in `PipelineContext`, refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.pipelines.pipeline_context.PipelineContext).

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-an-artifact.md

### Summary: Attaching Metadata to Artifacts in ZenML

In ZenML, metadata enhances artifacts by providing context such as size, structure, and performance metrics, which can be viewed in the ZenML dashboard for easier artifact tracking.

#### Logging Metadata for Artifacts
Artifacts are outputs from pipeline steps (e.g., datasets, models). To log metadata, use the `log_metadata` function with the artifact's name, version, or ID. Metadata can be any JSON-serializable value, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`.

**Example: Logging Metadata**
```python
import pandas as pd
from zenml import step, log_metadata
from zenml.metadata.metadata_types import StorageSize

@step
def process_data_step(dataframe: pd.DataFrame) -> pd.DataFrame:
    processed_dataframe = ...
    log_metadata(
        metadata={
            "row_count": len(processed_dataframe),
            "columns": list(processed_dataframe.columns),
            "storage_size": StorageSize(processed_dataframe.memory_usage().sum())
        },
        infer_artifact=True,
    )
    return processed_dataframe
```

#### Selecting the Artifact for Metadata Logging
1. **Using `infer_artifact`**: Automatically infers the output artifact of the step.
2. **Name and Version**: Specify both to attach metadata to a specific artifact version.
3. **Artifact Version ID**: Directly provide the ID to fetch and attach metadata.

#### Fetching Logged Metadata
Use the ZenML Client to retrieve logged metadata:
```python
from zenml.client import Client

client = Client()
artifact = client.get_artifact_version("my_artifact", "my_version")
print(artifact.run_metadata["metadata_key"])
```
*Note: Fetching by key returns the latest entry.*

#### Grouping Metadata in the Dashboard
To organize metadata into cards in the ZenML dashboard, pass a dictionary of dictionaries in the `metadata` parameter:
```python
log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="version",
)
```
In the dashboard, `model_metrics` and `data_details` will appear as separate cards with their respective data.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/logging-metadata.md

### Summary of ZenML Metadata Tracking

ZenML supports special metadata types for capturing specific information. Key types include:

- **Uri**: Represents a dataset source URI.
- **Path**: Specifies the filesystem path to a script.
- **DType**: Describes data types for specific columns.
- **StorageSize**: Indicates the size of processed data in bytes.

#### Example Usage:
```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize, DType, Uri, Path

log_metadata(
    metadata={
        "dataset_source": Uri("gs://my-bucket/datasets/source.csv"),
        "preprocessing_script": Path("/scripts/preprocess.py"),
        "column_types": {
            "age": DType("int"),
            "income": DType("float"),
            "score": DType("int")
        },
        "processed_data_size": StorageSize(2500000)
    },
)
```

These special types standardize metadata format, ensuring consistent and interpretable logging.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-run.md

### Attach Metadata to a Run in ZenML

In ZenML, you can log metadata to a pipeline run using the `log_metadata` function, which accepts a dictionary of key-value pairs. Values can be any JSON-serializable type, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Run

When logging metadata from within a pipeline step, use `log_metadata` to attach metadata with the key format `step_name::metadata_key`. This allows for consistent metadata keys across different steps during execution.

```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[
    ClassifierMixin,
    ArtifactConfig(name="sklearn_classifier", is_model_artifact=True)
]:
    """Train a model and log run-level metadata."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    # Log metadata at the run level
    log_metadata({
        "run_metrics": {"accuracy": accuracy, "precision": precision, "recall": recall}
    })
    return classifier
```

#### Manually Logging Metadata

You can also log metadata to a specific pipeline run using the run ID, which is useful for post-execution metrics.

```python
from zenml import log_metadata

log_metadata(
    {"post_run_info": {"some_metric": 5.0}},
    run_id_name_or_prefix="run_id_name_or_prefix"
)
```

#### Fetching Logged Metadata

To retrieve logged metadata, use the ZenML Client:

```python
from zenml.client import Client

client = Client()
run = client.get_pipeline_run("run_id_name_or_prefix")

print(run.run_metadata["metadata_key"])
```

**Note:** The fetched value for a specific key will always reflect the latest entry.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-step.md

### Summary: Attaching Metadata to a Step in ZenML

In ZenML, you can log metadata for a specific step using the `log_metadata` function, which allows you to attach a dictionary of key-value pairs as metadata. This metadata can include any JSON-serializable values, such as custom classes like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Step
When `log_metadata` is called within a step, it automatically attaches the metadata to the current step and its pipeline run, making it suitable for logging metrics available during execution.

**Example: Logging Metadata in a Step**
```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name="sklearn_classifier")]:
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    log_metadata(metadata={"evaluation_metrics": {"accuracy": accuracy, "precision": precision, "recall": recall}})
    return classifier
```

**Note:** If a pipeline step execution is cached, the cached run will copy the original metadata, excluding any manually generated entries post-execution.

#### Manually Logging Metadata After Execution
You can log metadata for a specific step after execution using identifiers for the pipeline, step, and run.

**Example: Manual Metadata Logging**
```python
from zenml import log_metadata

log_metadata(metadata={"additional_info": {"a_number": 3}}, step_name="step_name", run_id_name_or_prefix="run_id_name_or_prefix")

# or 

log_metadata(metadata={"additional_info": {"a_number": 3}}, step_id="step_id")
```

#### Fetching Logged Metadata
To retrieve logged metadata, use the ZenML Client:

**Example: Fetching Metadata**
```python
from zenml.client import Client

client = Client()
step = client.get_pipeline_run("pipeline_id").steps["step_name"]

print(step.run_metadata["metadata_key"])
```

**Note:** Fetching metadata by key returns the latest entry.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-model.md

### Summary: Attaching Metadata to a Model in ZenML

ZenML enables logging of metadata for models, providing context beyond individual artifact details. This metadata can include evaluation results, deployment information, or customer-specific details, aiding in model performance management across versions.

#### Logging Metadata

To log metadata, use the `log_metadata` function, which allows attaching key-value pairs, including metrics and JSON-serializable values (e.g., `Uri`, `Path`, `StorageSize`). 

**Example: Logging Metadata for a Model**

```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name="sklearn_classifier")]:
    """Train a model and log model metadata."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...
    
    log_metadata(
        metadata={
            "evaluation_metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall
            }
        },
        infer_model=True,
    )
    return classifier
```

In this example, metadata is associated with the model, useful for summarizing various pipeline steps and artifacts.

#### Selecting Models with `log_metadata`

ZenML offers flexible options for attaching metadata to model versions:
1. **Using `infer_model`**: Automatically infers the model from the step context.
2. **Model Name and Version**: Attach metadata to a specific model version using provided name and version.
3. **Model Version ID**: Directly attach metadata using a specific model version ID.

#### Fetching Logged Metadata

To retrieve attached metadata, use the ZenML Client:

```python
from zenml.client import Client

client = Client()
model = client.get_model_version("my_model", "my_version")

print(model.run_metadata["metadata_key"])
```

**Note**: Fetching metadata with a specific key returns the latest entry.

================================================================================

File: docs/book/how-to/model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md

### Summary: Accessing Meta Information in ZenML Pipelines

This documentation provides guidance on accessing real-time meta information within ZenML pipelines using the `StepContext`.

#### Fetching Metadata with `StepContext`

To retrieve information about the current pipeline or step, utilize the `zenml.get_step_context()` function:

```python
from zenml import step, get_step_context

@step
def my_step():
    step_context = get_step_context()
    pipeline_name = step_context.pipeline.name
    run_name = step_context.pipeline_run.name
    step_name = step_context.step_run.name
```

Additionally, the `StepContext` allows you to determine where the outputs of the current step will be stored and which Materializer will be used:

```python
from zenml import step, get_step_context

@step
def my_step():
    step_context = get_step_context()
    uri = step_context.get_output_artifact_uri()  # Output storage URI
    materializer = step_context.get_output_materializer()  # Output materializer
```

For further details on the attributes and methods available in `StepContext`, refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.steps.step_context.StepContext).

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/model-versions.md

# Model Versions Overview

Model versions allow tracking of different iterations during the machine learning training process, facilitating the full ML lifecycle with dashboard and API support. You can associate model versions with stages (e.g., production) and link them to non-technical artifacts like datasets.

## Explicitly Naming Model Versions

To explicitly name a model version, use the `version` argument in the `Model` object. If omitted, ZenML auto-generates a version number.

```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="1.0.5")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training happens here
```

If a model version exists, it automatically associates with the pipeline context.

## Templated Naming for Model Versions

For semantic naming, use templates in the `version` and/or `name` arguments. This generates unique, readable names for each run.

```python
from zenml import Model, step, pipeline

model = Model(name="{team}_my_model", version="experiment_with_phi_3_{date}_{time}")

@step(model=model)
def llm_trainer(...) -> ...:
    ...

@pipeline(model=model, substitutions={"team": "Team_A"})
def training_pipeline(...):
    # training happens here
```

This will produce a model version with a runtime-evaluated name, e.g., `experiment_with_phi_3_2024_08_30_12_42_53`. Standard substitutions include `{date}` and `{time}`.

## Fetching Model Versions by Stage

Assign stages (e.g., `production`, `staging`) to model versions for easier retrieval. Update a model version's stage via the CLI:

```shell
zenml model version update MODEL_NAME --stage=STAGE
```

You can then fetch the model version by its stage:

```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="production")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training happens here
```

## Autonumbering of Versions

ZenML automatically numbers model versions. If no version is specified, it generates a new version number.

```python
from zenml import Model, step

model = Model(name="my_model", version="even_better_version")

@step(model=model)
def svc_trainer(...) -> ...:
    ...
```

If `really_good_version` was the 5th version, `even_better_version` becomes the 6th.

```python
from zenml import Model

earlier_version = Model(name="my_model", version="really_good_version").number  # == 5
updated_version = Model(name="my_model", version="even_better_version").number  # == 6
```

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/README.md

# Use the Model Control Plane

A `Model` in ZenML is an entity that consolidates pipelines, artifacts, metadata, and essential business data, representing your ML products' business logic. It can be viewed as a "project" or "workspace."

**Key Points:**
- The technical model (model files with weights and parameters) is a primary artifact associated with a ZenML Model, but other artifacts like training data and production predictions are also included.
- Models are first-class entities in ZenML, accessible through the ZenML API, client, and the ZenML Pro dashboard.
- A Model captures lineage information and allows staging of different Model versions (e.g., `Production`), enabling decision-making on promotions based on business rules.
- The Model Control Plane provides a unified interface for managing models, integrating pipelines, artifacts, and business data with the technical model.

For a comprehensive example, refer to the [starter guide](../../../user-guide/starter-guide/track-ml-models.md).

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/associate-a-pipeline-with-a-model.md

### Summary of Documentation on Associating a Pipeline with a Model

To associate a pipeline with a model in ZenML, use the following code structure:

```python
from zenml import pipeline
from zenml import Model
from zenml.enums import ModelStages

@pipeline(
    model=Model(
        name="ClassificationModel",  # Unique model name
        tags=["MVP", "Tabular"],      # Tags for filtering
        version=ModelStages.LATEST     # Specify model version or stage
    )
)
def my_pipeline():
    ...
```

- **Model Association**: This code links the pipeline to the specified model. If the model exists, a new version is created. To attach to an existing version, specify the version explicitly.
  
- **Configuration Files**: Model configuration can also be defined in YAML files:

```yaml
model:
  name: text_classifier
  description: A breast cancer classifier
  tags: ["classifier", "sgd"]
```

This setup allows for organized model management and easy version control within ZenML.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/connecting-artifacts-via-a-model.md

### Summary: Structuring an MLOps Project

**Overview:**
An MLOps project typically consists of multiple pipelines that manage the flow of data and models. Key pipelines include:
- **Feature Engineering Pipeline:** Prepares raw data for training.
- **Training Pipeline:** Trains models using processed data.
- **Inference Pipeline:** Runs predictions on trained models.
- **Deployment Pipeline:** Deploys models to production.

The structure of these pipelines can vary based on project requirements, and information (artifacts, models, metadata) often needs to be shared between them.

### Common Patterns for Artifact Exchange

#### Pattern 1: Artifact Exchange via `Client`
This pattern facilitates the exchange of datasets between pipelines. For instance, a feature engineering pipeline produces datasets that are consumed by a training pipeline.

**Example Code:**
```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    train_data = client.get_artifact_version(name="iris_training_dataset")
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")
    sklearn_classifier = model_trainer(train_data)
    model_evaluator(model, sklearn_classifier)
```
*Note: Artifacts are referenced, not materialized in memory during the pipeline function.*

#### Pattern 2: Artifact Exchange via a `Model`
In this approach, models serve as the reference point for artifact exchange. A training pipeline may produce multiple models, with only the best being promoted to production. The inference pipeline can then access the latest promoted model without needing to know specific artifact IDs.

**Example Code:**
```python
from zenml import step, get_step_context

@step(enable_cache=False)
def predict(data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    model = get_step_context().model.get_model_artifact("trained_model")
    predictions = pd.Series(model.predict(data))
    return predictions
```

Alternatively, you can resolve the artifact at the pipeline level:
```python
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages

@step
def predict(model: ClassifierMixin, data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    return pd.Series(model.predict(data))

@pipeline(model=Model(name="iris_classifier", version=ModelStages.PRODUCTION))
def do_predictions():
    model = get_pipeline_context().model.get_model_artifact("trained_model")
    inference_data = load_data()
    predict(model=model, data=inference_data)

if __name__ == "__main__":
    do_predictions()
```

### Conclusion
Both artifact exchange patterns are valid; the choice depends on project needs and developer preferences. For detailed repository structure recommendations, refer to the best practices section.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/linking-model-binaries-data-to-models.md

# Linking Model Binaries/Data in ZenML

ZenML allows linking model artifacts generated during pipeline runs to models for lineage tracking and transparency. Artifacts can be linked in several ways:

## 1. Configuring the Model at the Pipeline Level
You can link artifacts by configuring the `model` parameter in the `@pipeline` or `@step` decorator:

```python
from zenml import Model, pipeline

model = Model(name="my_model", version="1.0.0")

@pipeline(model=model)
def my_pipeline():
    ...
```
This links all artifacts from the pipeline run to the specified model.

## 2. Saving Intermediate Artifacts
To save progress during long-running steps (e.g., training), use the `save_artifact` utility function. If the step has a Model context, it will link automatically.

```python
from zenml import step, Model
from zenml.artifacts.utils import save_artifact
import pandas as pd
from typing_extensions import Annotated
from zenml.artifacts.artifact_config import ArtifactConfig

@step(model=Model(name="MyModel", version="1.2.42"))
def trainer(trn_dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig("trained_model")]:
    for epoch in epochs:
        checkpoint = model.train(epoch)
        save_artifact(data=checkpoint, name="training_checkpoint", version=f"1.2.42_{epoch}")
    return model
```

## 3. Explicitly Linking Artifacts
To link an artifact to a model outside of a step context, use the `link_artifact_to_model` function:

```python
from zenml import step, Model, link_artifact_to_model, save_artifact
from zenml.client import Client

@step
def f_() -> None:
    new_artifact = save_artifact(data="Hello, World!", name="manual_artifact")
    link_artifact_to_model(artifact_version_id=new_artifact.id, model=Model(name="MyModel", version="0.0.42"))

existing_artifact = Client().get_artifact_version(name_id_or_prefix="existing_artifact")
link_artifact_to_model(artifact_version_id=existing_artifact.id, model=Model(name="MyModel", version="0.2.42"))
```

This documentation provides a concise overview of linking model artifacts in ZenML, ensuring that critical information is preserved while eliminating redundancy.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/promote-a-model.md

# Model Promotion in ZenML

## Stages and Promotion
ZenML model versions can progress through various lifecycle stages, which serve as metadata to identify their state. The available stages are:
- **staging**: Prepared for production.
- **production**: Actively running in production.
- **latest**: Represents the most recent version; cannot be promoted to this stage.
- **archived**: No longer relevant, typically after moving from another stage.

Model promotion can be done via:
1. **CLI**: 
   ```bash
   zenml model version update iris_logistic_regression --stage=...
   ```

2. **Cloud Dashboard**: Upcoming feature for promoting models directly from the ZenML Pro dashboard.

3. **Python SDK**: The most common method:
   ```python
   from zenml import Model
   from zenml.enums import ModelStages

   model = Model(name="iris_logistic_regression", version="1.2.3")
   model.set_stage(stage=ModelStages.PRODUCTION)

   latest_model = Model(name="iris_logistic_regression", version=ModelStages.LATEST)
   latest_model.set_stage(stage=ModelStages.STAGING)
   ```

Within a pipeline:
```python
from zenml import get_step_context, step, pipeline
from zenml.enums import ModelStages

@step
def promote_to_staging():
    model = get_step_context().model
    model.set_stage(ModelStages.STAGING, force=True)

@pipeline(...)
def train_and_promote_model():
    ...
    promote_to_staging(after=["train_and_evaluate"])
```

## Fetching Model Versions by Stage
You can load the appropriate model version by specifying the stage:
```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="production")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training happens here
```

This configuration allows for precise control over which model version is used in steps and pipelines.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/register-a-model.md

# Model Registration in ZenML

Models can be registered in ZenML through various methods: explicit registration via CLI, Python SDK, or implicit registration during a pipeline run. ZenML Pro users have access to a dashboard for model registration.

## Explicit CLI Registration
To register a model using the CLI, use the following command:

```bash
zenml model register iris_logistic_regression --license=... --description=...
```

Run `zenml model register --help` for available options. You can also add tags using the `--tag` option.

## Explicit Dashboard Registration
ZenML Pro users can register models directly from the cloud dashboard interface.

## Explicit Python SDK Registration
Register a model using the Python SDK as follows:

```python
from zenml import Model
from zenml.client import Client

Client().create_model(
    name="iris_logistic_regression",
    license="Copyright (c) ZenML GmbH 2023",
    description="Logistic regression model trained on the Iris dataset.",
    tags=["regression", "sklearn", "iris"],
)
```

## Implicit Registration by ZenML
Models are commonly registered implicitly during a pipeline run by specifying a `Model` object in the `@pipeline` decorator. Here’s an example of a training pipeline:

```python
from zenml import pipeline
from zenml import Model

@pipeline(
    enable_cache=False,
    model=Model(
        name="demo",
        license="Apache",
        description="Show case Model Control Plane.",
    ),
)
def train_and_promote_model():
    ...
```

Running this pipeline creates a new model version linked to the training artifacts.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/load-a-model-in-code.md

# Summary of ZenML Model Loading Documentation

## Loading a Model in Code

### 1. Load the Active Model in a Pipeline
You can load the active model to access its metadata and associated artifacts.

```python
from zenml import step, pipeline, get_step_context, Model

@pipeline(model=Model(name="my_model"))
def my_pipeline():
    ...

@step
def my_step():
    mv = get_step_context().model  # Get model from active step context
    print(mv.run_metadata["metadata_key"].value)  # Get metadata
    output = mv.get_artifact("my_dataset", "my_version")  # Fetch artifact
    output.run_metadata["accuracy"].value
```

### 2. Load Any Model via the Client
You can also load models using the `Client`.

```python
from zenml import step
from zenml.client import Client
from zenml.enums import ModelStages

@step
def model_evaluator_step():
    try:
        staging_zenml_model = Client().get_model_version(
            model_name_or_id="<INSERT_MODEL_NAME>",
            model_version_name_or_number_or_id=ModelStages.STAGING,
        )
    except KeyError:
        staging_zenml_model = None
```

This documentation outlines methods to load ZenML models, either through the active model in a pipeline or using the Client to access any model version.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/load-artifacts-from-model.md

### Summary of Documentation on Loading Artifacts from a Model

This documentation discusses how to load artifacts from a model in a two-pipeline project, where the first pipeline trains a model and the second performs batch inference using the trained model's artifacts.

#### Key Points:

1. **Model Context**: Use `get_pipeline_context().model` to access the model context during pipeline execution. This context is evaluated at runtime, not during pipeline compilation.

2. **Artifact Loading**: 
   - The method `model.get_model_artifact("trained_model")` retrieves the trained model artifact. This loading occurs during the step execution, allowing for delayed materialization.

3. **Alternative Method**: 
   - You can also use the `Client` class to directly fetch the model version:
     ```python
     from zenml.client import Client

     @pipeline
     def do_predictions():
         model = Client().get_model_version("iris_classifier", ModelStages.PRODUCTION)
         inference_data = load_data()
         predict(
             model=model.get_model_artifact("trained_model"),  
             data=inference_data,
         )
     ```

4. **Execution Timing**: In both approaches, the actual evaluation of the model artifact occurs only when the step is executed.

This concise overview retains all critical technical details necessary for understanding how to load artifacts from a model in ZenML pipelines.

================================================================================

File: docs/book/how-to/model-management-metrics/model-control-plane/delete-a-model.md

### Deleting Models in ZenML

**Overview**: Deleting a model or its specific version removes all links to artifacts and pipeline runs, along with associated metadata.

#### Deleting All Versions of a Model

- **CLI Command**:
  ```shell
  zenml model delete <MODEL_NAME>
  ```

- **Python SDK**:
  ```python
  from zenml.client import Client
  Client().delete_model(<MODEL_NAME>)
  ```

#### Deleting a Specific Version of a Model

- **CLI Command**:
  ```shell
  zenml model version delete <MODEL_VERSION_NAME>
  ```

- **Python SDK**:
  ```python
  from zenml.client import Client
  Client().delete_model_version(<MODEL_VERSION_ID>)
  ```

================================================================================

File: docs/book/how-to/contribute-to-zenml/README.md

# Contribute to ZenML

Thank you for considering contributing to ZenML! 

## How to Contribute

We welcome contributions in various forms, including new features, documentation improvements, integrations, or bug reports. For detailed guidelines on contributing, refer to the [ZenML contribution guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md), which outlines best practices and conventions.

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/contribute-to-zenml/implement-a-custom-integration.md

### Summary: Creating an External Integration for ZenML

ZenML aims to streamline the MLOps landscape by providing numerous integrations with popular tools. This guide is for those looking to contribute their own integrations to ZenML.

#### Step 1: Plan Your Integration
Identify the categories your integration fits into from the [ZenML categories list](../../component-guide/README.md). An integration may belong to multiple categories (e.g., cloud integrations like AWS/GCP/Azure).

#### Step 2: Create Stack Component Flavors
Develop individual stack component flavors corresponding to the identified categories. Test them as custom flavors before packaging. For example, to register a custom orchestrator flavor:

```shell
zenml orchestrator flavor register flavors.my_flavor.MyOrchestratorFlavor
```

Ensure ZenML is initialized at the root of your repository to avoid resolution issues.

#### Step 3: Create an Integration Class
1. **Clone Repo**: Clone the [ZenML repository](https://github.com/zenml-io/zenml) and set up your environment as per the [contributing guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).
2. **Create Integration Directory**: Structure your integration in `src/zenml/integrations/<your_integration>/` with subdirectories for artifact stores and flavors.

3. **Define Integration Name**: Add your integration name to `zenml/integrations/constants.py`:

```python
EXAMPLE_INTEGRATION = "<name-of-integration>"
```

4. **Create Integration Class**: In `__init__.py`, subclass the `Integration` class, set attributes, and define the `flavors` method:

```python
from zenml.integrations.constants import <EXAMPLE_INTEGRATION>
from zenml.integrations.integration import Integration
from zenml.stack import Flavor

class ExampleIntegration(Integration):
    NAME = <EXAMPLE_INTEGRATION>
    REQUIREMENTS = ["<INSERT PYTHON REQUIREMENTS HERE>"]

    @classmethod
    def flavors(cls):
        from zenml.integrations.<example_flavor> import <ExampleFlavor>
        return [<ExampleFlavor>]

ExampleIntegration.check_installation()
```

Refer to the [MLflow Integration](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/mlflow/__init__.py) for an example.

5. **Import the Integration**: Ensure your integration is imported in `src/zenml/integrations/__init__.py`.

#### Step 4: Create a PR
Submit a [pull request](https://github.com/zenml-io/zenml/compare) to ZenML for review by core maintainers.

Thank you for contributing to ZenML!

================================================================================

File: docs/book/how-to/data-artifact-management/README.md

# Data and Artifact Management in ZenML

This section outlines the management of data and artifacts within ZenML, focusing on key functionalities and processes.

### Key Concepts
- **Data Management**: Involves handling datasets used in machine learning workflows, ensuring they are versioned, reproducible, and accessible.
- **Artifact Management**: Refers to the handling of outputs generated during the ML pipeline, such as models, metrics, and visualizations.

### Core Features
1. **Versioning**: ZenML supports version control for datasets and artifacts, allowing users to track changes and revert to previous states.
2. **Storage**: Artifacts can be stored in various backends (e.g., local storage, cloud storage) to facilitate easy access and sharing.
3. **Metadata Tracking**: ZenML automatically tracks metadata associated with datasets and artifacts, providing insights into their usage and lineage.

### Code Snippet Example
```python
from zenml import pipeline

@pipeline
def my_pipeline():
    data = load_data()
    processed_data = preprocess(data)
    model = train_model(processed_data)
    save_artifact(model)

# Execute the pipeline
my_pipeline.run()
```

### Best Practices
- Regularly version datasets and artifacts to maintain reproducibility.
- Utilize cloud storage for scalability and collaboration.
- Monitor metadata for better tracking and auditing of ML workflows.

This summary encapsulates the essential aspects of data and artifact management in ZenML, providing a foundation for understanding its functionalities and best practices.

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/unmaterialized-artifacts.md

### Summary of Skipping Materialization of Artifacts in ZenML

**Overview**: In ZenML, pipelines are data-centric, where each step reads and writes artifacts to an artifact store. Materializers manage the serialization and deserialization of these artifacts. However, there are scenarios where you may want to skip materialization and use a reference to the artifact instead.

**Warning**: Skipping materialization can lead to unintended consequences for downstream tasks. Only do this if necessary.

### Skipping Materialization

To utilize an unmaterialized artifact, use `zenml.materializers.UnmaterializedArtifact`, which includes a `uri` property pointing to the artifact's storage path. Specify `UnmaterializedArtifact` as the type in the step function.

**Example Code**:
```python
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import step

@step
def my_step(my_artifact: UnmaterializedArtifact):
    pass
```

### Code Example

The following pipeline demonstrates the use of unmaterialized artifacts:

- `s1` and `s2` produce identical artifacts.
- `s3` consumes materialized artifacts, while `s4` consumes unmaterialized artifacts.

**Pipeline Structure**:
```
s1 -> s3 
s2 -> s4
```

**Example Code**:
```python
from typing_extensions import Annotated
from typing import Dict, List, Tuple
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import pipeline, step

@step
def step_1() -> Tuple[Annotated[Dict[str, str], "dict_"], Annotated[List[str], "list_"]]:
    return {"some": "data"}, []

@step
def step_2() -> Tuple[Annotated[Dict[str, str], "dict_"], Annotated[List[str], "list_"]]:
    return {"some": "data"}, []

@step
def step_3(dict_: Dict, list_: List) -> None:
    assert isinstance(dict_, dict)
    assert isinstance(list_, list)

@step
def step_4(dict_: UnmaterializedArtifact, list_: UnmaterializedArtifact) -> None:
    print(dict_.uri)
    print(list_.uri)

@pipeline
def example_pipeline():
    step_3(*step_1())
    step_4(*step_2())

example_pipeline()
```

For further examples of using `UnmaterializedArtifact`, refer to the documentation on triggering pipelines from another pipeline.

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/README.md

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I will be happy to assist you!

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/registering-existing-data.md

### Summary: Registering External Data as ZenML Artifacts

This documentation outlines how to register external data (folders and files) as ZenML artifacts for future use in machine learning pipelines.

#### Registering an Existing Folder as a ZenML Artifact
To register a folder containing data, follow these steps:

1. **Create a Folder and File**:
   ```python
   import os
   from uuid import uuid4
   from zenml.client import Client
   from zenml import register_artifact

   prefix = Client().active_stack.artifact_store.path
   preexisting_folder = os.path.join(prefix, f"my_test_folder_{uuid4()}")
   os.mkdir(preexisting_folder)
   with open(os.path.join(preexisting_folder, "test_file.txt"), "w") as f:
       f.write("test")
   ```

2. **Register the Folder**:
   ```python
   register_artifact(folder_or_file_uri=preexisting_folder, name="my_folder_artifact")
   ```

3. **Consume the Artifact**:
   ```python
   temp_artifact_folder_path = Client().get_artifact_version(name_id_or_prefix="my_folder_artifact").load()
   ```

#### Registering an Existing File as a ZenML Artifact
For registering a single file, the process is similar:

1. **Create a File**:
   ```python
   preexisting_file = os.path.join(preexisting_folder, "test_file.txt")
   with open(preexisting_file, "w") as f:
       f.write("test")
   ```

2. **Register the File**:
   ```python
   register_artifact(folder_or_file_uri=preexisting_file, name="my_file_artifact")
   ```

3. **Consume the Artifact**:
   ```python
   temp_artifact_file_path = Client().get_artifact_version(name_id_or_prefix="my_file_artifact").load()
   ```

#### Registering Checkpoints from a PyTorch Lightning Training Run
To register all checkpoints from a PyTorch Lightning training run:

1. **Set Up the Trainer**:
   ```python
   trainer = Trainer(default_root_dir=os.path.join(prefix, uuid4().hex), callbacks=[ModelCheckpoint(every_n_epochs=1, save_top_k=-1)])
   trainer.fit(model)
   ```

2. **Register Checkpoints**:
   ```python
   register_artifact(default_root_dir, name="all_my_model_checkpoints")
   ```

#### Custom Checkpoint Callback for ZenML
Extend the `ModelCheckpoint` to register each checkpoint as a separate artifact version:

```python
class ZenMLModelCheckpoint(ModelCheckpoint):
    def __init__(self, artifact_name: str, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.artifact_name = artifact_name

    def on_train_epoch_end(self, trainer, pl_module):
        super().on_train_epoch_end(trainer, pl_module)
        register_artifact(os.path.join(self.dirpath, self.filename_format.format(epoch=trainer.current_epoch)), self.artifact_name)
```

#### Example Pipeline
An example pipeline integrates data loading, model training, and prediction using the custom checkpointing:

```python
@pipeline(model=Model(name="LightningDemo"))
def train_pipeline(artifact_name: str = "my_model_ckpts"):
    train_loader = get_data()
    model = get_model()
    train_model(model, train_loader, 10, artifact_name)
    predict(get_pipeline_context().model.get_artifact(artifact_name), after=["train_model"])
```

This pipeline demonstrates how to manage checkpoints and artifacts effectively within ZenML.

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/datasets.md

# Summary of Custom Dataset Classes and Complex Data Flows in ZenML

## Overview
ZenML provides custom Dataset classes to manage complex data flows in machine learning projects, allowing efficient handling of various data sources (CSV, databases, cloud storage) and custom processing logic.

## Custom Dataset Classes
Custom Dataset classes encapsulate data loading, processing, and saving logic. They are beneficial when:
- Working with multiple data sources.
- Handling complex data structures.
- Implementing custom data processing.

### Implementation Example
A base `Dataset` class can be implemented for different data sources like CSV and BigQuery:

```python
from abc import ABC, abstractmethod
import pandas as pd
from google.cloud import bigquery
from typing import Optional

class Dataset(ABC):
    @abstractmethod
    def read_data(self) -> pd.DataFrame:
        pass

class CSVDataset(Dataset):
    def __init__(self, data_path: str, df: Optional[pd.DataFrame] = None):
        self.data_path = data_path
        self.df = df

    def read_data(self) -> pd.DataFrame:
        if self.df is None:
            self.df = pd.read_csv(self.data_path)
        return self.df

class BigQueryDataset(Dataset):
    def __init__(self, table_id: str, project: Optional[str] = None):
        self.table_id = table_id
        self.project = project
        self.client = bigquery.Client(project=self.project)

    def read_data(self) -> pd.DataFrame:
        query = f"SELECT * FROM `{self.table_id}`"
        return self.client.query(query).to_dataframe()

    def write_data(self) -> None:
        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = self.client.load_table_from_dataframe(self.df, self.table_id, job_config=job_config)
        job.result()
```

## Custom Materializers
Materializers in ZenML manage artifact serialization. Custom Materializers are necessary for custom Dataset classes:

### CSVDatasetMaterializer Example
```python
from zenml.materializers import BaseMaterializer
from zenml.io import fileio
import json
import tempfile
import pandas as pd

class CSVDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (CSVDataset,)

    def load(self, data_type: Type[CSVDataset]) -> CSVDataset:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            with fileio.open(os.path.join(self.uri, "data.csv"), "rb") as source_file:
                temp_file.write(source_file.read())
        return CSVDataset(temp_file.name)

    def save(self, dataset: CSVDataset) -> None:
        df = dataset.read_data()
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            df.to_csv(temp_file.name, index=False)
            with open(temp_file.name, "rb") as source_file:
                with fileio.open(os.path.join(self.uri, "data.csv"), "wb") as target_file:
                    target_file.write(source_file.read())
```

### BigQueryDatasetMaterializer Example
```python
class BigQueryDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (BigQueryDataset,)

    def load(self, data_type: Type[BigQueryDataset]) -> BigQueryDataset:
        with fileio.open(os.path.join(self.uri, "metadata.json"), "r") as f:
            metadata = json.load(f)
        return BigQueryDataset(table_id=metadata["table_id"], project=metadata["project"])

    def save(self, bq_dataset: BigQueryDataset) -> None:
        metadata = {"table_id": bq_dataset.table_id, "project": bq_dataset.project}
        with fileio.open(os.path.join(self.uri, "metadata.json"), "w") as f:
            json.dump(metadata, f)
        if bq_dataset.df is not None:
            bq_dataset.write_data()
```

## Managing Complex Pipelines
Design pipelines to handle different data sources effectively:

```python
@step
def extract_data_local(data_path: str) -> CSVDataset:
    return CSVDataset(data_path)

@step
def extract_data_remote(table_id: str) -> BigQueryDataset:
    return BigQueryDataset(table_id)

@step
def transform(dataset: Dataset) -> pd.DataFrame:
    df = dataset.read_data()
    # Transform data
    return df.copy()

@pipeline
def etl_pipeline(mode: str):
    raw_data = extract_data_local() if mode == "develop" else extract_data_remote(table_id="project.dataset.raw_table")
    return transform(raw_data)
```

## Best Practices
1. **Use a common base class**: This allows consistent handling of datasets.
2. **Specialized loading steps**: Implement separate steps for different datasets.
3. **Flexible pipelines**: Use configuration parameters or logic to adapt to data sources.
4. **Modular step design**: Create specific steps for tasks to enhance reusability and maintenance.

By following these practices, ZenML pipelines can efficiently manage complex data flows and adapt to changing requirements, leveraging custom Dataset classes throughout machine learning workflows.

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/manage-big-data.md

### Summary of Scaling Strategies for Big Data in ZenML

This documentation outlines strategies for managing large datasets in ZenML, focusing on scaling pipelines as data size increases. It categorizes datasets into three sizes and provides corresponding strategies for each.

#### Dataset Size Thresholds:
1. **Small datasets (up to a few GB)**: Handled in-memory with pandas.
2. **Medium datasets (up to tens of GB)**: Require chunking or out-of-core processing.
3. **Large datasets (hundreds of GB or more)**: Necessitate distributed processing frameworks.

#### Strategies for Small Datasets:
1. **Efficient Data Formats**: Use formats like Parquet instead of CSV.
   ```python
   import pyarrow.parquet as pq

   class ParquetDataset(Dataset):
       def __init__(self, data_path: str):
           self.data_path = data_path

       def read_data(self) -> pd.DataFrame:
           return pq.read_table(self.data_path).to_pandas()

       def write_data(self, df: pd.DataFrame):
           pq.write_table(pa.Table.from_pandas(df), self.data_path)
   ```

2. **Data Sampling**: Implement sampling methods in Dataset classes.
   ```python
   class SampleableDataset(Dataset):
       def sample_data(self, fraction: float = 0.1) -> pd.DataFrame:
           return self.read_data().sample(frac=fraction)
   ```

3. **Optimize Pandas Operations**: Use efficient operations to minimize memory usage.
   ```python
   @step
   def optimize_processing(df: pd.DataFrame) -> pd.DataFrame:
       df['new_column'] = df['column1'] + df['column2']
       df['mean_normalized'] = df['value'] - np.mean(df['value'])
       return df
   ```

#### Strategies for Medium Datasets:
1. **Chunking for CSV Datasets**: Process large files in chunks.
   ```python
   class ChunkedCSVDataset(Dataset):
       def __init__(self, data_path: str, chunk_size: int = 10000):
           self.data_path = data_path
           self.chunk_size = chunk_size

       def read_data(self):
           for chunk in pd.read_csv(self.data_path, chunksize=self.chunk_size):
               yield chunk
   ```

2. **Data Warehouses**: Use services like Google BigQuery for distributed processing.
   ```python
   @step
   def process_big_query_data(dataset: BigQueryDataset) -> BigQueryDataset:
       client = bigquery.Client()
       query = f"SELECT column1, AVG(column2) as avg_column2 FROM `{dataset.table_id}` GROUP BY column1"
       job_config = bigquery.QueryJobConfig(destination=f"{dataset.project}.{dataset.dataset}.processed_data")
       client.query(query, job_config=job_config).result()
       return BigQueryDataset(table_id=result_table_id)
   ```

#### Strategies for Very Large Datasets:
1. **Distributed Computing Frameworks**: Use frameworks like Apache Spark or Ray directly in ZenML pipelines.
   - **Apache Spark Example**:
     ```python
     from pyspark.sql import SparkSession

     @step
     def process_with_spark(input_data: str) -> None:
         spark = SparkSession.builder.appName("ZenMLSparkStep").getOrCreate()
         df = spark.read.csv(input_data, header=True)
         df.groupBy("column1").agg({"column2": "mean"}).write.csv("output_path", header=True)
         spark.stop()
     ```

   - **Ray Example**:
     ```python
     import ray

     @step
     def process_with_ray(input_data: str) -> None:
         ray.init()
         results = ray.get([process_partition.remote(part) for part in split_data(load_data(input_data))])
         save_results(combine_results(results), "output_path")
         ray.shutdown()
     ```

2. **Using Dask**: Integrate Dask for parallel computing.
   ```python
   import dask.dataframe as dd

   @step
   def create_dask_dataframe():
       return dd.from_pandas(pd.DataFrame({'A': range(1000), 'B': range(1000, 2000)}), npartitions=4)
   ```

3. **Using Numba**: Accelerate numerical computations with Numba.
   ```python
   from numba import jit

   @jit(nopython=True)
   def numba_function(x):
       return x * x + 2 * x - 1
   ```

#### Important Considerations:
- Ensure the execution environment has necessary frameworks installed.
- Manage resources effectively when using distributed frameworks.
- Implement error handling and data I/O strategies for large datasets.
- Choose scaling strategies based on dataset size, processing complexity, infrastructure, update frequency, and team expertise.

By following these strategies, ZenML pipelines can efficiently handle datasets of varying sizes, ensuring scalable machine learning workflows. For more details on creating custom Dataset classes, refer to the [custom dataset classes](datasets.md) documentation.

================================================================================

File: docs/book/how-to/data-artifact-management/complex-usecases/passing-artifacts-between-pipelines.md

### Structuring an MLOps Project

An MLOps project typically consists of multiple pipelines, such as:

- **Feature Engineering Pipeline**: Prepares raw data for training.
- **Training Pipeline**: Trains models using data from the feature engineering pipeline.
- **Inference Pipeline**: Runs batch predictions on the trained model.
- **Deployment Pipeline**: Deploys the trained model to a production endpoint.

The structure of these pipelines can vary based on project requirements, and sharing artifacts (models, metadata) between them is essential.

#### Pattern 1: Artifact Exchange via `Client`

In this pattern, the ZenML Client facilitates the exchange of artifacts between pipelines. For instance, a feature engineering pipeline generates datasets that the training pipeline consumes.

**Example Code:**
```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    train_data = client.get_artifact_version(name="iris_training_dataset")
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")
    sklearn_classifier = model_trainer(train_data)
    model_evaluator(model, sklearn_classifier)
```
*Note: Artifacts are referenced, not materialized in memory during pipeline compilation.*

#### Pattern 2: Artifact Exchange via `Model`

This approach uses a ZenML Model as a reference point for artifacts. For example, a training pipeline (`train_and_promote`) produces models, which are promoted based on accuracy. The inference pipeline (`do_predictions`) retrieves the latest promoted model without needing to know specific artifact IDs.

**Example Code:**
```python
from zenml import step, get_step_context

@step(enable_cache=False)
def predict(data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    model = get_step_context().model.get_model_artifact("trained_model")
    predictions = pd.Series(model.predict(data))
    return predictions
```
*Note: Disabling caching is crucial to avoid unexpected results.*

Alternatively, you can resolve the artifact at the pipeline level:

```python
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages

@step
def predict(model: ClassifierMixin, data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    return pd.Series(model.predict(data))

@pipeline(model=Model(name="iris_classifier", version=ModelStages.PRODUCTION))
def do_predictions():
    model = get_pipeline_context().model.get_model_artifact("trained_model")
    inference_data = load_data()
    predict(model=model, data=inference_data)

if __name__ == "__main__":
    do_predictions()
```

Both approaches are valid; the choice depends on user preference.

================================================================================

File: docs/book/how-to/data-artifact-management/visualize-artifacts/types-of-visualizations.md

### Types of Visualizations in ZenML

ZenML automatically saves and displays visualizations of various data types in the ZenML dashboard. These visualizations can also be accessed in Jupyter notebooks using the `artifact.visualize()` method.

**Examples of Default Visualizations:**
- Statistical representation of a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) as a PNG image.
- Drift detection reports from:
  - [Evidently](../../../component-guide/data-validators/evidently.md)
  - [Great Expectations](../../../component-guide/data-validators/great-expectations.md)
  - [whylogs](../../../component-guide/data-validators/whylogs.md)
- A [Hugging Face datasets viewer](https://zenml.io/integrations/huggingface) embedded as an HTML iframe.

Visualizations enhance data understanding and facilitate analysis within ZenML's ecosystem.

================================================================================

File: docs/book/how-to/data-artifact-management/visualize-artifacts/README.md

### ZenML Data Visualization Configuration

**Overview**: This documentation outlines how to configure ZenML to visualize data artifacts in the dashboard.

**Key Points**:
- ZenML allows easy association of visualizations with data artifacts.
- The dashboard provides a graphical representation of these artifacts.

**Visual Example**: 
- ![ZenML Artifact Visualizations](../../../.gitbook/assets/artifact_visualization_dashboard.png)

This configuration enhances the user experience by enabling clear insights into data artifacts through visual representations.

================================================================================

File: docs/book/how-to/data-artifact-management/visualize-artifacts/creating-custom-visualizations.md

### Creating Custom Visualizations in ZenML

ZenML allows you to create custom visualizations for artifacts using supported types: 

- **HTML:** Embedded HTML visualizations.
- **Image:** Visualizations of image data (e.g., Pillow images).
- **CSV:** Tables like pandas DataFrame `.describe()` output.
- **Markdown:** Markdown strings or pages.
- **JSON:** JSON strings or objects.

#### Methods to Add Custom Visualizations

1. **Special Return Types:** If you have HTML, Markdown, CSV, or JSON data, cast them to specific types in your step:
   - `zenml.types.HTMLString`
   - `zenml.types.MarkdownString`
   - `zenml.types.CSVString`
   - `zenml.types.JSONString`

   **Example:**
   ```python
   from zenml.types import CSVString

   @step
   def my_step() -> CSVString:
       return CSVString("a,b,c\n1,2,3")
   ```

2. **Materializers:** Override the `save_visualizations()` method in a custom materializer to extract visualizations for all artifacts of a specific data type. Refer to the [materializer docs](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md#optional-how-to-visualize-the-artifact) for details.

3. **Custom Return Type Class:** Create a custom class and materializer to visualize any data type. 

   **Steps:**
   1. Create a custom class for the data.
   2. Build a custom materializer with visualization logic in `save_visualizations()`.
   3. Return the custom class from your ZenML steps.

   **Example:**
   - **Custom Class:**
   ```python
   class FacetsComparison(BaseModel):
       datasets: List[Dict[str, Union[str, pd.DataFrame]]]
   ```

   - **Materializer:**
   ```python
   class FacetsMaterializer(BaseMaterializer):
       ASSOCIATED_TYPES = (FacetsComparison,)
       ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS

       def save_visualizations(self, data: FacetsComparison) -> Dict[str, VisualizationType]:
           html = ...  # Create visualization
           with fileio.open(os.path.join(self.uri, VISUALIZATION_FILENAME), "w") as f:
               f.write(html)
           return {visualization_path: VisualizationType.HTML}
   ```

   - **Step:**
   ```python
   @step
   def facets_visualization_step(reference: pd.DataFrame, comparison: pd.DataFrame) -> FacetsComparison:
       return FacetsComparison(datasets=[{"name": "reference", "table": reference}, {"name": "comparison", "table": comparison}])
   ```

#### Visualization Workflow
1. The step returns a `FacetsComparison`.
2. ZenML finds the `FacetsMaterializer` and calls `save_visualizations()`, creating and saving the visualization.
3. The visualization HTML file is displayed in the dashboard when accessed.

This process allows for flexible and powerful custom visualizations within ZenML.

================================================================================

File: docs/book/how-to/data-artifact-management/visualize-artifacts/disabling-visualizations.md

### Disabling Visualizations

To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level:

```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

This configuration prevents visualizations from being generated for the specified step or pipeline.

================================================================================

File: docs/book/how-to/data-artifact-management/visualize-artifacts/visualizations-in-dashboard.md

### Summary: Displaying Visualizations in the ZenML Dashboard

To display visualizations on the ZenML dashboard, the following steps are necessary:

1. **Service Connector Configuration**: 
   - Visualizations are stored in the artifact store. Users must configure a service connector to allow the ZenML server to access this store.
   - For detailed guidance, refer to the [service connector documentation](../../infrastructure-deployment/auth-management/README.md) and the [AWS S3 artifact store documentation](../../../component-guide/artifact-stores/s3.md).

2. **Local Artifact Store Limitation**: 
   - If using the default/local artifact store with a deployed ZenML, the server cannot access local files, resulting in visualizations not being displayed. A remote artifact store with an enabled service connector is required to view visualizations.

3. **Artifact Store Configuration**: 
   - If visualizations from a pipeline run are missing, ensure the ZenML server has the necessary dependencies and permissions for the artifact store. Additional details can be found on the [custom artifact store documentation page](../../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores).

This setup is crucial for successful visualization display in the ZenML dashboard.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/README.md

### Summary of ZenML Step Outputs and Pipeline

**Overview**: In ZenML, step outputs are stored in an artifact store, facilitating caching, lineage, and auditability. Utilizing type annotations enhances transparency, data passing between steps, and data serialization/deserialization (termed 'materialize').

**Key Points**:
- Use type annotations for outputs to improve code clarity and functionality.
- Data flows between steps in a ZenML pipeline, enabling structured processing.

**Code Example**:
```python
@step
def load_data(parameter: int) -> Dict[str, Any]:
    training_data = [[1, 2], [3, 4], [5, 6]]
    labels = [0, 1, 0]
    return {'features': training_data, 'labels': labels}

@step
def train_model(data: Dict[str, Any]) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}")

@pipeline  
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter)
    train_model(dataset)
```

**Explanation**:
- `load_data`: Accepts an integer parameter and returns a dictionary with training data and labels.
- `train_model`: Receives the dataset, computes sums of features and labels, and simulates model training.
- `simple_ml_pipeline`: Chains `load_data` and `train_model`, demonstrating data flow in a ZenML pipeline.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/artifacts-naming.md

### ZenML Artifact Naming Overview

In ZenML pipelines, managing artifact names is crucial for tracking outputs, especially when reusing steps with different inputs. ZenML leverages type annotations to determine artifact names, incrementing version numbers for artifacts with the same name. It supports both static and dynamic naming strategies.

#### Naming Strategies

1. **Static Naming**: Defined as string literals.
   ```python
   @step
   def static_single() -> Annotated[str, "static_output_name"]:
       return "null"
   ```

2. **Dynamic Naming**: Generated at runtime using string templates.

   - **Standard Placeholders**:
     - `{date}`: Current date (e.g., `2024_11_18`)
     - `{time}`: Current time (e.g., `11_07_09_326492`)
   ```python
   @step
   def dynamic_single_string() -> Annotated[str, "name_{date}_{time}"]:
       return "null"
   ```

   - **Custom Placeholders**: Provided via `substitutions` parameter.
   ```python
   @step(substitutions={"custom_placeholder": "some_substitute"})
   def dynamic_single_string() -> Annotated[str, "name_{custom_placeholder}_{time}"]:
       return "null"
   ```

   - **Using `with_options`**:
   ```python
   @step
   def extract_data(source: str) -> Annotated[str, "{stage}_dataset"]:
       ...
       return "my data"

   @pipeline
   def extraction_pipeline():
       extract_data.with_options(substitutions={"stage": "train"})(source="s3://train")
       extract_data.with_options(substitutions={"stage": "test"})(source="s3://test")
   ```

   **Substitution Scope**:
   - Set at `@pipeline`, `pipeline.with_options`, `@step`, or `step.with_options`.

3. **Multiple Output Handling**: Combine naming options for multiple artifacts.
   ```python
   @step
   def mixed_tuple() -> Tuple[
       Annotated[str, "static_output_name"],
       Annotated[str, "name_{date}_{time}"],
   ]:
       return "static_namer", "str_namer"
   ```

#### Caching Behavior

When caching is enabled, artifact names remain consistent across runs. Example:
```python
@step(substitutions={"custom_placeholder": "resolution"})
def demo() -> Tuple[
    Annotated[int, "name_{date}_{time}"],
    Annotated[int, "name_{custom_placeholder}"],
]:
    return 42, 43

@pipeline
def my_pipeline():
    demo()

if __name__ == "__main__":
    run_without_cache = my_pipeline.with_options(enable_cache=False)()
    run_with_cache = my_pipeline.with_options(enable_cache=True)()
```

**Output Example**:
```
['name_2024_11_21_14_27_33_750134', 'name_resolution']
```

This summary captures the key points of artifact naming in ZenML, including static and dynamic naming strategies, handling multiple outputs, and caching behavior.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/load-artifacts-into-memory.md

# Summary of Loading Artifacts in ZenML Pipelines

ZenML pipelines typically consume artifacts produced by one another directly, but external data may also be needed. For external artifacts from non-ZenML sources, use `ExternalArtifact`. For data exchange between ZenML pipelines, late materialization is essential, allowing the use of artifacts that do not yet exist at the time of pipeline compilation.

## Key Use Cases for Artifact Exchange
1. Grouping data products using ZenML Models.
2. Using the ZenML Client to manage artifacts.

**Recommendation:** Utilize models for artifact grouping and access. Refer to the documentation for loading artifacts from a ZenML Model.

## Exchanging Artifacts with Client Methods
If not using the Model Control Plane, artifacts can still be exchanged with late materialization. Below is a streamlined version of the `do_predictions` pipeline code:

```python
from typing import Annotated
from zenml import step, pipeline
from zenml.client import Client
import pandas as pd
from sklearn.base import ClassifierMixin

@step
def predict(model1: ClassifierMixin, model2: ClassifierMixin, model1_metric: float, model2_metric: float, data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    predictions = pd.Series(model1.predict(data)) if model1_metric < model2_metric else pd.Series(model2.predict(data))
    return predictions

@step
def load_data() -> pd.DataFrame:
    # load inference data
    ...

@pipeline
def do_predictions():
    model_42 = Client().get_artifact_version("trained_model", version="42")
    metric_42 = model_42.run_metadata["MSE"].value
    model_latest = Client().get_artifact_version("trained_model")
    metric_latest = model_latest.run_metadata["MSE"].value
    inference_data = load_data()
    
    predict(model1=model_42, model2=model_latest, model1_metric=metric_42, model2_metric=metric_latest, data=inference_data)

if __name__ == "__main__":
    do_predictions()
```

### Explanation of Code Changes
- The `predict` step now includes a metric comparison to select the best model dynamically.
- The `load_data` step is added for loading inference data.
- Calls to `Client().get_artifact_version()` and `model_latest.run_metadata["MSE"].value` are evaluated at execution time, ensuring the latest versions are used.

This approach ensures that the most current artifacts are utilized during pipeline execution rather than at compilation.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/artifact-versioning.md

### ZenML Data Storage Overview

ZenML integrates data versioning and lineage tracking into its core functionality, automatically managing artifacts generated during pipeline executions. Users can view the lineage of artifacts and interact with them through a dashboard, enhancing insights and reproducibility in machine learning workflows.

#### Artifact Creation and Caching
When a ZenML pipeline runs, it checks for changes in inputs, outputs, parameters, or configurations. Each step generates a new directory in the artifact store. If a step is new or modified, ZenML creates a unique directory structure with a unique ID and stores the data using appropriate materializers. If unchanged, ZenML may cache the step, saving time and resources.

This lineage tracking allows users to trace artifacts back to their origins, ensuring reproducibility and helping identify issues in pipelines. For artifact versioning and configuration details, refer to the [artifact management documentation](../../../user-guide/starter-guide/manage-artifacts.md).

#### Materializers
Materializers are essential for artifact management, handling serialization and deserialization to ensure consistent storage and retrieval. Each materializer stores data in unique directories within the artifact store. ZenML provides built-in materializers for common data types and uses `cloudpickle` for objects without a default materializer. Custom materializers can be created by extending the `BaseMaterializer` class.

**Warning:** The built-in `CloudpickleMaterializer` can handle any object but is not production-ready due to compatibility issues across Python versions and potential security risks. For robust solutions, consider building custom materializers.

When a pipeline runs, ZenML utilizes materializers to save and load artifacts through the ZenML `fileio` system, facilitating artifact caching and lineage tracking. An example of a default materializer (the `numpy` materializer) can be found [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy_materializer.py).

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/tagging.md

### Summary: Organizing Data with Tags in ZenML

ZenML allows users to organize machine learning artifacts and models using tags, enhancing workflow and discoverability. This guide covers how to assign tags to artifacts and models.

#### Assigning Tags to Artifacts

To tag artifact versions in a step or pipeline, use the `tags` property of `ArtifactConfig`:

**Python SDK Example:**
```python
from zenml import step, ArtifactConfig

@step
def training_data_loader() -> (
    Annotated[pd.DataFrame, ArtifactConfig(tags=["sklearn", "pre-training"])]
):
    ...
```

**CLI Example:**
```shell
# Tag the artifact
zenml artifacts update iris_dataset -t sklearn

# Tag the artifact version
zenml artifacts versions update iris_dataset raw_2023 -t sklearn
```

Tags like `sklearn` and `pre-training` will be assigned to all artifacts created by the step. ZenML Pro users can tag artifacts directly in the cloud dashboard.

#### Assigning Tags to Models

Models can also be tagged for organization. Tags are specified as key-value pairs when creating a model version:

**Python SDK Example:**
```python
from zenml.models import Model

# Define tags
tags = ["experiment", "v1", "classification-task"]

# Create a model version with tags
model = Model(name="iris_classifier", version="1.0.0", tags=tags)

@pipeline(model=model)
def my_pipeline(...):
    ...
```

You can also create or register models and their versions with tags:

```python
from zenml.client import Client

# Create a new model with tags
Client().create_model(name="iris_logistic_regression", tags=["classification", "iris-dataset"])

# Create a new model version with tags
Client().create_model_version(model_name_or_id="iris_logistic_regression", name="2", tags=["version-1", "experiment-42"])
```

To add tags to existing models using the CLI:

```shell
# Tag an existing model
zenml model update iris_logistic_regression --tag "classification"

# Tag a specific model version
zenml model version update iris_logistic_regression 2 --tag "experiment3"
```

### Important Notes
- During a pipeline run, models can be implicitly created without tags from the `Model` class.
- Tags improve the organization and filtering of ML assets within the ZenML ecosystem.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/get-arbitrary-artifacts-in-a-step.md

### Summary of Documentation

This documentation explains how to access artifacts in a step that may not originate from direct upstream steps. Artifacts can be fetched from other pipelines or steps using the ZenML client.

#### Key Points:
- Artifacts can be accessed using the ZenML client within a step.
- This allows for the retrieval of artifacts created and stored in the artifact store, which can be useful for integrating data from different sources.

#### Code Example:
```python
from zenml.client import Client
from zenml import step

@step
def my_step():
    client = Client()
    # Fetch an artifact
    output = client.get_artifact_version("my_dataset", "my_version")
    accuracy = output.run_metadata["accuracy"].value
```

#### Additional Resources:
- Refer to the [Managing artifacts](../../../user-guide/starter-guide/manage-artifacts.md) guide for information on the `ExternalArtifact` type and artifact passing between steps.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/handle-custom-data-types.md

### Summary of ZenML Materializers Documentation

#### Overview
ZenML pipelines are data-centric, where steps read and write artifacts to an artifact store. **Materializers** are responsible for the serialization and deserialization of artifacts, defining how they are stored and retrieved.

#### Built-In Materializers
ZenML includes several built-in materializers for common data types, which operate without user intervention:

| Materializer | Handled Data Types | Storage Format |
|--------------|---------------------|----------------|
| BuiltInMaterializer | `bool`, `float`, `int`, `str`, `None` | `.json` |
| BytesMaterializer | `bytes` | `.txt` |
| BuiltInContainerMaterializer | `dict`, `list`, `set`, `tuple` | Directory |
| NumpyMaterializer | `np.ndarray` | `.npy` |
| PandasMaterializer | `pd.DataFrame`, `pd.Series` | `.csv` (or `.gzip` with parquet) |
| PydanticMaterializer | `pydantic.BaseModel` | `.json` |
| ServiceMaterializer | `zenml.services.service.BaseService` | `.json` |
| StructuredStringMaterializer | `zenml.types.CSVString`, `zenml.types.HTMLString`, `zenml.types.MarkdownString` | `.csv`, `.html`, `.md` |

**Warning:** The `CloudpickleMaterializer` can handle any object but is not production-ready due to compatibility issues across Python versions.

#### Integration Materializers
ZenML also provides integration-specific materializers that can be activated by installing the respective integration. Examples include:

- **BentoMaterializer** for `bentoml.Bento` (`.bento`)
- **DeepchecksResultMaterializer** for `deepchecks.CheckResult` (`.json`)
- **LightGBMBoosterMaterializer** for `lgbm.Booster` (`.txt`)

#### Custom Materializers
To create a custom materializer:

1. **Define the Materializer:**
   ```python
   class MyMaterializer(BaseMaterializer):
       ASSOCIATED_TYPES = (MyObj,)
       ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

       def load(self, data_type: Type[MyObj]) -> MyObj:
           # Logic to load data
           ...

       def save(self, my_obj: MyObj) -> None:
           # Logic to save data
           ...
   ```

2. **Configure Steps to Use the Materializer:**
   ```python
   @step(output_materializers=MyMaterializer)
   def my_first_step() -> MyObj:
       return MyObj("my_object")
   ```

3. **Global Materializer Registration:**
   To use a custom materializer globally, register it:
   ```python
   materializer_registry.register_and_overwrite_type(key=pd.DataFrame, type_=FastPandasMaterializer)
   ```

#### Example of Materialization
A simple pipeline example with a custom object:
```python
@step
def my_first_step() -> MyObj:
    return MyObj("my_object")

@step
def my_second_step(my_obj: MyObj) -> None:
    logging.info(f"The following object was passed: `{my_obj.name}`")

@pipeline
def first_pipeline():
    output_1 = my_first_step()
    my_second_step(output_1)

first_pipeline()
```

To avoid warnings about unregistered materializers, implement a custom materializer for `MyObj` and configure it in the step.

#### Important Methods in BaseMaterializer
- **load(data_type)**: Defines how to read data from the artifact store.
- **save(data)**: Defines how to write data to the artifact store.
- **save_visualizations(data)**: Optionally saves visualizations of the artifact.
- **extract_metadata(data)**: Optionally extracts metadata from the artifact.

#### Notes
- Use `self.artifact_store` for compatibility across different artifact stores.
- Disable artifact visualization or metadata extraction at the pipeline or step level if needed.

This summary captures the essential details of using materializers in ZenML, including built-in options, integration materializers, and how to implement custom materializers effectively.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/delete-an-artifact.md

### Summary: Deleting Artifacts in ZenML

Currently, artifacts cannot be deleted directly to avoid breaking the ZenML database due to dangling references. However, you can delete artifacts that are no longer referenced by any pipeline runs using the following command:

```shell
zenml artifact prune
```

By default, this command removes artifacts from the underlying artifact store and the database. You can modify this behavior with the flags:
- `--only-artifact`: Deletes only the artifact.
- `--only-metadata`: Deletes only the database entry.

If you encounter errors due to local artifacts that no longer exist, use the `--ignore-errors` flag to continue pruning while suppressing error messages. Warning messages will still be displayed during the process.

================================================================================

File: docs/book/how-to/data-artifact-management/handle-data-artifacts/return-multiple-outputs-from-a-step.md

### Summary of Documentation on Using `Annotated` for Multiple Outputs

The `Annotated` type in ZenML allows a step to return multiple outputs with specific names, enhancing artifact retrieval and dashboard readability.

#### Code Example
```python
from typing import Annotated, Tuple
import pandas as pd
from zenml import step
from sklearn.model_selection import train_test_split

@step
def clean_data(data: pd.DataFrame) -> Tuple[
    Annotated[pd.DataFrame, "x_train"],
    Annotated[pd.DataFrame, "x_test"],
    Annotated[pd.Series, "y_train"],
    Annotated[pd.Series, "y_test"],
]:
    x = data.drop("target", axis=1)
    y = data["target"]
    return train_test_split(x, y, test_size=0.2, random_state=42)
```

#### Key Points
- The `clean_data` step accepts a pandas DataFrame and returns a tuple of four annotated outputs: `x_train`, `x_test`, `y_train`, and `y_test`.
- The data is split into features (`x`) and target (`y`), and then into training and testing sets using `train_test_split`.
- Annotated outputs facilitate easy identification and retrieval of artifacts in the pipeline and improve dashboard clarity.

================================================================================

File: docs/book/how-to/infrastructure-deployment/README.md

# Infrastructure and Deployment Summary

This section outlines the infrastructure setup and deployment processes for ZenML. Key components include:

1. **Infrastructure Requirements**: 
   - ZenML can be deployed on various cloud providers (AWS, GCP, Azure) and on-premises.
   - Ensure the environment meets prerequisites like Python version and necessary libraries.

2. **Deployment Options**:
   - **Local Deployment**: Suitable for development and testing. Install via pip:
     ```bash
     pip install zenml
     ```
   - **Cloud Deployment**: Use cloud services for scalability. Configure cloud credentials and set up ZenML with:
     ```bash
     zenml init
     ```

3. **Configuration**:
   - Configure ZenML using a `zenml.yaml` file to define pipelines, steps, and integrations.
   - Example configuration:
     ```yaml
     pipelines:
       - name: example_pipeline
         steps:
           - name: data_ingestion
           - name: model_training
     ```

4. **Version Control**:
   - Use Git for versioning pipelines and configurations to ensure reproducibility.

5. **Monitoring and Logging**:
   - Integrate with monitoring tools (e.g., Prometheus) for tracking performance and logs.

6. **Best Practices**:
   - Regularly update dependencies.
   - Use environment management tools (e.g., virtualenv, conda) to isolate project environments.

This summary encapsulates the essential elements of ZenML's infrastructure and deployment, providing a clear guide for setup and configuration.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/implement-a-custom-stack-component.md

# Custom Stack Component Flavor in ZenML

## Overview
ZenML allows for the creation of custom stack component flavors, enhancing composability and reusability in MLOps platforms. This guide covers the essentials of defining and implementing a custom flavor.

## Component Flavors
- **Component Type**: A broad category defining functionality (e.g., `artifact_store`).
- **Flavor**: Specific implementations of a component type (e.g., `local`, `s3`).

## Core Abstractions
1. **StackComponent**: Defines core functionality.
   ```python
   from zenml.stack import StackComponent

   class BaseArtifactStore(StackComponent):
       @abstractmethod
       def open(self, path, mode="r"):
           pass

       @abstractmethod
       def exists(self, path):
           pass
   ```

2. **StackComponentConfig**: Configures a stack component instance, separating configuration from implementation.
   ```python
   from zenml.stack import StackComponentConfig

   class BaseArtifactStoreConfig(StackComponentConfig):
       path: str
       SUPPORTED_SCHEMES: ClassVar[Set[str]]
   ```

3. **Flavor**: Combines `StackComponent` and `StackComponentConfig`, defining flavor name and type.
   ```python
   from zenml.enums import StackComponentType
   from zenml.stack import Flavor

   class LocalArtifactStoreFlavor(Flavor):
       @property
       def name(self) -> str:
           return "local"

       @property
       def type(self) -> StackComponentType:
           return StackComponentType.ARTIFACT_STORE

       @property
       def config_class(self) -> Type[LocalArtifactStoreConfig]:
           return LocalArtifactStoreConfig

       @property
       def implementation_class(self) -> Type[LocalArtifactStore]:
           return LocalArtifactStore
   ```

## Implementing a Custom Flavor
### Configuration Class
Define `SUPPORTED_SCHEMES` and additional configuration values:
```python
from zenml.artifact_stores import BaseArtifactStoreConfig
from zenml.utils.secret_utils import SecretField

class MyS3ArtifactStoreConfig(BaseArtifactStoreConfig):
    SUPPORTED_SCHEMES: ClassVar[Set[str]] = {"s3://"}
    key: Optional[str] = SecretField(default=None)
    secret: Optional[str] = SecretField(default=None)
    # Additional fields...
```

### Implementation Class
Implement abstract methods using S3:
```python
import s3fs
from zenml.artifact_stores import BaseArtifactStore

class MyS3ArtifactStore(BaseArtifactStore):
    _filesystem: Optional[s3fs.S3FileSystem] = None

    @property
    def filesystem(self) -> s3fs.S3FileSystem:
        if not self._filesystem:
            self._filesystem = s3fs.S3FileSystem(
                key=self.config.key,
                secret=self.config.secret,
                # Additional kwargs...
            )
        return self._filesystem

    def open(self, path, mode="r"):
        return self.filesystem.open(path=path, mode=mode)

    def exists(self, path):
        return self.filesystem.exists(path=path)
```

### Flavor Class
Combine configuration and implementation:
```python
from zenml.artifact_stores import BaseArtifactStoreFlavor

class MyS3ArtifactStoreFlavor(BaseArtifactStoreFlavor):
    @property
    def name(self):
        return 'my_s3_artifact_store'

    @property
    def implementation_class(self):
        return MyS3ArtifactStore

    @property
    def config_class(self):
        return MyS3ArtifactStoreConfig
```

## Registering the Flavor
Use the ZenML CLI to register:
```shell
zenml artifact-store flavor register flavors.my_flavor.MyS3ArtifactStoreFlavor
```

## Usage
After registration, use the custom flavor in stacks:
```shell
zenml artifact-store register <ARTIFACT_STORE_NAME> --flavor=my_s3_artifact_store --path='some-path'
zenml stack register <STACK_NAME> --artifact-store <ARTIFACT_STORE_NAME>
```

## Best Practices
- Execute `zenml init` at the repository root.
- Use the CLI to check required configuration values.
- Test flavors thoroughly before production use.
- Maintain clear documentation and clean code.

## Additional Resources
For specific stack component types, refer to the respective documentation links provided in the original text.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/export-stack-requirements.md

### Export Stack Requirements

To obtain the `pip` requirements for a specific stack, use the following CLI command:

```bash
zenml stack export-requirements <STACK-NAME> --output-file stack_requirements.txt
pip install -r stack_requirements.txt
```

This command exports the requirements to a file named `stack_requirements.txt`, which can then be used to install the necessary packages.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/README.md

### Managing Stacks & Components in ZenML

#### What is a Stack?
A **stack** in ZenML represents the configuration of infrastructure and tooling for executing pipelines. It consists of various components, each serving a specific function, such as:
- **Container Registry**: For managing container images.
- **Kubernetes Cluster**: Acts as an orchestrator.
- **Artifact Store**: For storing artifacts.
- **Experiment Tracker**: For tracking experiments (e.g., MLflow).

#### Organizing Execution Environments
ZenML allows running pipelines across multiple stacks, facilitating testing in different environments:
- **Local Development**: Data scientists can experiment locally.
- **Staging**: Test advanced features in a cloud environment.
- **Production**: Deploy the final pipeline on a production-grade stack.

**Benefits of Separate Stacks**:
- Prevents accidental production deployments.
- Reduces costs by using less powerful resources in staging.
- Controls access by assigning permissions to specific users.

#### Managing Credentials
Most stack components require credentials for infrastructure interaction. ZenML recommends using **Service Connectors** to manage these credentials securely, abstracting sensitive information from team members.

**Recommended Roles**:
- Limit Service Connector creation to individuals with direct cloud resource access to minimize credential leaks and simplify auditing.

**Recommended Workflow**:
1. Designate a small group to create Service Connectors.
2. Create a connector for development/staging environments for data scientists.
3. Create a separate connector for production to ensure safe resource usage.

#### Deploying and Managing Stacks
Deploying MLOps stacks can be complex due to:
- Specific requirements for tools (e.g., Kubernetes for Kubeflow).
- Difficulty in setting default infrastructure parameters.
- Potential installation issues (e.g., custom service accounts for Vertex AI).
- Need for proper permissions among components.
- Challenges in cleaning up resources post-experimentation.

ZenML aims to simplify the provisioning, configuration, and extension of stacks and components. 

#### Key Documentation Links
- [Deploy a Cloud Stack](./deploy-a-cloud-stack.md)
- [Register a Cloud Stack](./register-a-cloud-stack.md)
- [Deploy a Cloud Stack with Terraform](./deploy-a-cloud-stack-with-terraform.md)
- [Export and Install Stack Requirements](./export-stack-requirements.md)
- [Reference Secrets in Stack Configuration](./reference-secrets-in-stack-configuration.md)
- [Implement a Custom Stack Component](./implement-a-custom-stack-component.md)

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/deploy-a-cloud-stack.md

# Deploy a Cloud Stack with a Single Click

In ZenML, a **stack** represents your infrastructure configuration. Traditionally, creating a stack involves deploying infrastructure components and defining them in ZenML, which can be complex, especially in remote settings. To simplify this, ZenML offers a **1-click deployment feature** that allows you to deploy infrastructure on your chosen cloud provider effortlessly.

## Prerequisites
You need a deployed instance of ZenML (not a local server). For setup instructions, refer to the [ZenML deployment guide](../../../getting-started/deploying-zenml/README.md).

## Using the 1-Click Deployment Tool

### Dashboard
1. Go to the stacks page and click "+ New Stack".
2. Select "New Infrastructure".
3. Choose your cloud provider (AWS, GCP, or Azure).

#### AWS Deployment
- Select a region and stack name.
- Complete the configuration and click "Deploy in AWS" to be redirected to the AWS CloudFormation page.
- Log in to AWS, review configurations, and create the stack.

#### GCP Deployment
- Select a region and stack name.
- Click "Deploy in GCP" to start a Cloud Shell session.
- Trust the ZenML GitHub repository to authenticate.
- Follow prompts to create or select a GCP project, paste configuration values, and run the deployment script.

#### Azure Deployment
- Select a location and stack name.
- Click "Deploy in Azure" to start a Cloud Shell session.
- Paste the `main.tf` configuration into the Cloud Shell and run `terraform init --upgrade` and `terraform apply`.

### CLI
To create a remote stack via CLI, use:
```shell
zenml stack deploy -p {aws|gcp|azure}
```

#### AWS CLI
Follow prompts to deploy a CloudFormation stack, review configurations, and create the stack.

#### GCP CLI
Follow prompts to start a Cloud Shell session, authenticate, and run the deployment script.

#### Azure CLI
Follow prompts to open a `main.tf` file in Cloud Shell, paste the Terraform configuration, and run the necessary Terraform commands.

## Deployed Resources Overview

### AWS
- **Resources**: S3 bucket, ECR container registry, CloudBuild project, IAM roles.
- **Permissions**: Includes S3, ECR, CloudBuild, and SageMaker permissions.

### GCP
- **Resources**: GCS bucket, GCP Artifact Registry, Vertex AI permissions, Cloud Build permissions.
- **Permissions**: Includes roles for GCS, Artifact Registry, Vertex AI, and Cloud Build.

### Azure
- **Resources**: Resource Group, Storage Account, Container Registry, AzureML Workspace.
- **Permissions**: Includes permissions for Storage Account, Container Registry, and AzureML Workspace.

With this feature, you can deploy a cloud stack in a single click and start running your pipelines in a remote environment.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/register-a-cloud-stack.md

### Summary of ZenML Stack Wizard Documentation

**Overview**: ZenML's stack represents the configuration of your infrastructure. Traditionally, creating a stack involves deploying infrastructure and defining components in ZenML, which can be complex. The Stack Wizard simplifies this by allowing users to register a ZenML cloud stack using existing infrastructure.

**Options for Stack Creation**:
- **1-Click Deployment Tool**: For users without existing infrastructure.
- **Terraform Modules**: For those preferring manual infrastructure management.

### Using the Stack Wizard

**Access**: Available via CLI and dashboard.

#### Dashboard Steps:
1. Navigate to the stacks page.
2. Click "+ New Stack" and select "Use existing Cloud".
3. Choose a cloud provider and authentication method.

**Authentication Methods**:
- **AWS**:
  - AWS Secret Key
  - AWS STS Token
  - AWS IAM Role
  - AWS Session Token
  - AWS Federation Token
- **GCP**:
  - GCP User Account
  - GCP Service Account
  - GCP External Account
  - GCP OAuth 2.0 Token
  - GCP Service Account Impersonation
- **Azure**:
  - Azure Service Principal
  - Azure Access Token

After authentication, users can select existing resources to create stack components (artifact store, orchestrator, container registry).

#### CLI Command:
To register a remote stack:
```shell
zenml stack register <STACK_NAME> -p {aws|gcp|azure} -sc <SERVICE_CONNECTOR_ID_OR_NAME>
```
The wizard checks for local cloud provider credentials and offers options for auto-configuration or manual input.

### Defining Cloud Components
Users will define:
- **Artifact Store**
- **Orchestrator**
- **Container Registry**

For each component, users can choose to reuse existing components or create new ones based on available resources.

### Conclusion
The Stack Wizard streamlines the process of registering a cloud stack, enabling users to efficiently set up and run pipelines in a remote environment.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/deploy-a-cloud-stack-with-terraform.md

### Summary: Deploy a Cloud Stack Using Terraform

ZenML provides a collection of [Terraform modules](https://registry.terraform.io/modules/zenml-io/zenml-stack) to simplify the provisioning of cloud resources for AI/ML operations. These modules facilitate quick setup and integration with ZenML Stacks, enhancing machine learning infrastructure deployment.

#### Prerequisites
- A deployed ZenML server instance accessible from your cloud provider (not a local server).
- Create a service account and API key for programmatic access to the ZenML server using:
  ```shell
  zenml service-account create <account-name>
  ```
- Ensure Terraform (version 1.9 or later) is installed and authenticated with your cloud provider.

#### Using Terraform Stack Deployment Modules
1. Set up environment variables for ZenML server URL and API key:
   ```shell
   export ZENML_SERVER_URL="https://your-zenml-server.com"
   export ZENML_API_KEY="<your-api-key>"
   ```
2. Create a Terraform configuration file (e.g., `main.tf`):
   ```hcl
   terraform {
       required_providers {
           aws = { source = "hashicorp/aws" }
           zenml = { source = "zenml-io/zenml" }
       }
   }

   provider "zenml" {}

   module "zenml_stack" {
     source = "zenml-io/zenml-stack/<cloud-provider>"
     zenml_stack_name = "<your-stack-name>"
     orchestrator = "<your-orchestrator-type>"
   }

   output "zenml_stack_id" { value = module.zenml_stack.zenml_stack_id }
   output "zenml_stack_name" { value = module.zenml_stack.zenml_stack_name }
   ```
3. Run the following commands:
   ```shell
   terraform init
   terraform apply
   ```
4. Confirm changes by typing `yes` when prompted. Upon completion, the ZenML stack will be created and registered.

5. To use the stack:
   ```shell
   zenml integration install <list-of-required-integrations>
   zenml stack set <zenml_stack_id>
   ```

#### Cloud Provider Specifics
- **AWS**: Requires AWS CLI and credentials configured via `aws configure`.
- **GCP**: Requires `gcloud` CLI and credentials set up via `gcloud init`.
- **Azure**: Requires Azure CLI and credentials set up via `az login`.

#### Cleanup
To remove all resources provisioned by Terraform and delete the ZenML stack:
```shell
terraform destroy
```

This documentation provides a streamlined approach to deploying cloud stacks using Terraform with ZenML, ensuring efficient management of machine learning infrastructure. For detailed configurations and requirements for each cloud provider, refer to the respective Terraform module documentation.

================================================================================

File: docs/book/how-to/infrastructure-deployment/stack-deployment/reference-secrets-in-stack-configuration.md

### Summary: Referencing Secrets in Stack Configuration

Components in your stack may require sensitive information (e.g., passwords, tokens) for infrastructure connections. To securely configure these components, use secret references instead of direct values, following this syntax: `{{<SECRET_NAME>.<SECRET_KEY>}}`.

#### Example Usage

**CLI Example:**
```shell
# Create a secret named `mlflow_secret` with username and password
zenml secret create mlflow_secret \
    --username=admin \
    --password=abc123

# Reference the secret in the experiment tracker component
zenml experiment-tracker register mlflow \
    --flavor=mlflow \
    --tracking_username={{mlflow_secret.username}} \
    --tracking_password={{mlflow_secret.password}} \
    ...
```

#### Secret Validation

ZenML validates the existence of referenced secrets and keys before running a pipeline to prevent runtime failures. The validation can be controlled using the `ZENML_SECRET_VALIDATION_LEVEL` environment variable:

- `NONE`: Disables validation.
- `SECRET_EXISTS`: Validates only the existence of secrets.
- `SECRET_AND_KEY_EXISTS`: Validates both secret existence and key-value pairs (default).

#### Fetching Secrets in Steps

For centralized secrets management, access secrets directly within steps using the ZenML `Client` API:

```python
from zenml import step
from zenml.client import Client

@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    secret = Client().get_secret(<SECRET_NAME>)
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

### Additional Resources

- **Interact with Secrets**: Learn to create, list, and delete secrets using the ZenML CLI and Python SDK.

================================================================================

File: docs/book/how-to/infrastructure-deployment/infrastructure-as-code/terraform-stack-management.md

### Summary: Registering Existing Infrastructure with ZenML for Terraform Users

#### Overview
This guide helps advanced users integrate ZenML with their existing Terraform infrastructure. It covers the two-phase approach: Infrastructure Deployment and ZenML Registration.

#### Two-Phase Approach
1. **Infrastructure Deployment**: Managed by platform teams using existing Terraform configurations.
2. **ZenML Registration**: Registering existing resources as ZenML stack components.

#### Phase 1: Infrastructure Deployment
Example of existing GCP infrastructure:
```hcl
resource "google_storage_bucket" "ml_artifacts" {
  name     = "company-ml-artifacts"
  location = "US"
}

resource "google_artifact_registry_repository" "ml_containers" {
  repository_id = "ml-containers"
  format        = "DOCKER"
}
```

#### Phase 2: ZenML Registration

**Setup ZenML Provider**:
```hcl
terraform {
  required_providers {
    zenml = { source = "zenml-io/zenml" }
  }
}

provider "zenml" {
  # Configuration via environment variables
}
```
Generate API key:
```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```

**Create Service Connectors**:
```hcl
resource "zenml_service_connector" "gcp_connector" {
  name        = "gcp-${var.environment}-connector"
  type        = "gcp"
  auth_method = "service-account"  
  configuration = {
    project_id = var.project_id
    service_account_json = file("service-account.json")
  }
}
```

**Register Stack Components**:
```hcl
locals {
  component_configs = {
    artifact_store = { type = "artifact_store", flavor = "gcp", configuration = { path = "gs://${google_storage_bucket.ml_artifacts.name}" } }
    container_registry = { type = "container_registry", flavor = "gcp", configuration = { uri = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.ml_containers.repository_id}" } }
    orchestrator = { type = "orchestrator", flavor = "vertex", configuration = { project = var.project_id, region = var.region } }
  }
}

resource "zenml_stack_component" "components" {
  for_each = local.component_configs
  name     = "existing-${each.key}"
  type     = each.value.type
  flavor   = each.value.flavor
  configuration = each.value.configuration
  connector_id = zenml_service_connector.gcp_connector.id
}
```

**Assemble the Stack**:
```hcl
resource "zenml_stack" "ml_stack" {
  name = "${var.environment}-ml-stack"
  components = { for k, v in zenml_stack_component.components : k => v.id }
}
```

#### Practical Walkthrough: Registering Existing GCP Infrastructure
**Prerequisites**:
- GCS bucket for artifacts
- Artifact Registry repository
- Service account for ML operations
- Vertex AI enabled

**Variables Configuration**:
```hcl
variable "zenml_server_url" { type = string }
variable "zenml_api_key" { type = string, sensitive = true }
variable "project_id" { type = string }
variable "region" { type = string, default = "us-central1" }
variable "environment" { type = string }
variable "gcp_service_account_key" { type = string, sensitive = true }
```

**Main Configuration**:
```hcl
terraform {
  required_providers {
    zenml = { source = "zenml-io/zenml" }
    google = { source = "hashicorp/google" }
  }
}

provider "zenml" { server_url = var.zenml_server_url; api_key = var.zenml_api_key }
provider "google" { project = var.project_id; region = var.region }

resource "google_storage_bucket" "artifacts" { name = "${var.project_id}-zenml-artifacts-${var.environment}"; location = var.region }
resource "google_artifact_registry_repository" "containers" { location = var.region; repository_id = "zenml-containers-${var.environment}"; format = "DOCKER" }

resource "zenml_service_connector" "gcp" {
  name        = "gcp-${var.environment}"
  type        = "gcp"
  auth_method = "service-account"
  configuration = { project_id = var.project_id; region = var.region; service_account_json = var.gcp_service_account_key }
}

resource "zenml_stack_component" "artifact_store" {
  name   = "gcs-${var.environment}"
  type   = "artifact_store"
  flavor = "gcp"
  configuration = { path = "gs://${google_storage_bucket.artifacts.name}/artifacts" }
  connector_id = zenml_service_connector.gcp.id
}

resource "zenml_stack" "gcp_stack" {
  name = "gcp-${var.environment}"
  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator      = zenml_stack_component.orchestrator.id
  }
}
```

**Outputs Configuration**:
```hcl
output "stack_id" { value = zenml_stack.gcp_stack.id }
output "stack_name" { value = zenml_stack.gcp_stack.name }
```

**terraform.tfvars Configuration**:
```hcl
zenml_server_url = "https://your-zenml-server.com"
project_id       = "your-gcp-project-id"
region           = "us-central1"
environment      = "dev"
```
Set sensitive variables in environment:
```bash
export TF_VAR_zenml_api_key="your-zenml-api-key"
export TF_VAR_gcp_service_account_key=$(cat path/to/service-account-key.json)
```

#### Usage Instructions
1. Initialize Terraform:
   ```bash
   terraform init
   ```
2. Install ZenML integrations:
   ```bash
   zenml integration install gcp
   ```
3. Review planned changes:
   ```bash
   terraform plan
   ```
4. Apply configuration:
   ```bash
   terraform apply
   ```
5. Set the stack as active:
   ```bash
   zenml stack set $(terraform output -raw stack_name)
   ```
6. Verify configuration:
   ```bash
   zenml stack describe
   ```

#### Best Practices
- Use appropriate IAM roles and permissions.
- Securely manage credentials.
- Consider Terraform workspaces for multiple environments.
- Regularly back up Terraform state files.
- Version control Terraform configurations, excluding sensitive files.

For more details, refer to the [ZenML provider documentation](https://registry.terraform.io/providers/zenml-io/zenml/latest).

================================================================================

File: docs/book/how-to/infrastructure-deployment/infrastructure-as-code/README.md

### Integrate with Infrastructure as Code

**Infrastructure as Code (IaC)** is the practice of managing and provisioning infrastructure through code rather than manual processes. This section outlines how to integrate ZenML with popular IaC tools like [Terraform](https://www.terraform.io/).

![ZenML stack on Terraform Registry](../../../.gitbook/assets/terraform_providers_screenshot.png) 

Leverage IaC to effectively manage your ZenML stacks and components.

================================================================================

File: docs/book/how-to/infrastructure-deployment/infrastructure-as-code/best-practices.md

# Summary: Best Practices for Using IaC with ZenML

## Overview
This documentation outlines best practices for architecting scalable ML infrastructure using ZenML and Terraform. It addresses challenges such as supporting multiple teams, maintaining security, and allowing rapid iteration.

## ZenML Approach
ZenML utilizes **stack components** as abstractions over infrastructure resources, promoting a component-based architecture for reusability and consistency.

### Part 1: Stack Component Architecture
- **Problem**: Different teams require varied ML infrastructure configurations.
- **Solution**: Create reusable Terraform modules for ZenML stack components.

**Base Infrastructure Example**:
```hcl
resource "random_id" "suffix" { byte_length = 6 }

module "base_infrastructure" {
  source = "./modules/base_infra"
  environment = var.environment
  project_id  = var.project_id
  region      = var.region
  resource_prefix = "zenml-${var.environment}-${random_id.suffix.hex}"
}

resource "zenml_service_connector" "base_connector" {
  name        = "${var.environment}-base-connector"
  type        = "gcp"
  auth_method = "service-account"
  configuration = { project_id = var.project_id, region = var.region, service_account_json = module.base_infrastructure.service_account_key }
}
```

Teams can extend the base stack:
```hcl
resource "zenml_stack_component" "training_orchestrator" {
  name   = "${var.environment}-training-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"
  configuration = { location = var.region, machine_type = "n1-standard-8", gpu_enabled = true }
}
```

### Part 2: Environment Management and Authentication
- **Problem**: Different environments require distinct configurations and authentication methods.
- **Solution**: Use environment-specific configurations with flexible service connectors.

**Environment-Specific Connector Example**:
```hcl
locals {
  env_config = {
    dev = { machine_type = "n1-standard-4", gpu_enabled = false, auth_method = "service-account", auth_configuration = { service_account_json = file("dev-sa.json") } }
    prod = { machine_type = "n1-standard-8", gpu_enabled = true, auth_method = "external-account", auth_configuration = { external_account_json = file("prod-sa.json") } }
  }
}

resource "zenml_service_connector" "env_connector" {
  name        = "${var.environment}-connector"
  type        = "gcp"
  auth_method = local.env_config[var.environment].auth_method
  dynamic "configuration" { for_each = try(local.env_config[var.environment].auth_configuration, {}); content { key = configuration.key; value = configuration.value } }
}
```

### Part 3: Resource Sharing and Isolation
- **Problem**: Need for strict isolation of data and security across ML projects.
- **Solution**: Implement resource scoping with project isolation.

**Project Isolation Example**:
```hcl
locals {
  project_paths = { fraud_detection = "projects/fraud_detection/${var.environment}", recommendation = "projects/recommendation/${var.environment}" }
}

resource "zenml_stack_component" "project_artifact_stores" {
  for_each = local.project_paths
  name   = "${each.key}-artifact-store"
  type   = "artifact_store"
  configuration = { path = "gs://${var.shared_bucket}/${each.value}" }
}
```

### Part 4: Advanced Stack Management Practices
1. **Stack Component Versioning**:
   ```hcl
   locals { stack_version = "1.2.0" }
   resource "zenml_stack" "versioned_stack" { name = "stack-v${local.stack_version}" }
   ```

2. **Service Connector Management**:
   ```hcl
   resource "zenml_service_connector" "env_connector" {
     name        = "${var.environment}-${var.purpose}-connector"
     auth_method = var.environment == "prod" ? "workload-identity" : "service-account"
   }
   ```

3. **Component Configuration Management**:
   ```hcl
   locals {
     base_configs = { orchestrator = { location = var.region, project = var.project_id } }
     env_configs = { dev = { orchestrator = { machine_type = "n1-standard-4" } }, prod = { orchestrator = { machine_type = "n1-standard-8" } } }
   }
   ```

4. **Stack Organization and Dependencies**:
   ```hcl
   module "ml_stack" {
     source = "./modules/ml_stack"
     depends_on = [module.base_infrastructure, module.security]
   }
   ```

5. **State Management**:
   ```hcl
   terraform { backend "gcs" { prefix = "terraform/state" } }
   ```

## Conclusion
Utilizing ZenML and Terraform for ML infrastructure allows for a flexible, maintainable, and secure environment. Following these best practices ensures a clean infrastructure codebase and effective management of ML operations.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/service-connectors-guide.md

# Service Connectors Guide Summary

This documentation provides a comprehensive guide for managing Service Connectors to connect ZenML with external resources. Key sections include terminology, types of Service Connectors, registration, and connecting Stack Components to resources.

## Key Sections

1. **Terminology**: Introduces essential terms related to Service Connectors, including:
   - **Service Connector Types**: Represents specific implementations that define capabilities and required configurations.
   - **Resource Types**: Logical classifications of resources based on access protocols or vendors (e.g., `kubernetes-cluster`, `docker-registry`).
   - **Resource Names**: Unique identifiers for resource instances accessible via Service Connectors.

2. **Service Connector Types**: 
   - Examples include AWS, GCP, Azure, Kubernetes, and Docker connectors.
   - Each type supports various authentication methods and resource types.
   - Commands to explore types:
     ```sh
     zenml service-connector list-types
     zenml service-connector describe-type <type>
     ```

3. **Registering Service Connectors**:
   - Service Connectors can be configured as multi-type (access multiple resource types), multi-instance (access multiple resources of the same type), or single-instance (access a single resource).
   - Example command to register a multi-type AWS Service Connector:
     ```sh
     zenml service-connector register aws-multi-type --type aws --auto-configure
     ```

4. **Connecting Stack Components**:
   - Stack Components can connect to external resources using registered Service Connectors.
   - Use interactive CLI mode for ease:
     ```sh
     zenml artifact-store connect <component-name> -i
     ```

5. **Resource Discovery**:
   - Use commands to find accessible resources:
     ```sh
     zenml service-connector list-resources
     zenml service-connector list-resources --resource-type <type>
     ```

6. **Verification**:
   - Verify Service Connector configurations and access permissions:
     ```sh
     zenml service-connector verify <connector-name>
     ```

7. **Local Client Configuration**:
   - Configure local CLI tools (e.g., `kubectl`, Docker) with credentials from Service Connectors:
     ```sh
     zenml service-connector login <connector-name> --resource-type <type> --resource-id <id>
     ```

8. **End-to-End Examples**:
   - Detailed examples for AWS, GCP, and Azure Service Connectors are provided to illustrate complete workflows from registration to execution.

## Important Commands

- List Service Connector Types:
  ```sh
  zenml service-connector list-types
  ```

- Register a Service Connector:
  ```sh
  zenml service-connector register <name> --type <type> --auto-configure
  ```

- Connect a Stack Component:
  ```sh
  zenml <component-type> connect <component-name> --connector <connector-name>
  ```

- Verify Service Connector:
  ```sh
  zenml service-connector verify <connector-name>
  ```

This guide serves as a foundational resource for integrating ZenML with various external services through Service Connectors, ensuring secure and efficient access to necessary resources.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/best-security-practices.md

### Summary of Best Practices for Service Connector Authentication Methods

#### Overview
Service Connectors for cloud providers support various authentication methods. While no unified standard exists, identifiable patterns can guide the choice of authentication methods. This document outlines best practices for using these methods effectively.

#### Username and Password
- **Avoid using primary account passwords** for authentication. Instead, opt for session tokens, API keys, or API tokens.
- Passwords are the least secure method and should not be shared or used for automated workloads.
- Cloud platforms typically require the exchange of account/password credentials for long-lived credentials.

#### Implicit Authentication
- Provides immediate access to cloud resources without configuration but may limit portability.
- **Security Risk**: Can grant users access to resources configured for the ZenML Server. Disabled by default; enable via `ZENML_ENABLE_IMPLICIT_AUTH_METHODS`.
- Utilizes locally stored credentials, environment variables, and cloud workload metadata for authentication.

##### Examples of Implicit Authentication:
- **AWS**: Uses instance metadata service for EC2, ECS, EKS, etc.
- **GCP**: Accesses resources via attached service accounts.
- **Azure**: Uses Managed Identity services.

#### Long-lived Credentials (API Keys, Account Keys)
- Preferred for production environments, especially when sharing results.
- Cloud platforms do not use account passwords directly; they exchange them for long-lived credentials.
- Different cloud providers have varying names for these credentials (e.g., AWS Access Keys, GCP Service Account Credentials).

##### Credential Types:
- **User Credentials**: Tied to human users, broad permissions; not recommended for sharing.
- **Service Credentials**: Used for automated access, can have restricted permissions; better for sharing.

#### Generating Temporary and Down-scoped Credentials
- **Temporary Credentials**: Issued from long-lived credentials, expire after a set duration.
- **Down-scoped Credentials**: Limit permissions to the minimum required for specific resources.

##### Example of Temporary Credentials:
```sh
zenml service-connector register gcp-implicit --type gcp --auth-method implicit --project_id=zenml-core
```

#### Impersonating Accounts and Assuming Roles
- Offers flexibility and control but requires setup of multiple permission-bearing accounts.
- Long-lived credentials are used to obtain short-lived tokens with limited permissions.
  
##### Example of GCP Account Impersonation:
```sh
zenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl
```

#### Short-lived Credentials
- Temporary credentials can be manually configured or auto-generated.
- Useful for granting temporary access without exposing long-lived credentials.

##### Example of Short-lived Credentials:
```sh
AWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token
```

### Conclusion
Choosing the appropriate authentication method for Service Connectors is crucial for security and usability. Long-lived credentials, temporary tokens, and impersonation strategies provide a robust framework for managing access to cloud resources while minimizing risks.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/gcp-service-connector.md

### Summary of GCP Service Connectors Documentation

**Overview**: The ZenML GCP Service Connector enables authentication and access to various GCP resources like GCS buckets, GKE clusters, and GCR registries. It supports multiple authentication methods, including user accounts, service accounts, and OAuth 2.0 tokens, prioritizing security by issuing short-lived tokens.

#### Key Features:
- **Authentication Methods**:
  - **Implicit Authentication**: Uses Application Default Credentials (ADC) and is disabled by default for security.
  - **GCP User Account**: Generates temporary OAuth 2.0 tokens from user credentials.
  - **GCP Service Account**: Uses service account credentials to generate temporary tokens.
  - **Service Account Impersonation**: Allows temporary token generation by impersonating another service account.
  - **External Account**: Uses GCP Workload Identity for authentication with external cloud providers.
  - **OAuth 2.0 Token**: Requires manual token management.

#### Resource Types:
1. **Generic GCP Resource**: Connects to any GCP service using OAuth 2.0 tokens.
2. **GCS Bucket**: Requires specific permissions (e.g., `storage.buckets.list`).
3. **GKE Kubernetes Cluster**: Requires permissions like `container.clusters.list`.
4. **GAR and Legacy GCR**: Supports both Google Artifact Registry and legacy Google Container Registry, requiring specific permissions for each.

#### Prerequisites:
- Install ZenML GCP integration using:
  ```bash
  pip install "zenml[connectors-gcp]"
  ```
  or
  ```bash
  zenml integration install gcp
  ```

#### Example Commands:
- **List Connector Types**:
  ```bash
  zenml service-connector list-types --type gcp
  ```
  
- **Register a Service Connector**:
  ```bash
  zenml service-connector register gcp-implicit --type gcp --auth-method implicit --auto-configure
  ```

- **Describe a Service Connector**:
  ```bash
  zenml service-connector describe gcp-implicit
  ```

- **Verify Access to Resource Types**:
  ```bash
  zenml service-connector verify gcp-user-account --resource-type kubernetes-cluster
  ```

#### Local Client Provisioning:
- The local `gcloud`, `kubectl`, and Docker CLIs can be configured with credentials from the GCP Service Connector. The `gcloud` CLI can only be configured if the connector uses user or service account authentication.

#### Stack Components:
- The GCP Service Connector can link various Stack Components (e.g., GCS Artifact Store, Kubernetes Orchestrator) to GCP resources, simplifying resource management without manual credential configuration.

#### End-to-End Examples:
1. **Multi-Type GCP Service Connector**: Connects GKE, GCS, and GCR using a single connector.
2. **Single-Instance Connectors**: Each resource (e.g., GCS, GCR) has its own connector for specific Stack Components.

This documentation provides a comprehensive guide for configuring and utilizing GCP Service Connectors within ZenML, ensuring secure and efficient access to GCP resources.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/README.md

### ZenML Service Connectors Overview

**Purpose**: ZenML Service Connectors facilitate secure connections between ZenML deployments and various cloud providers (AWS, GCP, Azure, Kubernetes, etc.), enabling seamless access to infrastructure resources.

#### Key Concepts

- **MLOps Complexity**: Integrating multiple third-party services requires managing authentication and authorization for secure access.
- **Service Connectors**: Abstract the complexity of authentication, allowing users to focus on pipeline development without worrying about security configurations.

#### Use Case Example: AWS S3 Bucket Connection

1. **Connecting to AWS S3**:
   - Use the AWS Service Connector to link ZenML with an S3 bucket.
   - Alternatives for direct connection include embedding credentials in Stack Components or using ZenML secrets, but these methods have significant security and usability drawbacks.

2. **Service Connector Registration**:
   - Register a Service Connector with auto-configuration to simplify the setup process:
   ```sh
   zenml service-connector register aws-s3 --type aws --auto-configure --resource-type s3-bucket
   ```

3. **Connecting Stack Components**:
   - Register an S3 Artifact Store and connect it to the AWS Service Connector:
   ```sh
   zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
   zenml artifact-store connect s3-zenfiles --connector aws-s3
   ```

#### Authentication Methods

- **AWS Service Connector** supports multiple authentication methods:
  - Implicit
  - Secret-key
  - STS token
  - IAM role
  - Session token
  - Federation token

- **Security Practices**: The Service Connector generates short-lived credentials, minimizing security risks associated with long-lived credentials.

#### Example Pipeline

A simple pipeline demonstrates the use of the connected S3 Artifact Store:
```python
from zenml import step, pipeline

@step
def simple_step_one() -> str:
    return "Hello World!"

@step
def simple_step_two(msg: str) -> None:
    print(msg)

@pipeline
def simple_pipeline() -> None:
    message = simple_step_one()
    simple_step_two(msg=message)

if __name__ == "__main__":
    simple_pipeline()
```
Run the pipeline:
```sh
python run.py
```

#### Conclusion

ZenML Service Connectors streamline the integration of cloud resources into MLOps workflows, providing a secure and efficient way to manage authentication and access. For more details, refer to the [Service Connector Guide](./service-connectors-guide.md) and related documentation on security best practices and specific connectors for AWS, GCP, Azure, and Docker.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/kubernetes-service-connector.md

### Kubernetes Service Connector Overview

The ZenML Kubernetes Service Connector enables authentication and connection to Kubernetes clusters, providing access to generic clusters via pre-authenticated Kubernetes Python clients and local `kubectl` configuration.

#### Prerequisites
- Install the connector:
  - For only the Kubernetes Service Connector: 
    ```shell
    pip install "zenml[connectors-kubernetes]"
    ```
  - For the entire Kubernetes ZenML integration:
    ```shell
    zenml integration install kubernetes
    ```
- Local `kubectl` configuration is not required for accessing Kubernetes clusters.

#### Resource Types
- Supports only `kubernetes-cluster` resource type, identified by a user-friendly name during registration.

#### Authentication Methods
1. Username and password (not recommended for production).
2. Authentication token (with or without client certificates). For local K3D clusters, an empty token can be used.

**Warning**: Credentials configured in the Service Connector are directly used for authentication, so using API tokens with client certificates is advisable.

#### Auto-configuration
Fetch credentials from local `kubectl` during registration:
```sh
zenml service-connector register kube-auto --type kubernetes --auto-configure
```

#### Example Command Output
```text
Successfully registered service connector `kube-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES ┃
┠───────────────────────┼────────────────┨
┃ 🌀 kubernetes-cluster │ 35.185.95.223  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

#### Describe Command
To view details of the service connector:
```sh
zenml service-connector describe kube-auto 
```

#### Example Command Output
```text
Service connector 'kube-auto' of type 'kubernetes' ...
┃ AUTH METHOD      │ token                                ┃
┃ RESOURCE NAME    │ 35.175.95.223                        ┃
...
┃ server                │ https://35.175.95.223 ┃
┃ token                 │ [HIDDEN]              ┃
...
```

**Note**: Credentials may have limited lifetime, particularly with third-party authentication providers.

#### Local Client Provisioning
Configure the local Kubernetes client with:
```sh
zenml service-connector login kube-auto 
```

#### Example Command Output
```text
Updated local kubeconfig with the cluster details. The current kubectl context was set to '35.185.95.223'.
```

#### Stack Components Use
The Kubernetes Service Connector can be utilized in Orchestrator and Model Deployer stack components, allowing management of Kubernetes workloads without explicit `kubectl` configuration in the target environment.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/aws-service-connector.md

### Summary of AWS Service Connector Documentation

The **ZenML AWS Service Connector** allows seamless integration with AWS resources like S3 buckets, EKS Kubernetes clusters, and ECR container registries, facilitating authentication and access management. It supports various authentication methods, including AWS secret keys, IAM roles, STS tokens, and implicit authentication. The connector can generate temporary STS tokens with minimal permissions and can auto-configure using AWS CLI credentials.

#### Key Features:
- **Resource Types Supported**:
  - **Generic AWS Resource**: Connects to any AWS service using a pre-configured boto3 session.
  - **S3 Bucket**: Requires specific IAM permissions (e.g., `s3:ListBucket`, `s3:GetObject`).
  - **EKS Cluster**: Requires permissions like `eks:ListClusters` and must be added to the `aws-auth` ConfigMap for access.
  - **ECR Registry**: Requires permissions for actions like `ecr:DescribeRepositories` and `ecr:PutImage`.

- **Authentication Methods**:
  - **Implicit Authentication**: Uses environment variables or IAM roles; disabled by default for security.
  - **AWS Secret Key**: Long-lived credentials; not recommended for production.
  - **STS Token**: Temporary tokens that need regular renewal.
  - **IAM Role**: Generates temporary STS credentials by assuming a role.
  - **Session Token**: Generates temporary session tokens for IAM users.
  - **Federation Token**: Generates tokens for federated users; requires specific permissions.

#### Configuration Commands:
- **List AWS Service Connector Types**:
  ```shell
  zenml service-connector list-types --type aws
  ```

- **Register a Service Connector**:
  ```shell
  zenml service-connector register -i --type aws
  ```

- **Verify Access to Resources**:
  ```shell
  zenml service-connector verify <connector-name> --resource-type <resource-type>
  ```

- **Example of Registering a Service Connector with Auto-Configuration**:
  ```shell
  AWS_PROFILE=connectors zenml service-connector register aws-auto --type aws --auto-configure
  ```

#### Local Client Provisioning:
The connector can configure local AWS CLI, Kubernetes `kubectl`, and Docker CLI with credentials extracted from the Service Connector. Local configurations are short-lived and require regular refreshes.

#### Stack Components Use:
The AWS Service Connector can connect various ZenML Stack Components, enabling workflows that utilize S3 for artifact storage, EKS for orchestration, and ECR for container management without needing explicit credentials in the environment.

#### Example Workflow:
1. **Register AWS Service Connector**.
2. **Connect Stack Components** (S3 Artifact Store, EKS Orchestrator, ECR Registry).
3. **Run a Pipeline** to validate the setup.

This documentation provides a comprehensive guide for configuring and using the AWS Service Connector within ZenML, ensuring secure and efficient access to AWS resources.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/azure-service-connector.md

### Summary of Azure Service Connector Documentation

#### Overview
The ZenML Azure Service Connector enables authentication and access to Azure resources like Blob storage, AKS Kubernetes clusters, and ACR container registries. It supports automatic configuration and credential detection via the Azure CLI.

#### Prerequisites
- To install the Azure Service Connector:
  - `pip install "zenml[connectors-azure]"` (for the connector only)
  - `zenml integration install azure` (for the full Azure integration)
- Azure CLI installation is recommended for quick setup and auto-configuration, but not mandatory.

#### Resource Types
1. **Generic Azure Resource**: Connects to any Azure service using generic credentials.
2. **Azure Blob Storage**: Requires specific IAM permissions (e.g., `Storage Blob Data Contributor`). Resource names can be specified as URIs or container names.
3. **AKS Kubernetes Cluster**: Requires permissions like `Azure Kubernetes Service Cluster Admin Role`. Resource names can include the resource group.
4. **ACR Container Registry**: Requires permissions like `AcrPull` and `AcrPush`. Resource names can be specified as URIs or registry names.

#### Authentication Methods
- **Implicit Authentication**: Uses environment variables or Azure CLI credentials. Requires explicit enabling due to security risks.
- **Service Principal**: Uses client ID and secret for authentication. Requires prior setup of an Azure service principal.
- **Access Token**: Uses temporary tokens but is limited to short-term use and does not support Blob storage.

#### Configuration Examples
- **Implicit Authentication**:
  ```sh
  zenml service-connector register azure-implicit --type azure --auth-method implicit --auto-configure
  ```
- **Service Principal Authentication**:
  ```sh
  zenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=<tenant_id> --client_id=<client_id> --client_secret=<client_secret>
  ```

#### Local Client Provisioning
The Azure CLI, Kubernetes `kubectl`, and Docker CLI can be configured with credentials from the Azure Service Connector. Example for Kubernetes:
```sh
zenml service-connector login azure-service-principal --resource-type kubernetes-cluster --resource-id=<cluster_id>
```

#### Stack Components Usage
The Azure Service Connector can link:
- **Azure Artifact Store** to Blob storage.
- **Kubernetes Orchestrator** to AKS clusters.
- **Container Registry** to ACR.

#### End-to-End Example
1. Set up an Azure service principal with necessary permissions.
2. Register a multi-type Azure Service Connector.
3. Connect an Azure Blob Storage Artifact Store, AKS Orchestrator, and ACR.
4. Register and set an active stack.
5. Run a simple pipeline to validate the setup.

#### Example Pipeline Code
```python
from zenml import pipeline, step

@step
def step_1() -> str:
    return "world"

@step(enable_cache=False)
def step_2(input_one: str, input_two: str) -> None:
    print(f"{input_one} {input_two}")

@pipeline
def my_pipeline():
    output_step_one = step_1()
    step_2(input_one="hello", input_two=output_step_one)

if __name__ == "__main__":
    my_pipeline()
```

This documentation provides essential details for configuring and using the Azure Service Connector with ZenML, ensuring efficient access to Azure resources for machine learning workflows.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/docker-service-connector.md

### Summary: Configuring Docker Service Connectors for ZenML

The ZenML Docker Service Connector facilitates authentication with Docker/OCI container registries and manages Docker clients. It provides pre-authenticated `python-docker` clients to Stack Components.

#### Key Commands

- **List Docker Service Connector Types:**
  ```shell
  zenml service-connector list-types --type docker
  ```

- **Register a DockerHub Service Connector:**
  ```sh
  zenml service-connector register dockerhub --type docker -in
  ```

- **Login to DockerHub:**
  ```sh
  zenml service-connector login dockerhub
  ```

#### Resource Types
- The connector supports `docker-registry` resource types, identified by:
  - DockerHub: `docker.io` or `https://index.docker.io/v1/<repository-name>`
  - Generic OCI registry: `https://host:port/<repository-name>`

#### Authentication Methods
- Supports username/password or access tokens; API tokens are recommended over passwords.

#### Important Notes
- Credentials are stored unencrypted in the local Docker configuration file.
- The connector does not support generating short-lived credentials or auto-discovery of local Docker client credentials.
- Currently, ZenML does not automatically configure Docker credentials for container runtimes like Kubernetes.

#### Example Output
When registering a service connector, users will be prompted for:
- Service connector name
- Description
- Username and password/token
- Registry URL (optional)

Successful registration confirms access to the specified resources.

For further enhancements or features, users are encouraged to provide feedback via Slack or GitHub.

================================================================================

File: docs/book/how-to/infrastructure-deployment/auth-management/hyperai-service-connector.md

### HyperAI Service Connector Overview

The ZenML HyperAI Service Connector enables authentication with HyperAI instances for deploying pipeline runs. It provides pre-authenticated Paramiko SSH clients to connected Stack Components.

#### Listing Connector Types
To list available HyperAI service connector types, use:
```shell
$ zenml service-connector list-types --type hyperai
```

#### Connector Details
| NAME                      | TYPE       | RESOURCE TYPES     | AUTH METHODS     | LOCAL | REMOTE |
|---------------------------|------------|--------------------|-------------------|-------|--------|
| HyperAI Service Connector  | 🤖 hyperai | 🤖 hyperai-instance | rsa-key, dsa-key, ecdsa-key, ed25519-key | ✅    | ✅     |

### Prerequisites
Install the HyperAI integration:
```shell
$ zenml integration install hyperai
```

### Resource Types
The connector supports HyperAI instances.

### Authentication Methods
SSH connections are established in the background. Supported methods include:
1. RSA key
2. DSA (DSS) key
3. ECDSA key
4. ED25519 key

**Warning:** SSH private keys are distributed to clients running pipelines, granting unrestricted access to HyperAI instances.

### Configuration Requirements
When configuring the Service Connector, provide:
- At least one `hostname`
- `username` for login
- Optionally, an `ssh_passphrase`

You can either:
1. Create separate connectors for each HyperAI instance with different SSH keys.
2. Use a single SSH key across multiple instances, selecting the instance when creating the HyperAI orchestrator component.

### Auto-configuration
This Service Connector does not support auto-discovery of authentication credentials. Feedback can be provided via [Slack](https://zenml.io/slack) or by creating an issue on [GitHub](https://github.com/zenml-io/zenml/issues).

### Stack Components Usage
The HyperAI Service Connector is utilized by the HyperAI Orchestrator for deploying pipeline runs to HyperAI instances.

================================================================================

File: docs/book/how-to/handle-data-artifacts/visualize-artifacts.md

### Summary: Configuring ZenML for Data Visualizations

ZenML supports automatic visualization of various data types, viewable in the ZenML dashboard or Jupyter notebooks using the `artifact.visualize()` method. Supported visualization types include:

- **HTML:** For embedded HTML visualizations.
- **Image:** For image data (e.g., Pillow images).
- **CSV:** For tabular data (e.g., pandas DataFrame).
- **Markdown:** For Markdown content.

#### Accessing Visualizations

To display visualizations on the dashboard, the ZenML server must access the artifact store. This requires configuring a **service connector** to grant access. For example, using an AWS S3 artifact store is detailed in the respective documentation. 

**Note:** The default/local artifact store does not allow server access to local files, so a remote artifact store is necessary for visualization.

#### Custom Visualizations

Custom visualizations can be added in two main ways:

1. **Using Special Return Types:** Return HTML, Markdown, or CSV data by casting them to specific types:
   - `zenml.types.HTMLString`
   - `zenml.types.MarkdownString`
   - `zenml.types.CSVString`

   **Example:**
   ```python
   from zenml.types import CSVString

   @step
   def my_step() -> CSVString:
       return CSVString("a,b,c\n1,2,3")
   ```

2. **Using Custom Materializers:** Override the `save_visualizations()` method in a materializer to handle specific data types. 

3. **Custom Return Type and Materializer:** Create a custom class for your data, build a corresponding materializer, and return the custom class from your steps.

   **Example:**
   - **Custom Class:**
   ```python
   class FacetsComparison(BaseModel):
       datasets: List[Dict[str, Union[str, pd.DataFrame]]]
   ```

   - **Materializer:**
   ```python
   class FacetsMaterializer(BaseMaterializer):
       def save_visualizations(self, data: FacetsComparison) -> Dict[str, VisualizationType]:
           html = ...  # Create visualization
           return {visualization_path: VisualizationType.HTML}
   ```

   - **Step:**
   ```python
   @step
   def facets_visualization_step(reference: pd.DataFrame, comparison: pd.DataFrame) -> FacetsComparison:
       return FacetsComparison(datasets=[{"name": "reference", "table": reference}, {"name": "comparison", "table": comparison}])
   ```

#### Disabling Visualizations

To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level:

```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

This summary encapsulates the essential configurations and methods for visualizing artifacts in ZenML, ensuring clarity and conciseness while retaining critical technical details.

================================================================================

File: docs/book/how-to/popular-integrations/gcp-guide.md

# Minimal GCP Stack Setup Guide

This guide outlines the steps to quickly set up a minimal production stack on Google Cloud Platform (GCP) for ZenML.

## Steps to Set Up

### 1. Choose a GCP Project
Select or create a GCP project in the Google Cloud console. Ensure a billing account is attached.

```bash
gcloud projects create <PROJECT_ID> --billing-project=<BILLING_PROJECT>
```

### 2. Enable GCloud APIs
Enable the following APIs in your GCP project:
- Cloud Functions API
- Cloud Run Admin API
- Cloud Build API
- Artifact Registry API
- Cloud Logging API

### 3. Create a Dedicated Service Account
Create a service account with the following roles:
- AI Platform Service Agent
- Storage Object Admin

### 4. Create a JSON Key for Your Service Account
Generate a JSON key for the service account.

```bash
export JSON_KEY_FILE_PATH=<JSON_KEY_FILE_PATH>
```

### 5. Create a Service Connector in ZenML
Authenticate ZenML with GCP using the service account.

```bash
zenml integration install gcp \
&& zenml service-connector register gcp_connector \
--type gcp \
--auth-method service-account \
--service_account_json=@${JSON_KEY_FILE_PATH} \
--project_id=<GCP_PROJECT_ID>
```

### 6. Create Stack Components

#### Artifact Store
Create a GCS bucket and register it as an artifact store.

```bash
export ARTIFACT_STORE_NAME=gcp_artifact_store
zenml artifact-store register ${ARTIFACT_STORE_NAME} --flavor gcp --path=gs://<YOUR_BUCKET_NAME>
zenml artifact-store connect ${ARTIFACT_STORE_NAME} -i
```

#### Orchestrator
Register Vertex AI as the orchestrator.

```bash
export ORCHESTRATOR_NAME=gcp_vertex_orchestrator
zenml orchestrator register ${ORCHESTRATOR_NAME} --flavor=vertex --project=<PROJECT_NAME> --location=europe-west2
zenml orchestrator connect ${ORCHESTRATOR_NAME} -i
```

#### Container Registry
Register the GCP container registry.

```bash
export CONTAINER_REGISTRY_NAME=gcp_container_registry
zenml container-registry register ${CONTAINER_REGISTRY_NAME} --flavor=gcp --uri=<GCR-URI>
zenml container-registry connect ${CONTAINER_REGISTRY_NAME} -i
```

### 7. Create Stack
Register the stack with the created components.

```bash
export STACK_NAME=gcp_stack
zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

## Cleanup
To delete the project and all associated resources:

```bash
gcloud project delete <PROJECT_ID_OR_NUMBER>
```

## Best Practices
- **IAM and Least Privilege**: Grant minimum permissions necessary for ZenML operations.
- **Resource Labeling**: Implement consistent labeling for GCP resources.

```bash
gcloud storage buckets update gs://your-bucket-name --update-labels=project=zenml,environment=production
```

- **Cost Management**: Use GCP's Cost Management tools to monitor spending.

```bash
gcloud billing budgets create --billing-account=BILLING_ACCOUNT_ID --display-name="ZenML Monthly Budget" --budget-amount=1000 --threshold-rule=percent=90
```

- **Backup Strategy**: Regularly back up critical data and configurations.

```bash
gsutil versioning set on gs://your-bucket-name
```

By following these steps and best practices, you can efficiently set up and manage a GCP stack for your ZenML projects.

================================================================================

File: docs/book/how-to/popular-integrations/azure-guide.md

# Azure Stack Setup for ZenML Pipelines

This guide outlines the steps to set up a minimal production stack on Azure for running ZenML pipelines.

## Prerequisites
- Active Azure account
- ZenML installed
- ZenML Azure integration: `zenml integration install azure`

## Steps to Set Up Azure Stack

### 1. Create Service Principal
1. Go to Azure portal > App Registrations > `+ New registration`.
2. Register the app and note the Application ID and Tenant ID.
3. Under `Certificates & secrets`, create a client secret and note its value.

### 2. Create Resource Group and AzureML Instance
1. In Azure portal, go to `Resource Groups` > `+ Create`.
2. After creating the resource group, navigate to it and select `+ Create` to add a new resource.
3. Search for and select `Azure Machine Learning` to create an AzureML workspace, which includes a storage account, key vault, and application insights.

### 3. Create Role Assignments
1. In the resource group, go to `Access control (IAM)` > `+ Add role assignment`.
2. Assign the following roles to your registered app:
   - AzureML Compute Operator
   - AzureML Data Scientist
   - AzureML Registry User

### 4. Create ZenML Azure Service Connector
Register the service connector with the following command:
```bash
zenml service-connector register azure_connector --type azure \
  --auth-method service-principal \
  --client_secret=<CLIENT_SECRET> \
  --tenant_id=<TENANT_ID> \
  --client_id=<APPLICATION_ID>
```

### 5. Create Stack Components
- **Artifact Store (Azure Blob Storage)**:
  Create a container in the storage account and register it:
  ```bash
  zenml artifact-store register azure_artifact_store -f azure \
    --path=<PATH_TO_YOUR_CONTAINER> \
    --connector azure_connector
  ```

- **Orchestrator (AzureML)**:
  Register the orchestrator:
  ```bash
  zenml orchestrator register azure_orchestrator -f azureml \
      --subscription_id=<YOUR_AZUREML_SUBSCRIPTION_ID> \
      --resource_group=<NAME_OF_YOUR_RESOURCE_GROUP> \
      --workspace=<NAME_OF_YOUR_AZUREML_WORKSPACE> \
      --connector azure_connector
  ```

- **Container Registry (Azure Container Registry)**:
  Register the container registry:
  ```bash
  zenml container-registry register azure_container_registry -f azure \
    --uri=<URI_TO_YOUR_AZURE_CONTAINER_REGISTRY> \
    --connector azure_connector
  ```

### 6. Create ZenML Stack
Register the stack using the components:
```shell
zenml stack register azure_stack \
    -o azure_orchestrator \
    -a azure_artifact_store \
    -c azure_container_registry \
    --set
```

### 7. Run a ZenML Pipeline
Define and run a simple pipeline:
```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from Azure!"

@pipeline
def azure_pipeline():
    hello_world()

if __name__ == "__main__":
    azure_pipeline()
```
Save as `run.py` and execute:
```shell
python run.py
```

## Next Steps
- Explore ZenML's [production guide](../../user-guide/production-guide/README.md) for best practices.
- Check ZenML's [integrations](../../component-guide/README.md) with other tools.
- Join the [ZenML community](https://zenml.io/slack) for support and networking.

================================================================================

File: docs/book/how-to/popular-integrations/skypilot.md

### Summary of ZenML SkyPilot VM Orchestrator Documentation

**Overview**: The ZenML SkyPilot VM Orchestrator enables provisioning and management of VMs across cloud providers (AWS, GCP, Azure, Lambda Labs) for ML pipelines, enhancing cost efficiency and GPU availability.

#### Prerequisites:
- Install ZenML SkyPilot integration for your cloud provider:  
  ```bash
  zenml integration install <PROVIDER> skypilot_<PROVIDER>
  ```
- Ensure Docker is running.
- Set up a remote artifact store and container registry.
- Have a remote ZenML deployment.
- Obtain necessary permissions for VM provisioning.
- Configure a service connector for cloud authentication (not required for Lambda Labs).

#### Configuration Steps:

**For AWS, GCP, Azure**:
1. Install SkyPilot integration and provider-specific connectors.
2. Register a service connector with required credentials.
3. Register and connect the orchestrator to the service connector.
4. Register and activate a stack with the orchestrator.

```bash
zenml service-connector register <PROVIDER>-skypilot-vm -t <PROVIDER> --auto-configure
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_<PROVIDER>  
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <PROVIDER>-skypilot-vm
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

**For Lambda Labs**:
1. Install SkyPilot Lambda integration.
2. Register a secret for your API key.
3. Register the orchestrator using the API key.
4. Register and activate a stack with the orchestrator.

```bash
zenml secret create lambda_api_key --scope user --api_key=<KEY>
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_lambda --api_key={{lambda_api_key.api_key}}
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

#### Running a Pipeline:
Once configured, run ZenML pipelines using the SkyPilot VM Orchestrator, where each step executes in a Docker container on a provisioned VM.

#### Additional Configuration:
You can customize the orchestrator with cloud-specific `Settings` objects to define VM specifications:

```python
from zenml.integrations.skypilot_<PROVIDER>.flavors.skypilot_orchestrator_<PROVIDER>_vm_flavor import Skypilot<PROVIDER>OrchestratorSettings

skypilot_settings = Skypilot<PROVIDER>OrchestratorSettings(
   cpus="2",
   memory="16", 
   accelerators="V100:2",
   use_spot=True,
   region=<REGION>,
)

@pipeline(settings={"orchestrator": skypilot_settings})
```

Resource allocation can be specified per step:

```python
@step(settings={"orchestrator": high_resource_settings})  
def resource_intensive_step():
   ...
```

For further details, refer to the [full SkyPilot VM Orchestrator documentation](../../component-guide/orchestrators/skypilot-vm.md).

================================================================================

File: docs/book/how-to/popular-integrations/mlflow.md

### MLflow Experiment Tracker with ZenML

The MLflow Experiment Tracker integration in ZenML allows logging and visualizing pipeline step information using MLflow without additional code.

#### Prerequisites
- Install ZenML MLflow integration: 
  ```bash
  zenml integration install mlflow -y
  ```
- An MLflow deployment (local or remote with proxied artifact storage).

#### Configuring the Experiment Tracker
1. **Local Deployment**: 
   - Suitable for local ZenML runs, no extra configuration needed.
   ```bash
   zenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow
   zenml stack register custom_stack -e mlflow_experiment_tracker ... --set
   ```

2. **Remote Deployment**: 
   - Requires authentication (ZenML secrets recommended).
   ```bash
   zenml secret create mlflow_secret --username=<USERNAME> --password=<PASSWORD>
   zenml experiment-tracker register mlflow --flavor=mlflow --tracking_username={{mlflow_secret.username}} --tracking_password={{mlflow_secret.password}} ...
   ```

#### Using the Experiment Tracker
- Enable the experiment tracker with the `@step` decorator and use MLflow logging:
```python
import mlflow

@step(experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>")
def train_step(...):
   mlflow.tensorflow.autolog()
   mlflow.log_param(...)
   mlflow.log_metric(...)
   mlflow.log_artifact(...)
```

#### Viewing Results
- Retrieve the MLflow experiment URL for a ZenML run:
```python
last_run = client.get_pipeline("<PIPELINE_NAME>").last_run
tracking_url = last_run.get_step("<STEP_NAME>").run_metadata["experiment_tracker_url"].value
```

#### Additional Configuration
- Configure the experiment tracker with `MLFlowExperimentTrackerSettings`:
```python
from zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import MLFlowExperimentTrackerSettings

mlflow_settings = MLFlowExperimentTrackerSettings(nested=True, tags={"key": "value"})

@step(experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>", settings={"experiment_tracker": mlflow_settings})
```

For more advanced options, refer to the [full MLflow Experiment Tracker documentation](../../component-guide/experiment-trackers/mlflow.md).

================================================================================

File: docs/book/how-to/popular-integrations/README.md

# ZenML Integrations Guide

ZenML integrates with various tools in the data science and machine learning ecosystem. This guide outlines how to connect ZenML with popular tools.

### Key Points:
- ZenML is designed for seamless integration with favorite data science tools.
- The guide provides instructions for integrating ZenML with these tools.

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/popular-integrations/kubernetes.md

### Summary: Deploying ZenML Pipelines on Kubernetes

The ZenML Kubernetes Orchestrator enables running ML pipelines on a Kubernetes cluster without the need for Kubernetes coding, serving as a simpler alternative to orchestrators like Airflow or Kubeflow.

#### Prerequisites
To use the Kubernetes Orchestrator, ensure you have:
- ZenML `kubernetes` integration: `zenml integration install kubernetes`
- Docker installed and running
- `kubectl` installed
- A remote artifact store and container registry in your ZenML stack
- A deployed Kubernetes cluster
- (Optional) Configured `kubectl` context for the cluster

#### Deploying the Orchestrator
You need a Kubernetes cluster to run the orchestrator. Various deployment methods exist; refer to the [cloud guide](../../user-guide/cloud-guide/cloud-guide.md) for options.

#### Configuring the Orchestrator
Configuration can be done in two ways:

1. **Using a Service Connector** (recommended for cloud-managed clusters):
   ```bash
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubernetes
   zenml service-connector list-resources --resource-type kubernetes-cluster -e
   zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
   zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
   ```

2. **Using `kubectl` Context**:
   ```bash
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor=kubernetes --kubernetes_context=<KUBERNETES_CONTEXT>
   zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
   ```

#### Running a Pipeline
To run a ZenML pipeline:
```bash
python your_pipeline.py
```
This command creates a Kubernetes pod for each pipeline step. Use `kubectl` commands to interact with the pods. For more details, consult the [full Kubernetes Orchestrator documentation](../../component-guide/orchestrators/kubernetes.md).

================================================================================

File: docs/book/how-to/popular-integrations/aws-guide.md

# AWS Stack Setup for ZenML Pipelines

## Overview
This guide provides steps to set up a minimal production stack on AWS for running ZenML pipelines. It includes creating an IAM role with specific permissions for ZenML to authenticate with AWS resources.

## Prerequisites
- Active AWS account with permissions for S3, SageMaker, ECR, and ECS.
- ZenML installed.
- AWS CLI installed and configured.

## Steps

### 1. Set Up Credentials and Local Environment
1. **Choose AWS Region**: Select the region for deployment (e.g., `us-east-1`).
2. **Create IAM Role**:
   - Get your AWS account ID:
     ```shell
     aws sts get-caller-identity --query Account --output text
     ```
   - Create `assume-role-policy.json`:
     ```json
     {
       "Version": "2012-10-17",
       "Statement": [
         {
           "Effect": "Allow",
           "Principal": {
             "AWS": "arn:aws:iam::<YOUR_ACCOUNT_ID>:root",
             "Service": "sagemaker.amazonaws.com"
           },
           "Action": "sts:AssumeRole"
         }
       ]
     }
     ```
   - Replace `<YOUR_ACCOUNT_ID>` and create the role:
     ```shell
     aws iam create-role --role-name zenml-role --assume-role-policy-document file://assume-role-policy.json
     ```
3. **Attach Policies**:
   ```shell
   aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
   aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
   aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
   ```
4. **Install ZenML AWS Integration**:
   ```shell
   zenml integration install aws s3 -y
   ```

### 2. Create a Service Connector in ZenML
Register an AWS Service Connector:
```shell
zenml service-connector register aws_connector \
  --type aws \
  --auth-method iam-role \
  --role_arn=<ROLE_ARN> \
  --region=<YOUR_REGION> \
  --aws_access_key_id=<YOUR_ACCESS_KEY_ID> \
  --aws_secret_access_key=<YOUR_SECRET_ACCESS_KEY>
```

### 3. Create Stack Components
#### Artifact Store (S3)
1. Create an S3 bucket:
   ```shell
   aws s3api create-bucket --bucket your-bucket-name
   ```
2. Register the S3 Artifact Store:
   ```shell
   zenml artifact-store register cloud_artifact_store -f s3 --path=s3://your-bucket-name --connector aws_connector
   ```

#### Orchestrator (SageMaker Pipelines)
1. Create a SageMaker domain (if not already created).
2. Register the SageMaker Pipelines orchestrator:
   ```shell
   zenml orchestrator register sagemaker-orchestrator --flavor=sagemaker --region=<YOUR_REGION> --execution_role=<ROLE_ARN>
   ```

#### Container Registry (ECR)
1. Create an ECR repository:
   ```shell
   aws ecr create-repository --repository-name zenml --region <YOUR_REGION>
   ```
2. Register the ECR container registry:
   ```shell
   zenml container-registry register ecr-registry --flavor=aws --uri=<ACCOUNT_ID>.dkr.ecr.<YOUR_REGION>.amazonaws.com --connector aws_connector
   ```

### 4. Create Stack
```shell
export STACK_NAME=aws_stack

zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} \
    -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

### 5. Run a Pipeline
Define and run a simple ZenML pipeline:
```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from SageMaker!"

@pipeline
def aws_sagemaker_pipeline():
    hello_world()

if __name__ == "__main__":
    aws_sagemaker_pipeline()
```
Execute:
```shell
python run.py
```

## Cleanup
To avoid charges, delete resources:
```shell
# Delete S3 bucket
aws s3 rm s3://your-bucket-name --recursive
aws s3api delete-bucket --bucket your-bucket-name

# Delete SageMaker domain
aws sagemaker delete-domain --domain-id <DOMAIN_ID>

# Delete ECR repository
aws ecr delete-repository --repository-name zenml --force

# Detach policies and delete IAM role
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
aws iam delete-role --role-name zenml-role
```

## Conclusion
This guide provides a streamlined process for setting up an AWS stack with ZenML, enabling scalable and efficient machine learning pipeline management. Following best practices for IAM roles, resource tagging, cost management, and backup strategies will enhance security and efficiency in your AWS environment.

================================================================================

File: docs/book/how-to/popular-integrations/kubeflow.md

### Summary of Kubeflow Orchestrator Documentation

**Overview**: The ZenML Kubeflow Orchestrator enables running ML pipelines on Kubeflow without writing Kubeflow code.

#### Prerequisites:
- Install ZenML `kubeflow` integration: `zenml integration install kubeflow`
- Docker installed and running
- `kubectl` installed (optional)
- Kubernetes cluster with Kubeflow Pipelines
- Remote artifact store and container registry in ZenML stack
- Remote ZenML server deployed in the cloud
- Kubernetes context name (optional)

#### Configuring the Orchestrator:
1. **Using a Service Connector** (recommended for cloud-managed clusters):
   ```bash
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubeflow
   zenml service-connector list-resources --resource-type kubernetes-cluster -e  
   zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
   zenml stack update -o <ORCHESTRATOR_NAME>
   ```

2. **Using `kubectl`**:
   ```bash  
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor=kubeflow --kubernetes_context=<KUBERNETES_CONTEXT>
   zenml stack update -o <ORCHESTRATOR_NAME>
   ```

#### Running a Pipeline:
Execute any ZenML pipeline using:
```bash
python your_pipeline.py
```
This creates a Kubernetes pod for each pipeline step, viewable in the Kubeflow UI.

#### Additional Configuration:
Configure the orchestrator with `KubeflowOrchestratorSettings`:
```python
from zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import KubeflowOrchestratorSettings

kubeflow_settings = KubeflowOrchestratorSettings(
   client_args={},  
   user_namespace="my_namespace",
   pod_settings={
       "affinity": {...},
       "tolerations": [...]
   }
)

@pipeline(settings={"orchestrator": kubeflow_settings})
```

#### Multi-Tenancy Deployments:
For multi-tenant setups, register the orchestrator with:
```bash
zenml orchestrator register <NAME> --flavor=kubeflow --kubeflow_hostname=<KUBEFLOW_HOSTNAME>
```
Provide namespace, username, and password in settings:
```python
kubeflow_settings = KubeflowOrchestratorSettings(
   client_username="admin",
   client_password="abc123", 
   user_namespace="namespace_name"
)

@pipeline(settings={"orchestrator": kubeflow_settings})
```

For further details, refer to the [full Kubeflow Orchestrator documentation](../../component-guide/orchestrators/kubeflow.md).

================================================================================

File: docs/book/how-to/project-setup-and-management/interact-with-secrets.md

### ZenML Secrets Overview

**ZenML Secrets** are secure groupings of **key-value pairs** stored in the ZenML secrets store, each identified by a **name** for easy reference in pipelines and stacks.

### Creating Secrets

#### CLI Method
To create a secret named `<SECRET_NAME>` with key-value pairs:

```shell
zenml secret create <SECRET_NAME> \
    --<KEY_1>=<VALUE_1> \
    --<KEY_2>=<VALUE_2>

# Using JSON or YAML format
zenml secret create <SECRET_NAME> \
    --values='{"key1":"value2","key2":"value2"}'
```

For interactive creation:

```shell
zenml secret create <SECRET_NAME> -i
```

For large values or special characters, use the `@` syntax to read from a file:

```bash
zenml secret create <SECRET_NAME> \
   --key=@path/to/file.txt
```

#### Python SDK Method
Using the ZenML client API:

```python
from zenml.client import Client

client = Client()
client.create_secret(
    name="my_secret",
    values={"username": "admin", "password": "abc123"}
)
```

### Managing Secrets
You can list, update, and delete secrets via CLI. Use `zenml stack register-secrets [<STACK_NAME>]` to interactively register missing secrets for a stack.

### Scoping Secrets
Secrets can be scoped to users. By default, they are scoped to the active user. To create a user-scoped secret:

```shell
zenml secret create <SECRET_NAME> \
    --scope user \
    --<KEY_1>=<VALUE_1> \
    --<KEY_2>=<VALUE_2>
```

### Accessing Secrets
To reference secrets in stack components, use the syntax `{{<SECRET_NAME>.<SECRET_KEY>}}`. For example:

```shell
zenml secret create mlflow_secret \
    --username=admin \
    --password=abc123

zenml experiment-tracker register mlflow \
    --tracking_username={{mlflow_secret.username}} \
    --tracking_password={{mlflow_secret.password}}
```

ZenML validates the existence of secrets and keys before running a pipeline. Control validation with the `ZENML_SECRET_VALIDATION_LEVEL` environment variable:

- `NONE`: Disable validation.
- `SECRET_EXISTS`: Validate only the existence of secrets.
- `SECRET_AND_KEY_EXISTS`: Default; validates both secret and key existence.

### Fetching Secret Values in Steps
To access secrets in steps:

```python
from zenml import step
from zenml.client import Client

@step
def secret_loader() -> None:
    secret = Client().get_secret(<SECRET_NAME>)
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

This allows secure access to sensitive information without hard-coding credentials.

================================================================================

File: docs/book/how-to/project-setup-and-management/README.md

# Project Setup and Management

This section outlines the essential steps for setting up and managing ZenML projects.

## Key Steps:

1. **Project Initialization**:
   - Use `zenml init` to create a new ZenML project directory.
   - This command sets up the necessary file structure and configuration.

2. **Configuration**:
   - Configure your project using `zenml configure`.
   - Specify components like version control, storage, and orchestrators.

3. **Pipeline Creation**:
   - Define pipelines using decorators and functions.
   - Example:
     ```python
     @pipeline
     def my_pipeline():
         step1 = step1_function()
         step2 = step2_function(step1)
     ```

4. **Running Pipelines**:
   - Execute pipelines with `zenml run my_pipeline`.
   - Monitor progress and logs via the ZenML dashboard.

5. **Version Control**:
   - Integrate with Git for versioning.
   - Use `.zenml` directory to track project changes.

6. **Collaboration**:
   - Share projects by pushing to a remote repository.
   - Ensure team members have access to the same configurations.

7. **Best Practices**:
   - Maintain clear documentation for pipelines and configurations.
   - Regularly update dependencies and ZenML versions.

This guide provides a foundational understanding of setting up and managing ZenML projects effectively.

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/stacks-pipelines-models.md

# Organizing Stacks, Pipelines, Models, and Artifacts in ZenML

ZenML's architecture revolves around stacks, pipelines, models, and artifacts, which are essential for organizing your ML workflow.

## Key Concepts

- **Stacks**: Configuration of tools and infrastructure for running pipelines, including components like orchestrators and artifact stores. Stacks enable seamless transitions between environments (local, staging, production) and can be reused across multiple pipelines, promoting consistency and reducing configuration overhead.

- **Pipelines**: Sequences of tasks in your ML workflow, such as data preparation, training, and evaluation. It’s advisable to separate pipelines by task type for modularity and easier management. This allows independent execution and better organization of runs.

- **Models**: Collections of related pipelines, artifacts, and metadata, acting as a "project" that spans multiple pipelines. Models facilitate data transfer between pipelines, such as moving a trained model from training to inference.

- **Artifacts**: Outputs of pipeline steps that can be tracked and reused. Proper naming of artifacts aids in identification and traceability across pipeline runs. Artifacts can be associated with models for better organization.

## Organizing Your Workflow

1. **Pipelines**: Create separate pipelines for distinct tasks (e.g., feature engineering, training, inference) to enhance modularity and manageability.

2. **Models**: Use models to group related artifacts and pipelines. The Model Control Plane helps manage model versions and stages.

3. **Artifacts**: Track outputs of pipeline steps and log metadata for traceability. Each unique execution produces a new artifact version.

## Example Workflow

1. Team members create three pipelines: feature engineering, training, and inference.
2. They use a shared `default` stack for local development.
3. Alice’s inference pipeline references the model artifact produced by Bob’s training pipeline.
4. The Model Control Plane helps manage model versions, allowing Alice to use the correct version in her pipeline.
5. Alice’s inference pipeline generates a new artifact (predictions), which can be logged as metadata.

## Guidelines for Organization

- **Models**: One model per use case; group related resources.
- **Stacks**: Separate stacks for different environments; share production and staging stacks.
- **Naming**: Consistent naming conventions; use tags for organization; document configurations and dependencies.

Following these principles will help maintain a scalable and organized MLOps workflow in ZenML.

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/README.md

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I will be happy to assist you!

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/shared-components-for-teams.md

# Shared Libraries and Logic for Teams

## Overview
This guide focuses on sharing code libraries within teams using ZenML, emphasizing what can be shared and how to distribute shared components.

## What Can Be Shared
ZenML supports sharing several custom components:

### Custom Flavors
- Create in a shared repository.
- Implement as per ZenML documentation.
- Register using ZenML CLI:
  ```bash
  zenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>
  ```

### Custom Steps
- Create and share via a separate repository, referenced like Python modules.

### Custom Materializers
- Create in a shared repository and implement as per ZenML documentation. Team members can import these into their projects.

## How to Distribute Shared Components

### Shared Private Wheels
- Packages Python code for internal distribution.
- **Benefits**: Easy installation, version and dependency management, privacy, and smooth integration.

#### Setting Up
1. Create a private PyPI server (e.g., AWS CodeArtifact).
2. Build code into wheel format.
3. Upload the wheel to the private server.
4. Configure pip to use the private server.
5. Install packages using pip.

### Using Shared Libraries with `DockerSettings`
ZenML generates a `Dockerfile` at runtime. Use `DockerSettings` to include shared libraries.

#### Installing Shared Libraries
Specify requirements directly:
```python
from zenml.config import DockerSettings
from zenml import pipeline

docker_settings = DockerSettings(
    requirements=["my-simple-package==0.1.0"],
    environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ.get('PYPI_TOKEN', '')}@my-private-pypi-server.com/{os.environ.get('PYPI_USERNAME', '')}/"}
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```
Or use a requirements file:
```python
docker_settings = DockerSettings(requirements="/path/to/requirements.txt")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```
The `requirements.txt` should include:
```
--extra-index-url https://YOURTOKEN@my-private-pypi-server.com/YOURUSERNAME/
my-simple-package==0.1.0
```

## Best Practices
- **Version Control**: Use Git for shared code repositories.
- **Access Controls**: Implement security measures for private servers.
- **Documentation**: Maintain clear and comprehensive documentation for shared components.
- **Regular Updates**: Keep shared libraries updated and communicate changes.
- **Continuous Integration**: Set up CI for quality assurance and compatibility.

By following these guidelines, teams can enhance collaboration, maintain consistency, and accelerate development within the ZenML framework.

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/access-management.md

# Access Management and Roles in ZenML

This guide outlines the management of user roles and responsibilities in ZenML, emphasizing the importance of access management for security and efficiency.

## Typical Roles in an ML Project
- **Data Scientists**: Develop and run pipelines.
- **MLOps Platform Engineers**: Manage infrastructure and stack components.
- **Project Owners**: Oversee ZenML deployment and user access.

Roles may vary in your team, but responsibilities can be aligned with the roles mentioned.

### Creating Roles
You can create roles in ZenML Pro with specific permissions and assign them to Users or Teams. For more details, refer to the [Roles in ZenML Pro](../../../getting-started/zenml-pro/roles.md).

## Service Connectors
Service connectors integrate external cloud services with ZenML, abstracting credentials and configurations. Only MLOps Platform Engineers should manage these connectors, while Data Scientists can use them to create stack components without accessing sensitive credentials.

### Example Permissions
- **Data Scientist**: Can use connectors but cannot create, update, or delete them.
- **MLOps Platform Engineer**: Can create, update, delete connectors, and read their secret values.

RBAC features are available only in ZenML Pro. More on roles can be found [here](../../../getting-started/zenml-pro/roles.md).

## Server Upgrades
Project Owners decide when to upgrade the ZenML server, considering team requirements. MLOps Platform Engineers typically perform the upgrade, ensuring data backup and no service disruption. For best practices, see the [Best Practices for Upgrading ZenML Servers](../../advanced-topics/manage-zenml-server/best-practices-upgrading-zen.md).

## Pipeline Migration and Maintenance
Data Scientists own the pipeline code, while Platform Engineers ensure compatibility with new ZenML versions. Both should review release notes and migration guides during upgrades.

## Best Practices for Access Management
- **Regular Audits**: Review user access and permissions periodically.
- **Role-Based Access Control (RBAC)**: Streamline permission management.
- **Least Privilege**: Grant minimal permissions necessary.
- **Documentation**: Maintain clear records of roles and access policies.

RBAC is only available for ZenML Pro users. Following these guidelines ensures a secure and collaborative ZenML environment.

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/project-templates/create-your-own-template.md

### Creating Your Own ZenML Template

To standardize and share ML workflows, you can create a ZenML template using the Copier library. Follow these steps:

1. **Create a Repository**: Set up a new repository to store your template's code and configuration files.

2. **Define ML Workflows**: Use existing ZenML templates (e.g., the [starter template](https://github.com/zenml-io/template-starter)) as a base to define your ML steps and pipelines.

3. **Create `copier.yml`**: This file defines the template's parameters and default values. Refer to the [Copier documentation](https://copier.readthedocs.io/en/stable/creating/) for details.

4. **Test Your Template**: Use the Copier CLI to generate a new project:

   ```bash
   copier copy https://github.com/your-username/your-template.git your-project
   ```

5. **Use Your Template with ZenML**: Initialize a new ZenML project with your template:

   ```bash
   zenml init --template https://github.com/your-username/your-template.git
   ```

   For a specific version, use:

   ```bash
   zenml init --template https://github.com/your-username/your-template.git --template-tag v1.0.0
   ```

6. **Keep It Updated**: Regularly update your template to align with best practices and changes in your workflows.

For practical experience, install the `e2e_batch` template using:

```bash
mkdir e2e_batch
cd e2e_batch
zenml init --template e2e_batch --template-with-defaults
```

This guide helps you create and utilize your ZenML template effectively.

================================================================================

File: docs/book/how-to/project-setup-and-management/collaborate-with-team/project-templates/README.md

### ZenML Project Templates Overview

ZenML provides project templates to help users quickly understand the framework and start building ML pipelines. These templates cover major use cases and include a simple CLI.

#### Available Project Templates

| Project Template [Short name] | Tags | Description |
|-------------------------------|------|-------------|
| [Starter template](https://github.com/zenml-io/template-starter) [code: starter] | basic, scikit-learn | Basic ML components for starting with ZenML, including parameterized steps, a model training pipeline, and a simple CLI, using scikit-learn. |
| [E2E Training with Batch Predictions](https://github.com/zenml-io/template-e2e-batch) [code: e2e_batch] | etl, hp-tuning, model-promotion, drift-detection, batch-prediction, scikit-learn | A comprehensive template with pipelines for data loading, preprocessing, hyperparameter tuning, model training, evaluation, promotion, drift detection, and batch inference. |
| [NLP Training Pipeline](https://github.com/zenml-io/template-nlp) [code: nlp] | nlp, hp-tuning, model-promotion, training, pytorch, gradio, huggingface | An NLP pipeline for tokenization, training, hyperparameter tuning, evaluation, and deployment of BERT or GPT-2 models, with local testing using Gradio. |

#### Using a Project Template

To use the templates, install ZenML with the templates extras:

```bash
pip install zenml[templates]
```

**Note:** These templates differ from 'Run Templates' used for triggering pipelines. More on Run Templates can be found [here](https://docs.zenml.io/how-to/trigger-pipelines).

To generate a project from a template, use:

```bash
zenml init --template <short_name_of_template>
# Example: zenml init --template e2e_batch
```

For default values, add `--template-with-defaults`:

```bash
zenml init --template <short_name_of_template> --template-with-defaults
# Example: zenml init --template e2e_batch --template-with-defaults
```

#### Collaboration Invitation

ZenML invites users with personal projects to collaborate and share their experiences to enhance the platform. Interested users can join the [ZenML Slack](https://zenml.io/slack/) for discussions.

================================================================================

File: docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/connect-your-git-repository.md

### Summary of ZenML Code Repository Documentation

**Overview**: Connecting a Git repository to ZenML allows for tracking code versions and speeding up Docker image builds by avoiding unnecessary rebuilds when source code changes.

#### Registering a Code Repository
1. **Install Integration**:
   To use a specific code repository, install the corresponding ZenML integration:
   ```bash
   zenml integration install <INTEGRATION_NAME>
   ```

2. **Register Repository**:
   Use the CLI to register the code repository:
   ```bash
   zenml code-repository register <NAME> --type=<TYPE> [--CODE_REPOSITORY_OPTIONS]
   ```

#### Available Implementations
- **GitHub**:
  - Install GitHub integration:
    ```bash
    zenml integration install github
    ```
  - Register GitHub repository:
    ```bash
    zenml code-repository register <NAME> --type=github \
    --url=<GITHUB_URL> --owner=<OWNER> --repository=<REPOSITORY> \
    --token=<GITHUB_TOKEN>
    ```
  - **Token Generation**:
    1. Go to GitHub settings > Developer settings > Personal access tokens.
    2. Generate a new token with `contents` read-only access.

- **GitLab**:
  - Install GitLab integration:
    ```bash
    zenml integration install gitlab
    ```
  - Register GitLab repository:
    ```bash
    zenml code-repository register <NAME> --type=gitlab \
    --url=<GITLAB_URL> --group=<GROUP> --project=<PROJECT> \
    --token=<GITLAB_TOKEN>
    ```
  - **Token Generation**:
    1. Go to GitLab settings > Access Tokens.
    2. Create a token with necessary scopes (e.g., `read_repository`).

#### Custom Code Repository
To implement a custom code repository:
1. Subclass `zenml.code_repositories.BaseCodeRepository` and implement the required methods:
   ```python
   class BaseCodeRepository(ABC):
       @abstractmethod
       def login(self) -> None:
           pass

       @abstractmethod
       def download_files(self, commit: str, directory: str, repo_sub_directory: Optional[str]) -> None:
           pass

       @abstractmethod
       def get_local_context(self, path: str) -> Optional["LocalRepositoryContext"]:
           pass
   ```

2. Register the custom repository:
   ```bash
   zenml code-repository register <NAME> --type=custom --source=my_module.MyRepositoryClass [--CODE_REPOSITORY_OPTIONS]
   ```

This documentation provides essential steps for integrating and managing code repositories within ZenML, including GitHub and GitLab support, and guidelines for custom implementations.

================================================================================

File: docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/README.md

# Setting up a Well-Architected ZenML Project

This guide outlines best practices for structuring ZenML projects to enhance scalability, maintainability, and team collaboration.

## Importance of a Well-Architected Project
A well-architected ZenML project is vital for efficient machine learning operations (MLOps), providing a foundation for developing, deploying, and maintaining ML models.

## Key Components

### Repository Structure
- Organize folders for pipelines, steps, and configurations.
- Maintain clear separation of concerns and consistent naming conventions.
- Refer to the [Set up repository guide](./best-practices.md) for details.

### Version Control and Collaboration
- Integrate with Git for efficient code management and collaboration.
- Enables faster pipeline builds by reusing images and downloading code directly from the repository.
- Learn more in the [Set up a repository guide](./best-practices.md).

### Stacks, Pipelines, Models, and Artifacts
- **Stacks**: Define infrastructure and tool configurations.
- **Models**: Represent ML models and metadata.
- **Pipelines**: Encapsulate ML workflows.
- **Artifacts**: Track data and model outputs.
- Explore organization in the [Organizing Stacks, Pipelines, Models, and Artifacts guide](./stacks-pipelines-models.md).

### Access Management and Roles
- Define roles (data scientists, MLOps engineers, etc.) and set up service connectors.
- Manage authorizations and establish maintenance processes.
- Use [Teams in ZenML Pro](../../../getting-started/zenml-pro/teams.md) for role assignments.
- Review strategies in the [Access Management and Roles guide](./access-management-and-roles.md).

### Shared Components and Libraries
- Promote code reuse with custom flavors, steps, and shared libraries.
- Handle authentication for specific libraries.
- Learn about sharing code in the [Shared Libraries and Logic for Teams guide](./shared_components_for_teams.md).

### Project Templates
- Utilize pre-made and custom templates to ensure consistency.
- Discover more in the [Project Templates guide](./project-templates.md).

### Migration and Maintenance
- Implement strategies for migrating legacy code and upgrading ZenML servers.
- Find best practices in the [Migration and Maintenance guide](../../advanced-topics/manage-zenml-server/best-practices-upgrading-zenml.md#upgrading-your-code).

## Getting Started
Begin by exploring the guides in this section for detailed information on project setup and management. Regularly review and refine your project structure to meet evolving team needs. Following these guidelines will help create a robust, scalable, and collaborative MLOps environment.

================================================================================

File: docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/set-up-repository.md

### Recommended Repository Structure and Best Practices for ZenML

#### Project Structure
A recommended structure for ZenML projects is as follows:

```markdown
.
├── .dockerignore
├── Dockerfile
├── steps
│   ├── loader_step
│   │   ├── loader_step.py
│   │   └── requirements.txt (optional)
│   └── training_step
├── pipelines
│   ├── training_pipeline
│   │   ├── training_pipeline.py
│   │   └── requirements.txt (optional)
│   └── deployment_pipeline
├── notebooks
│   └── *.ipynb
├── requirements.txt
├── .zen
└── run.py
```

- The `steps` and `pipelines` folders contain the respective components of your project.
- Simpler projects may keep steps directly in the `steps` folder without subfolders.

#### Code Repository Registration
Registering your repository allows ZenML to track code versions for pipeline runs and can speed up Docker image builds by avoiding unnecessary rebuilds. More details can be found in the [connecting your Git repository](https://docs.zenml.io/how-to/setting-up-a-project-repository/connect-your-git-repository) documentation.

#### Steps
- Store each step in separate Python files to manage utilities, dependencies, and Dockerfiles.
- Use the `logging` module to log messages, which will be recorded in the ZenML dashboard.

```python
from zenml.logger import get_logger

logger = get_logger(__name__)

@step
def training_data_loader():
    logger.info("My logs")
```

#### Pipelines
- Keep pipelines in separate Python files and separate execution from definition to prevent immediate execution upon import.
- Avoid naming pipelines or instances "pipeline" to prevent conflicts with the imported `pipeline` decorator.

#### .dockerignore
Use a `.dockerignore` file to exclude unnecessary files (e.g., data, virtual environments) from Docker images, reducing size and build time.

#### Dockerfile
ZenML uses an official Docker image by default. You can provide a custom `Dockerfile` if needed.

#### Notebooks
Organize all Jupyter notebooks in a dedicated folder.

#### .zen
Run `zenml init` at the project root to define the project scope, which helps resolve import paths and store configurations. This is especially important for projects using Jupyter notebooks.

#### run.py
Place your pipeline runners in the root directory to ensure proper resolution of imports relative to the project root. If no `.zen` file is defined, this will implicitly define the source's root.

================================================================================

File: docs/book/how-to/customize-docker-builds/how-to-use-a-private-pypi-repository.md

### How to Use a Private PyPI Repository

To use a private PyPI repository that requires authentication, follow these steps:

1. **Store Credentials Securely**: Use environment variables for credentials.
2. **Configure Package Managers**: Set up `pip` or `poetry` to utilize these credentials during package installation.
3. **Custom Docker Images**: Consider using Docker images pre-configured with the necessary authentication.

#### Example Code for Authentication Setup

```python
import os
from my_simple_package import important_function
from zenml.config import DockerSettings
from zenml import step, pipeline

docker_settings = DockerSettings(
    requirements=["my-simple-package==0.1.0"],
    environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ['PYPI_TOKEN']}@my-private-pypi-server.com/{os.environ['PYPI_USERNAME']}/"}
)

@step
def my_step():
    return important_function()

@pipeline(settings={"docker": docker_settings})
def my_pipeline():
    my_step()

if __name__ == "__main__":
    my_pipeline()
```

**Important Note**: Handle credentials with care and use secure methods for managing and distributing authentication information within your team.

================================================================================

File: docs/book/how-to/customize-docker-builds/README.md

### Customize Docker Builds in ZenML

ZenML executes pipeline steps sequentially in the local Python environment. However, when using remote orchestrators or step operators, it builds Docker images to run pipelines in an isolated environment. This section covers how to manage the dockerization process.

**Key Points:**
- **Execution Environment:** Local Python for local runs; Docker images for remote orchestrators or step operators.
- **Isolation:** Docker provides a well-defined environment for pipeline execution.

For more details, refer to the sections on [cloud orchestration](../../user-guide/production-guide/cloud-orchestration.md) and [step operators](../../component-guide/step-operators/step-operators.md).

================================================================================

File: docs/book/how-to/customize-docker-builds/docker-settings-on-a-step.md

### Summary of Docker Settings Customization in ZenML

In ZenML, you can customize Docker settings at the step level, allowing different steps in a pipeline to use distinct Docker images. By default, all steps inherit the Docker image defined at the pipeline level. 

**Customizing Docker Settings in Step Decorator:**
You can specify a different Docker image for a step by using the `DockerSettings` in the step decorator.

```python
from zenml import step
from zenml.config import DockerSettings

@step(
  settings={
    "docker": DockerSettings(
      parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime"
    )
  }
)
def training(...):
    ...
```

**Customizing Docker Settings in Configuration File:**
Alternatively, you can define Docker settings in a configuration file.

```yaml
steps:
  training:
    settings:
      docker:
        parent_image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
        required_integrations:
          - gcp
          - github
        requirements:
          - zenml
          - numpy
```

This allows for flexibility in managing dependencies and integrations specific to each step.

================================================================================

File: docs/book/how-to/customize-docker-builds/specify-pip-dependencies-and-apt-packages.md

### Summary of Specifying Pip Dependencies and Apt Packages in ZenML

**Context**: This documentation outlines how to specify pip and apt dependencies for remote pipelines in ZenML. It is important to note that these configurations do not apply to local pipelines.

**Key Points**:

1. **Docker Image Creation**: When a pipeline is executed with a remote orchestrator, a Dockerfile is generated dynamically to build the Docker image.

2. **Default Behavior**: ZenML installs all packages required by the active stack automatically.

3. **Specifying Additional Packages**:
   - **Replicate Local Environment**:
     ```python
     docker_settings = DockerSettings(replicate_local_python_environment="pip_freeze")
     ```
   - **Custom Command for Requirements**:
     ```python
     docker_settings = DockerSettings(replicate_local_python_environment=["poetry", "export", "--extras=train", "--format=requirements.txt"])
     ```
   - **List of Requirements in Code**:
     ```python
     docker_settings = DockerSettings(requirements=["torch==1.12.0", "torchvision"])
     ```
   - **Requirements File**:
     ```python
     docker_settings = DockerSettings(requirements="/path/to/requirements.txt")
     ```
   - **ZenML Integrations**:
     ```python
     from zenml.integrations.constants import PYTORCH, EVIDENTLY
     docker_settings = DockerSettings(required_integrations=[PYTORCH, EVIDENTLY])
     ```
   - **Apt Packages**:
     ```python
     docker_settings = DockerSettings(apt_packages=["git"])
     ```
   - **Disable Automatic Requirement Installation**:
     ```python
     docker_settings = DockerSettings(install_stack_requirements=False)
     ```

4. **Custom Docker Settings for Steps**:
   ```python
   docker_settings = DockerSettings(requirements=["tensorflow"])
   @step(settings={"docker": docker_settings})
   def my_training_step(...):
       ...
   ```

5. **Installation Order**:
   - Local Python environment packages
   - Stack requirements (if not disabled)
   - Required integrations
   - Explicitly specified requirements

6. **Installer Arguments**:
   ```python
   docker_settings = DockerSettings(python_package_installer_args={"timeout": 1000})
   ```

7. **Experimental Installer**: Use `uv` for faster package installation:
   ```python
   docker_settings = DockerSettings(python_package_installer="uv")
   ```

**Note**: If issues arise with `uv`, revert to `pip`. For detailed integration with PyTorch, refer to the [Astral Docs](https://docs.astral.sh/uv/guides/integration/pytorch/). 

This summary retains critical information and code examples while ensuring clarity and conciseness.

================================================================================

File: docs/book/how-to/customize-docker-builds/how-to-reuse-builds.md

### Summary of Build Reuse in ZenML

#### Overview
This documentation explains how to reuse builds in ZenML to enhance pipeline efficiency. A build encapsulates a pipeline and its stack, including Docker images and optionally the pipeline code.

#### What is a Build?
A build represents a specific execution of a pipeline with its associated stack. It contains the necessary Docker images and can optionally include the pipeline code. To list builds for a pipeline, use:

```bash
zenml pipeline builds list --pipeline_id='startswith:ab53ca'
```

To create a build manually:

```bash
zenml pipeline build --stack vertex-stack my_module.my_pipeline_instance
```

#### Reusing Builds
ZenML automatically reuses existing builds that match the pipeline and stack. You can specify a build ID to force the use of a particular build. However, note that reusing a build will execute the code in the Docker image, not your local code changes. To ensure local changes are included, disconnect your code from the build by either registering a code repository or using the artifact store.

#### Using the Artifact Store
ZenML can upload your code to the artifact store by default unless a code repository is detected and the `allow_download_from_artifact_store` flag is set to `False`.

#### Connecting Code Repositories
Connecting a git repository allows for faster Docker builds by avoiding the need to include source files in the image. ZenML will automatically reuse appropriate builds when a clean repository state is maintained. To register a code repository, ensure the relevant integrations are installed:

```sh
zenml integration install github
```

#### Detecting Local Code Repositories
ZenML checks if the files used in a pipeline run are tracked in registered code repositories by computing the source root and verifying its inclusion in a local checkout.

#### Tracking Code Versions
When a local code repository is detected, ZenML stores a reference to the current commit for the pipeline run. This reference is only tracked if the local checkout is clean, ensuring the pipeline runs with the exact code version.

#### Best Practices
- Ensure the local checkout is clean and the latest commit is pushed to avoid file download failures.
- For options to disable or enforce file downloading, refer to the relevant documentation.

This summary retains critical technical details and provides concise guidance on using builds effectively in ZenML.

================================================================================

File: docs/book/how-to/customize-docker-builds/which-files-are-built-into-the-image.md

### ZenML Image Building and File Management

ZenML determines the root directory for source files based on the following:

1. If `zenml init` has been executed in the current or a parent directory, that directory is used as the root.
2. If not, the parent directory of the executing Python file is used. For example, running `python /path/to/file.py` sets the source root to `/path/to`.

You can control file handling in the Docker image using the `DockerSettings` attributes:

- **`allow_download_from_code_repository`**: If `True` and the files are in a registered code repository with no local changes, files will be downloaded from the repository instead of included in the image.
- **`allow_download_from_artifact_store`**: If the previous option is `False` or no suitable repository exists, and this is `True`, ZenML will archive and upload your code to the artifact store.
- **`allow_including_files_in_images`**: If both previous options are `False`, and this is `True`, files will be included in the Docker image, requiring a new image build for any code changes.

**Warning**: Setting all attributes to `False` is not recommended, as it may lead to unexpected behavior. You must ensure all files are correctly positioned in the Docker images used for pipeline execution.

### File Exclusion and Inclusion

- **Excluding Files**: Use a `.gitignore` file to exclude files when downloading from a code repository.
- **Including Files**: Use a `.dockerignore` file to exclude files when building the Docker image. This can be done by:
  - Placing a `.dockerignore` file in the source root directory.
  - Specifying a `.dockerignore` file explicitly:

```python
docker_settings = DockerSettings(build_config={"dockerignore": "/path/to/.dockerignore"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

This setup helps manage which files are included or excluded in the Docker image, optimizing the build process.

================================================================================

File: docs/book/how-to/customize-docker-builds/use-a-prebuilt-image.md

### Summary: Using a Prebuilt Image for ZenML Pipeline Execution

ZenML allows you to skip building a Docker image for your pipeline by using a prebuilt image. This can save time and costs, especially when dependencies are large or internet speeds are slow. However, using a prebuilt image means you won't receive updates to your code or dependencies unless they are included in the image.

#### Setting Up DockerSettings
To use a prebuilt image, configure the `DockerSettings` class:

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

Ensure the specified image is pushed to a registry accessible by your orchestrator.

#### Requirements for the Parent Image
The `parent_image` must contain:
- All dependencies required by your pipeline.
- Optionally, your code files if no code repository is registered and `allow_download_from_artifact_store` is `False`.

If using an image built by ZenML from a previous run, it can be reused as long as it was built for the same stack.

#### Stack and Integration Requirements
To ensure your image meets stack requirements:

```python
from zenml.client import Client

stack_name = <YOUR_STACK>
Client().set_active_stack(stack_name)
active_stack = Client().active_stack
stack_requirements = active_stack.requirements()
```

For integration dependencies:

```python
from zenml.integrations.registry import integration_registry
from zenml.integrations.constants import HUGGINGFACE, PYTORCH

required_integrations = [PYTORCH, HUGGINGFACE]
integration_requirements = set(
    itertools.chain.from_iterable(
        integration_registry.select_integration_requirements(
            integration_name=integration,
            target_os=OperatingSystemType.LINUX,
        )
        for integration in required_integrations
    )
)
```

#### Project-Specific and System Packages
Add project-specific requirements in your `Dockerfile`:

```Dockerfile
RUN pip install <ANY_ARGS> -r FILE
```

Include necessary `apt` packages:

```Dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends YOUR_APT_PACKAGES
```

#### Code Files
Ensure your pipeline and step code is available:
- If a code repository is registered, ZenML will handle it.
- If `allow_download_from_artifact_store` is `True`, ZenML will upload your code.
- If both options are disabled, include your code files in the image (not recommended).

Your code should be in the `/app` directory, and Python, `pip`, and `zenml` must be installed in the image.

================================================================================

File: docs/book/how-to/customize-docker-builds/docker-settings-on-a-pipeline.md

### Summary: Using Docker Images to Run Your Pipeline

#### Overview
When running a pipeline with a remote orchestrator, a Dockerfile is dynamically generated at runtime to build a Docker image using the ZenML image builder. The Dockerfile includes:

1. **Base Image**: Starts from a parent image with ZenML installed, defaulting to the official ZenML image for the active Python environment. Custom base images can be specified.
2. **Pip Dependencies**: Automatically installs required integrations and additional dependencies as needed.
3. **Source Files**: Optionally copies source files into the Docker container for execution.
4. **Environment Variables**: Sets user-defined environment variables.

#### Configuring Docker Settings
Docker settings can be configured using the `DockerSettings` class:

```python
from zenml.config import DockerSettings
```

**Pipeline Configuration**: Apply settings to all steps:

```python
docker_settings = DockerSettings()
@pipeline(settings={"docker": docker_settings})
def my_pipeline():
    my_step()
```

**Step Configuration**: Apply settings to individual steps for specialized images:

```python
@step(settings={"docker": docker_settings})
def my_step():
    pass
```

**YAML Configuration**: Use a YAML file for settings:

```yaml
settings:
    docker:
        ...
steps:
  step_name:
    settings:
        docker:
            ...
```

#### Docker Build Options
To specify build options for the image builder:

```python
docker_settings = DockerSettings(build_config={"build_options": {...}})
@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**MacOS ARM Architecture**: Specify the target platform for local Docker caching:

```python
docker_settings = DockerSettings(build_config={"build_options": {"platform": "linux/amd64"}})
@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

#### Custom Parent Images
You can specify a custom pre-built parent image or a Dockerfile. Ensure the image has Python, pip, and ZenML installed.

**Using a Pre-built Parent Image**:

```python
docker_settings = DockerSettings(parent_image="my_registry.io/image_name:tag")
@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**Skipping Docker Builds**:

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)
@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**Warning**: Using a pre-built image may lead to unintended behavior. Ensure code files are included in the specified image.

For more details on configuration options, refer to the [DockerSettings documentation](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings).

================================================================================

File: docs/book/how-to/customize-docker-builds/use-your-own-docker-files.md

# Using Custom Docker Files in ZenML

ZenML allows you to build a parent Docker image dynamically for each pipeline execution by specifying a custom Dockerfile, build context directory, and build options. The build process is as follows:

- **No Dockerfile Specified**: If requirements or environment configurations necessitate an image build, ZenML will create one. Otherwise, it uses the `parent_image`.
  
- **Dockerfile Specified**: ZenML builds an image from the specified Dockerfile. If further requirements necessitate an additional image, ZenML will build a second image; otherwise, the first image is used for the pipeline.

The installation of requirements follows this order (each step is optional):
1. Local Python environment packages.
2. Packages from the `requirements` attribute.
3. Packages from `required_integrations` and stack requirements.

Depending on the `DockerSettings` configuration, the intermediate image may also be used directly for executing pipeline steps.

### Example Code
```python
docker_settings = DockerSettings(
    dockerfile="/path/to/dockerfile",
    build_context_root="/path/to/build/context",
    parent_image_build_config={
        "build_options": ...,
        "dockerignore": ...
    }
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

================================================================================

File: docs/book/how-to/customize-docker-builds/define-where-an-image-is-built.md

### Image Builder Definition in ZenML

ZenML executes pipeline steps sequentially in the local Python environment. For remote orchestrators or step operators, it builds Docker images for isolated execution environments. By default, these environments are created locally using the Docker client, which requires Docker installation and permissions.

ZenML provides **image builders**, a specialized stack component for building and pushing Docker images in a dedicated environment. Even without a configured image builder, ZenML defaults to the local image builder to ensure consistency across builds, using the client environment.

Users do not need to interact directly with image builders in their code. As long as the desired image builder is included in the active ZenML stack, it will be automatically utilized by any component requiring container image builds. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/manage-zenml-server/README.md

# Manage Your ZenML Server

This section provides guidance on best practices for upgrading your ZenML server, using it in production, and troubleshooting. It includes recommended upgrade steps and migration guides for transitioning between specific versions.

## Key Points:
- **Upgrading**: Follow the recommended steps for a smooth upgrade process.
- **Production Use**: Tips for effectively utilizing ZenML in a production environment.
- **Troubleshooting**: Common issues and their resolutions.
- **Migration Guides**: Instructions for moving between certain ZenML versions.

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/manage-zenml-server/upgrade-zenml-server.md

# ZenML Server Upgrade Guide

## Overview
This guide outlines how to upgrade your ZenML server based on the deployment method. Always refer to the [best practices for upgrading ZenML](./best-practices-upgrading-zenml.md) before proceeding.

## General Recommendations
- Upgrade promptly after a new version release to benefit from improvements and fixes.
- Ensure data persistence (on persistent storage or external MySQL) before upgrading. Consider performing a backup.

## Upgrade Methods

### Docker
1. **Delete Existing Container**:
   ```bash
   docker ps  # Find your container ID
   docker stop <CONTAINER_ID>
   docker rm <CONTAINER_ID>
   ```

2. **Deploy New Version**:
   ```bash
   docker run -it -d -p 8080:8080 --name <CONTAINER_NAME> zenmldocker/zenml-server:<VERSION>
   ```
   - Find available versions [here](https://hub.docker.com/r/zenmldocker/zenml-server/tags).

### Kubernetes with Helm
1. **Pull Latest Helm Chart**:
   ```bash
   git clone https://github.com/zenml-io/zenml.git
   git pull
   cd src/zenml/zen_server/deploy/helm/
   ```

2. **Reuse or Extract Values**:
   - Use your existing `custom-values.yaml` or extract values:
   ```bash
   helm -n <namespace> get values zenml-server > custom-values.yaml
   ```

3. **Upgrade Release**:
   ```bash
   helm -n <namespace> upgrade zenml-server . -f custom-values.yaml
   ```
   - Avoid changing the container image tag in the Helm chart unless necessary.

## Important Notes
- **Downgrading**: Not supported and may cause unexpected behavior.
- **Python Client Version**: Should match the server version.

This summary provides essential steps and considerations for upgrading the ZenML server across different deployment methods.

================================================================================

File: docs/book/how-to/manage-zenml-server/using-zenml-server-in-prod.md

# Best Practices for Using ZenML Server in Production

## Overview
This guide provides best practices for deploying ZenML servers in production environments, focusing on autoscaling, performance optimization, database management, ingress setup, monitoring, and backup strategies.

## Autoscaling Replicas
To handle larger, longer-running pipelines, set up autoscaling based on your deployment environment:

### Kubernetes with Helm
Enable autoscaling using the Helm chart:
```yaml
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
```

### ECS (AWS)
1. Go to the ECS console and select your ZenML service.
2. Click "Update Service."
3. Enable autoscaling and set task limits.

### Cloud Run (GCP)
1. Access the Cloud Run console and select your service.
2. Click "Edit & Deploy new Revision."
3. Set minimum and maximum instances.

### Docker Compose
Scale your service using:
```bash
docker compose up --scale zenml-server=N
```

## High Connection Pool Values
Increase server performance by adjusting thread pool size:
```yaml
zenml:
  threadPoolSize: 100
```
Ensure `zenml.database.poolSize` and `zenml.database.maxOverflow` are set appropriately.

## Scaling the Backing Database
Monitor and scale your database based on:
- **CPU Utilization**: Scale if consistently above 50%.
- **Freeable Memory**: Scale if below 100-200 MB.

## Setting Up Ingress/Load Balancer
Securely expose your ZenML server:

### Kubernetes with Helm
Enable ingress:
```yaml
zenml:
  ingress:
    enabled: true
    className: "nginx"
```

### ECS
Use Application Load Balancers for traffic routing.

### Cloud Run
Utilize Cloud Load Balancing for service traffic.

### Docker Compose
Set up an NGINX server as a reverse proxy.

## Monitoring
Use appropriate tools for monitoring based on your deployment:

### Kubernetes with Helm
Set up Prometheus and Grafana. Example query for CPU utilization:
```
sum by(namespace) (rate(container_cpu_usage_seconds_total{namespace=~"zenml.*"}[5m]))
```

### ECS
Utilize CloudWatch for metrics like CPU and memory utilization.

### Cloud Run
Use Cloud Monitoring for metrics on CPU and memory usage.

## Backups
Implement a backup strategy to protect critical data:
- Automated backups with a retention period (e.g., 30 days).
- Periodic exports to external storage (e.g., S3, GCS).
- Manual backups before server upgrades.

================================================================================

File: docs/book/how-to/manage-zenml-server/troubleshoot-your-deployed-server.md

# Troubleshooting Tips for ZenML Deployment

## Viewing Logs
To debug issues in your ZenML deployment, analyzing logs is essential. The method to view logs differs based on whether you are using Kubernetes or Docker.

### Kubernetes
1. **Check running pods:**
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> get pods
   ```
2. **Get logs for all pods:**
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml
   ```
3. **Get logs for a specific container:**
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml -c <CONTAINER_NAME>
   ```
   - Use `zenml-db-init` for Init state errors, otherwise use `zenml`.
   - Use `--tail` to limit lines or `--follow` for real-time logs.

### Docker
1. **If deployed using `zenml login --local --docker`:**
   ```shell
   zenml logs -f
   ```
2. **If deployed using `docker run`:**
   ```shell
   docker logs zenml -f
   ```
3. **If deployed using `docker compose`:**
   ```shell
   docker compose -p zenml logs -f
   ```

## Fixing Database Connection Problems
Common MySQL connection issues can be diagnosed through the `zenml-db-init` logs:

- **Access Denied Error:**
  - Check username and password.
- **Can't Connect to MySQL Server:**
  - Verify the host settings.
  
Test connection with:
```bash
mysql -h <HOST> -u <USER> -p
```
For Kubernetes, use `kubectl port-forward` to connect to the database locally.

## Fixing Database Initialization Problems
If you encounter `Revision not found` errors after migrating ZenML versions, you may need to recreate the database:

1. **Log in to MySQL:**
   ```bash
   mysql -h <HOST> -u <NAME> -p
   ```
2. **Drop the existing database:**
   ```sql
   drop database <NAME>;
   ```
3. **Create a new database:**
   ```sql
   create database <NAME>;
   ```
4. **Restart your Kubernetes pods or Docker container** to reinitialize the database.

================================================================================

File: docs/book/how-to/manage-zenml-server/best-practices-upgrading-zenml.md

### Best Practices for Upgrading ZenML

#### Upgrading Your Server
To ensure a successful upgrade of your ZenML server, follow these best practices:

1. **Data Backups**:
   - **Database Backup**: Create a backup of your MySQL database before upgrading to allow rollback if necessary.
   - **Automated Backups**: Set up daily automated backups using managed services like AWS RDS, Google Cloud SQL, or Azure Database for MySQL.

2. **Upgrade Strategies**:
   - **Staged Upgrade**: Use two ZenML server instances (old and new) for gradual migration of services.
   - **Team Coordination**: Coordinate upgrade timing among teams to minimize disruption.
   - **Separate ZenML Servers**: Consider dedicated instances for teams needing different upgrade schedules. ZenML Pro supports multi-tenancy for this purpose.

3. **Minimizing Downtime**:
   - **Upgrade Timing**: Schedule upgrades during low-activity periods.
   - **Avoid Mid-Pipeline Upgrades**: Be cautious of upgrades that might interrupt long-running pipelines.

#### Upgrading Your Code
When upgrading your code for compatibility with a new ZenML version, consider the following:

1. **Testing and Compatibility**:
   - **Local Testing**: Test locally after upgrading (`pip install zenml --upgrade`) and run old pipelines for compatibility checks.
   - **End-to-End Testing**: Develop simple tests to ensure compatibility with your pipeline code. Refer to ZenML's [test suite](https://github.com/zenml-io/zenml/tree/main/tests) for examples.
   - **Artifact Compatibility**: Be cautious with pickle-based materializers. Load older artifacts to check compatibility:

   ```python
   from zenml.client import Client

   artifact = Client().get_artifact_version('YOUR_ARTIFACT_ID')
   loaded_artifact = artifact.load()
   ```

2. **Dependency Management**:
   - **Python Version**: Ensure your Python version is compatible with the new ZenML version. Refer to the [installation guide](../../getting-started/installation.md).
   - **External Dependencies**: Check for compatibility of external dependencies with the new ZenML version, as older versions may no longer be supported. Review the [release notes](https://github.com/zenml-io/zenml/releases).

3. **Handling API Changes**:
   - **Changelog Review**: Always review the [changelog](https://github.com/zenml-io/zenml/releases) for breaking changes or new syntax.
   - **Migration Scripts**: Use available [migration scripts](migration-guide/migration-guide.md) for database schema changes.

By adhering to these best practices, you can minimize risks and ensure a smoother upgrade process for your ZenML server and code. Adapt these guidelines to fit your specific environment and infrastructure needs.

================================================================================

File: docs/book/how-to/manage-zenml-server/connecting-to-zenml/connect-in-with-your-user-interactive.md

# ZenML User Authentication Overview

## Authentication Process
Authenticate clients with the ZenML Server using the ZenML CLI:

```bash
zenml login https://...
```

This command initiates a browser-based validation process. You can choose to trust your device:

- **Trust this device**: Issues a 30-day token.
- **Do not trust**: Issues a 24-hour token.

## Device Management Commands
- List authorized devices:

```bash
zenml authorized-device list
```

- Inspect a specific device:

```bash
zenml authorized-device describe <DEVICE_ID>
```

- Invalidate a token for a device:

```bash
zenml authorized-device lock <DEVICE_ID>
```

## Summary of Steps
1. Use `zenml login <URL>` to connect to the ZenML server.
2. Decide whether to trust the device.
3. Check authorized devices with `zenml authorized-device list`.
4. Lock a device with `zenml authorized-device lock <DEVICE_ID>`.

## Security Notice
Using the ZenML CLI ensures secure interactions with your ZenML tenants. Regularly manage device trust levels and revoke access as needed, as each token can provide access to sensitive data and infrastructure.

================================================================================

File: docs/book/how-to/manage-zenml-server/connecting-to-zenml/README.md

### Connecting to ZenML

After deploying ZenML, there are multiple methods to connect to the server. For detailed deployment instructions, refer to the [production guide](../../../user-guide/production-guide/deploying-zenml.md). 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

File: docs/book/how-to/manage-zenml-server/connecting-to-zenml/connect-with-a-service-account.md

# Connecting with a Service Account in ZenML

To authenticate to a ZenML server from non-interactive environments (e.g., CI/CD workloads), you can create a service account and use an API key for authentication.

### Creating a Service Account
Use the following command to create a service account and generate an API key:
```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```
The API key will be displayed in the output and cannot be retrieved later.

### Authenticating with the API Key
You can authenticate using the API key in two ways:

1. **CLI Method**:
   ```bash
   zenml login https://... --api-key
   ```

2. **Environment Variables** (suitable for automated environments):
   ```bash
   export ZENML_STORE_URL=https://...
   export ZENML_STORE_API_KEY=<API_KEY>
   ```
   After setting these variables, you can interact with the server without needing to run `zenml login`.

### Managing Service Accounts and API Keys
- List service accounts:
  ```bash
  zenml service-account list
  ```
- List API keys for a service account:
  ```bash
  zenml service-account api-key <SERVICE_ACCOUNT_NAME> list
  ```
- Describe a service account or API key:
  ```bash
  zenml service-account describe <SERVICE_ACCOUNT_NAME>
  zenml service-account api-key <SERVICE_ACCOUNT_NAME> describe <API_KEY_NAME>
  ```

### Rotating API Keys
API keys do not expire, but it's recommended to rotate them regularly:
```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME>
```
To retain the old key for a specified period (e.g., 60 minutes):
```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME> --retain 60
```

### Deactivating Service Accounts or API Keys
To deactivate a service account or API key:
```bash
zenml service-account update <SERVICE_ACCOUNT_NAME> --active false
zenml service-account api-key <SERVICE_ACCOUNT_NAME> update <API_KEY_NAME> --active false
```
This action prevents further authentication using the deactivated account or key.

### Summary of Steps
1. Create a service account and API key: `zenml service-account create`.
2. Authenticate using the API key via CLI or environment variables.
3. List service accounts and API keys.
4. Rotate API keys regularly.
5. Deactivate unused service accounts or API keys.

### Important Notice
API keys are critical for accessing data and infrastructure. Regularly rotate and deactivate keys that are no longer needed to maintain security.

================================================================================

File: docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-sixty.md

### Migration Guide: ZenML 0.58.2 to 0.60.0 (Pydantic 2 Edition)

**Overview:**
ZenML has upgraded to Pydantic v2, introducing critical updates and stricter validation. Users may encounter new validation errors due to these changes. For issues, contact us on [GitHub](https://github.com/zenml-io/zenml) or [Slack](https://zenml.io/slack-invite).

**Key Dependency Changes:**
- **SQLModel:** Upgraded from `0.0.8` to `0.0.18` for compatibility with Pydantic v2.
- **SQLAlchemy:** Upgraded from v1 to v2. Users of SQLAlchemy should refer to [their migration guide](https://docs.sqlalchemy.org/en/20/changelog/migration_20.html).

**Pydantic v2 Features:**
- Enhanced performance using Rust.
- New features in model design, configuration, validation, and serialization. For more details, see the [Pydantic migration guide](https://docs.pydantic.dev/2.7/migration/).

**Integration Changes:**
- **Airflow:** Dependencies removed due to incompatibility with SQLAlchemy v1. Use ZenML for pipeline creation and a separate environment for Airflow.
- **AWS:** Upgraded `sagemaker` to version `2.172.0` to support `protobuf` 4.
- **Evidently:** Updated to version `0.4.16` for Pydantic v2 compatibility.
- **Feast:** Removed extra `redis` dependency for compatibility.
- **GCP & Kubeflow:** Upgraded `kfp` dependency to v2, removing Pydantic dependency.
- **Great Expectations:** Updated dependency to `great-expectations>=0.17.15,<1.0` for Pydantic v2 support.
- **MLflow:** Compatible with both Pydantic versions, but may downgrade to v1 due to installation order. Watch for deprecation warnings.
- **Label Studio:** Updated to support Pydantic v2 with the new `label-studio-sdk` 1.0 version.
- **Skypilot:** `skypilot[azure]` integration deactivated due to incompatibility with `azurecli`.
- **TensorFlow:** Requires `tensorflow>=2.12.0` to resolve dependency issues with `protobuf` 4.
- **Tekton:** Updated to use `kfp` v2, ensuring compatibility.

**Warning:**
Upgrading to ZenML 0.60.0 may lead to dependency issues, especially with integrations not supporting Pydantic v2. It is recommended to set up a fresh Python environment for the upgrade.

================================================================================

File: docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-thirty.md

### Migration Guide: ZenML 0.20.0-0.23.0 to 0.30.0-0.39.1

**Important Note:** Migrating to ZenML `0.30.0` involves non-reversible database changes. Downgrading to versions `<=0.23.0` is not possible post-migration. If using an older version, first follow the [0.20.0 Migration Guide](migration-zero-twenty.md) to avoid database migration issues.

**Key Changes:**
- The `ml-pipelines-sdk` dependency has been removed.
- Pipeline runs and artifacts are now stored natively in the ZenML database.

**Migration Steps:**
1. Install ZenML `0.30.0`:
   ```bash
   pip install zenml==0.30.0
   zenml version  # Confirm version is 0.30.0
   ```

**Database Migration:** This will occur automatically upon executing any `zenml` CLI command after installation.

================================================================================

File: docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-twenty.md

### Migration Guide: ZenML 0.13.2 to 0.20.0

**Last Updated:** 2023-07-24

ZenML 0.20.0 introduces significant architectural changes that are not backward compatible. This guide outlines the necessary steps to migrate your ZenML stacks and pipelines with minimal disruption.

#### Key Changes:
- **Metadata Store:** ZenML now manages its own Metadata Store. If using remote Metadata Stores, replace them with a ZenML server deployment.
- **ZenML Dashboard:** A new dashboard is included for managing deployments.
- **Removal of Profiles:** ZenML Profiles are replaced by Projects. Existing Profiles must be manually migrated.
- **Decoupled Stack Component Configuration:** Stack component configuration is now separate from implementation. Custom implementations may need updates.
- **Improved Collaboration:** Users can share Stacks and Components when connected to a ZenML server.

#### Migration Steps:
1. **Backup Existing Metadata:** Before upgrading, back up all metadata stores.
2. **Upgrade ZenML:** Use `pip install zenml==0.20.0`.
3. **Connect to ZenML Server:** If using a server, connect with `zenml connect`.
4. **Migrate Pipeline Runs:**
   - For SQLite: 
     ```bash
     zenml pipeline runs migrate PATH/TO/LOCAL/STORE/metadata.db
     ```
   - For other stores (MySQL):
     ```bash
     zenml pipeline runs migrate DATABASE_NAME --database_type=mysql --mysql_host=URL/TO/MYSQL --mysql_username=MYSQL_USERNAME --mysql_password=MYSQL_PASSWORD
     ```

#### New CLI Commands:
- **Deploy Server:** `zenml deploy --aws`
- **Start Local Server:** `zenml up`
- **Check Server Status:** `zenml status`

#### Dashboard Access:
Launch the ZenML Dashboard locally with:
```bash
zenml up
```
Access it at `http://127.0.0.1:8237`.

#### Profile Migration:
1. Update to ZenML 0.20.0 to invalidate existing Profiles.
2. Use:
   ```bash
   zenml profile list
   zenml profile migrate /path/to/profile
   ```
   to migrate stacks and components.

#### Configuration Changes:
- **Rename Classes:**
  - `Repository` → `Client`
  - `BaseStepConfig` → `BaseParameters`
- **Configuration Rework:** Use `BaseSettings` for pipeline configurations. Remove deprecated decorators like `@enable_xxx`.

#### Example Migration:
For a step with a tracker:
```python
@step(
    experiment_tracker="mlflow_stack_comp_name",
    settings={
        "experiment_tracker.mlflow": {
            "experiment_name": "name",
            "nested": False
        }
    }
)
```

#### Future Changes:
- Potential removal of the secrets manager from the stack.
- Deprecation of `StepContext`.

#### Reporting Issues:
For bugs or feature requests, engage with the ZenML community on [Slack](https://zenml.io/slack) or submit a [GitHub Issue](https://github.com/zenml-io/zenml/issues/new/choose). 

This guide provides essential details for migrating to ZenML 0.20.0, ensuring users can transition effectively while adapting to new features and configurations.

================================================================================

File: docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-forty.md

# Migration Guide: ZenML 0.39.1 to 0.41.0

ZenML versions 0.40.0 to 0.41.0 introduced a new syntax for defining steps and pipelines. The old syntax is deprecated and will be removed in future releases.

## Overview

### Old Syntax Example
```python
from zenml.steps import BaseParameters, Output, StepContext, step
from zenml.pipelines import pipeline

class MyStepParameters(BaseParameters):
    param_1: int
    param_2: Optional[float] = None

@step
def my_step(params: MyStepParameters, context: StepContext) -> Output(int_output=int, str_output=str):
    result = int(params.param_1 * (params.param_2 or 1))
    result_uri = context.get_output_artifact_uri()
    return result, result_uri

@pipeline
def my_pipeline(my_step):
    my_step()

step_instance = my_step(params=MyStepParameters(param_1=17))
pipeline_instance = my_pipeline(my_step=step_instance)
pipeline_instance.run(schedule=schedule)
```

### New Syntax Example
```python
from typing import Annotated, Optional, Tuple
from zenml import get_step_context, pipeline, step

@step
def my_step(param_1: int, param_2: Optional[float] = None) -> Tuple[Annotated[int, "int_output"], Annotated[str, "str_output"]]:
    result = int(param_1 * (param_2 or 1))
    result_uri = get_step_context().get_output_artifact_uri()
    return result, result_uri

@pipeline
def my_pipeline():
    my_step(param_1=17)

my_pipeline = my_pipeline.with_options(enable_cache=False)
my_pipeline()
```

## Key Changes

### Defining Steps
- **Old:** Use `BaseParameters` to define parameters.
- **New:** Parameters are defined directly in the step function. Optionally, use `pydantic.BaseModel` for grouping.

### Calling Steps
- **Old:** Use `my_step.entrypoint()`.
- **New:** Call the step directly with `my_step()`.

### Defining Pipelines
- **Old:** Steps are arguments in the pipeline function.
- **New:** Steps are called directly within the pipeline function.

### Configuring Pipelines
- **Old:** Use `pipeline_instance.configure(...)`.
- **New:** Use `with_options(...)` method.

### Running Pipelines
- **Old:** Create an instance and call `pipeline_instance.run(...)`.
- **New:** Call the pipeline directly.

### Scheduling Pipelines
- **Old:** Schedule via `pipeline_instance.run(schedule=...)`.
- **New:** Set schedule using `with_options(...)`.

### Fetching Pipeline Runs
- **Old:** Access runs with `pipeline.get_runs()`.
- **New:** Use `pipeline.last_run` or `pipeline.runs[0]`.

### Controlling Step Execution Order
- **Old:** Use `step.after(...)`.
- **New:** Pass `after` argument when calling a step.

### Defining Steps with Multiple Outputs
- **Old:** Use `Output` class.
- **New:** Use `Tuple` with optional custom output names.

### Accessing Run Information Inside Steps
- **Old:** Pass `StepContext` as an argument.
- **New:** Use `get_step_context()` to access run information.

For more detailed information, refer to the ZenML documentation on [parameterizing steps](../../pipeline-development/build-pipelines/use-pipeline-step-parameters.md) and [scheduling pipelines](../../pipeline-development/build-pipelines/schedule-a-pipeline.md).

================================================================================

File: docs/book/how-to/manage-zenml-server/migration-guide/migration-guide.md

### ZenML Migration Guide Summary

Migration is required for ZenML releases with breaking changes, specifically for minor version increments (e.g., `0.X` to `0.Y`) and major version increments (e.g., `0.1.X` to `0.2.X`).

#### Release Type Examples:
- **No Breaking Changes**: `0.40.2` to `0.40.3` (no migration needed)
- **Minor Breaking Changes**: `0.40.3` to `0.41.0` (migration required)
- **Major Breaking Changes**: `0.39.1` to `0.40.0` (significant code changes)

#### Major Migration Guides:
Follow these guides sequentially if multiple migrations are needed:
- [0.13.2 → 0.20.0](migration-zero-twenty.md)
- [0.23.0 → 0.30.0](migration-zero-thirty.md)
- [0.39.1 → 0.41.0](migration-zero-forty.md)
- [0.58.2 → 0.60.0](migration-zero-sixty.md)

#### Release Notes:
For minor breaking changes, refer to the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases) for details on changes introduced.

================================================================================

