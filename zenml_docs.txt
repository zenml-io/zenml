# Debugging ZenML Issues

This guide provides steps to debug common issues with ZenML and seek help effectively.

### When to Get Help
Before asking for help, check the following resources:
- Search Slack using the built-in search.
- Look for issues on [GitHub](https://github.com/zenml-io/zenml/issues).
- Search the [documentation](https://docs.zenml.io).
- Review the [common errors](debug-and-solve-issues.md#most-common-errors) section.
- Analyze [additional logs](debug-and-solve-issues.md#41-additional-logs) and [client/server logs](debug-and-solve-issues.md#client-and-server-logs).

If you still need assistance, post your question on [Slack](https://zenml.io/slack).

### How to Post on Slack
Provide the following information for effective troubleshooting:

1. **System Information**: Run and share the output of:
   ```shell
   zenml info -a -s
   ```
   For specific package issues, use:
   ```shell
   zenml info -p <package_name>
   ```

2. **What Happened**: Briefly describe:
   - Your goal.
   - Expected outcome.
   - Actual outcome.

3. **Reproduce the Error**: Detail the steps to reproduce the error.

4. **Relevant Log Output**: Attach relevant logs and the full error traceback. Include outputs from:
   ```shell
   zenml status
   zenml stack describe
   ```

### Additional Logs
If default logs are insufficient, increase verbosity by setting:
```shell
export ZENML_LOGGING_VERBOSITY=DEBUG
```
Refer to documentation for setting environment variables on [Linux](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/), [macOS](https://youngstone89.medium.com/setting-up-environment-variables-in-mac-os-28e5941c771c), and [Windows](https://www.computerhope.com/issues/ch000549.htm).

### Client and Server Logs
For server-related issues, view logs with:
```shell
zenml logs
```

### Common Errors
1. **Error initializing rest store**:
   ```bash
   RuntimeError: Error initializing rest store with URL 'http://127.0.0.1:8237': Connection refused
   ```
   Solution: Run `zenml login --local` after each machine restart.

2. **Column 'step_configuration' cannot be null**:
   ```bash
   sqlalchemy.exc.IntegrityError: (1048, "Column 'step_configuration' cannot be null")
   ```
   Solution: Ensure step configuration length is within limits.

3. **'NoneType' object has no attribute 'name'**:
   ```shell
   AttributeError: 'NoneType' object has no attribute 'name'
   ```
   Solution: Register an experiment tracker:
   ```shell
   zenml experiment-tracker register mlflow_tracker --flavor=mlflow
   zenml stack update -e mlflow_tracker
   ```

This guide aims to streamline the debugging process and enhance communication when seeking help.

================================================================================

# Pipeline Development in ZenML

This section details the key components of pipeline development in ZenML.

## Key Components:
- **Pipeline Definition**: Define a pipeline using decorators and functions.
- **Steps**: Each step in the pipeline is a function that processes data.
- **Artifacts**: Outputs from steps that can be used as inputs for subsequent steps.
- **Execution**: Pipelines can be executed locally or in the cloud.

## Example Code:
```python
from zenml.pipelines import pipeline

@pipeline
def my_pipeline():
    step1 = step_function1()
    step2 = step_function2(step1)
```

## Important Notes:
- Ensure steps are stateless for better scalability.
- Use ZenML's built-in integrations for data sources and storage.
- Monitor pipeline execution for performance optimization.

This concise overview captures the essential elements of pipeline development in ZenML.

================================================================================

# Limitations of Defining Steps in Notebook Cells

To run ZenML steps defined in notebook cells remotely with a remote orchestrator or step operator, the following conditions must be met:

- The cell can only contain Python code; Jupyter magic commands or shell commands (starting with `%` or `!`) are not allowed.
- The cell **must not** call code from other notebook cells. However, functions or classes imported from Python files are permitted.
- The cell **must not** rely on imports from previous cells; it must perform all necessary imports, including ZenML imports like `from zenml import step`.

================================================================================

# Run Remote Pipelines from Notebooks

ZenML allows you to define and execute steps and pipelines in Jupyter Notebooks remotely. The code from notebook cells is extracted and run as Python modules in Docker containers. To ensure proper execution, notebook cells must adhere to specific conditions.

## Key Sections:
- **Limitations of Defining Steps in Notebook Cells**: [Read more](limitations-of-defining-steps-in-notebook-cells.md)
- **Run a Single Step from a Notebook**: [Read more](run-a-single-step-from-a-notebook.md)

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

# Running a Single Step from a Notebook

To execute a single step remotely from a notebook, call the step like a regular Python function. ZenML will create a pipeline with that step and run it on the active stack. Be aware of the [limitations](limitations-of-defining-steps-in-notebook-cells.md) when defining remote steps.

```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from typing import Tuple, Annotated

@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(X_train: pd.DataFrame, y_train: pd.Series, gamma: float = 0.001) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
    """Train a sklearn SVC classifier."""
    model = SVC(gamma=gamma)
    model.fit(X_train, y_train)
    train_acc = model.score(X_train, y_train)
    print(f"Train accuracy: {train_acc}")
    return model, train_acc

X_train = pd.DataFrame(...)
y_train = pd.Series(...)

# Execute the step
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

================================================================================

# Configuration Overview

## Sample YAML Configuration
A sample YAML configuration file is provided below, highlighting key configurations. For a complete list of keys, refer to [this page](./autogenerate-a-template-yaml-file.md).

```yaml
build: dcd6fafb-c200-4e85-8328-428bef98d804

enable_artifact_metadata: True
enable_artifact_visualization: False
enable_cache: False
enable_step_logs: True

extra: 
  any_param: 1
  another_random_key: "some_string"

model:
  name: "classification_model"
  version: production
  audience: "Data scientists"
  description: "This classifies hotdogs and not hotdogs"
  ethics: "No ethical implications"
  license: "Apache 2.0"
  limitations: "Only works for hotdogs"
  tags: ["sklearn", "hotdog", "classification"]

parameters: 
  dataset_name: "another_dataset"

run_name: "my_great_run"

schedule:
  catchup: true
  cron_expression: "* * * * *"

settings:
  docker:
    apt_packages: ["curl"]
    copy_files: True
    dockerfile: "Dockerfile"
    dockerignore: ".dockerignore"
    environment:
      ZENML_LOGGING_VERBOSITY: DEBUG
    parent_image: "zenml-io/zenml-cuda"
    requirements: ["torch"]
    skip_build: False
  
  resources:
    cpu_count: 2
    gpu_count: 1
    memory: "4Gb"

steps:
  train_model:
    parameters:
      data_source: "best_dataset"
    experiment_tracker: "mlflow_production"
    step_operator: "vertex_gpu"
    outputs: {}
    failure_hook_source: {}
    success_hook_source: {}
    enable_artifact_metadata: True
    enable_artifact_visualization: True
    enable_cache: False
    enable_step_logs: True
    extra: {}
    model: {}
    settings:
      docker: {}
      resources: {}
      step_operator.sagemaker:
        estimator_args:
          instance_type: m7g.medium
```

## Key Configuration Parameters

### `enable_XXX` Flags
These boolean flags control various configurations:
- `enable_artifact_metadata`: Attach metadata to artifacts.
- `enable_artifact_visualization`: Attach visualizations of artifacts.
- `enable_cache`: Enable caching.
- `enable_step_logs`: Enable step logs.

```yaml
enable_artifact_metadata: True
enable_artifact_visualization: True
enable_cache: True
enable_step_logs: True
```

### `build` ID
Specifies the UUID of the Docker image to use. If provided, Docker image building is skipped.

```yaml
build: <INSERT-BUILD-ID-HERE>
```

### Model Configuration
Defines the ZenML model for the pipeline.

```yaml
model:
  name: "ModelName"
  version: "production"
  description: An example model
  tags: ["classifier"]
```

### Pipeline and Step Parameters
Parameters can be defined at both the pipeline and step levels.

```yaml
parameters:
    gamma: 0.01

steps:
    trainer:
        parameters:
            gamma: 0.001
```

### Setting the `run_name`
Specify a unique `run_name` for each execution.

```yaml
run_name: <INSERT_RUN_NAME_HERE>  
```

### Stack Component Runtime Settings
Settings for Docker and resource configurations.

#### Docker Settings
Example configuration for Docker settings:

```yaml
settings:
  docker:
    requirements:
      - pandas
```

#### Resource Settings
Defines resource settings for the pipeline.

```yaml
resources:
  cpu_count: 2
  gpu_count: 1
  memory: "4Gb"
```

### Step-specific Configuration
Certain configurations can only be applied at the step level, such as:
- `experiment_tracker`: Name of the experiment tracker for the step.
- `step_operator`: Name of the step operator for the step.
- `outputs`: Configuration for output artifacts.

For more details on configurations, refer to the specific orchestrator documentation.

================================================================================

ZenML allows easy configuration and execution of pipelines using YAML files. These files enable runtime configuration of parameters, caching behavior, and stack components. Key topics include:

- **What can be configured**: [Configuration options](what-can-be-configured.md)
- **Configuration hierarchy**: [Hierarchy details](configuration-hierarchy.md)
- **Autogenerate a template YAML file**: [Template generation](autogenerate-a-template-yaml-file.md)

For more information, refer to the linked sections.

================================================================================

### Autogenerate a Template YAML File

To create a YAML configuration template for your pipeline, use the `.write_run_configuration_template()` method. This generates a YAML file with all options commented out, allowing you to select relevant settings.

#### Code Example
```python
from zenml import pipeline

@pipeline(enable_cache=True)
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter=parameter)
    train_model(dataset)

simple_ml_pipeline.write_run_configuration_template(path="<Insert_path_here>")
```

#### Example of a Generated YAML Configuration Template
```yaml
build: Union[PipelineBuildBase, UUID, NoneType]
enable_artifact_metadata: Optional[bool]
enable_artifact_visualization: Optional[bool]
enable_cache: Optional[bool]
enable_step_logs: Optional[bool]
extra: Mapping[str, Any]
model:
  name: str
  save_models_to_registry: bool
  tags: Optional[List[str]]
parameters: Optional[Mapping[str, Any]]
steps:
  load_data:
    name: Optional[str]
    parameters: {}
    settings:
      resources:
        cpu_count: Optional[PositiveFloat]
        gpu_count: Optional[NonNegativeInt]
        memory: Optional[ConstrainedStrValue]
  train_model:
    name: Optional[str]
    parameters: {}
    settings:
      resources:
        cpu_count: Optional[PositiveFloat]
        gpu_count: Optional[NonNegativeInt]
        memory: Optional[ConstrainedStrValue]
```

**Note:** To configure your pipeline with a specific stack, use `write_run_configuration_template(stack=<Insert_stack_here>)`.

================================================================================

### Summary: Configuring Runtime Settings in ZenML

**Overview**  
Settings in ZenML configure runtime configurations for stack components and pipelines, including resource requirements, containerization processes, and component-specific configurations. All configurations are managed through `BaseSettings`.

**Types of Settings**  
1. **General Settings**: Applicable to all pipelines, e.g.:
   - `DockerSettings`: Docker configurations.
   - `ResourceSettings`: Resource specifications.

2. **Stack-Component-Specific Settings**: Runtime configurations for specific components, identified by keys like `<COMPONENT_CATEGORY>` or `<COMPONENT_CATEGORY>.<COMPONENT_FLAVOR>`. Examples include:
   - `SkypilotAWSOrchestratorSettings`
   - `KubeflowOrchestratorSettings`
   - `MLflowExperimentTrackerSettings`
   - `WandbExperimentTrackerSettings`
   - `WhylogsDataValidatorSettings`
   - `SagemakerStepOperatorSettings`
   - `VertexStepOperatorSettings`
   - `AzureMLStepOperatorSettings`

**Registration-Time vs Real-Time Settings**  
Settings registered at component registration are static, while runtime settings can change per pipeline execution. For instance, the `tracking_url` is fixed, but `experiment_name` can vary.

**Default Values**  
Default values can be set during component registration, which apply unless overridden at runtime.

**Key Specification for Settings**  
Use keys in the format `<COMPONENT_CATEGORY>` or `<COMPONENT_CATEGORY>.<COMPONENT_FLAVOR>`. If only the category is specified, ZenML applies settings to the corresponding component flavor in the stack.

**Code Examples**  
Using settings in Python:
```python
@step(step_operator="nameofstepoperator", settings={"step_operator": {"estimator_args": {"instance_type": "m7g.medium"}}})
def my_step():
    ...

@step(step_operator="nameofstepoperator", settings={"step_operator": SagemakerStepOperatorSettings(instance_type="m7g.medium")})
def my_step():
    ...
```

Using settings in YAML:
```yaml
steps:
  my_step:
    step_operator: "nameofstepoperator"
    settings:
      step_operator:
        estimator_args:
          instance_type: m7g.medium
```

This summary captures the essential technical details regarding the configuration of runtime settings in ZenML, ensuring clarity and conciseness.

================================================================================

# Extracting Configuration from a Pipeline Run

To retrieve the configuration used in a completed pipeline run, load the pipeline run and access its `config` attribute or that of a specific step.

```python
from zenml.client import Client

pipeline_run = Client().get_pipeline_run(<PIPELINE_RUN_NAME>)
pipeline_run.config  # General configuration
pipeline_run.steps[<STEP_NAME>].config  # Step-specific configuration
```

================================================================================

### Configuration Files in ZenML

**Best Practice:** Use a YAML configuration file to separate configuration from code. 

**Applying Configuration:**
Use the `with_options(config_path=<PATH_TO_CONFIG>)` pattern to apply configuration to a pipeline.

**Example YAML Configuration:**
```yaml
enable_cache: False
parameters:
  dataset_name: "best_dataset"  
steps:
  load_data:
    enable_cache: False
```

**Example Python Code:**
```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def simple_ml_pipeline(dataset_name: str):
    load_data(dataset_name)

if __name__ == "__main__":
    simple_ml_pipeline.with_options(config_path=<INSERT_PATH_TO_CONFIG_YAML>)()
```

**Functionality:** This setup runs `simple_ml_pipeline` with caching disabled for `load_data` and `dataset_name` set to `best_dataset`.

================================================================================

### Configuration Hierarchy

In ZenML, configuration settings follow these rules:

- Code configurations override YAML file configurations.
- Step-level configurations override pipeline-level configurations.
- Attribute dictionaries are merged.

### Example Code

```python
from zenml import pipeline, step
from zenml.config import ResourceSettings

@step
def load_data(parameter: int) -> dict:
    ...

@step(settings={"resources": ResourceSettings(gpu_count=1, memory="2GB")})
def train_model(data: dict) -> None:
    ...

@pipeline(settings={"resources": ResourceSettings(cpu_count=2, memory="1GB")}) 
def simple_ml_pipeline(parameter: int):
    ...

# Merged configurations
train_model.configuration.settings["resources"]
# -> cpu_count: 2, gpu_count=1, memory="2GB"

simple_ml_pipeline.configuration.settings["resources"]
# -> cpu_count: 2, memory="1GB"
```

================================================================================

### Creating Pipeline Variants for Local Development and Production

When developing ZenML pipelines, it's useful to have different variants for local development and production. This allows for quick iteration during development while maintaining a robust setup for production. Variants can be created using:

1. **Configuration Files**
2. **Code Implementation**
3. **Environment Variables**

#### 1. Using Configuration Files

ZenML allows pipeline configurations via YAML files. Example configuration for development:

```yaml
enable_cache: False
parameters:
    dataset_name: "small_dataset"
steps:
    load_data:
        enable_cache: False
```

To apply this configuration:

```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def ml_pipeline(dataset_name: str):
    load_data(dataset_name)

if __name__ == "__main__":
    ml_pipeline.with_options(config_path="path/to/config.yaml")()
```

Create separate files for development (`config_dev.yaml`) and production (`config_prod.yaml`).

#### 2. Implementing Variants in Code

You can create variants directly in your code:

```python
import os
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "small_dataset" if is_dev else "full_dataset"
    load_data(dataset)

if __name__ == "__main__":
    is_dev = os.environ.get("ZENML_ENVIRONMENT") == "dev"
    ml_pipeline(is_dev=is_dev)
```

This method uses a boolean flag to switch between variants.

#### 3. Using Environment Variables

Environment variables can determine which variant to run:

```python
import os

config_path = "config_dev.yaml" if os.environ.get("ZENML_ENVIRONMENT") == "dev" else "config_prod.yaml"
ml_pipeline.with_options(config_path=config_path)()
```

Run your pipeline with:
```bash
ZENML_ENVIRONMENT=dev python run.py
```
or
```bash
ZENML_ENVIRONMENT=prod python run.py
```

### Development Variant Considerations

For faster iteration and debugging in development:

- Use smaller datasets
- Specify a local execution stack
- Reduce training epochs
- Decrease batch size
- Use a smaller base model

Example configuration:

```yaml
parameters:
    dataset_path: "data/small_dataset.csv"
epochs: 1
batch_size: 16
stack: local_stack
```

Or in code:

```python
@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "data/small_dataset.csv" if is_dev else "data/full_dataset.csv"
    epochs = 1 if is_dev else 100
    batch_size = 16 if is_dev else 64
    
    load_data(dataset)
    train_model(epochs=epochs, batch_size=batch_size)
```

By creating different pipeline variants, you can efficiently test and debug locally while maintaining a full-scale configuration for production. This approach enhances your development workflow without compromising production integrity.

================================================================================

# Develop Locally

This section outlines best practices for developing pipelines locally, allowing for faster iteration and reduced costs. It is common to work with a smaller subset of data or synthetic data. ZenML supports local development, with guidance on transitioning to remote hardware for execution.

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

# Keeping Your Pipeline Runs Clean

## Clean Development Practices
To avoid cluttering the server during pipeline development, ZenML offers several options:

### Run Locally
To run a local server, disconnect from the remote server:
```bash
zenml login --local
```
Reconnect with:
```bash
zenml login <remote-url>
```

### Unlisted Runs
Create pipeline runs without associating them explicitly:
```python
pipeline_instance.run(unlisted=True)
```
Unlisted runs won’t appear on the pipeline's dashboard, keeping the history focused.

### Deleting Pipeline Runs
To delete a specific run:
```bash
zenml pipeline runs delete <PIPELINE_RUN_NAME_OR_ID>
```
To delete all runs from the last 24 hours:
```python
#!/usr/bin/env python3
import datetime
from zenml.client import Client

def delete_recent_pipeline_runs():
    zc = Client()
    time_filter = (datetime.datetime.utcnow() - datetime.timedelta(hours=24)).strftime("%Y-%m-%d %H:%M:%S")
    recent_runs = zc.list_pipeline_runs(created=f"gt:{time_filter}")
    for run in recent_runs:
        zc.delete_pipeline_run(run.id)
    print(f"Deleted {len(recent_runs)} pipeline runs.")

if __name__ == "__main__":
    delete_recent_pipeline_runs()
```

### Deleting Pipelines
To delete an entire pipeline:
```bash
zenml pipeline delete <PIPELINE_ID_OR_NAME>
```

### Unique Pipeline Names
Assign unique names to each run:
```python
training_pipeline = training_pipeline.with_options(run_name="custom_pipeline_run_name")
training_pipeline()
```

### Models
To delete a model:
```bash
zenml model delete <MODEL_NAME>
```

### Pruning Artifacts
To delete unreferenced artifacts:
```bash
zenml artifact prune
```
Use `--only-artifact` or `--only-metadata` flags for specific deletions.

### Cleaning Your Environment
For a complete reset of your local environment:
```bash
zenml clean
```
Use the `--local` flag to delete local files related to the active stack. 

By utilizing these methods, you can maintain a clean and organized pipeline dashboard, focusing on essential runs for your project.

================================================================================

### Schedule a Pipeline

**Supported Orchestrators:**
| Orchestrator | Scheduling Support |
|--------------|--------------------|
| [Airflow](../../../component-guide/orchestrators/airflow.md) | ✅ |
| [AzureML](../../../component-guide/orchestrators/azureml.md) | ✅ |
| [Databricks](../../../component-guide/orchestrators/databricks.md) | ✅ |
| [HyperAI](../../component-guide/orchestrators/hyperai.md) | ✅ |
| [Kubeflow](../../../component-guide/orchestrators/kubeflow.md) | ✅ |
| [Kubernetes](../../../component-guide/orchestrators/kubernetes.md) | ✅ |
| [Local](../../../component-guide/orchestrators/local.md) | ⛔️ |
| [LocalDocker](../../../component-guide/orchestrators/local-docker.md) | ⛔️ |
| [Sagemaker](../../../component-guide/orchestrators/sagemaker.md) | ⛔️ |
| [Skypilot (AWS, Azure, GCP, Lambda)](../../../component-guide/orchestrators/skypilot-vm.md) | ⛔️ |
| [Tekton](../../../component-guide/orchestrators/tekton.md) | ⛔️ |
| [Vertex](../../../component-guide/orchestrators/vertex.md) | ✅ |

### Set a Schedule
```python
from zenml.config.schedule import Schedule
from zenml import pipeline
from datetime import datetime

@pipeline()
def my_pipeline(...):
    ...

# Scheduling options
schedule = Schedule(cron_expression="5 14 * * 3")  # Cron expression
# or
schedule = Schedule(start_time=datetime.now(), interval_second=1800)  # Human-readable

my_pipeline = my_pipeline.with_options(schedule=schedule)
my_pipeline()
```
For more scheduling options, refer to the [SDK docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.schedule.Schedule).

### Pause/Stop a Schedule
The method to pause or stop a scheduled run varies by orchestrator. For instance, in Kubeflow, use the UI for this purpose. Consult your orchestrator's documentation for specific instructions. 

**Note:** ZenML schedules the run, but users are responsible for managing the lifecycle of the schedule. Running a pipeline with a schedule multiple times creates unique scheduled pipelines. 

### See Also
Learn about supported orchestrators [here](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

### Deleting Pipelines

To delete a pipeline, use either the CLI or the Python SDK:

#### CLI
```shell
zenml pipeline delete <PIPELINE_NAME>
```

#### Python SDK
```python
from zenml.client import Client

Client().delete_pipeline(<PIPELINE_NAME>)
```

**Note:** Deleting a pipeline does not remove associated runs or artifacts.

For deleting multiple pipelines, the Python SDK is recommended. Use the following script if pipelines share a prefix:

```python
from zenml.client import Client

client = Client()
pipelines_list = client.list_pipelines(name="startswith:test_pipeline", size=100)
target_pipeline_ids = [p.id for p in pipelines_list.items]

if input(f"Found {len(target_pipeline_ids)} pipelines. Delete? (y/n): ").lower() == 'y':
    for pid in target_pipeline_ids:
        client.delete_pipeline(pid)
    print("Deletion complete")
else:
    print("Deletion cancelled")
```

### Deleting Pipeline Runs

To delete a pipeline run, use the CLI or the Python SDK:

#### CLI
```shell
zenml pipeline runs delete <RUN_NAME_OR_ID>
```

#### Python SDK
```python
from zenml.client import Client

Client().delete_pipeline_run(<RUN_NAME_OR_ID>)
```

================================================================================

### Runtime Configuration of a Pipeline

To run a pipeline with a different configuration, use the [`pipeline.with_options`](../../pipeline-development/use-configuration-files/README.md) method. You can configure options in two ways:

1. Explicitly:  
   ```python
   with_options(steps="trainer", parameters={"param1": 1})
   ```
   
2. By passing a YAML file:  
   ```python
   with_options(config_file="path_to_yaml_file")
   ```

For triggering a pipeline from a client or another pipeline, use the `PipelineRunConfiguration` object. More details can be found [here](../../pipeline-development/trigger-pipelines/use-templates-python.md#advanced-usage-run-a-template-from-another-pipeline). 

For further information on using config files, refer to the [configuration files documentation](../../pipeline-development/use-configuration-files/README.md).

================================================================================

### Summary: Reuse Steps Between Pipelines

ZenML enables the composition of pipelines to reduce code duplication by extracting common functionalities into separate functions.

#### Code Example:
```python
from zenml import pipeline

@pipeline
def data_loading_pipeline(mode: str):
    data = training_data_loader_step() if mode == "train" else test_data_loader_step()
    return preprocessing_step(data)

@pipeline
def training_pipeline():
    training_data = data_loading_pipeline(mode="train")
    model = training_step(data=training_data)
    test_data = data_loading_pipeline(mode="test")
    evaluation_step(model=model, data=test_data)
```

**Key Points:**
- The `data_loading_pipeline` serves as a step within the `training_pipeline`.
- Only the parent pipeline is visible in the dashboard.
- For triggering a pipeline from another, refer to the advanced usage documentation. 

For more on orchestrators, see [orchestrators.md](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

### Building a Pipeline with ZenML

To create a pipeline, use the `@step` and `@pipeline` decorators.

```python
from zenml import pipeline, step

@step
def load_data() -> dict:
    return {'features': [[1, 2], [3, 4], [5, 6]], 'labels': [0, 1, 0]}

@step
def train_model(data: dict) -> None:
    print(f"Trained model using {len(data['features'])} data points.")

@pipeline
def simple_ml_pipeline():
    train_model(load_data())
```

Run the pipeline with:
```python
simple_ml_pipeline()
```

Execution logs are available on the ZenML dashboard, which requires a running ZenML server (local or remote). For more advanced pipeline features, refer to the following topics:

- Configure pipeline/step parameters
- Name and annotate step outputs
- Control caching behavior
- Run pipeline from another pipeline
- Control execution order of steps
- Customize step invocation IDs
- Name pipeline runs
- Use failure/success hooks
- Hyperparameter tuning
- Attach and fetch metadata within steps
- Enable or disable log storing
- Access secrets in a step

For detailed documentation, see the respective links provided.

================================================================================

### Summary of Documentation on Pipeline and Step Parameters

**Parameterization of Steps and Pipelines**  
Steps and pipelines can be parameterized like standard Python functions. Inputs to a step can be either an **artifact** (output from another step) or a **parameter** (explicitly provided value). Only JSON-serializable values can be passed as parameters; for non-JSON-serializable objects, use [External Artifacts](../../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline).

**Example Code:**
```python
from zenml import step, pipeline

@step
def my_step(input_1: int, input_2: int) -> None:
    pass

@pipeline
def my_pipeline():
    int_artifact = some_other_step()
    my_step(input_1=int_artifact, input_2=42)
```

**Using YAML Configuration Files**  
Parameters can also be defined in a YAML configuration file, allowing for easier updates without modifying the code.

**Example YAML:**
```yaml
parameters:
  environment: production
steps:
  my_step:
    parameters:
      input_2: 42
```

**Example Code with YAML:**
```python
from zenml import step, pipeline

@step
def my_step(input_1: int, input_2: int) -> None:
    ...

@pipeline
def my_pipeline(environment: str):
    ...

if __name__ == "__main__":
    my_pipeline.with_options(config_paths="config.yaml")()
```

**Conflict Handling**  
Conflicts may arise if parameters are defined in both the YAML file and the code. The system will notify you of any conflicts.

**Example of Conflict:**
```yaml
parameters:
    some_param: 24
steps:
  my_step:
    parameters:
      input_2: 42
```
```python
@pipeline
def my_pipeline(some_param: int):
    my_step(input_1=42, input_2=43)

if __name__ == "__main__":
    my_pipeline(23)
```

**Caching Behavior**  
- **Parameters**: A step is cached only if all parameter values match previous executions.
- **Artifacts**: A step is cached only if all input artifacts match previous executions. If upstream steps are not cached, the step will always execute.

### See Also
- [Use configuration files to set parameters](use-pipeline-step-parameters.md)
- [How caching works and how to control it](control-caching-behavior.md)

================================================================================

# Reference Environment Variables in Configurations

ZenML allows referencing environment variables in configurations using the syntax `${ENV_VARIABLE_NAME}`.

## In-code Example

```python
from zenml import step

@step(extra={"value_from_environment": "${ENV_VAR}"})
def my_step() -> None:
    ...
```

## Configuration File Example

```yaml
extra:
  value_from_environment: ${ENV_VAR}
  combined_value: prefix_${ENV_VAR}_suffix
```

================================================================================

# Naming Pipeline Runs

Pipeline run names are automatically generated using the current date and time, as shown below:

```bash
Pipeline run training_pipeline-2023_05_24-12_41_04_576473 has finished in 3.742s.
```

To customize the run name, use the `run_name` parameter with the `with_options()` method:

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name"
)
training_pipeline()
```

Ensure that pipeline run names are unique. For multiple runs or scheduled executions, compute the run name dynamically or use placeholders that ZenML will replace. Placeholders can be set in the `@pipeline` decorator or `pipeline.with_options` function. Standard placeholders include:

- `{date}`: Current date (e.g., `2024_11_27`)
- `{time}`: Current UTC time (e.g., `11_07_09_326492`)

Example of using placeholders in a custom run name:

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name_{experiment_name}_{date}_{time}"
)
training_pipeline()
```

================================================================================

### Run Pipelines Asynchronously

By default, pipelines run synchronously, displaying logs in the terminal. To run them asynchronously, configure the orchestrator with `synchronous=False` either in the pipeline code or a YAML config file.

**Python Code Example:**
```python
from zenml import pipeline

@pipeline(settings={"orchestrator": {"synchronous": False}})
def my_pipeline():
    ...
```

**YAML Configuration Example:**
```yaml
settings:
  orchestrator.<STACK_NAME>:
    synchronous: false
```

For more details, refer to the [orchestrators documentation](../../../component-guide/orchestrators/orchestrators.md).

================================================================================

### Hyperparameter Tuning with ZenML

**Note:** Hyperparameter tuning is not fully supported in ZenML yet, but it is planned for future updates.

#### Basic Implementation

You can implement hyperparameter tuning using a simple pipeline:

```python
@pipeline
def my_pipeline(step_count: int) -> None:
    data = load_data_step()
    after = []
    for i in range(step_count):
        train_step(data, learning_rate=i * 0.0001, name=f"train_step_{i}")
        after.append(f"train_step_{i}")
    model = select_model_step(..., after=after)
```

This example demonstrates a basic grid search over learning rates. After training, `select_model_step` identifies the best-performing hyperparameters.

#### E2E Example

To see a complete example, refer to the `Hyperparameter tuning stage` in [`pipelines/training.py`](../../../../examples/e2e/pipelines/training.py):

```python
after = []
search_steps_prefix = "hp_tuning_search_"
for i, model_search_configuration in enumerate(MetaConfig.model_search_space):
    step_name = f"{search_steps_prefix}{i}"
    hp_tuning_single_search(
        model_metadata=ExternalArtifact(value=model_search_configuration),
        id=step_name,
        dataset_trn=dataset_trn,
        dataset_tst=dataset_tst,
        target=target,
    )
    after.append(step_name)

best_model_config = hp_tuning_select_best_model(
    search_steps_prefix=search_steps_prefix, after=after
)
```

#### Challenges

Currently, you cannot programmatically pass a variable number of artifacts into a step. Instead, `select_model_step` queries all artifacts produced by previous steps:

```python
from zenml import step, get_step_context
from zenml.client import Client

@step
def select_model_step():
    run_name = get_step_context().pipeline_run.name
    run = Client().get_pipeline_run(run_name)

    trained_models_by_lr = {}
    for step_name, step in run.steps.items():
        if step_name.startswith("train_step"):
            for output_name, output in step.outputs.items():
                if output_name == "<NAME_OF_MODEL_OUTPUT_IN_TRAIN_STEP>":
                    model = output.load()
                    lr = step.config.parameters["learning_rate"]
                    trained_models_by_lr[lr] = model
    
    for lr, model in trained_models_by_lr.items():
        ...
```

#### Additional Resources

For more tailored hyperparameter search implementations, check the following files in the `steps/hp_tuning` folder:
- [`hp_tuning_single_search`](../../../../examples/e2e/steps/hp_tuning/hp_tuning_single_search.py): Performs randomized search for hyperparameters.
- [`hp_tuning_select_best_model`](../../../../examples/e2e/steps/hp_tuning/hp_tuning_select_best_model.py): Finds the best hyperparameters based on previous searches.

================================================================================

### Control Caching Behavior in ZenML

By default, ZenML caches steps in pipelines when code and parameters remain unchanged.

#### Example Code

```python
@step(enable_cache=True) 
def load_data(parameter: int) -> dict:
    ...

@step(enable_cache=False) 
def train_model(data: dict) -> None:
    ...

@pipeline(enable_cache=True) 
def simple_ml_pipeline(parameter: int):
    ...
```

**Note:** Caching occurs only when code and parameters are unchanged.

#### Modifying Cache Settings

You can change caching behavior after initial setup:

```python
my_step.configure(enable_cache=...)
my_pipeline.configure(enable_cache=...)
```

For YAML configuration, refer to [use-configuration-files](../../pipeline-development/use-configuration-files/).

================================================================================

# Running an Individual Step on Your Stack

To execute a single step in ZenML, call the step like a regular Python function. ZenML will create an unlisted pipeline to run it on the active stack. This run will appear in the "Runs" tab of the dashboard.

## Example Code

```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from typing import Tuple, Annotated

@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(X_train: pd.DataFrame, y_train: pd.Series, gamma: float = 0.001) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
    """Train a sklearn SVC classifier."""
    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())
    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
    print(f"Train accuracy: {train_acc}")
    return model, train_acc

X_train = pd.DataFrame(...)
y_train = pd.Series(...)

# Call the step directly
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

## Running the Step Function Directly

To run the step function without ZenML, use the `entrypoint(...)` method:

```python
model, train_acc = svc_trainer.entrypoint(X_train=X_train, y_train=y_train)
```

### Default Behavior

Set the environment variable `ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK` to `True` to make calling a step directly invoke the underlying function without using ZenML.

================================================================================

# Control Execution Order of Steps

ZenML determines the execution order of pipeline steps based on data dependencies. For example, `step_3` depends on the outputs of `step_1` and `step_2`, allowing both to run in parallel before `step_3` starts.

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1()
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

To specify non-data dependencies, use invocation IDs to enforce execution order. For a single step: `my_step(after="other_step")`. For multiple steps: `my_step(after=["other_step", "other_step_2"])`. 

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1(after="step_2")
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

In this example, `step_1` will only start after `step_2` has completed.

================================================================================

### Summary: Inspecting a Finished Pipeline Run and Its Outputs

#### Overview
After a pipeline run is completed, you can access various outputs and metadata programmatically, including models, datasets, and lineage information.

#### Pipeline Hierarchy
The structure of pipelines consists of:
- **Pipelines** → **Runs** → **Steps** → **Artifacts**

#### Fetching Pipelines
- **Get a Specific Pipeline:**
  ```python
  from zenml.client import Client
  pipeline_model = Client().get_pipeline("first_pipeline")
  ```

- **List All Pipelines:**
  - **Python:**
    ```python
    pipelines = Client().list_pipelines()
    ```
  - **CLI:**
    ```shell
    zenml pipeline list
    ```

#### Pipeline Runs
- **Get All Runs of a Pipeline:**
  ```python
  runs = pipeline_model.runs
  ```

- **Get the Last Run:**
  ```python
  last_run = pipeline_model.last_run  # OR: pipeline_model.runs[0]
  ```

- **Execute and Get Latest Run:**
  ```python
  run = training_pipeline()
  ```

- **Fetch a Specific Run:**
  ```python
  pipeline_run = Client().get_pipeline_run("first_pipeline-2023_06_20-16_20_13_274466")
  ```

#### Run Information
- **Status:**
  ```python
  status = run.status
  ```

- **Configuration:**
  ```python
  pipeline_config = run.config
  ```

- **Component Metadata:**
  ```python
  run_metadata = run.run_metadata
  orchestrator_url = run_metadata["orchestrator_url"].value
  ```

#### Steps in a Run
- **Get All Steps:**
  ```python
  steps = run.steps
  ```

- **Access Step Information:**
  ```python
  step = run.steps["first_step"]
  ```

#### Artifacts
- **Inspect Output Artifacts:**
  ```python
  output = step.outputs["output_name"]  # or step.output for single output
  my_pytorch_model = output.load()
  ```

- **Fetch Artifacts Directly:**
  ```python
  artifact = Client().get_artifact('iris_dataset')
  output = artifact.versions['2022']
  ```

#### Artifact Metadata
- **Access Metadata:**
  ```python
  output_metadata = output.run_metadata
  storage_size_in_bytes = output_metadata["storage_size"].value
  ```

- **Visualizations:**
  ```python
  output.visualize()
  ```

#### Fetching Information During Run Execution
To fetch information from within a running pipeline:
```python
from zenml import get_step_context
from zenml.client import Client

@step
def my_step():
    current_run_name = get_step_context().pipeline_run.name
    current_run = Client().get_pipeline_run(current_run_name)
    previous_run = current_run.pipeline.runs[1]
```

#### Code Example
Combining concepts into a script:
```python
from typing_extensions import Tuple, Annotated
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC
from zenml import pipeline, step
from zenml.client import Client

@step
def training_data_loader() -> Tuple[Annotated[pd.DataFrame, "X_train"], Annotated[pd.DataFrame, "X_test"], Annotated[pd.Series, "y_train"], Annotated[pd.Series, "y_test"]]:
    iris = load_iris(as_frame=True)
    return train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)

@step
def svc_trainer(X_train: pd.DataFrame, y_train: pd.Series, gamma: float = 0.001) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
    model = SVC(gamma=gamma).fit(X_train.to_numpy(), y_train.to_numpy())
    return model, model.score(X_train.to_numpy(), y_train.to_numpy())

@pipeline
def training_pipeline(gamma: float = 0.002):
    X_train, X_test, y_train, y_test = training_data_loader()
    svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)

if __name__ == "__main__":
    last_run = training_pipeline()
    model = last_run.steps["svc_trainer"].outputs["trained_model"].load()
```

This summary captures the essential technical information while maintaining clarity and conciseness.

================================================================================

# Access Secrets in a Step

ZenML secrets are **key-value pairs** securely stored in the ZenML secrets store, each with a **name** for easy reference in pipelines. For configuration and creation details, refer to the [platform guide on secrets](../../../getting-started/deploying-zenml/secret-management.md).

You can access secrets in your steps using the ZenML `Client` API, allowing you to query APIs without hard-coding access keys:

```python
from zenml import step
from zenml.client import Client
from somewhere import authenticate_to_some_api

@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    secret = Client().get_secret("<SECRET_NAME>")
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

### See Also:
- [Learn how to create and manage secrets](../../interact-with-secrets.md)
- [Find out more about the secrets backend in ZenML](../../../getting-started/deploying-zenml/secret-management.md)

================================================================================

# Get Past Pipeline/Step Runs

To retrieve past pipeline or step runs, use the `get_pipeline` method with the `last_run` property or index into the runs:

```python
from zenml.client import Client

client = Client()
# Retrieve a pipeline by its name
p = client.get_pipeline("mlflow_train_deploy_pipeline")
# Get the latest run of this pipeline
latest_run = p.last_run
# Access runs by index
first_run = p[0]
```

================================================================================

### Step Output Typing and Annotation

**Step Outputs**: Outputs are stored in your artifact store. Annotate and name them for clarity.

#### Type Annotations
- **Benefits**:
  - **Type Validation**: Ensures correct input types from upstream steps.
  - **Better Serialization**: Allows ZenML to select the appropriate materializer based on type annotations. Custom materializers can be created if needed.

**Warning**: The built-in `CloudpickleMaterializer` is not production-ready due to compatibility issues across Python versions and potential security risks.

#### Code Examples
```python
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> float:
    return number ** 0.5

@step
def divide(a: int, b: int) -> Tuple[int, int]:
    return a // b, a % b
```

To enforce type annotations, set `ZENML_ENFORCE_TYPE_ANNOTATIONS=True`. ZenML will raise exceptions for missing annotations.

#### Tuple vs Multiple Outputs
- **Convention**: 
  - Return a tuple literal (e.g., `return (1, 2)`) for multiple outputs.
  - Other cases are treated as a single output of type `Tuple`.

#### Output Naming
- Default names: `output` for single outputs and `output_0, output_1, ...` for multiple outputs.
- Use `Annotated` for custom names:
```python
from typing_extensions import Annotated
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> Annotated[float, "custom_output_name"]:
    return number ** 0.5

@step
def divide(a: int, b: int) -> Tuple[Annotated[int, "quotient"], Annotated[int, "remainder"]]:
    return a // b, a % b
```

If no custom names are provided, artifacts will be named `{pipeline_name}::{step_name}::output` or `{pipeline_name}::{step_name}::output_{i}`.

### See Also
- [Output Annotation](../../data-artifact-management/handle-data-artifacts/return-multiple-outputs-from-a-step.md)
- [Custom Data Types](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md)

================================================================================

### Running Failure and Success Hooks After Step Execution

**Overview**: Hooks allow actions to be performed after a step's execution, useful for notifications, logging, or resource cleanup. There are two types of hooks: 
- `on_failure`: Triggered when a step fails.
- `on_success`: Triggered when a step succeeds.

**Defining Hooks**: Hooks are defined as callback functions accessible within the pipeline repository. The `on_failure` hook can accept a `BaseException` argument to access the exception that caused the failure.

```python
from zenml import step

def on_failure(exception: BaseException):
    print(f"Step failed: {exception}")

def on_success():
    print("Step succeeded!")

@step(on_failure=on_failure)
def my_failing_step() -> int:
    raise ValueError("Error")

@step(on_success=on_success)
def my_successful_step() -> int:
    return 1
```

**Pipeline-Level Hooks**: Hooks can also be defined at the pipeline level, which apply to all steps unless overridden by step-level hooks.

```python
@pipeline(on_failure=on_failure, on_success=on_success)
def my_pipeline(...):
    ...
```

**Accessing Step Information**: Use `get_step_context()` within hooks to access the current pipeline run or step details.

```python
from zenml import get_step_context

def on_failure(exception: BaseException):
    context = get_step_context()
    print(context.step_run.name)
    print(context.step_run.config.parameters)
    print("Step failed!")

@step(on_failure=on_failure)
def my_step(some_parameter: int = 1):
    raise ValueError("My exception")
```

**Using Alerter Component**: Integrate the Alerter component to send notifications on step success or failure.

```python
from zenml import get_step_context, Client

def notify_on_failure() -> None:
    step_context = get_step_context()
    alerter = Client().active_stack.alerter
    if alerter and step_context.pipeline_run.config.extra["notify_on_failure"]:
        alerter.post(message=build_message(status="failed"))
```

**OpenAI ChatGPT Failure Hook**: This hook generates potential fixes for exceptions using OpenAI's API. Ensure the OpenAI integration is installed and your API key is stored in a ZenML secret.

```shell
zenml integration install openai
zenml secret create openai --api_key=<YOUR_API_KEY>
```

Use the hook in your pipeline:

```python
from zenml.integration.openai.hooks import openai_chatgpt_alerter_failure_hook

@step(on_failure=openai_chatgpt_alerter_failure_hook)
def my_step(...):
    ...
```

### Summary
Hooks in ZenML facilitate post-execution actions for steps, with options for success and failure notifications, and can leverage external services like OpenAI for enhanced error handling.

================================================================================

### Step Retry Configuration in ZenML

ZenML offers a built-in retry mechanism to automatically retry steps upon failure, useful for handling intermittent issues. You can configure the following parameters for retries:

- **max_retries:** Maximum retry attempts.
- **delay:** Initial delay (in seconds) before the first retry.
- **backoff:** Multiplier for the delay after each retry.

#### Example with @step Decorator

You can set the retry configuration directly in your step definition:

```python
from zenml.config.retry_config import StepRetryConfig

@step(
    retry=StepRetryConfig(
        max_retries=3, 
        delay=10, 
        backoff=2
    )
)
def my_step() -> None:
    raise Exception("This is a test exception")
```

**Note:** Infinite retries are not supported. Setting `max_retries` to a high value will still enforce an internal limit to prevent infinite loops. Choose a reasonable `max_retries` based on your use case.

### See Also:
- [Failure/Success Hooks](use-failure-success-hooks.md)
- [Configure Pipelines](../../pipeline-development/use-configuration-files/how-to-use-config.md)

================================================================================

# Tagging Pipeline Runs

You can specify tags for your pipeline runs in the following ways:

1. **Configuration File**:
   ```yaml
   # config.yaml
   tags:
     - tag_in_config_file
   ```

2. **In Code**:
   Using the `@pipeline` decorator:
   ```python
   @pipeline(tags=["tag_on_decorator"])
   def my_pipeline():
       ...
   ```

   Or with the `with_options` method:
   ```python
   my_pipeline = my_pipeline.with_options(tags=["tag_on_with_options"])
   ```

Tags from all specified locations will be merged and applied to the pipeline run.

================================================================================

# Custom Step Invocation ID in ZenML

When invoking a ZenML step in a pipeline, a unique **invocation ID** is generated. This ID can be used to define the execution order of steps or to fetch invocation details post-execution.

## Example Code
```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def example_pipeline():
    my_step()  # First invocation ID: `my_step`
    my_step()  # Second invocation ID: `my_step_2`
    my_step(id="my_custom_invocation_id")  # Custom invocation ID
```

Ensure custom IDs are unique within the pipeline.

================================================================================

# GPU Resource Management in ZenML

## Scaling Machine Learning Pipelines
To leverage powerful hardware or distribute tasks, ZenML allows running steps on GPU-backed hardware using `ResourceSettings`.

### Specify Resource Requirements
For resource-intensive steps, specify the required resources:

```python
from zenml.config import ResourceSettings
from zenml import step

@step(settings={"resources": ResourceSettings(cpu_count=8, gpu_count=2, memory="8GB")})
def training_step(...) -> ...:
    # train a model
```

If the orchestrator supports it, this will allocate the specified resources. For orchestrators like Skypilot that use specific settings:

```python
from zenml import step
from zenml.integrations.skypilot.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings

skypilot_settings = SkypilotAWSOrchestratorSettings(cpus="2", memory="16", accelerators="V100:2")

@step(settings={"orchestrator": skypilot_settings})
def training_step(...) -> ...:
    # train a model
```

Refer to orchestrator documentation for specific resource support.

### Ensure CUDA-Enabled Container
To utilize GPUs, ensure your environment has CUDA tools. Key steps include:

1. **Specify a CUDA-enabled parent image**:

```python
from zenml import pipeline
from zenml.config import DockerSettings

docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

2. **Add ZenML as a pip requirement**:

```python
docker_settings = DockerSettings(
    parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
    requirements=["zenml==0.39.1", "torchvision"]
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

Choose images carefully to avoid compatibility issues between local and remote environments. Check cloud provider documentation for prebuilt images.

### Reset CUDA Cache
Resetting the CUDA cache can prevent issues during GPU-intensive tasks:

```python
import gc
import torch

def cleanup_memory() -> None:
    while gc.collect():
        torch.cuda.empty_cache()

@step
def training_step(...):
    cleanup_memory()
    # train a model
```

Use this function judiciously as it may affect others using the same GPU.

## Multi-GPU Training
ZenML supports multi-GPU training on a single node. To implement this, create a script that handles parallel training and call it from within the step. This approach is currently being improved for better integration.

For assistance, connect with the ZenML community on Slack.

================================================================================

# Distributed Training with Hugging Face's Accelerate in ZenML

ZenML integrates with [Hugging Face's Accelerate library](https://github.com/huggingface/accelerate) for seamless distributed training, allowing you to leverage multiple GPUs or nodes.

## Using 🤗 Accelerate in ZenML Steps

You can enable distributed execution in training steps using the `run_with_accelerate` decorator:

```python
from zenml import step, pipeline
from zenml.integrations.huggingface.steps import run_with_accelerate

@run_with_accelerate(num_processes=4, multi_gpu=True)
@step
def training_step(some_param: int, ...):
    ...

@pipeline
def training_pipeline(some_param: int, ...):
    training_step(some_param, ...)
```

### Configuration Options
The `run_with_accelerate` decorator accepts several arguments:
- `num_processes`: Number of processes for training.
- `cpu`: Force training on CPU.
- `multi_gpu`: Enable distributed GPU training.
- `mixed_precision`: Set mixed precision mode ('no', 'fp16', 'bf16').

### Important Notes
1. Use the `@` syntax for the decorator directly on steps.
2. Use keyword arguments for step calls.
3. Misuse raises a `RuntimeError` with guidance.

For a complete example, see the [llm-lora-finetuning](https://github.com/zenml-io/zenml-projects/blob/main/llm-lora-finetuning/README.md) project.

## Ensure Your Container is Accelerate-Ready

To utilize Accelerate, ensure your environment is correctly configured:

### 1. Specify a CUDA-enabled Parent Image

Example using a CUDA-enabled PyTorch image:

```python
from zenml.config import DockerSettings

docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

### 2. Add Accelerate as a Requirement

Ensure Accelerate is included in your container:

```python
docker_settings = DockerSettings(
    parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
    requirements=["accelerate", "torchvision"]
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

## Training Across Multiple GPUs

ZenML's Accelerate integration supports training on multiple GPUs, enhancing performance for large datasets or complex models. Key steps include:
- Wrapping your training step with `run_with_accelerate`.
- Configuring Accelerate arguments (e.g., `num_processes`, `multi_gpu`).
- Ensuring compatibility of your training code with distributed training.

For assistance, connect with us on [Slack](https://zenml.io/slack). By using Accelerate with ZenML, you can efficiently scale your training processes while maintaining pipeline structure.

================================================================================

### Create a Template Using ZenML CLI

**Note:** This feature is available only in [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

To create a run template, use the ZenML CLI:

```bash
zenml pipeline create-run-template <PIPELINE_SOURCE_PATH> --name=<TEMPLATE_NAME>
```
*Replace `<PIPELINE_SOURCE_PATH>` with `run.my_pipeline` if defined in `run.py`.*

**Warning:** Ensure you have an active **remote stack** or specify one with the `--stack` option.

================================================================================

### Trigger a Pipeline in ZenML

To execute a pipeline in ZenML, use the pipeline function as shown below:

```python
from zenml import step, pipeline

@step
def load_data() -> dict:
    return {'features': [[1, 2], [3, 4], [5, 6]], 'labels': [0, 1, 0]}

@step
def train_model(data: dict) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}.")

@pipeline
def simple_ml_pipeline():
    train_model(load_data())

if __name__ == "__main__":
    simple_ml_pipeline()
```

### Other Pipeline Triggering Methods

You can also trigger pipelines with a remote stack (orchestrator, artifact store, and container registry).

### Run Templates

Run Templates are pre-defined, parameterized configurations for ZenML pipelines, allowing easy execution from the ZenML dashboard or via the Client/REST API. This feature is exclusive to ZenML Pro users. 

For more details, refer to:
- [Use templates: Python SDK](use-templates-python.md)
- [Use templates: CLI](use-templates-cli.md)
- [Use templates: Dashboard](use-templates-dashboard.md)
- [Use templates: REST API](use-templates-rest-api.md)

================================================================================

### ZenML Template Creation and Execution

**Note:** This feature is available only in [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Create a Template

To create a run template using the ZenML client:

```python
from zenml.client import Client

run = Client().get_pipeline_run(<RUN_NAME_OR_ID>)
Client().create_run_template(name=<TEMPLATE_NAME>, deployment_id=run.deployment_id)
```

**Warning:** Select a pipeline run executed on a **remote stack** (with remote orchestrator, artifact store, and container registry).

Alternatively, create a template directly from your pipeline definition:

```python
from zenml import pipeline

@pipeline
def my_pipeline():
    ...

template = my_pipeline.create_run_template(name=<TEMPLATE_NAME>)
```

#### Run a Template

To run a template:

```python
from zenml.client import Client

template = Client().get_run_template(<TEMPLATE_NAME>)
config = template.config_template

# [OPTIONAL] Modify the config here

Client().trigger_pipeline(template_id=template.id, run_configuration=config)
```

This triggers a new run on the same stack as the original.

#### Advanced Usage: Run a Template from Another Pipeline

You can trigger a pipeline within another pipeline:

```python
import pandas as pd
from zenml import pipeline, step
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml.artifacts.utils import load_artifact
from zenml.client import Client
from zenml.config.pipeline_run_configuration import PipelineRunConfiguration

@step
def trainer(data_artifact_id: str):
    df = load_artifact(data_artifact_id)

@pipeline
def training_pipeline():
    trainer()

@step
def load_data() -> pd.DataFrame:
    ...

@step
def trigger_pipeline(df: UnmaterializedArtifact):
    run_config = PipelineRunConfiguration(
        steps={"trainer": {"parameters": {"data_artifact_id": df.id}}}
    )
    Client().trigger_pipeline("training_pipeline", run_configuration=run_config)

@pipeline
def loads_data_and_triggers_training():
    df = load_data()
    trigger_pipeline(df)
```

For more details, refer to the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) and [`trigger_pipeline`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client) documentation, as well as information on Unmaterialized Artifacts [here](../../data-artifact-management/complex-usecases/unmaterialized-artifacts.md).

================================================================================

### ZenML Dashboard: Create and Run a Template

**Note:** This feature is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Create a Template
1. Navigate to a pipeline run executed on a remote stack (with a remote orchestrator, artifact store, and container registry).
2. Click `+ New Template`, name it, and click `Create`.

#### Run a Template
- To run a template:
  - Click `Run a Pipeline` on the main `Pipelines` page, or
  - Go to a specific template page and click `Run Template`.
  
You will be directed to the `Run Details` page, where you can upload a `.yaml` configuration file or modify the configuration using the editor. 

Once executed, the template runs on the same stack as the original run.

================================================================================

### Create and Run a Template Over the ZenML REST API

**Note:** This feature is available only in [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

## Run a Template

To trigger a pipeline from the REST API, ensure you have created at least one run template for the pipeline. Follow these steps:

1. **Get Pipeline ID:**
   ```shell
   curl -X 'GET' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/pipelines?name=<PIPELINE_NAME>' \
     -H 'accept: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>'
   ```

2. **Get Template ID:**
   ```shell
   curl -X 'GET' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates?pipeline_id=<PIPELINE_ID>' \
     -H 'accept: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>'
   ```

3. **Trigger Pipeline:**
   ```shell
   curl -X 'POST' \
     '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates/<TEMPLATE_ID>/runs' \
     -H 'accept: application/json' \
     -H 'Content-Type: application/json' \
     -H 'Authorization: Bearer <YOUR_TOKEN>' \
     -d '{
       "steps": {"model_trainer": {"parameters": {"model_type": "rf"}}}
     }'
   ```

A successful response indicates that your pipeline has been re-triggered with the specified configuration. 

**Additional Information:** For obtaining a bearer token, refer to the [API reference](../../../reference/api-reference.md#using-a-bearer-token-to-access-the-api-programmatically).

================================================================================

# Handling Dependency Conflicts in ZenML

## Overview
ZenML is designed to be stack- and integration-agnostic, which may lead to dependency conflicts when used with other libraries. You can install integration-specific dependencies using the command:

```bash
zenml integration install ...
```

To check if all ZenML requirements are met after installing additional dependencies, run:

```bash
zenml integration list
```

## Resolving Dependency Conflicts

### Use `pip-compile`
Utilize `pip-compile` from the [pip-tools package](https://pip-tools.readthedocs.io/) to create a static `requirements.txt` file for consistency across environments. For more details, refer to the [gitflow repository](https://github.com/zenml-io/zenml-gitflow#-software-requirements-management).

### Use `pip check`
Run `pip check` to verify compatibility of your environment's dependencies. This command will list any conflicts.

### Known Issues
ZenML has strict dependency requirements. For example, it requires `click~=8.0.3` for its CLI. Using a higher version may cause issues.

### Manual Installation
You can manually install integration dependencies, though this is not recommended. The command `zenml integration install ...` executes a `pip install` for the required packages.

To export integration requirements, use:

```bash
# Export to a file
zenml integration export-requirements --output-file integration-requirements.txt INTEGRATION_NAME

# Print to console
zenml integration export-requirements INTEGRATION_NAME
```

If using a remote orchestrator, update the dependencies in a `DockerSettings` object to ensure proper functionality.

================================================================================

# Configure Python Environments

ZenML deployments involve multiple environments for managing dependencies and configurations. 

## Environment Overview
- **Client Environment (Runner Environment)**: Where ZenML pipelines are compiled (e.g., in `run.py`). Types include:
  - Local development
  - CI runner
  - ZenML Pro runner
  - `runner` image orchestrated by the ZenML server

### Key Steps in Client Environment:
1. Compile pipeline via `@pipeline` function.
2. Create/trigger pipeline and step build environments if running remotely.
3. Trigger a run in the orchestrator.

**Note**: The `@pipeline` function is called only in the client environment, focusing on compile-time logic.

## ZenML Server Environment
The ZenML server is a FastAPI application managing pipelines and metadata, including the ZenML Dashboard. Install dependencies during deployment if using custom integrations. 

## Execution Environments
When running locally, the client and execution environments are the same. For remote execution, ZenML builds Docker images (execution environments) starting from a base image containing ZenML and Python, adding pipeline dependencies. Follow the [containerize your pipeline](../../infrastructure-deployment/customize-docker-builds/README.md) guide for configuration.

## Image Builder Environment
Execution environments are typically created locally using the Docker client, requiring installation and permissions. ZenML provides [image builders](../../../component-guide/image-builders/image-builders.md) for building and pushing Docker images in a specialized environment. If no image builder is configured, ZenML defaults to the local image builder for consistency.

For more details, refer to the respective guides linked above.

================================================================================

### Configure the Server Environment

The ZenML server environment is set up using environment variables, which must be configured before deploying your server instance. For a complete list of available environment variables, refer to [the documentation](../../../reference/environment-variables.md).

================================================================================

### Disabling Colorful Logging in ZenML

ZenML uses colorful logging by default for better readability. To disable this feature, set the following environment variable:

```bash
ZENML_LOGGING_COLORS_DISABLED=true
```

Setting this variable in the client environment (e.g., local machine) will disable colorful logging for remote pipeline runs as well. To disable it locally while keeping it enabled for remote runs, set the variable in your pipeline's environment:

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_COLORS_DISABLED": "false"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Alternatively, configure pipeline options
my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

================================================================================

### Disabling Rich Traceback Output in ZenML

ZenML uses the [`rich`](https://rich.readthedocs.io/en/stable/traceback.html) library for enhanced traceback output during pipeline debugging. To disable this feature, set the following environment variable:

```bash
export ZENML_ENABLE_RICH_TRACEBACK=false
```

This change will only affect local pipeline runs. To disable rich tracebacks for remote runs, set the environment variable in your pipeline's environment:

```python
docker_settings = DockerSettings(environment={"ZENML_ENABLE_RICH_TRACEBACK": "false"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure options
my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

================================================================================

# Viewing Logs on the Dashboard

ZenML captures logs during step execution using a logging handler. Users can utilize the Python logging module or print statements, which ZenML will log.

```python
import logging
from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")
    print("World.")
```

Logs are stored in the artifact store of your stack and can be viewed on the dashboard if the ZenML server has access to it. Access conditions include:

- **Local ZenML Server**: Both local and remote artifact stores may be accessible based on client configuration.
- **Deployed ZenML Server**: Logs from runs on a local artifact store are not accessible. Logs from a remote artifact store **may be** accessible if configured with a service connector.

For configuration details, refer to the production guide on [remote artifact stores](../../user-guide/production-guide/remote-storage.md). If configured correctly, logs will display on the dashboard.

**Note**: To disable log storage due to performance or storage limits, follow [these instructions](./enable-or-disable-logs-storing.md).

================================================================================

# Configuring ZenML's Default Logging Behavior

## Control Logging

ZenML generates different types of logs:

- **ZenML Server**: Produces server logs similar to any FastAPI server.
- **Client or Runner Environment**: Logs are generated during pipeline execution, including pre- and post-run steps.
- **Execution Environment**: Logs are created at the orchestrator level during pipeline step execution, typically using Python's `logging` module.

This section explains how to manage logging behavior across these environments.

================================================================================

### Setting Logging Verbosity in ZenML

By default, ZenML logging verbosity is set to `INFO`. To change it, set the environment variable:

```bash
export ZENML_LOGGING_VERBOSITY=INFO
```

Available options: `INFO`, `WARN`, `ERROR`, `CRITICAL`, `DEBUG`. Note that this setting affects only local pipeline runs. For remote pipeline runs, set the variable in the pipeline's environment:

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_VERBOSITY": "DEBUG"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure pipeline options
my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

================================================================================

# ZenML Logging Configuration

ZenML captures logs during step execution using a logging handler. Users can utilize the default Python logging module or print statements, which ZenML will store in the artifact store.

## Example Code
```python
import logging
from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")
    print("World.")
```

Logs can be viewed on the dashboard, but require a connected cloud artifact store. For more details, refer to [viewing logs](./view-logs-on-the-dashboard.md).

## Disabling Log Storage

To disable log storage:

1. Use the `enable_step_logs` parameter in the `@step` or `@pipeline` decorator:
```python
from zenml import pipeline, step

@step(enable_step_logs=False)
def my_step() -> None:
    ...

@pipeline(enable_step_logs=False)
def my_pipeline():
    ...
```

2. Set the environmental variable `ZENML_DISABLE_STEP_LOGS_STORAGE` to `true` in the execution environment:
```python
docker_settings = DockerSettings(environment={"ZENML_DISABLE_STEP_LOGS_STORAGE": "true"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure pipeline options
my_pipeline = my_pipeline.with_options(settings={"docker": docker_settings})
```

================================================================================

# Configuring ZenML

This guide outlines how to configure ZenML's default behavior in various scenarios. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

# Model Management and Metrics

This section details managing models and tracking metrics in ZenML.

================================================================================

# Track Metrics and Metadata

ZenML offers a unified `log_metadata` function to log and manage metrics and metadata for models, artifacts, steps, and runs through a single interface. You can also choose to log the same metadata for related entities automatically.

### Basic Usage

To log metadata within a step:

```python
from zenml import step, log_metadata

@step
def my_step() -> ...:
    log_metadata(metadata={"accuracy": 0.91})
```

This logs `accuracy` for the step, its pipeline run, and optionally its model version.

### Additional Use-Cases

The `log_metadata` function allows specifying the target entity (model, artifact, step, or run). For more details, refer to:
- [Log metadata to a step](attach-metadata-to-a-step.md)
- [Log metadata to a run](attach-metadata-to-a-run.md)
- [Log metadata to an artifact](attach-metadata-to-an-artifact.md)
- [Log metadata to a model](attach-metadata-to-a-model.md)

**Note:** Older methods like `log_model_metadata`, `log_artifact_metadata`, and `log_step_metadata` are deprecated. Use `log_metadata` for all future implementations.

================================================================================

# Grouping Metadata in the Dashboard

To group key-value pairs in the ZenML dashboard, use a dictionary of dictionaries in the `metadata` parameter. This organizes metadata into cards for better visualization. 

### Example Code:
```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize

log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="my_artifact_version",
)
```

In the ZenML dashboard, "model_metrics" and "data_details" will display as separate cards with their respective key-value pairs.

================================================================================

### Fetch Metadata During Pipeline Composition

#### Pipeline Configuration with `PipelineContext`

To access pipeline configuration during composition, use the `zenml.get_pipeline_context()` function to obtain the `PipelineContext`.

```python
from zenml import get_pipeline_context, pipeline

@pipeline(
    extra={
        "complex_parameter": [
            ("sklearn.tree", "DecisionTreeClassifier"),
            ("sklearn.ensemble", "RandomForestClassifier"),
        ]
    }
)
def my_pipeline():
    context = get_pipeline_context()
    after = []
    for i, model_search_configuration in enumerate(context.extra["complex_parameter"]):
        step_name = f"hp_tuning_search_{i}"
        cross_validation(
            model_package=model_search_configuration[0],
            model_class=model_search_configuration[1],
            id=step_name
        )
        after.append(step_name)
    select_best_model(search_steps_prefix="hp_tuning_search_", after=after)
```

For more details on `PipelineContext` attributes and methods, refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.pipelines.pipeline_context.PipelineContext).

================================================================================

# Attach Metadata to an Artifact

In ZenML, metadata enhances artifacts by providing context such as size, structure, or performance metrics, accessible via the ZenML dashboard for easier inspection and tracking.

## Logging Metadata for Artifacts

Artifacts are outputs from pipeline steps (e.g., datasets, models). Use the `log_metadata` function to associate metadata with an artifact, specifying the artifact name, version, or ID. Metadata can be any JSON-serializable value, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`.

### Example of Logging Metadata

```python
import pandas as pd
from zenml import step, log_metadata
from zenml.metadata.metadata_types import StorageSize

@step
def process_data_step(dataframe: pd.DataFrame) -> pd.DataFrame:
    processed_dataframe = ...
    log_metadata(
        metadata={
            "row_count": len(processed_dataframe),
            "columns": list(processed_dataframe.columns),
            "storage_size": StorageSize(processed_dataframe.memory_usage().sum())
        },
        infer_artifact=True,
    )
    return processed_dataframe
```

### Selecting the Artifact for Metadata Logging

1. **Using `infer_artifact`**: Automatically selects the output artifact of the step.
2. **Name and Version**: Attach metadata to a specific artifact version using both name and version.
3. **Artifact Version ID**: Directly attach metadata using the version ID.

## Fetching Logged Metadata

Retrieve logged metadata with the ZenML Client:

```python
from zenml.client import Client

client = Client()
artifact = client.get_artifact_version("my_artifact", "my_version")
print(artifact.run_metadata["metadata_key"])
```

> **Note**: Fetching metadata by key returns the latest entry.

## Grouping Metadata in the Dashboard

Pass a dictionary of dictionaries to group metadata into cards in the ZenML dashboard for better organization:

```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize

log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="version",
)
```

In the ZenML dashboard, `model_metrics` and `data_details` will appear as separate cards.

================================================================================

### Tracking Your Metadata

ZenML supports special metadata types to capture specific information. Key types include `Uri`, `Path`, `DType`, and `StorageSize`. 

**Example Usage:**
```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize, DType, Uri, Path

log_metadata({
    "dataset_source": Uri("gs://my-bucket/datasets/source.csv"),
    "preprocessing_script": Path("/scripts/preprocess.py"),
    "column_types": {
        "age": DType("int"),
        "income": DType("float"),
        "score": DType("int")
    },
    "processed_data_size": StorageSize(2500000)
})
```

**Key Points:**
- `Uri`: Indicates dataset source.
- `Path`: Specifies the filesystem path to a script.
- `DType`: Describes data types of columns.
- `StorageSize`: Indicates size of processed data in bytes.

These types standardize metadata format for consistent logging.

================================================================================

### Attach Metadata to a Run in ZenML

In ZenML, you can log metadata to a pipeline run using the `log_metadata` function, which accepts a dictionary of key-value pairs. Values can be any JSON-serializable type, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Run
When logging metadata from a pipeline step, use `log_metadata` to attach metadata with the pattern `step_name::metadata_key`. This allows for consistent metadata keys across different steps during execution.

```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[
    ClassifierMixin,
    ArtifactConfig(name="sklearn_classifier", is_model_artifact=True)
]:
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    log_metadata({
        "run_metrics": {"accuracy": accuracy, "precision": precision, "recall": recall}
    })
    return classifier
```

#### Manually Logging Metadata
You can also log metadata to a specific pipeline run using the run ID, useful for post-execution metrics.

```python
from zenml import log_metadata

log_metadata({"post_run_info": {"some_metric": 5.0}}, run_id_name_or_prefix="run_id_name_or_prefix")
```

#### Fetching Logged Metadata
Retrieve logged metadata using the ZenML Client:

```python
from zenml.client import Client

client = Client()
run = client.get_pipeline_run("run_id_name_or_prefix")

print(run.run_metadata["metadata_key"])
```

> **Note:** Fetching metadata with a specific key returns the latest entry.

================================================================================

### Attach Metadata to a Step in ZenML

In ZenML, use the `log_metadata` function to attach metadata (key-value pairs) to a step during or after execution. The metadata can include any JSON-serializable value, including custom classes like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Step

When called within a step, `log_metadata` attaches the metadata to the executing step and its pipeline run, suitable for logging metrics available during execution.

```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name="sklearn_classifier")]:
    """Train a model and log evaluation metrics."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    log_metadata(metadata={"evaluation_metrics": {"accuracy": accuracy, "precision": precision, "recall": recall}})
    return classifier
```

> **Note:** In cached pipeline executions, metadata from the original step execution is copied to the cached run. Manually generated metadata post-execution is not included.

#### Manually Logging Metadata After Execution

You can log metadata for a specific step after execution using identifiers for the pipeline, step, and run.

```python
from zenml import log_metadata

log_metadata(metadata={"additional_info": {"a_number": 3}}, step_name="step_name", run_id_name_or_prefix="run_id_name_or_prefix")

# or 

log_metadata(metadata={"additional_info": {"a_number": 3}}, step_id="step_id")
```

#### Fetching Logged Metadata

To fetch logged metadata, use the ZenML Client:

```python
from zenml.client import Client

client = Client()
step = client.get_pipeline_run("pipeline_id").steps["step_name"]

print(step.run_metadata["metadata_key"])
```

> **Note:** Fetching metadata by key returns the latest entry.

================================================================================

### Attach Metadata to a Model

ZenML allows logging metadata for models, providing context beyond artifact details. This metadata can include evaluation results, deployment info, or customer-specific details, aiding in model management and performance interpretation across versions.

#### Logging Metadata for Models

Use the `log_metadata` function to attach key-value metadata to a model, including metrics and JSON-serializable values (e.g., `Uri`, `Path`, `StorageSize`).

**Example:**
```python
from typing import Annotated
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier
from zenml import step, log_metadata, ArtifactConfig

@step
def train_model(dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig(name="sklearn_classifier")]:
    """Train a model and log metadata."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...
    
    log_metadata(metadata={"evaluation_metrics": {"accuracy": accuracy, "precision": precision, "recall": recall}}, infer_model=True)
    return classifier
```

The metadata is linked to the model, summarizing various steps and artifacts in the pipeline.

#### Selecting Models with `log_metadata`

Options for attaching metadata to model versions:
1. **Using `infer_model`**: Attaches metadata inferred from the step context.
2. **Model Name and Version**: Attaches metadata to a specific model version.
3. **Model Version ID**: Directly attaches metadata to the specified model version.

#### Fetching Logged Metadata

Retrieve attached metadata using the ZenML Client.

**Example:**
```python
from zenml.client import Client

client = Client()
model = client.get_model_version("my_model", "my_version")
print(model.run_metadata["metadata_key"])
```

*Note: Fetching metadata by key returns the latest entry.*

================================================================================

### Accessing Meta Information in Real-Time

#### Fetch Metadata Within Steps

To access information about the currently running pipeline or step, use the `zenml.get_step_context()` function to obtain the `StepContext`:

```python
from zenml import step, get_step_context

@step
def my_step():
    context = get_step_context()
    pipeline_name = context.pipeline.name
    run_name = context.pipeline_run.name
    step_name = context.step_run.name
```

You can also determine where the outputs will be stored and which Materializer class will be used:

```python
from zenml import step, get_step_context

@step
def my_step():
    context = get_step_context()
    uri = context.get_output_artifact_uri()
    materializer = context.get_output_materializer()
```

For more details on `StepContext` attributes and methods, refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.steps.step_context.StepContext).

================================================================================

# Model Versions Overview

Model versions track iterations of your training process, allowing you to associate them with stages (e.g., production, staging) and link them to artifacts like datasets. Versions are created automatically during training, but can also be explicitly named via the `version` argument in the `Model` object.

## Explicitly Naming Model Versions

To explicitly name a model version:

```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="1.0.5")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training happens here
```

If a model version exists, it is automatically associated with the pipeline.

## Templated Naming for Model Versions

For continuous projects, use templated naming for unique, semantically meaningful versions:

```python
from zenml import Model, step, pipeline

model = Model(name="{team}_my_model", version="experiment_with_phi_3_{date}_{time}")

@step(model=model)
def llm_trainer(...) -> ...:
    ...

@pipeline(model=model, substitutions={"team": "Team_A"})
def training_pipeline(...):
    # training happens here
```

This will produce a runtime-evaluated model version name, e.g., `experiment_with_phi_3_2024_08_30_12_42_53`.

### Standard Substitutions
- `{date}`: current date (e.g., `2024_11_27`)
- `{time}`: current UTC time (e.g., `11_07_09_326492`)

## Fetching Model Versions by Stage

Assign stages to model versions (e.g., `production`) for semantic retrieval:

```shell
zenml model version update MODEL_NAME --stage=STAGE
```

To fetch a model version by stage:

```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="production")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training happens here
```

## Autonumbering of Versions

ZenML automatically numbers model versions. If no version is specified, a new version is generated:

```python
from zenml import Model, step

model = Model(name="my_model", version="even_better_version")

@step(model=model)
def svc_trainer(...) -> ...:
    ...
```

ZenML tracks the version sequence:

```python
from zenml import Model

earlier_version = Model(name="my_model", version="really_good_version").number  # == 5
updated_version = Model(name="my_model", version="even_better_version").number  # == 6
```

================================================================================

# Use the Model Control Plane

A `Model` in ZenML is an entity that consolidates pipelines, artifacts, metadata, and business data, encapsulating your ML product's logic. It can be viewed as a "project" or "workspace." 

**Key Points:**
- The technical model (model files with weights and parameters) is a primary artifact associated with a ZenML Model, but training data and production predictions are also included.
- Models are first-class citizens in ZenML, accessible via the ZenML API, client, and [ZenML Pro](https://zenml.io/pro) dashboard.
- Models capture lineage information and support version staging, allowing for business rule-based promotion of model versions.
- The Model Control Plane provides a unified interface for managing models, integrating pipelines, artifacts, and technical models.

For a complete example, refer to the [starter guide](../../../user-guide/starter-guide/track-ml-models.md).

================================================================================

# Associate a Pipeline with a Model

To associate a pipeline with a model in ZenML, use the following code:

```python
from zenml import pipeline
from zenml import Model
from zenml.enums import ModelStages

@pipeline(
    model=Model(
        name="ClassificationModel",  # Unique model name
        tags=["MVP", "Tabular"],      # Tags for filtering
        version=ModelStages.LATEST     # Specify model version
    )
)
def my_pipeline():
    ...
```

If the model exists, a new version will be created. To attach the pipeline to an existing model version, specify it accordingly.

You can also define the model configuration in a YAML file:

```yaml
model:
  name: text_classifier
  description: A breast cancer classifier
  tags: ["classifier", "sgd"]
```

================================================================================

### Structuring an MLOps Project

#### Overview
An MLOps project typically consists of multiple pipelines, including:
- **Feature Engineering Pipeline**: Prepares raw data for training.
- **Training Pipeline**: Trains models using data from the feature engineering pipeline.
- **Inference Pipeline**: Runs batch predictions on the trained model.
- **Deployment Pipeline**: Deploys the trained model to a production endpoint.

The structure of these pipelines can vary based on project requirements, and information (artifacts, models, metadata) often needs to be shared between them.

#### Common Patterns for Artifact Exchange

**Pattern 1: Artifact Exchange via `Client`**
To exchange artifacts between pipelines, use the ZenML Client. For example, in a feature engineering and training pipeline:

```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    train_data = client.get_artifact_version(name="iris_training_dataset")
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")
    sklearn_classifier = model_trainer(train_data)
    model_evaluator(model, sklearn_classifier)
```
*Note: Artifacts are references, not materialized in memory during the pipeline function.*

**Pattern 2: Artifact Exchange via `Model`**
Using a ZenML Model as a reference can simplify exchanges. For instance, in a `train_and_promote` and `do_predictions` pipeline:

```python
from zenml import step, get_step_context

@step(enable_cache=False)
def predict(data: pd.DataFrame) -> pd.Series:
    model = get_step_context().model.get_model_artifact("trained_model")
    return pd.Series(model.predict(data))
```

Alternatively, resolve the artifact at the pipeline level:

```python
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd

@step
def predict(model: ClassifierMixin, data: pd.DataFrame) -> pd.Series:
    return pd.Series(model.predict(data))

@pipeline(model=Model(name="iris_classifier", version=ModelStages.PRODUCTION))
def do_predictions():
    model = get_pipeline_context().model.get_model_artifact("trained_model")
    predict(model=model, data=load_data())

if __name__ == "__main__":
    do_predictions()
```

Both approaches are valid; choose based on preference.

================================================================================

# Linking Model Binaries/Data to Models

Artifacts generated during pipeline runs can be linked to models in ZenML for lineage tracking and transparency. Here are the methods to link artifacts:

## Configuring the Model at Pipeline Level

Use the `model` parameter in the `@pipeline` or `@step` decorator:

```python
from zenml import Model, pipeline

model = Model(name="my_model", version="1.0.0")

@pipeline(model=model)
def my_pipeline():
    ...
```

This links all artifacts from the pipeline run to the specified model.

## Saving Intermediate Artifacts

To save progress during long-running steps, use the `save_artifact` utility function. If the step has the Model context configured, it will be automatically linked.

```python
from zenml import step, Model
from zenml.artifacts.utils import save_artifact
import pandas as pd
from typing_extensions import Annotated
from zenml.artifacts.artifact_config import ArtifactConfig

@step(model=Model(name="MyModel", version="1.2.42"))
def trainer(trn_dataset: pd.DataFrame) -> Annotated[ClassifierMixin, ArtifactConfig("trained_model")]:
    for epoch in epochs:
        checkpoint = model.train(epoch)
        save_artifact(data=checkpoint, name="training_checkpoint", version=f"1.2.42_{epoch}")
    return model
```

## Linking Artifacts Explicitly

To link an artifact outside of a step, use the `link_artifact_to_model` function:

```python
from zenml import step, Model, link_artifact_to_model, save_artifact
from zenml.client import Client

@step
def f_() -> None:
    new_artifact = save_artifact(data="Hello, World!", name="manual_artifact")
    link_artifact_to_model(artifact_version_id=new_artifact.id, model=Model(name="MyModel", version="0.0.42"))

existing_artifact = Client().get_artifact_version(name_id_or_prefix="existing_artifact")
link_artifact_to_model(artifact_version_id=existing_artifact.id, model=Model(name="MyModel", version="0.2.42"))
```

================================================================================

# Promote a Model

## Stages and Promotion
Model stages represent the lifecycle progress of different versions in ZenML. A model version can be promoted through the Dashboard, ZenML CLI, or Python SDK. Stages include:
- `staging`: Ready for production.
- `production`: Active in production.
- `latest`: Virtual stage for the most recent version; cannot be promoted to.
- `archived`: No longer relevant.

### Promotion Methods

#### CLI
Use the following command to promote a model version:
```bash
zenml model version update iris_logistic_regression --stage=...
```

#### Cloud Dashboard
Promotion via the ZenML Pro dashboard will be available soon.

#### Python SDK
The most common method for promoting models:
```python
from zenml import Model
from zenml.enums import ModelStages

MODEL_NAME = "iris_logistic_regression"
model = Model(name=MODEL_NAME, version="1.2.3")
model.set_stage(stage=ModelStages.PRODUCTION)

latest_model = Model(name=MODEL_NAME, version=ModelStages.LATEST)
latest_model.set_stage(stage=ModelStages.STAGING)
```

In a pipeline context, retrieve the model from the step context:
```python
from zenml import get_step_context, step, pipeline
from zenml.enums import ModelStages

@step
def promote_to_staging():
    model = get_step_context().model
    model.set_stage(ModelStages.STAGING, force=True)

@pipeline
def train_and_promote_model():
    promote_to_staging(after=["train_and_evaluate"])
```

## Fetching Model Versions by Stage
Load the appropriate model version by specifying the `version`:
```python
from zenml import Model, step, pipeline

model = Model(name="my_model", version="production")

@step(model=model)
def svc_trainer(...) -> ...:
    ...

@pipeline(model=model)
def training_pipeline(...):
    # training logic
```


================================================================================

# Model Registration in ZenML

Models can be registered in several ways: explicitly via CLI or Python SDK, or implicitly during a pipeline run.

## Explicit CLI Registration
Use the following command to register a model:

```bash
zenml model register iris_logistic_regression --license=... --description=...
```
Run `zenml model register --help` for options. Tags can be added using `--tag`.

## Explicit Dashboard Registration
Users of [ZenML Pro](https://zenml.io/pro) can register models directly from the cloud dashboard.

## Explicit Python SDK Registration
Register a model using the Python SDK as follows:

```python
from zenml.client import Client

Client().create_model(
    name="iris_logistic_regression",
    license="Copyright (c) ZenML GmbH 2023",
    description="Logistic regression model trained on the Iris dataset.",
    tags=["regression", "sklearn", "iris"],
)
```

## Implicit Registration by ZenML
Models can be registered implicitly during a pipeline run by specifying a `Model` object in the `@pipeline` decorator:

```python
from zenml import pipeline, Model

@pipeline(
    enable_cache=False,
    model=Model(
        name="demo",
        license="Apache",
        description="Showcase Model Control Plane.",
    ),
)
def train_and_promote_model():
    ...
```

Running this pipeline creates a new model version linked to the artifacts.

================================================================================

# Loading a ZenML Model

## Load the Active Model in a Pipeline
You can load the active model to access its metadata and associated artifacts:

```python
from zenml import step, pipeline, get_step_context, Model

@pipeline(model=Model(name="my_model"))
def my_pipeline():
    ...

@step
def my_step():
    mv = get_step_context().model  # Get model from active step context
    print(mv.run_metadata["metadata_key"].value)  # Get metadata
    output = mv.get_artifact("my_dataset", "my_version")  # Fetch artifact
    output.run_metadata["accuracy"].value
```

## Load Any Model via the Client
Alternatively, use the `Client` to load a model:

```python
from zenml import step
from zenml.client import Client
from zenml.enums import ModelStages

@step
def model_evaluator_step():
    try:
        staging_zenml_model = Client().get_model_version(
            model_name_or_id="<INSERT_MODEL_NAME>",
            model_version_name_or_number_or_id=ModelStages.STAGING,
        )
    except KeyError:
        staging_zenml_model = None
```

This documentation provides methods to load models in ZenML, either through the active pipeline context or using the Client API.

================================================================================

# Loading Artifacts from a Model

In a two-pipeline project, the first pipeline trains a model, and the second performs batch inference using the trained model artifacts. Understanding when and how to load these artifacts is crucial.

### Example Code

```python
from typing_extensions import Annotated
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd
from sklearn.base import ClassifierMixin

@step
def predict(model: ClassifierMixin, data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    return pd.Series(model.predict(data))

@pipeline(model=Model(name="iris_classifier", version=ModelStages.PRODUCTION))
def do_predictions():
    model = get_pipeline_context().model
    inference_data = load_data()
    predict(model=model.get_model_artifact("trained_model"), data=inference_data)

if __name__ == "__main__":
    do_predictions()
```

### Key Points

- Use `get_pipeline_context().model` to access the model context during pipeline execution.
- Model versioning is dynamic; the `Production` version may change before execution.
- Artifact loading occurs during step execution, allowing for delayed materialization.

### Alternative Code Using Client

```python
from zenml.client import Client

@pipeline
def do_predictions():
    model = Client().get_model_version("iris_classifier", ModelStages.PRODUCTION)
    inference_data = load_data()
    predict(model=model.get_model_artifact("trained_model"), data=inference_data)
```

In this version, artifact evaluation happens at runtime.

================================================================================

# Delete a Model

Deleting a model or its specific version removes all links to artifacts, pipeline runs, and associated metadata.

## Deleting All Versions of a Model

### CLI
```shell
zenml model delete <MODEL_NAME>
```

### Python SDK
```python
from zenml.client import Client

Client().delete_model(<MODEL_NAME>)
```

## Delete a Specific Version of a Model

### CLI
```shell
zenml model version delete <MODEL_VERSION_NAME>
```

### Python SDK
```python
from zenml.client import Client

Client().delete_model_version(<MODEL_VERSION_ID>)
```

================================================================================

# Contribute to ZenML

Thank you for considering contributing to ZenML! We welcome contributions such as new features, documentation improvements, integrations, or bug reports. 

For detailed guidelines on contributing, including best practices and conventions, please refer to the [ZenML contribution guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).

================================================================================

# Creating an External Integration for ZenML

ZenML aims to organize the MLOps landscape by providing numerous integrations with popular tools and allowing users to implement custom stack components. This guide outlines how to contribute your integration to ZenML.

### Step 1: Plan Your Integration
Identify the categories your integration belongs to from the [ZenML categories list](../../component-guide/README.md). Note that an integration can belong to multiple categories (e.g., cloud integrations like AWS/GCP/Azure).

### Step 2: Create Stack Component Flavors
Develop individual stack component flavors based on the selected categories. Test them as custom flavors before packaging. For example, to register a custom orchestrator flavor:

```shell
zenml orchestrator flavor register flavors.my_flavor.MyOrchestratorFlavor
```

Ensure ZenML is initialized at the root of your repository to avoid resolution issues.

List available flavors:

```shell
zenml orchestrator flavor list
```

Refer to the [extensibility documentation](../../component-guide/README.md) for more details.

### Step 3: Create an Integration Class
Once flavors are ready, package them into your integration:

1. **Clone the ZenML Repository**: Follow the [contributing guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md) to set up your environment.
   
2. **Create Integration Directory**: Structure your integration in `src/zenml/integrations/<example-integration>/` as follows:

```
/src/zenml/integrations/
    <example-integration>/
        ├── artifact-stores/
        ├── flavors/
        └── __init__.py
```

3. **Define Integration Name**: In `zenml/integrations/constants.py`, add:

```python
EXAMPLE_INTEGRATION = "<name-of-integration>"
```

4. **Create Integration Class**: In `src/zenml/integrations/<YOUR_INTEGRATION>/__init__.py`:

```python
from zenml.integrations.constants import EXAMPLE_INTEGRATION
from zenml.integrations.integration import Integration
from zenml.stack import Flavor

class ExampleIntegration(Integration):
    NAME = EXAMPLE_INTEGRATION
    REQUIREMENTS = ["<INSERT PYTHON REQUIREMENTS HERE>"]

    @classmethod
    def flavors(cls):
        from zenml.integrations.<example_flavor> import ExampleFlavor
        return [ExampleFlavor]

ExampleIntegration.check_installation()
```

Refer to the [MLflow Integration](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/mlflow/__init__.py) for an example.

5. **Import the Integration**: Ensure it is imported in `src/zenml/integrations/__init__.py`.

### Step 4: Create a PR
Submit a [pull request](https://github.com/zenml-io/zenml/compare) to ZenML for review. Thank you for your contribution!

================================================================================

# Data and Artifact Management

This section addresses the management of data and artifacts in ZenML. It includes key processes and best practices for handling these components effectively.

================================================================================

### Skip Materialization of Artifacts

**Unmaterialized Artifacts**  
In ZenML, a pipeline's steps are interconnected through their inputs and outputs, which are managed by **materializers**. Materializers handle the serialization and deserialization of artifacts stored in the artifact store. 

However, there are cases where you may want to **skip materialization** and use a reference to the artifact instead. Note that this may affect downstream tasks that depend on materialized artifacts; use this approach cautiously.

**How to Skip Materialization**  
To utilize an unmaterialized artifact, use `zenml.materializers.UnmaterializedArtifact`, which provides a `uri` property pointing to the artifact's storage path. Specify `UnmaterializedArtifact` as the type in your step:

```python
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import step

@step
def my_step(my_artifact: UnmaterializedArtifact):
    pass
```

**Code Example**  
The following pipeline demonstrates the use of unmaterialized artifacts:

```python
from typing_extensions import Annotated
from typing import Dict, List, Tuple
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import pipeline, step

@step
def step_1() -> Tuple[Annotated[Dict[str, str], "dict_"], Annotated[List[str], "list_"]]:
    return {"some": "data"}, []

@step
def step_2() -> Tuple[Annotated[Dict[str, str], "dict_"], Annotated[List[str], "list_"]]:
    return {"some": "data"}, []

@step
def step_3(dict_: Dict, list_: List) -> None:
    assert isinstance(dict_, dict)
    assert isinstance(list_, list)

@step
def step_4(dict_: UnmaterializedArtifact, list_: UnmaterializedArtifact) -> None:
    print(dict_.uri)
    print(list_.uri)

@pipeline
def example_pipeline():
    step_3(*step_1())
    step_4(*step_2())

example_pipeline()
```

This pipeline shows `s3` consuming materialized artifacts and `s4` consuming unmaterialized artifacts, allowing direct access to their URIs.

================================================================================

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

================================================================================

# Register Existing Data as a ZenML Artifact

## Overview
Register external data (folders or files) as ZenML artifacts for future use without materializing them.

## Register Existing Folder as a ZenML Artifact
To register a folder:

```python
import os
from uuid import uuid4
from pathlib import Path
from zenml.client import Client
from zenml import register_artifact

prefix = Client().active_stack.artifact_store.path
folder_path = os.path.join(prefix, f"my_test_folder_{uuid4()}")
os.mkdir(folder_path)
with open(os.path.join(folder_path, "test_file.txt"), "w") as f:
    f.write("test")

register_artifact(folder_path, name="my_folder_artifact")

# Load and verify the artifact
loaded_folder = Client().get_artifact_version("my_folder_artifact").load()
assert isinstance(loaded_folder, Path) and os.path.isdir(loaded_folder)
with open(os.path.join(loaded_folder, "test_file.txt"), "r") as f:
    assert f.read() == "test"
```

## Register Existing File as a ZenML Artifact
To register a file:

```python
import os
from uuid import uuid4
from pathlib import Path
from zenml.client import Client
from zenml import register_artifact

prefix = Client().active_stack.artifact_store.path
file_path = os.path.join(prefix, f"my_test_folder_{uuid4()}", "test_file.txt")
os.makedirs(os.path.dirname(file_path), exist_ok=True)
with open(file_path, "w") as f:
    f.write("test")

register_artifact(file_path, name="my_file_artifact")

# Load and verify the artifact
loaded_file = Client().get_artifact_version("my_file_artifact").load()
assert isinstance(loaded_file, Path) and not os.path.isdir(loaded_file)
with open(loaded_file, "r") as f:
    assert f.read() == "test"
```

## Register Checkpoints of a Pytorch Lightning Training Run
To register checkpoints during training:

```python
from zenml.client import Client
from zenml import register_artifact
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from uuid import uuid4

prefix = Client().active_stack.artifact_store.path
root_dir = os.path.join(prefix, uuid4().hex)

trainer = Trainer(
    default_root_dir=root_dir,
    callbacks=[ModelCheckpoint(every_n_epochs=1, save_top_k=-1)]
)
trainer.fit(model)

register_artifact(root_dir, name="all_my_model_checkpoints")
```

## Custom Checkpoint Callback
To register checkpoints as separate artifact versions:

```python
from zenml.client import Client
from zenml import register_artifact
from zenml import get_step_context
from zenml.exceptions import StepContextError
from pytorch_lightning.callbacks import ModelCheckpoint

class ZenMLModelCheckpoint(ModelCheckpoint):
    def __init__(self, artifact_name: str, *args, **kwargs):
        try:
            zenml_model = get_step_context().model
        except StepContextError:
            raise RuntimeError("Can only be called from within a step.")
        self.artifact_name = artifact_name
        self.default_root_dir = os.path.join(Client().active_stack.artifact_store.path, str(zenml_model.version))
        super().__init__(*args, **kwargs)

    def on_train_epoch_end(self, trainer, pl_module):
        super().on_train_epoch_end(trainer, pl_module)
        register_artifact(os.path.join(self.dirpath, self.filename_format.format(epoch=trainer.current_epoch)), self.artifact_name)
```

## Example Pipeline with Pytorch Lightning
A complete example of a training pipeline with checkpoints:

```python
from zenml import step, pipeline
from torch.utils.data import DataLoader
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from pytorch_lightning import Trainer, LightningModule

@step
def get_data() -> DataLoader:
    dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())
    return DataLoader(dataset)

@step
def get_model() -> LightningModule:
    # Define and return the model
    pass

@step
def train_model(model: LightningModule, train_loader: DataLoader, epochs: int, artifact_name: str):
    chkpt_cb = ZenMLModelCheckpoint(artifact_name=artifact_name)
    trainer = Trainer(default_root_dir=chkpt_cb.default_root_dir, max_epochs=epochs, callbacks=[chkpt_cb])
    trainer.fit(model, train_loader)

@pipeline
def train_pipeline(artifact_name: str = "my_model_ckpts"):
    train_loader = get_data()
    model = get_model()
    train_model(model, train_loader, 10, artifact_name)

if __name__ == "__main__":
    train_pipeline()
```

This concise documentation provides essential information on registering external data and managing artifacts in ZenML, particularly for Pytorch Lightning training runs.

================================================================================

# Custom Dataset Classes and Complex Data Flows in ZenML

## Overview
Custom Dataset classes in ZenML encapsulate data loading, processing, and saving logic for various data sources, aiding in managing complex data flows in machine learning projects.

### Use Cases
- Handling multiple data sources (CSV, databases, cloud storage)
- Managing complex data structures
- Implementing custom data processing

## Implementing Dataset Classes

### Base Dataset Class
```python
from abc import ABC, abstractmethod
import pandas as pd
from google.cloud import bigquery
from typing import Optional

class Dataset(ABC):
    @abstractmethod
    def read_data(self) -> pd.DataFrame:
        pass
```

### CSV Dataset Implementation
```python
class CSVDataset(Dataset):
    def __init__(self, data_path: str, df: Optional[pd.DataFrame] = None):
        self.data_path = data_path
        self.df = df

    def read_data(self) -> pd.DataFrame:
        if self.df is None:
            self.df = pd.read_csv(self.data_path)
        return self.df
```

### BigQuery Dataset Implementation
```python
class BigQueryDataset(Dataset):
    def __init__(self, table_id: str, project: Optional[str] = None):
        self.table_id = table_id
        self.project = project
        self.client = bigquery.Client(project=self.project)

    def read_data(self) -> pd.DataFrame:
        return self.client.query(f"SELECT * FROM `{self.table_id}`").to_dataframe()

    def write_data(self) -> None:
        self.client.load_table_from_dataframe(self.df, self.table_id, job_config=bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")).result()
```

## Creating Custom Materializers
Custom Materializers handle serialization and deserialization of artifacts.

### CSV Materializer
```python
class CSVDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (CSVDataset,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[CSVDataset]) -> CSVDataset:
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            with fileio.open(os.path.join(self.uri, "data.csv"), "rb") as source_file:
                temp_file.write(source_file.read())
        dataset = CSVDataset(temp_file.name)
        dataset.read_data()
        return dataset

    def save(self, dataset: CSVDataset) -> None:
        df = dataset.read_data()
        df.to_csv(temp_file.name, index=False)
        with open(temp_file.name, "rb") as source_file:
            with fileio.open(os.path.join(self.uri, "data.csv"), "wb") as target_file:
                target_file.write(source_file.read())
```

### BigQuery Materializer
```python
class BigQueryDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (BigQueryDataset,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[BigQueryDataset]) -> BigQueryDataset:
        with fileio.open(os.path.join(self.uri, "metadata.json"), "r") as f:
            metadata = json.load(f)
        return BigQueryDataset(metadata["table_id"], metadata["project"])

    def save(self, bq_dataset: BigQueryDataset) -> None:
        with fileio.open(os.path.join(self.uri, "metadata.json"), "w") as f:
            json.dump({"table_id": bq_dataset.table_id, "project": bq_dataset.project}, f)
        if bq_dataset.df is not None:
            bq_dataset.write_data()
```

## Pipeline Management
Design flexible pipelines for multiple data sources.

### Example Pipeline
```python
@step(output_materializer=CSVDatasetMaterializer)
def extract_data_local(data_path: str = "data/raw_data.csv") -> CSVDataset:
    return CSVDataset(data_path)

@step(output_materializer=BigQueryDatasetMaterializer)
def extract_data_remote(table_id: str) -> BigQueryDataset:
    return BigQueryDataset(table_id)

@step
def transform(dataset: Dataset) -> pd.DataFrame:
    return dataset.read_data().copy()  # Apply transformations here

@pipeline
def etl_pipeline(mode: str = "develop"):
    raw_data = extract_data_local() if mode == "develop" else extract_data_remote(table_id="project.dataset.raw_table")
    return transform(raw_data)
```

## Best Practices
1. **Common Base Class**: Use the `Dataset` base class for consistent handling.
2. **Specialized Steps**: Create separate steps for loading different datasets.
3. **Flexible Pipelines**: Use parameters or conditional logic to adapt to data sources.
4. **Modular Design**: Create steps for specific tasks to promote code reuse.

By following these practices, you can build adaptable ZenML pipelines that efficiently manage complex data flows and multiple data sources. For scaling strategies, refer to [scaling strategies for big data](manage-big-data.md).

================================================================================

# Scaling Strategies for Big Data in ZenML

## Dataset Size Thresholds
1. **Small datasets (up to a few GB)**: Handled in-memory with pandas.
2. **Medium datasets (up to tens of GB)**: Require chunking or out-of-core processing.
3. **Large datasets (hundreds of GB or more)**: Necessitate distributed processing frameworks.

## Strategies for Small Datasets
1. **Efficient Data Formats**: Use Parquet instead of CSV.
   ```python
   import pyarrow.parquet as pq

   class ParquetDataset(Dataset):
       def read_data(self) -> pd.DataFrame:
           return pq.read_table(self.data_path).to_pandas()

       def write_data(self, df: pd.DataFrame):
           pq.write_table(pa.Table.from_pandas(df), self.data_path)
   ```

2. **Data Sampling**:
   ```python
   class SampleableDataset(Dataset):
       def sample_data(self, fraction: float = 0.1) -> pd.DataFrame:
           return self.read_data().sample(frac=fraction)

   @step
   def analyze_sample(dataset: SampleableDataset) -> Dict[str, float]:
       sample = dataset.sample_data()
       return {"mean": sample["value"].mean(), "std": sample["value"].std()}
   ```

3. **Optimize Pandas Operations**:
   ```python
   @step
   def optimize_processing(df: pd.DataFrame) -> pd.DataFrame:
       df['new_column'] = df['column1'] + df['column2']
       df['mean_normalized'] = df['value'] - np.mean(df['value'])
       return df
   ```

## Handling Medium Datasets
### Chunking for CSV Datasets
```python
class ChunkedCSVDataset(Dataset):
    def read_data(self):
        for chunk in pd.read_csv(self.data_path, chunksize=self.chunk_size):
            yield chunk

@step
def process_chunked_csv(dataset: ChunkedCSVDataset) -> pd.DataFrame:
    return pd.concat(process_chunk(chunk) for chunk in dataset.read_data())
```

### Data Warehouses
Utilize data warehouses like Google BigQuery:
```python
@step
def process_big_query_data(dataset: BigQueryDataset) -> BigQueryDataset:
    client = bigquery.Client()
    query = f"SELECT column1, AVG(column2) as avg_column2 FROM `{dataset.table_id}` GROUP BY column1"
    job_config = bigquery.QueryJobConfig(destination=f"{dataset.project}.{dataset.dataset}.processed_data")
    client.query(query, job_config=job_config).result()
    return BigQueryDataset(table_id=result_table_id)
```

## Approaches for Very Large Datasets
### Using Apache Spark
```python
from pyspark.sql import SparkSession

@step
def process_with_spark(input_data: str) -> None:
    spark = SparkSession.builder.appName("ZenMLSparkStep").getOrCreate()
    df = spark.read.csv(input_data, header=True)
    df.groupBy("column1").agg({"column2": "mean"}).write.csv("output_path", header=True)
    spark.stop()
```

### Using Ray
```python
import ray

@step
def process_with_ray(input_data: str) -> None:
    ray.init()
    results = ray.get([process_partition.remote(part) for part in split_data(load_data(input_data))])
    save_results(combine_results(results), "output_path")
    ray.shutdown()
```

### Using Dask
```python
import dask.dataframe as dd

@step
def create_dask_dataframe():
    return dd.from_pandas(pd.DataFrame({'A': range(1000), 'B': range(1000, 2000)}), npartitions=4)

@step
def process_dask_dataframe(df: dd.DataFrame) -> dd.DataFrame:
    return df.map_partitions(lambda x: x ** 2)

@pipeline
def dask_pipeline():
    df = create_dask_dataframe()
    compute_result(process_dask_dataframe(df))
```

### Using Numba
```python
from numba import jit

@jit(nopython=True)
def numba_function(x):
    return x * x + 2 * x - 1

@step
def apply_numba_function(data: np.ndarray) -> np.ndarray:
    return numba_function(data)
```

## Important Considerations
1. **Environment Setup**: Ensure necessary frameworks are installed.
2. **Resource Management**: Coordinate resource allocation with ZenML.
3. **Error Handling**: Implement proper error handling.
4. **Data I/O**: Use intermediate storage for large datasets.
5. **Scaling**: Ensure infrastructure supports computation scale.

## Choosing the Right Scaling Strategy
- **Dataset size**: Start simple and scale as needed.
- **Processing complexity**: Use appropriate tools for the task.
- **Infrastructure**: Ensure compute resources are adequate.
- **Update frequency**: Consider how often data changes.
- **Team expertise**: Choose familiar technologies.

By applying these strategies, you can efficiently manage large datasets in ZenML. For more details on custom Dataset classes, refer to [custom dataset classes](datasets.md).

================================================================================

### Structuring an MLOps Project

MLOps projects consist of multiple pipelines, such as:
- **Feature Engineering Pipeline**: Prepares raw data for training.
- **Training Pipeline**: Trains models using data from the feature engineering pipeline.
- **Inference Pipeline**: Runs predictions on trained models.
- **Deployment Pipeline**: Deploys models to production.

The structure of these pipelines can vary based on project requirements, but sharing artifacts (models, metadata) between them is essential.

#### Pattern 1: Artifact Exchange via `Client`

In this pattern, the ZenML Client facilitates the exchange of datasets between pipelines. For example:

```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    train_data = client.get_artifact_version(name="iris_training_dataset")
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")
    model_evaluator(model_trainer(train_data))
```

**Note**: Artifacts are referenced, not materialized in memory during the pipeline function.

#### Pattern 2: Artifact Exchange via `Model`

This pattern uses ZenML Model as a reference point. For instance, in a `train_and_promote` pipeline, models are promoted based on accuracy, and the `do_predictions` pipeline uses the latest promoted model without needing artifact IDs.

Example code for the `do_predictions` pipeline:

```python
from zenml import step, get_step_context

@step(enable_cache=False)
def predict(data: pd.DataFrame) -> pd.Series:
    model = get_step_context().model.get_model_artifact("trained_model")
    return pd.Series(model.predict(data))
```

To avoid unexpected results from caching, you can disable caching or resolve artifacts at the pipeline level:

```python
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd

@step
def predict(model: ClassifierMixin, data: pd.DataFrame) -> pd.Series:
    return pd.Series(model.predict(data))

@pipeline(model=Model(name="iris_classifier", version=ModelStages.PRODUCTION))
def do_predictions():
    model = get_pipeline_context().model.get_model_artifact("trained_model")
    predict(model=model, data=load_data())

if __name__ == "__main__":
    do_predictions()
```

Choose the approach based on your project needs.

================================================================================

### Types of Visualizations in ZenML

ZenML automatically saves visualizations for various data types, accessible via the ZenML dashboard or Jupyter notebooks using the `artifact.visualize()` method.

**Default Visualizations Include:**
- Statistical representation of a [Pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html) as a PNG image.
- Drift detection reports from [Evidently](../../../component-guide/data-validators/evidently.md), [Great Expectations](../../../component-guide/data-validators/great-expectations.md), and [whylogs](../../../component-guide/data-validators/whylogs.md).
- A [Hugging Face](https://zenml.io/integrations/huggingface) datasets viewer embedded as an HTML iframe.

![ZenML Artifact Visualizations](../../../.gitbook/assets/artifact_visualization_dashboard.png)  
![output.visualize() Output](../../../.gitbook/assets/artifact_visualization_evidently.png)  
![Hugging Face datasets viewer](../../../.gitbook/assets/artifact_visualization_huggingface.gif)

================================================================================

--- icon: chart-scatter description: Configuring ZenML for data visualizations in the dashboard. --- 

# Visualize Artifacts

ZenML allows easy association of visualizations with data and artifacts. 

![ZenML Artifact Visualizations](../../../.gitbook/assets/artifact_visualization_dashboard.png) 

<figure>
  <img src="https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc" alt="ZenML Scarf">
</figure>

================================================================================

# Creating Custom Visualizations in ZenML

ZenML supports several visualization types for artifacts:

- **HTML:** Embedded HTML visualizations (e.g., data validation reports)
- **Image:** Visualizations of image data (e.g., Pillow images)
- **CSV:** Tables (e.g., pandas DataFrame `.describe()` output)
- **Markdown:** Markdown strings or pages
- **JSON:** JSON strings or objects

## Adding Custom Visualizations

You can add custom visualizations in three ways:

1. **Special Return Types:** Cast HTML, Markdown, CSV, or JSON data to specific types in your step.
2. **Custom Materializers:** Define visualization logic for specific data types by overriding the `save_visualizations()` method.
3. **Custom Return Types:** Create a custom class and materializer for any other visualizations.

### Visualization via Special Return Types

Return visualizations by casting data to the following types:

- `zenml.types.HTMLString`
- `zenml.types.MarkdownString`
- `zenml.types.CSVString`
- `zenml.types.JSONString`

**Example:**

```python
from zenml.types import CSVString

@step
def my_step() -> CSVString:
    return CSVString("a,b,c\n1,2,3")
```

### Visualization via Materializers

To visualize artifacts automatically, override the `save_visualizations()` method in a custom materializer. More details can be found in the [materializer docs](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md#optional-how-to-visualize-the-artifact).

### Creating a Custom Visualization

To create a custom visualization:

1. Define a **custom class** for the data.
2. Implement a **custom materializer** with visualization logic.
3. Return the custom class from your ZenML steps.

**Example: Facets Data Skew Visualization**

1. **Custom Class:**

```python
class FacetsComparison(BaseModel):
    datasets: List[Dict[str, Union[str, pd.DataFrame]]]
```

2. **Materializer:**

```python
class FacetsMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (FacetsComparison,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS

    def save_visualizations(self, data: FacetsComparison) -> Dict[str, VisualizationType]:
        html = ...  # Create visualization
        visualization_path = os.path.join(self.uri, VISUALIZATION_FILENAME)
        with fileio.open(visualization_path, "w") as f:
            f.write(html)
        return {visualization_path: VisualizationType.HTML}
```

3. **Step:**

```python
@step
def facets_visualization_step(reference: pd.DataFrame, comparison: pd.DataFrame) -> FacetsComparison:
    return FacetsComparison(datasets=[{"name": "reference", "table": reference}, {"name": "comparison", "table": comparison}])
```

### Workflow

When `facets_visualization_step` is executed:

1. It creates and returns a `FacetsComparison`.
2. ZenML finds the `FacetsMaterializer`, calls `save_visualizations()`, and saves the visualization as an HTML file.
3. The visualization is displayed in the dashboard when the artifact is accessed.

================================================================================

### Disabling Visualizations

To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level:

```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

================================================================================

### Displaying Visualizations in the Dashboard

To display visualizations on the ZenML dashboard, the following steps are necessary:

#### Configuring a Service Connector
Visualizations are stored in the [artifact store](../../../component-guide/artifact-stores/artifact-stores.md). To view them on the dashboard, the ZenML server must have access to this store. Refer to the [service connector](../../infrastructure-deployment/auth-management/README.md) documentation for configuration details. For an example, see the [AWS S3](../../../component-guide/artifact-stores/s3.md) documentation.

> **Note:** When using the default/local artifact store with a deployed ZenML, the server cannot access local files, and visualizations will not display. Use a service connector and a remote artifact store to view visualizations.

#### Configuring Artifact Stores
If visualizations from a pipeline run are missing, check if the ZenML server has the necessary dependencies or permissions for the artifact store. For more details, see the [custom artifact store documentation](../../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores).

================================================================================

### Summary of ZenML Step Outputs and Pipeline

Step outputs in ZenML are stored in an artifact store, enabling caching, lineage, and auditability. Using type annotations enhances transparency, facilitates data passing between steps, and allows for serialization/deserialization (materialization).

#### Code Example

```python
@step
def load_data(parameter: int) -> Dict[str, Any]:
    training_data = [[1, 2], [3, 4], [5, 6]]
    labels = [0, 1, 0]
    return {'features': training_data, 'labels': labels}

@step
def train_model(data: Dict[str, Any]) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}")

@pipeline  
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter)
    train_model(dataset)
```

### Key Points
- **Steps**: `load_data` returns training data and labels; `train_model` processes this data.
- **Pipeline**: `simple_ml_pipeline` chains the steps, demonstrating data flow in ZenML.

================================================================================

### ZenML Artifact Naming Overview

In ZenML, artifact naming is crucial for managing outputs from pipeline steps, especially when reusing steps with different inputs. ZenML employs type annotations to determine artifact names, incrementing version numbers for artifacts with the same name. It supports both static and dynamic naming strategies.

#### Naming Strategies

1. **Static Naming**: Defined as string literals.
   ```python
   @step
   def static_single() -> Annotated[str, "static_output_name"]:
       return "null"
   ```

2. **Dynamic Naming**:
   - **Using Standard Placeholders**:
     ```python
     @step
     def dynamic_single_string() -> Annotated[str, "name_{date}_{time}"]:
         return "null"
     ```
     Placeholders:
     - `{date}`: Current date (e.g., `2024_11_18`)
     - `{time}`: Current time (e.g., `11_07_09_326492`)

   - **Using Custom Placeholders**:
     ```python
     @step(substitutions={"custom_placeholder": "some_substitute"})
     def dynamic_single_string() -> Annotated[str, "name_{custom_placeholder}_{time}"]:
         return "null"
     ```

   - **Dynamic Redefinition with `with_options`**:
     ```python
     @step
     def extract_data(source: str) -> Annotated[str, "{stage}_dataset"]:
         return "my data"

     @pipeline
     def extraction_pipeline():
         extract_data.with_options(substitutions={"stage": "train"})(source="s3://train")
         extract_data.with_options(substitutions={"stage": "test"})(source="s3://test")
     ```

#### Multiple Output Handling
Combine naming options for multiple artifacts:
```python
@step
def mixed_tuple() -> Tuple[
    Annotated[str, "static_output_name"],
    Annotated[str, "name_{date}_{time}"],
]:
    return "static_namer", "str_namer"
```

#### Caching Behavior
When caching is enabled, output artifact names remain consistent across runs:
```python
@step(substitutions={"custom_placeholder": "resolution"})
def demo() -> Tuple[
    Annotated[int, "name_{date}_{time}"],
    Annotated[int, "name_{custom_placeholder}"],
]:
    return 42, 43

@pipeline
def my_pipeline():
    demo()

if __name__ == "__main__":
    run_without_cache = my_pipeline.with_options(enable_cache=False)()
    run_with_cache = my_pipeline.with_options(enable_cache=True)()

    assert set(run_without_cache.steps["demo"].outputs.keys()) == set(
        run_with_cache.steps["demo"].outputs.keys()
    )
```

### Summary
ZenML provides flexible artifact naming through static and dynamic strategies, utilizing placeholders for customization. Caching maintains consistent artifact names across runs, aiding in output management.

================================================================================

# Loading Artifacts into Memory

ZenML pipeline steps typically consume artifacts from one another, but external data may also be required. For external artifacts, use [ExternalArtifact](../../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline). For data exchange between ZenML pipelines, late materialization is essential, allowing the use of not-yet-existing artifacts as step inputs.

## Use Cases for Artifact Exchange
1. Grouping data products using ZenML Models.
2. Using [ZenML Client](../../../reference/python-client.md#client-methods) for data integration.

**Recommendation:** Use models for artifact access across pipelines. Learn to load artifacts from a ZenML Model [here](../../model-management-metrics/model-control-plane/load-artifacts-from-model.md).

## Client Methods for Artifact Exchange
If not using the Model Control Plane, late materialization can still facilitate data exchange. Here’s a revised version of the `do_predictions` pipeline:

```python
from typing import Annotated
from zenml import step, pipeline
from zenml.client import Client
import pandas as pd
from sklearn.base import ClassifierMixin

@step
def predict(model1: ClassifierMixin, model2: ClassifierMixin, model1_metric: float, model2_metric: float, data: pd.DataFrame) -> Annotated[pd.Series, "predictions"]:
    predictions = pd.Series(model1.predict(data)) if model1_metric < model2_metric else pd.Series(model2.predict(data))
    return predictions

@step
def load_data() -> pd.DataFrame:
    ...

@pipeline
def do_predictions():
    model_42 = Client().get_artifact_version("trained_model", version="42")
    metric_42 = model_42.run_metadata["MSE"].value
    model_latest = Client().get_artifact_version("trained_model")
    metric_latest = model_latest.run_metadata["MSE"].value

    inference_data = load_data()
    predict(model1=model_42, model2=model_latest, model1_metric=metric_42, model2_metric=metric_latest, data=inference_data)

if __name__ == "__main__":
    do_predictions()
```

In this code, the `predict` step compares models based on MSE, ensuring predictions are made with the best-performing model. The `load_data` step loads inference data, and artifact retrieval occurs at execution time, ensuring the latest versions are used.

================================================================================

# How ZenML Stores Data

ZenML integrates data versioning and lineage into its core functionality. Each pipeline run generates automatically tracked artifacts, allowing users to view the lineage and interact with artifacts via a dashboard. Key features include artifact management, caching, lineage tracking, and visualization, which enhance insights, streamline experimentation, and ensure reproducibility in machine learning workflows.

## Artifact Creation and Caching

During a pipeline run, ZenML checks for changes in inputs, outputs, parameters, or configurations. Each step creates a new directory in the artifact store. If a step is modified, a new directory structure with a unique ID is created; otherwise, ZenML may cache the step to save time and resources. This caching allows users to focus on experimenting without rerunning unchanged pipeline parts.

ZenML enables tracing artifacts back to their origins, providing insights into data processing and transformations, which is crucial for reproducibility and identifying pipeline issues. For artifact versioning and configuration, refer to the [documentation](../../../user-guide/starter-guide/manage-artifacts.md).

## Saving and Loading Artifacts with Materializers

Materializers handle the serialization and deserialization of artifacts, ensuring consistent storage and retrieval from the artifact store. Each materializer stores data in unique directories. ZenML offers built-in materializers for common data types and uses `cloudpickle` for objects without a default materializer.

Custom materializers can be created by extending the `BaseMaterializer` class. Note that the built-in `CloudpickleMaterializer` is not production-ready due to compatibility issues across Python versions and potential security risks. For robust artifact storage, consider building a custom materializer.

When a pipeline runs, ZenML uses materializers to save and load artifacts through the `fileio` system, simplifying interactions with various data formats and enabling artifact caching and lineage tracking. An example of a default materializer (the `numpy` materializer) can be found [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy_materializer.py).

================================================================================

# Organizing Data with Tags in ZenML

ZenML allows you to use tags to organize and filter your machine learning artifacts and models, enhancing workflow and discoverability.

## Assigning Tags to Artifacts

To tag artifact versions of a step or pipeline, use the `tags` property of `ArtifactConfig`:

### Python SDK
```python
from zenml import step, ArtifactConfig

@step
def training_data_loader() -> Annotated[pd.DataFrame, ArtifactConfig(tags=["sklearn", "pre-training"])]:
    ...
```

### CLI
```shell
# Tag the artifact
zenml artifacts update iris_dataset -t sklearn

# Tag the artifact version
zenml artifacts versions update iris_dataset raw_2023 -t sklearn
```

Tags like `sklearn` and `pre-training` will be assigned to all artifacts created by this step. ZenML Pro users can tag artifacts directly in the cloud dashboard.

## Assigning Tags to Models

You can also tag models for semantic organization. Tags can be specified as key-value pairs when creating a model version.

### Model Creation with Tags
```python
from zenml.models import Model

model = Model(name="iris_classifier", version="1.0.0", tags=["experiment", "v1", "classification-task"])

@pipeline(model=model)
def my_pipeline(...):
    ...
```

### Creating or Updating Models with Tags
```python
from zenml.client import Client

# Create a new model with tags
Client().create_model(name="iris_logistic_regression", tags=["classification", "iris-dataset"])

# Create a new model version with tags
Client().create_model_version(model_name_or_id="iris_logistic_regression", name="2", tags=["version-1", "experiment-42"])
```

### Adding Tags to Existing Models via CLI
```shell
# Tag an existing model
zenml model update iris_logistic_regression --tag "classification"

# Tag a specific model version
zenml model version update iris_logistic_regression 2 --tag "experiment3"
```

This concise tagging system helps in efficiently managing and retrieving your ML assets.

================================================================================

### Summary

Artifacts can be accessed in a step without needing direct upstream connections. You can fetch artifacts from other steps or pipelines using the ZenML client.

#### Code Example
```python
from zenml.client import Client
from zenml import step

@step
def my_step():
    output = Client().get_artifact_version("my_dataset", "my_version")
    return output.run_metadata["accuracy"].value
```

This method allows you to utilize previously created artifacts stored in the artifact store.

### See Also
- [Managing artifacts](../../../user-guide/starter-guide/manage-artifacts.md) - Learn about the `ExternalArtifact` type and artifact transfer between steps.

================================================================================

### Summary: Using Materializers in ZenML

#### Overview
ZenML pipelines are data-centric, where each step reads and writes artifacts to an artifact store. **Materializers** manage how artifacts are serialized and deserialized during this process.

#### Built-In Materializers
ZenML includes several built-in materializers for common data types, which operate automatically without user intervention:

| Materializer | Handled Data Types | Storage Format |
|--------------|---------------------|----------------|
| `BuiltInMaterializer` | `bool`, `float`, `int`, `str`, `None` | `.json` |
| `BytesMaterializer` | `bytes` | `.txt` |
| `BuiltInContainerMaterializer` | `dict`, `list`, `set`, `tuple` | Directory |
| `NumpyMaterializer` | `np.ndarray` | `.npy` |
| `PandasMaterializer` | `pd.DataFrame`, `pd.Series` | `.csv` (or `.gzip` with `parquet`) |
| `PydanticMaterializer` | `pydantic.BaseModel` | `.json` |
| `ServiceMaterializer` | `zenml.services.service.BaseService` | `.json` |
| `StructuredStringMaterializer` | `zenml.types.CSVString`, `HTMLString`, `MarkdownString` | `.csv`, `.html`, `.md` |

**Warning**: The `CloudpickleMaterializer` can handle any object but is not production-ready due to compatibility issues across Python versions.

#### Integration Materializers
ZenML also offers integration-specific materializers, activated by installing the respective integration. Each materializer handles specific data types and storage formats.

#### Custom Materializers
To use a custom materializer:
1. **Define the Materializer**:
   - Subclass `BaseMaterializer`.
   - Set `ASSOCIATED_TYPES` and `ASSOCIATED_ARTIFACT_TYPE`.

   ```python
   class MyMaterializer(BaseMaterializer):
       ASSOCIATED_TYPES = (MyObj,)
       ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

       def load(self, data_type: Type[MyObj]) -> MyObj:
           # Load logic
           ...

       def save(self, my_obj: MyObj) -> None:
           # Save logic
           ...
   ```

2. **Configure Steps**:
   - Use the materializer in the step decorator or via the `configure()` method.

   ```python
   @step(output_materializers=MyMaterializer)
   def my_first_step() -> MyObj:
       return MyObj("my_object")
   ```

3. **Global Configuration**:
   - Register a materializer globally to override built-in ones.

   ```python
   materializer_registry.register_and_overwrite_type(key=pd.DataFrame, type_=FastPandasMaterializer)
   ```

#### Example of Custom Materializer
Here's a simple example of a custom materializer for a class `MyObj`:

```python
class MyObj:
    def __init__(self, name: str):
        self.name = name

class MyMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (MyObj,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[MyObj]) -> MyObj:
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'r') as f:
            return MyObj(f.read())

    def save(self, my_obj: MyObj) -> None:
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'w') as f:
            f.write(my_obj.name)

@step
def my_first_step() -> MyObj:
    return MyObj("my_object")

my_first_step.configure(output_materializers=MyMaterializer)
```

#### Important Notes
- Ensure compatibility with custom artifact stores by adjusting the materializer logic as needed.
- Use `get_temporary_directory(...)` for temporary directories in custom materializers.
- Optionally, implement visualization and metadata extraction methods in your materializer.

This concise guide covers the essential aspects of using materializers in ZenML, focusing on both built-in and custom implementations.

================================================================================

### Delete an Artifact

Artifacts cannot be deleted directly to avoid breaking the ZenML database. However, you can delete artifacts not referenced by any pipeline runs using:

```shell
zenml artifact prune
```

This command removes artifacts from the underlying [artifact store](../../../component-guide/artifact-stores/artifact-stores.md) and the database. Use the `--only-artifact` and `--only-metadata` flags to control this behavior. If you encounter errors due to local artifacts that no longer exist, add the `--ignore-errors` flag to continue pruning while still receiving warning messages in the terminal.

================================================================================

### Summary: Returning Multiple Outputs with Annotated

Use the `Annotated` type to return and name multiple outputs from a step, enhancing artifact retrieval and dashboard readability.

#### Code Example
```python
from typing import Annotated, Tuple
import pandas as pd
from zenml import step
from sklearn.model_selection import train_test_split

@step
def clean_data(data: pd.DataFrame) -> Tuple[
    Annotated[pd.DataFrame, "x_train"],
    Annotated[pd.DataFrame, "x_test"],
    Annotated[pd.Series, "y_train"],
    Annotated[pd.Series, "y_test"],
]:
    x = data.drop("target", axis=1)
    y = data["target"]
    return train_test_split(x, y, test_size=0.2, random_state=42)
```

#### Key Points
- The `clean_data` step processes a DataFrame and returns training and testing sets for features and target.
- Outputs are annotated for easy identification and display on the pipeline dashboard.

================================================================================

# Infrastructure and Deployment

This section outlines the infrastructure setup and deployment processes in ZenML. 

Key Points:
- **Infrastructure Setup**: Details on configuring cloud resources and local environments.
- **Deployment**: Guidelines for deploying ZenML pipelines, including CI/CD integration.
- **Best Practices**: Recommendations for optimizing performance and scalability.

Ensure to follow these practices for effective infrastructure management and deployment in ZenML.

================================================================================

# Custom Stack Component Flavor Guide

## Overview
ZenML allows for custom solutions in MLOps through modular stack component flavors. This guide explains how to create and use custom flavors in ZenML.

## Component Flavors
- **Component Type**: Defines functionality (e.g., `artifact_store`).
- **Flavors**: Specific implementations of component types (e.g., `local`, `s3`).

## Core Abstractions
1. **StackComponent**: Defines core functionality. Example:
    ```python
    from zenml.stack import StackComponent

    class BaseArtifactStore(StackComponent):
        @abstractmethod
        def open(self, path, mode="r"):
            pass

        @abstractmethod
        def exists(self, path):
            pass
    ```

2. **StackComponentConfig**: Configures stack component instances, separating static and dynamic configurations.
    ```python
    from zenml.stack import StackComponentConfig

    class BaseArtifactStoreConfig(StackComponentConfig):
        path: str
        SUPPORTED_SCHEMES: ClassVar[Set[str]]
    ```

3. **Flavor**: Combines the implementation and configuration, defining the flavor's name and type.
    ```python
    from zenml.enums import StackComponentType
    from zenml.stack import Flavor

    class LocalArtifactStoreFlavor(Flavor):
        @property
        def name(self) -> str:
            return "local"

        @property
        def type(self) -> StackComponentType:
            return StackComponentType.ARTIFACT_STORE

        @property
        def config_class(self) -> Type[LocalArtifactStoreConfig]:
            return LocalArtifactStoreConfig

        @property
        def implementation_class(self) -> Type[LocalArtifactStore]:
            return LocalArtifactStore
    ```

## Implementing a Custom Flavor
### Configuration Class
Define the configuration for your custom flavor:
```python
from zenml.artifact_stores import BaseArtifactStoreConfig
from zenml.utils.secret_utils import SecretField

class MyS3ArtifactStoreConfig(BaseArtifactStoreConfig):
    SUPPORTED_SCHEMES: ClassVar[Set[str]] = {"s3://"}
    key: Optional[str] = SecretField(default=None)
    secret: Optional[str] = SecretField(default=None)
    token: Optional[str] = SecretField(default=None)
    client_kwargs: Optional[Dict[str, Any]] = None
    config_kwargs: Optional[Dict[str, Any]] = None
    s3_additional_kwargs: Optional[Dict[str, Any]] = None
```

### Implementation Class
Implement the abstract methods:
```python
import s3fs
from zenml.artifact_stores import BaseArtifactStore

class MyS3ArtifactStore(BaseArtifactStore):
    _filesystem: Optional[s3fs.S3FileSystem] = None

    @property
    def filesystem(self) -> s3fs.S3FileSystem:
        if not self._filesystem:
            self._filesystem = s3fs.S3FileSystem(
                key=self.config.key,
                secret=self.config.secret,
                token=self.config.token,
                client_kwargs=self.config.client_kwargs,
                config_kwargs=self.config.config_kwargs,
                s3_additional_kwargs=self.config.s3_additional_kwargs,
            )
        return self._filesystem

    def open(self, path, mode="r"):
        return self.filesystem.open(path=path, mode=mode)

    def exists(self, path):
        return self.filesystem.exists(path=path)
```

### Flavor Class
Combine the implementation and configuration:
```python
from zenml.artifact_stores import BaseArtifactStoreFlavor

class MyS3ArtifactStoreFlavor(BaseArtifactStoreFlavor):
    @property
    def name(self):
        return 'my_s3_artifact_store'

    @property
    def implementation_class(self):
        from ... import MyS3ArtifactStore
        return MyS3ArtifactStore

    @property
    def config_class(self):
        from ... import MyS3ArtifactStoreConfig
        return MyS3ArtifactStoreConfig
```

## Registering the Flavor
Use the ZenML CLI to register your flavor:
```shell
zenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>
```

## Usage
After registration, use your custom flavor:
```shell
zenml artifact-store register <ARTIFACT_STORE_NAME> \
    --flavor=my_s3_artifact_store \
    --path='some-path'

zenml stack register <STACK_NAME> \
    --artifact-store <ARTIFACT_STORE_NAME>
```

## Best Practices
- Execute `zenml init` consistently.
- Test flavors thoroughly before production use.
- Keep code clean and well-documented.
- Refer to existing flavors for guidance.

## Additional Resources
For specific stack component types, refer to the corresponding documentation links provided in the original text.

================================================================================

### Export Stack Requirements

To export the `pip` requirements of your stack, use the following CLI command:

```bash
zenml stack export-requirements <STACK-NAME> --output-file stack_requirements.txt
pip install -r stack_requirements.txt
```

This command saves the requirements to a file and installs them.

================================================================================

# Managing Stacks & Components

## What is a Stack?
A **stack** in ZenML represents the configuration of infrastructure and tooling for pipeline execution. It consists of various components, each responsible for specific tasks, such as:
- **Container Registry**
- **Kubernetes Cluster** (orchestrator)
- **Artifact Store**
- **Experiment Tracker** (e.g., MLflow)

## Organizing Execution Environments
ZenML allows running pipelines across multiple stacks, facilitating testing in different environments:
1. Local experimentation
2. Staging in a cloud environment
3. Production deployment

**Benefits of Separate Stacks:**
- Prevents incorrect deployments (e.g., staging to production)
- Reduces costs by using less powerful resources for staging
- Controls access by limiting permissions to specific stacks

## Managing Credentials
Most stack components require credentials for infrastructure interaction. ZenML recommends using **Service Connectors** to manage these credentials securely.

### Recommended Roles
- Limit Service Connector creation to individuals with direct cloud resource access to minimize credential leaks and enable instant revocation of compromised credentials.

### Recommended Workflow
1. Designate a small group to create Service Connectors.
2. Create one connector for development/staging.
3. Create a separate connector for production to prevent accidental resource usage.

## Deploying and Managing Stacks
Deploying MLOps stacks can be complex due to:
- Tool-specific requirements (e.g., Kubernetes for Kubeflow)
- Difficulty in setting reasonable infrastructure defaults
- Need for additional installations for security
- Ensuring components have the correct permissions
- Challenges in resource cleanup post-experimentation

This section provides guidance on provisioning, configuring, and extending stacks in ZenML.

### Key Documentation Links
- [Deploy a Cloud Stack](./deploy-a-cloud-stack.md)
- [Register a Cloud Stack](./register-a-cloud-stack.md)
- [Deploy a Cloud Stack with Terraform](./deploy-a-cloud-stack-with-terraform.md)
- [Export and Install Stack Requirements](./export-stack-requirements.md)
- [Reference Secrets in Stack Configuration](./reference-secrets-in-stack-configuration.md)
- [Implement a Custom Stack Component](./implement-a-custom-stack-component.md)

================================================================================

# Deploy a Cloud Stack with a Single Click

ZenML's **stack** represents your infrastructure configuration. Traditionally, creating a stack involves deploying infrastructure and defining components, which can be complex and time-consuming. To simplify this, ZenML offers a **1-click deployment feature** that allows you to deploy infrastructure on your chosen cloud provider effortlessly.

## Getting Started

To use the 1-click deployment tool, you need a deployed ZenML instance (not a local server). Set up your instance by following the [deployment guide](../../../getting-started/deploying-zenml/README.md).

### Deployment Options

You can deploy via the **Dashboard** or **CLI**.

#### Dashboard Deployment

1. Go to the stacks page and click "+ New Stack".
2. Select "New Infrastructure".
3. Choose your cloud provider (AWS, GCP, Azure) and configure the stack.

**AWS Deployment:**
- Select region and name.
- Click "Deploy in AWS" to access CloudFormation.
- Log in to AWS, review, and create the stack.

**GCP Deployment:**
- Select region and name.
- Click "Deploy in GCP" to start a Cloud Shell session.
- Review the ZenML repository, check "Trust repo", and authenticate.
- Configure your deployment using values from the ZenML dashboard and run the provided script.

**Azure Deployment:**
- Select location and name.
- Click "Deploy in Azure" to access Cloud Shell.
- Paste the `main.tf` content and run `terraform init --upgrade` and `terraform apply`.

#### CLI Deployment

Use the following command to deploy:

```shell
zenml stack deploy -p {aws|gcp|azure}
```

### What Will Be Deployed?

**AWS:**
- S3 bucket (Artifact Store)
- ECR (Container Registry)
- CloudBuild project (Image Builder)
- IAM user/role with necessary permissions.

**GCP:**
- GCS bucket (Artifact Store)
- GCP Artifact Registry (Container Registry)
- Vertex AI and Cloud Build permissions.
- GCP Service Account with necessary permissions.

**Azure:**
- Azure Resource Group
- Azure Storage Account (Artifact Store)
- Azure Container Registry (Container Registry)
- AzureML Workspace (Orchestrator)
- Azure Service Principal with necessary permissions.

With this setup, you can start running your pipelines in a remote environment.

================================================================================

### Summary: Registering a Cloud Stack in ZenML

In ZenML, a **stack** represents your infrastructure configuration. Traditionally, creating a stack involves deploying infrastructure and defining components with authentication, which can be complex, especially remotely. The **Stack Wizard** simplifies this by allowing you to register a ZenML cloud stack using existing infrastructure.

#### Alternatives for Stack Creation
- **1-click Deployment Tool**: For those without existing infrastructure.
- **Terraform Modules**: For manual infrastructure management.

### Using the Stack Wizard
The Stack Wizard is accessible via the CLI or dashboard.

#### Dashboard Steps:
1. Go to the stacks page and click "+ New Stack".
2. Select "Use existing Cloud" and choose your cloud provider.
3. Fill in authentication details based on the selected provider.

#### CLI Command:
To register a stack, use:
```shell
zenml stack register <STACK_NAME> -p {aws|gcp|azure} -sc <SERVICE_CONNECTOR_ID_OR_NAME>
```
The wizard checks for existing credentials in your environment and offers options for auto-configuration or manual setup.

### Authentication Methods
**AWS**:
- Options include AWS Secret Key, STS Token, IAM Role, Session Token, and Federation Token.

**GCP**:
- Options include User Account, Service Account, External Account, OAuth 2.0 Token, and Service Account Impersonation.

**Azure**:
- Options include Service Principal and Access Token.

### Defining Cloud Components
You will define three essential components for your stack:
1. **Artifact Store**
2. **Orchestrator**
3. **Container Registry**

You can reuse existing components or create new ones based on available resources from the service connector.

### Conclusion
Using the Stack Wizard, you can efficiently register a cloud stack and start running pipelines in a remote environment.

================================================================================

# Deploy a Cloud Stack with Terraform

ZenML provides [Terraform modules](https://registry.terraform.io/modules/zenml-io/zenml-stack) for provisioning cloud resources and integrating them with ZenML Stacks, enhancing AI/ML operations. Users can create custom Terraform configurations based on these modules.

## Prerequisites
- A deployed ZenML server instance accessible from your cloud provider.
- Create a service account and API key for Terraform access:
  ```shell
  zenml service-account create <account-name>
  ```
- Install Terraform (version 1.9 or higher).
- Authenticate with your cloud provider via its CLI or SDK.

## Using Terraform Stack Deployment Modules
1. Set up the ZenML Terraform provider using environment variables:
   ```shell
   export ZENML_SERVER_URL="https://your-zenml-server.com"
   export ZENML_API_KEY="<your-api-key>"
   ```
2. Create a `main.tf` file with the following structure (replace `<cloud provider>` with `aws`, `gcp`, or `azure`):
   ```hcl
   terraform {
       required_providers {
           aws = { source = "hashicorp/aws" }
           zenml = { source = "zenml-io/zenml" }
       }
   }

   provider "zenml" {}
   module "zenml_stack" {
       source = "zenml-io/zenml-stack/<cloud-provider>"
       zenml_stack_name = "<your-stack-name>"
       orchestrator = "<your-orchestrator-type>"
   }
   output "zenml_stack_id" {
       value = module.zenml_stack.zenml_stack_id
   }
   output "zenml_stack_name" {
       value = module.zenml_stack.zenml_stack_name
   }
   ```
3. Run:
   ```shell
   terraform init
   terraform apply
   ```
4. Confirm changes by typing `yes` when prompted.

5. After provisioning, use the ZenML stack:
   ```shell
   zenml integration install <list-of-required-integrations>
   zenml stack set <zenml_stack_id>
   ```

## Cloud Provider Specifics

### AWS
- **Authentication**: Install [AWS CLI](https://aws.amazon.com/cli/) and run `aws configure`.
- **Example Configuration**:
   ```hcl
   provider "aws" { region = "eu-central-1" }
   ```

### GCP
- **Authentication**: Install [gcloud CLI](https://cloud.google.com/sdk/gcloud) and run `gcloud init`.
- **Example Configuration**:
   ```hcl
   provider "google" { region = "europe-west3"; project = "my-project" }
   ```

### Azure
- **Authentication**: Install [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/) and run `az login`.
- **Example Configuration**:
   ```hcl
   provider "azurerm" { features { resource_group { prevent_deletion_if_contains_resources = false } } }
   ```

## Cleanup
To remove all resources and delete the ZenML stack:
```shell
terraform destroy
``` 

This concise guide retains essential technical details for deploying a cloud stack with Terraform using ZenML.

================================================================================

### Reference Secrets in Stack Configuration

Components in your stack may require sensitive information (e.g., passwords, tokens) for infrastructure connections. Use secret references to securely configure these components by referencing a secret instead of directly specifying values. The syntax for referencing a secret is: `{{<SECRET_NAME>.<SECRET_KEY>}}`.

**Example: CLI Usage**
```shell
# Create a secret named `mlflow_secret` with username and password
zenml secret create mlflow_secret --username=admin --password=abc123

# Reference the secret in the experiment tracker component
zenml experiment-tracker register mlflow \
    --flavor=mlflow \
    --tracking_username={{mlflow_secret.username}} \
    --tracking_password={{mlflow_secret.password}} \
    ...
```

ZenML validates the existence of all referenced secrets and keys before running a pipeline to prevent failures due to missing secrets. The validation can be controlled using the `ZENML_SECRET_VALIDATION_LEVEL` environment variable:
- `NONE`: Disables validation.
- `SECRET_EXISTS`: Validates only the existence of secrets.
- `SECRET_AND_KEY_EXISTS`: (default) Validates both secret existence and key-value pairs.

### Fetching Secret Values in Steps
For centralized secrets management, access secrets within your steps using the ZenML `Client` API:

```python
from zenml import step
from zenml.client import Client

@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    secret = Client().get_secret(<SECRET_NAME>)
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

### See Also
- [Interact with secrets](../../interact-with-secrets.md): Instructions for creating, listing, and deleting secrets using ZenML CLI and Python SDK.

================================================================================

# ZenML Integration with Terraform - Quick Guide

## Overview
This guide helps advanced users integrate ZenML with existing Terraform-managed infrastructure. It focuses on registering existing resources with ZenML using the ZenML provider.

## Two-Phase Approach
1. **Infrastructure Deployment**: Creating cloud resources.
2. **ZenML Registration**: Registering these resources as ZenML stack components.

## Phase 1: Infrastructure Deployment
Example of existing GCP infrastructure:
```hcl
resource "google_storage_bucket" "ml_artifacts" {
  name     = "company-ml-artifacts"
  location = "US"
}

resource "google_artifact_registry_repository" "ml_containers" {
  repository_id = "ml-containers"
  format        = "DOCKER"
}
```

## Phase 2: ZenML Registration

### Setup the ZenML Provider
Configure the ZenML provider:
```hcl
terraform {
  required_providers {
    zenml = { source = "zenml-io/zenml" }
  }
}

provider "zenml" {
  # Load configuration from environment variables
}
```
Generate an API key:
```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```

### Create Service Connectors
Create a service connector for authentication:
```hcl
resource "zenml_service_connector" "gcp_connector" {
  name        = "gcp-${var.environment}-connector"
  type        = "gcp"
  auth_method = "service-account"  
  
  configuration = {
    project_id = var.project_id
    service_account_json = file("service-account.json")
  }
}
```

### Register Stack Components
Register components:
```hcl
locals {
  component_configs = {
    artifact_store = {
      type = "artifact_store"
      flavor = "gcp"
      configuration = { path = "gs://${google_storage_bucket.ml_artifacts.name}" }
    }
    container_registry = {
      type = "container_registry"
      flavor = "gcp"
      configuration = { uri = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.ml_containers.repository_id}" }
    }
    orchestrator = {
      type = "orchestrator"
      flavor = "vertex"
      configuration = { project = var.project_id, region = var.region }
    }
  }
}

resource "zenml_stack_component" "components" {
  for_each = local.component_configs
  
  name          = "existing-${each.key}"
  type          = each.value.type
  flavor        = each.value.flavor
  configuration = each.value.configuration
  connector_id  = zenml_service_connector.gcp_connector.id
}
```

### Assemble the Stack
Combine components into a stack:
```hcl
resource "zenml_stack" "ml_stack" {
  name = "${var.environment}-ml-stack"
  
  components = { for k, v in zenml_stack_component.components : k => v.id }
}
```

## Complete Example for GCP Infrastructure
### Prerequisites
- GCS bucket for artifacts
- Artifact Registry repository
- Service account for ML operations
- Vertex AI enabled

### Variables Configuration
```hcl
variable "zenml_server_url" { type = string }
variable "zenml_api_key" { type = string, sensitive = true }
variable "project_id" { type = string }
variable "region" { type = string, default = "us-central1" }
variable "environment" { type = string }
variable "gcp_service_account_key" { type = string, sensitive = true }
```

### Main Configuration
```hcl
terraform {
  required_providers {
    zenml = { source = "zenml-io/zenml" }
    google = { source = "hashicorp/google" }
  }
}

provider "zenml" {
  server_url = var.zenml_server_url
  api_key    = var.zenml_api_key
}

provider "google" {
  project = var.project_id
  region  = var.region
}

resource "google_storage_bucket" "artifacts" {
  name     = "${var.project_id}-zenml-artifacts-${var.environment}"
  location = var.region
}

resource "google_artifact_registry_repository" "containers" {
  location      = var.region
  repository_id = "zenml-containers-${var.environment}"
  format        = "DOCKER"
}

resource "zenml_service_connector" "gcp" {
  name        = "gcp-${var.environment}"
  type        = "gcp"
  auth_method = "service-account"
  configuration = {
    project_id = var.project_id
    region     = var.region
    service_account_json = var.gcp_service_account_key
  }
}

resource "zenml_stack_component" "artifact_store" {
  name   = "gcs-${var.environment}"
  type   = "artifact_store"
  flavor = "gcp"
  configuration = { path = "gs://${google_storage_bucket.artifacts.name}/artifacts" }
  connector_id = zenml_service_connector.gcp.id
}

resource "zenml_stack_component" "container_registry" {
  name   = "gcr-${var.environment}"
  type   = "container_registry"
  flavor = "gcp"
  configuration = { uri = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.containers.repository_id}" }
  connector_id = zenml_service_connector.gcp.id
}

resource "zenml_stack_component" "orchestrator" {
  name   = "vertex-${var.environment}"
  type   = "orchestrator"
  flavor = "vertex"
  configuration = { location = var.region, synchronous = true }
  connector_id = zenml_service_connector.gcp.id
}

resource "zenml_stack" "gcp_stack" {
  name = "gcp-${var.environment}"
  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator      = zenml_stack_component.orchestrator.id
  }
}
```

### Outputs Configuration
```hcl
output "stack_id" { value = zenml_stack.gcp_stack.id }
output "stack_name" { value = zenml_stack.gcp_stack.name }
output "artifact_store_path" { value = "${google_storage_bucket.artifacts.name}/artifacts" }
output "container_registry_uri" { value = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.containers.repository_id}" }
```

### terraform.tfvars Configuration
```hcl
zenml_server_url = "https://your-zenml-server.com"
project_id       = "your-gcp-project-id"
region           = "us-central1"
environment      = "dev"
```
Store sensitive variables in environment variables:
```bash
export TF_VAR_zenml_api_key="your-zenml-api-key"
export TF_VAR_gcp_service_account_key=$(cat path/to/service-account-key.json)
```

### Usage Instructions
1. Initialize Terraform:
   ```bash
   terraform init
   ```
2. Install ZenML integrations:
   ```bash
   zenml integration install gcp
   ```
3. Review planned changes:
   ```bash
   terraform plan
   ```
4. Apply configuration:
   ```bash
   terraform apply
   ```
5. Set the new stack as active:
   ```bash
   zenml stack set $(terraform output -raw stack_name)
   ```
6. Verify configuration:
   ```bash
   zenml stack describe
   ```

## Key Points
- Use appropriate IAM roles and permissions.
- Follow security best practices for credential management.
- Adapt the guide for AWS and Azure by changing provider configurations and resource types.

================================================================================

--- icon: network-wired description: > Use Infrastructure as Code to manage ZenML stacks and components. --- # Integrate with Infrastructure as Code [Infrastructure as Code (IaC)](https://aws.amazon.com/what-is/iac) enables managing and provisioning infrastructure through code. This section demonstrates integrating ZenML with popular IaC tools like [Terraform](https://www.terraform.io/). ![ZenML stack on Terraform Registry](../../../.gitbook/assets/terraform_providers_screenshot.png)

================================================================================

# Best Practices for Using IaC with ZenML

## Architecting ML Infrastructure with ZenML and Terraform

### The Challenge
System architects must establish scalable ML infrastructure that:
- Supports multiple teams with varying requirements
- Operates across dev, staging, and prod environments
- Maintains security and compliance
- Enables rapid iteration without bottlenecks

### The ZenML Approach
ZenML uses stack components as abstractions over infrastructure resources. This guide outlines effective architecture using Terraform with the ZenML provider.

## Part 1: Foundation - Stack Component Architecture

### Problem
Different teams require unique ML infrastructure configurations while ensuring consistency and reusability.

### Solution: Component-Based Architecture
Break down infrastructure into reusable modules corresponding to ZenML stack components:

```hcl
# modules/zenml_stack_base/main.tf
terraform {
  required_providers {
    zenml = { source = "zenml-io/zenml" }
    google = { source = "hashicorp/google" }
  }
}

resource "random_id" "suffix" { byte_length = 6 }

module "base_infrastructure" {
  source = "./modules/base_infra"
  environment = var.environment
  project_id  = var.project_id
  region      = var.region
  resource_prefix = "zenml-${var.environment}-${random_id.suffix.hex}"
}

resource "zenml_service_connector" "base_connector" {
  name        = "${var.environment}-base-connector"
  type        = "gcp"
  auth_method = "service-account"
  configuration = {
    project_id = var.project_id
    region     = var.region
    service_account_json = module.base_infrastructure.service_account_key
  }
  labels = { environment = var.environment }
}

resource "zenml_stack_component" "artifact_store" {
  name   = "${var.environment}-artifact-store"
  type   = "artifact_store"
  flavor = "gcp"
  configuration = { path = "gs://${module.base_infrastructure.artifact_store_bucket}/artifacts" }
  connector_id = zenml_service_connector.base_connector.id
}

resource "zenml_stack" "base_stack" {
  name = "${var.environment}-base-stack"
  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator       = zenml_stack_component.orchestrator.id
  }
  labels = { environment = var.environment, type = "base" }
}
```

Teams can extend this base stack:

```hcl
# team_configs/training_stack.tf
resource "zenml_stack_component" "training_orchestrator" {
  name   = "${var.environment}-training-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"
  configuration = {
    location      = var.region
    machine_type  = "n1-standard-8"
    gpu_enabled   = true
    synchronous   = true
  }
  connector_id = zenml_service_connector.base_connector.id
}

resource "zenml_stack" "training_stack" {
  name = "${var.environment}-training-stack"
  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator       = zenml_stack_component.training_orchestrator.id
  }
  labels = { environment = var.environment, type = "training" }
}
```

## Part 2: Environment Management and Authentication

### Problem
Different environments require distinct authentication methods, resource configurations, and isolation.

### Solution: Environment Configuration Pattern
Create a flexible service connector setup that adapts to the environment:

```hcl
locals {
  env_config = {
    dev = { machine_type = "n1-standard-4", gpu_enabled = false, auth_method = "service-account", auth_configuration = { service_account_json = file("dev-sa.json") } }
    prod = { machine_type = "n1-standard-8", gpu_enabled = true, auth_method = "external-account", auth_configuration = { external_account_json = file("prod-sa.json") } }
  }
}

resource "zenml_service_connector" "env_connector" {
  name        = "${var.environment}-connector"
  type        = "gcp"
  auth_method = local.env_config[var.environment].auth_method
  dynamic "configuration" {
    for_each = try(local.env_config[var.environment].auth_configuration, {})
    content { key = configuration.key; value = configuration.value }
  }
}

resource "zenml_stack_component" "env_orchestrator" {
  name   = "${var.environment}-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"
  configuration = {
    location     = var.region
    machine_type = local.env_config[var.environment].machine_type
    gpu_enabled  = local.env_config[var.environment].gpu_enabled
  }
  connector_id = zenml_service_connector.env_connector.id
  labels = { environment = var.environment }
}
```

## Part 3: Resource Sharing and Isolation

### Problem
ML projects require strict isolation of data and security.

### Solution: Resource Scoping Pattern
Implement resource sharing with project isolation:

```hcl
locals {
  project_paths = {
    fraud_detection = "projects/fraud_detection/${var.environment}"
    recommendation  = "projects/recommendation/${var.environment}"
  }
}

resource "zenml_stack_component" "project_artifact_stores" {
  for_each = local.project_paths
  name   = "${each.key}-artifact-store"
  type   = "artifact_store"
  flavor = "gcp"
  configuration = { path = "gs://${var.shared_bucket}/${each.value}" }
  connector_id = zenml_service_connector.env_connector.id
  labels = { project = each.key, environment = var.environment }
}

resource "zenml_stack" "project_stacks" {
  for_each = local.project_paths
  name = "${each.key}-stack"
  components = {
    artifact_store = zenml_stack_component.project_artifact_stores[each.key].id
    orchestrator   = zenml_stack_component.project_orchestrator.id
  }
  labels = { project = each.key, environment = var.environment }
}
```

## Part 4: Advanced Stack Management Practices

1. **Stack Component Versioning**
```hcl
locals {
  stack_version = "1.2.0"
  common_labels = { version = local.stack_version, managed_by = "terraform", environment = var.environment }
}

resource "zenml_stack" "versioned_stack" {
  name   = "stack-v${local.stack_version}"
  labels = local.common_labels
}
```

2. **Service Connector Management**
```hcl
resource "zenml_service_connector" "env_connector" {
  name        = "${var.environment}-${var.purpose}-connector"
  type        = var.connector_type
  auth_method = var.environment == "prod" ? "workload-identity" : "service-account"
  resource_type = var.resource_type
  resource_id   = var.resource_id
  labels = merge(local.common_labels, { purpose = var.purpose })
}
```

3. **Component Configuration Management**
```hcl
locals {
  base_configs = {
    orchestrator = { location = var.region, project = var.project_id }
    artifact_store = { path_prefix = "gs://${var.bucket_name}" }
  }
  
  env_configs = {
    dev = { orchestrator = { machine_type = "n1-standard-4" } }
    prod = { orchestrator = { machine_type = "n1-standard-8" } }
  }
}

resource "zenml_stack_component" "configured_component" {
  name   = "${var.environment}-${var.component_type}"
  type   = var.component_type
  configuration = merge(local.base_configs[var.component_type], try(local.env_configs[var.environment][var.component_type], {}))
}
```

4. **Stack Organization and Dependencies**
```hcl
module "ml_stack" {
  source = "./modules/ml_stack"
  depends_on = [module.base_infrastructure, module.security]
  components = {
    artifact_store     = module.storage.artifact_store_id
    container_registry = module.container.registry_id
    orchestrator       = var.needs_orchestrator ? module.compute.orchestrator_id : null
    experiment_tracker = var.needs_tracking ? module.mlflow.tracker_id : null
  }
  labels = merge(local.common_labels, { stack_type = "ml-platform" })
}
```

5. **State Management**
```hcl
terraform {
  backend "gcs" { prefix = "terraform/state" }
  workspace_prefix = "zenml-"
}

data "terraform_remote_state" "infrastructure" {
  backend = "gcs"
  config = { bucket = var.state_bucket, prefix = "terraform/infrastructure" }
}
```

### Conclusion
Using ZenML and Terraform for ML infrastructure enables a flexible, maintainable, and secure environment. The ZenML provider streamlines the process while adhering to best practices in infrastructure management.

================================================================================

# Service Connectors Guide Summary

This guide provides comprehensive instructions for managing Service Connectors to connect ZenML to external resources. Key sections include:

1. **Getting Started**:
   - Familiarize with [terminology](service-connectors-guide.md#terminology).
   - Explore [Service Connector Types](service-connectors-guide.md#cloud-provider-service-connector-types) for various implementations.
   - Learn about [Registering Service Connectors](service-connectors-guide.md#register-service-connectors) for quick setup.
   - Connect Stack Components to resources using available Service Connectors.

2. **Terminology**:
   - **Service Connector Types**: Identify specific implementations and their capabilities (e.g., AWS Service Connector for S3, EKS).
   - **Resource Types**: Classify resources based on access protocols or vendors (e.g., `kubernetes-cluster`, `docker-registry`).
   - **Resource Names**: Unique identifiers for resource instances (e.g., S3 bucket names).

3. **Service Connector Types**:
   - Examples of Service Connector Types include AWS, GCP, Azure, Kubernetes, and Docker.
   - Use CLI commands like `zenml service-connector list-types` to explore available types.

4. **Registering Service Connectors**:
   - Register connectors with commands like:
     ```sh
     zenml service-connector register aws-multi-type --type aws --auto-configure
     ```
   - Different scopes: multi-type (multiple resource types), multi-instance (multiple resources of the same type), single-instance (one resource).

5. **Verification**:
   - Verify configurations using:
     ```sh
     zenml service-connector verify <connector-name>
     ```
   - Scope verification to specific resource types or names.

6. **Connecting Stack Components**:
   - Use interactive CLI mode to connect components:
     ```sh
     zenml artifact-store connect <component-name> -i
     ```

7. **Resource Discovery**:
   - Discover available resources with:
     ```sh
     zenml service-connector list-resources
     ```

8. **End-to-End Examples**:
   - Refer to specific examples for AWS, GCP, and Azure Service Connectors for practical implementation guidance.

### Example Commands
- List Service Connector Types:
  ```sh
  zenml service-connector list-types
  ```
- Register a Service Connector:
  ```sh
  zenml service-connector register aws-multi-type --type aws --auto-configure
  ```
- Verify a Service Connector:
  ```sh
  zenml service-connector verify aws-multi-type
  ```
- Connect a Stack Component:
  ```sh
  zenml artifact-store connect s3-zenfiles --connector aws-multi-type
  ```

This summary encapsulates the essential technical information and commands necessary for managing Service Connectors in ZenML, ensuring clarity and conciseness.

================================================================================

# Security Best Practices for Service Connectors

Service Connectors for cloud providers support various authentication methods. While no unified standard exists, identifiable patterns can guide the selection of appropriate methods.

## Username and Password
- **Avoid using primary account passwords** for authentication. Use alternatives like session tokens or API keys whenever possible.
- Passwords are the least secure method and should not be shared or used for automated workloads. Cloud platforms often require exchanging passwords for long-lived credentials.

## Implicit Authentication
- Provides immediate access to cloud resources without configuration but may limit portability.
- **Security Risk**: Implicit authentication can grant access to resources configured for the ZenML Server. It is disabled by default and must be explicitly enabled via the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable.

### Examples of Implicit Authentication:
- **AWS**: Uses instance metadata service to load credentials.
- **GCP**: Accesses resources via service account attached to the workload.
- **Azure**: Utilizes Azure Managed Identity for access.

### GCP Implicit Authentication Example:
```sh
zenml service-connector register gcp-implicit --type gcp --auth-method implicit --project_id=zenml-core
```

## Long-Lived Credentials (API Keys, Account Keys)
- Ideal for production environments, especially when combined with mechanisms for generating short-lived tokens or impersonating accounts.
- Cloud platforms do not use account passwords directly; instead, they exchange them for long-lived credentials.

### Credential Types:
- **User Credentials**: Tied to human users, not recommended for sharing.
- **Service Credentials**: Used for automated processes, better for sharing due to restricted permissions.

## Generating Temporary and Down-Scoped Credentials
- **Temporary Credentials**: Issued to clients with limited lifetimes, reducing exposure risk.
- **Down-Scoped Credentials**: Limit permissions to the minimum required for specific resources.

### AWS Temporary Credentials Example:
```sh
zenml service-connector describe eks-zenhacks-cluster
```

## Impersonating Accounts and Assuming Roles
- Requires setup of multiple accounts/roles but offers flexibility and control.
- Long-lived credentials are exchanged for short-lived tokens with limited permissions.

### GCP Account Impersonation Example:
```sh
zenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl
```

## Short-Lived Credentials
- Temporary credentials configured in Service Connectors, ideal for granting temporary access without exposing long-lived credentials.
- Example of auto-configuration for AWS short-lived credentials:
```sh
AWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token
```

### Summary
- Use secure authentication methods, prioritize long-lived and service credentials, and consider the implications of implicit authentication.
- Implement temporary and down-scoped credentials for enhanced security in production environments.

================================================================================

### GCP Service Connector Overview

The ZenML GCP Service Connector enables authentication and access to GCP resources, including GCS buckets, GKE clusters, and GCR registries. It supports various authentication methods: user accounts, service accounts, OAuth 2.0 tokens, and implicit authentication. By default, it issues short-lived OAuth 2.0 tokens for enhanced security.

#### Key Features:
- **Resource Types**: Supports generic GCP resources, GCS buckets, GKE clusters, and GAR/GCR registries.
- **Authentication Methods**:
  - **Implicit**: Automatically discovers credentials from environment variables or local ADC files.
  - **User Account**: Uses long-lived credentials, generating temporary OAuth tokens.
  - **Service Account**: Requires a service account key JSON, generating temporary tokens by default.
  - **Impersonation**: Generates temporary STS credentials by impersonating another service account.
  - **External Account**: Uses GCP workload identity federation for authentication with AWS or Azure credentials.
  - **OAuth 2.0 Token**: Requires manual token management.

### Prerequisites
- Install ZenML GCP integration:
  ```bash
  pip install "zenml[connectors-gcp]"
  ```
- Optionally, install the GCP CLI for easier configuration.

### Resource Types and Permissions
- **Generic GCP Resource**: Provides a google-auth credentials object for any GCP service.
- **GCS Bucket**: Requires permissions like `storage.buckets.list`, `storage.objects.create`, etc.
- **GKE Cluster**: Requires permissions such as `container.clusters.list`.
- **GAR/GCR**: Requires permissions for artifact management.

### Example Commands
1. **List Service Connector Types**:
   ```bash
   zenml service-connector list-types --type gcp
   ```

2. **Register a GCP Service Connector**:
   ```bash
   zenml service-connector register gcp-implicit --type gcp --auth-method implicit --auto-configure
   ```

3. **Describe a Service Connector**:
   ```bash
   zenml service-connector describe gcp-implicit
   ```

### Local Client Configuration
Local clients like `gcloud`, `kubectl`, and Docker can be configured using credentials from the GCP Service Connector. Ensure the connector is set to use user account or service account methods with temporary tokens enabled.

### Stack Components Integration
The GCP Service Connector can connect various ZenML Stack Components, such as:
- GCS Artifact Store
- Kubernetes Orchestrator
- GCP Container Registry

### End-to-End Workflow Example
1. **Install ZenML and Configure GCP CLI**:
   ```bash
   zenml integration install -y gcp
   gcloud auth application-default login
   ```

2. **Register a Multi-Type GCP Service Connector**:
   ```bash
   zenml service-connector register gcp-demo-multi --type gcp --auto-configure
   ```

3. **Connect Stack Components**:
   - Register and connect GCS Artifact Store:
     ```bash
     zenml artifact-store register gcs-zenml-bucket-sl --flavor gcp --path=gs://zenml-bucket-sl
     zenml artifact-store connect gcs-zenml-bucket-sl --connector gcp-demo-multi
     ```

4. **Run a Simple Pipeline**:
   ```python
   from zenml import pipeline, step

   @step
   def step_1() -> str:
       return "world"

   @step(enable_cache=False)
   def step_2(input_one: str, input_two: str) -> None:
       print(f"{input_one} {input_two}")

   @pipeline
   def my_pipeline():
       output_step_one = step_1()
       step_2(input_one="hello", input_two=output_step_one)

   if __name__ == "__main__":
       my_pipeline()
   ```

This concise summary captures the essential technical details and commands necessary for configuring and using the GCP Service Connector with ZenML.

================================================================================

# ZenML Service Connectors Overview

ZenML enables seamless connections to cloud providers and infrastructure services, essential for MLOps platforms. It simplifies the complex task of managing authentication and authorization across various services, such as AWS S3, Kubernetes, and GCR.

## Key Features of Service Connectors
- **Abstraction of Complexity**: Service Connectors handle authentication, allowing developers to focus on pipeline code without worrying about security details.
- **Unified Access**: Multiple Stack Components can use the same Service Connector, promoting reusability and reducing redundancy.

## Use Case: Connecting to AWS S3
To connect ZenML to an AWS S3 bucket using the AWS Service Connector, follow these steps:

### 1. List Available Service Connector Types
```sh
zenml service-connector list-types
```

### 2. Register the AWS Service Connector
Ensure the AWS CLI is configured on your local machine. Then, register the connector:
```sh
zenml service-connector register aws-s3 --type aws --auto-configure --resource-type s3-bucket
```

### 3. Connect an Artifact Store to the S3 Bucket
```sh
zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
zenml artifact-store connect s3-zenfiles --connector aws-s3
```

### 4. Example Pipeline
Create a simple pipeline:
```python
from zenml import step, pipeline

@step
def simple_step_one() -> str:
    return "Hello World!"

@step
def simple_step_two(msg: str) -> None:
    print(msg)

@pipeline
def simple_pipeline() -> None:
    message = simple_step_one()
    simple_step_two(msg=message)

if __name__ == "__main__":
    simple_pipeline()
```
Run the pipeline:
```sh
python run.py
```

## Security Best Practices
Service Connectors enforce security best practices by managing credentials securely, generating short-lived tokens, and minimizing direct access to sensitive information.

## Additional Resources
- [Service Connector Guide](./service-connectors-guide.md)
- [Security Best Practices](./best-security-practices.md)
- [Docker Service Connector](./docker-service-connector.md)
- [Kubernetes Service Connector](./kubernetes-service-connector.md)
- [AWS Service Connector](./aws-service-connector.md)
- [GCP Service Connector](./gcp-service-connector.md)
- [Azure Service Connector](./azure-service-connector.md)

This overview provides a concise understanding of how to utilize ZenML Service Connectors for connecting to various cloud services while ensuring security and ease of use.

================================================================================

# Kubernetes Service Connector

The ZenML Kubernetes Service Connector enables authentication and connection to Kubernetes clusters, allowing access via pre-authenticated Kubernetes Python clients and local `kubectl` configuration.

## Prerequisites

- Install the connector:
  - `pip install "zenml[connectors-kubernetes]"` for prerequisites only.
  - `zenml integration install kubernetes` for the full integration.
- Local `kubectl` configuration is not required for accessing clusters.

### List Connector Types
```shell
$ zenml service-connector list-types --type kubernetes
```
```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password     │ ✅    │ ✅     ┃
┃                              │               │                       │ token        │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Resource Types
- Supports authentication to generic Kubernetes clusters (`kubernetes-cluster`).

## Authentication Methods
1. Username and password (not recommended for production).
2. Authentication token (can be empty for local K3D clusters).

**Warning**: Credentials are distributed directly to clients; use API tokens with client certificates when possible.

## Auto-configuration
Fetch credentials from local `kubectl` during registration:
```sh
zenml service-connector register kube-auto --type kubernetes --auto-configure
```
**Example Output**:
```
Successfully registered service connector `kube-auto` with access to:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES ┃
┠───────────────────────┼────────────────┨
┃ 🌀 kubernetes-cluster │ 35.185.95.223  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

### Describe Service Connector
```sh
zenml service-connector describe kube-auto 
```
**Example Output**:
```
Service connector 'kube-auto' of type 'kubernetes'...
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ ID               │ 4315e8eb-fcbd-4938-a4d7-a9218ab372a1 ┃
┃ NAME             │ kube-auto                            ┃
┃ AUTH METHOD      │ token                                ┃
┃ RESOURCE NAME    │ 35.175.95.223                        ┃
┃ OWNER            │ default                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

**Info**: Credentials may have a limited lifetime, affecting connectivity.

## Local Client Provisioning
Configure local `kubectl` with:
```sh
zenml service-connector login kube-auto 
```
**Example Output**:
```
Updated local kubeconfig with the cluster details...
```

## Stack Components Usage
The Kubernetes Service Connector is utilized in Orchestrator and Model Deployer stack components, managing Kubernetes workloads without explicit `kubectl` configurations.

================================================================================

### AWS Service Connector Documentation Summary

The **ZenML AWS Service Connector** allows connection to AWS resources such as S3 buckets, EKS clusters, and ECR registries, supporting various authentication methods (long-lived AWS keys, IAM roles, STS tokens, implicit authentication). It generates temporary STS tokens with minimal permissions and auto-configures credentials from the AWS CLI.

#### Key Features:
- **Authentication Methods**: 
  - **Implicit**: Uses environment variables or local AWS CLI configuration.
  - **Secret Key**: Long-lived credentials; not recommended for production.
  - **STS Token**: Temporary tokens; requires manual refresh.
  - **IAM Role**: Assumes a role for temporary credentials.
  - **Federation Token**: For federated users; requires permissions for `GetFederationToken`.

- **Resource Types**:
  - **Generic AWS Resource**: Access to any AWS service.
  - **S3 Bucket**: Requires specific IAM permissions (e.g., `s3:ListBucket`, `s3:GetObject`).
  - **EKS Cluster**: Requires permissions (e.g., `eks:ListClusters`).
  - **ECR Registry**: Requires permissions (e.g., `ecr:DescribeRepositories`).

#### Configuration Commands:
- **List AWS Service Connector Types**:
  ```shell
  zenml service-connector list-types --type aws
  ```

- **Register Service Connector**:
  ```shell
  zenml service-connector register aws-implicit --type aws --auth-method implicit --region=us-east-1
  ```

- **Verify Access**:
  ```shell
  zenml service-connector verify aws-implicit --resource-type s3-bucket
  ```

#### Auto-Configuration:
The connector can auto-discover credentials from the AWS CLI. Example command:
```shell
AWS_PROFILE=connectors zenml service-connector register aws-auto --type aws --auto-configure
```

#### Local Client Provisioning:
Local AWS CLI, Kubernetes `kubectl`, and Docker CLI can be configured with credentials from the AWS Service Connector. Example for Kubernetes:
```shell
zenml service-connector login aws-session-token --resource-type kubernetes-cluster --resource-id zenhacks-cluster
```

#### Stack Components:
The AWS Service Connector integrates with ZenML Stack Components such as S3 Artifact Store, Kubernetes Orchestrator, and ECR Container Registry, allowing seamless resource management without explicit credentials in the environment.

#### Example Workflow:
1. Configure AWS CLI with IAM credentials.
2. Register a multi-type AWS Service Connector.
3. Connect Stack Components (S3, EKS, ECR) to the Service Connector.
4. Run a simple pipeline to validate the setup.

### Example Pipeline Code:
```python
from zenml import pipeline, step

@step
def step_1() -> str:
    return "world"

@step(enable_cache=False)
def step_2(input_one: str, input_two: str) -> None:
    print(f"{input_one} {input_two}")

@pipeline
def my_pipeline():
    output_step_one = step_1()
    step_2(input_one="hello", input_two=output_step_one)

if __name__ == "__main__":
    my_pipeline()
```

This summary captures the essential technical details of the AWS Service Connector in ZenML, focusing on its configuration, authentication methods, resource types, and integration with Stack Components.

================================================================================

### Azure Service Connector Overview

The ZenML Azure Service Connector enables authentication and access to Azure resources like Blob storage, AKS clusters, and ACR registries. It supports automatic credential configuration via the Azure CLI and specialized authentication for various Azure services.

#### Prerequisites
- Install the Azure Service Connector:
  - For Azure Service Connector only: 
    ```bash
    pip install "zenml[connectors-azure]"
    ```
  - For full Azure integration:
    ```bash
    zenml integration install azure
    ```
- Azure CLI setup is recommended for auto-configuration but not mandatory.

#### Resource Types
1. **Generic Azure Resource**: Connects to any Azure service using generic azure-identity credentials.
2. **Azure Blob Storage**: Requires permissions like `Storage Blob Data Contributor`. Resource name formats:
   - URI: `{az|abfs}://{container-name}`
   - Name: `{container-name}`
   - Only service principal authentication is supported.
3. **AKS Kubernetes Cluster**: Requires `Azure Kubernetes Service Cluster Admin Role`. Resource name formats:
   - `[{resource-group}/]{cluster-name}`
4. **ACR Container Registry**: Requires permissions like `AcrPull` and `AcrPush`. Resource name formats:
   - URI: `[https://]{registry-name}.azurecr.io`
   - Name: `{registry-name}`

#### Authentication Methods
- **Implicit Authentication**: Uses environment variables or Azure CLI. Needs explicit enabling due to security risks.
- **Service Principal**: Requires client ID and secret for authentication.
- **Access Token**: Temporary tokens that require regular updates; not suitable for blob storage.

#### Example Commands
- Register an implicit service connector:
  ```bash
  zenml service-connector register azure-implicit --type azure --auth-method implicit --auto-configure
  ```
- Register a service principal connector:
  ```bash
  zenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=<tenant_id> --client_id=<client_id> --client_secret=<client_secret>
  ```

#### Local Client Configuration
- Configure local Kubernetes CLI:
  ```bash
  zenml service-connector login azure-service-principal --resource-type kubernetes-cluster --resource-id=<cluster-id>
  ```
- Configure local Docker CLI:
  ```bash
  zenml service-connector login azure-service-principal --resource-type docker-registry --resource-id=<registry-id>
  ```

#### Stack Components Usage
- Connect Azure Artifact Store to Blob Storage:
  ```bash
  zenml artifact-store register azure-demo --flavor azure --path=az://demo-zenmlartifactstore
  zenml artifact-store connect azure-demo --connector azure-service-principal
  ```
- Connect Kubernetes Orchestrator to AKS:
  ```bash
  zenml orchestrator register aks-demo-cluster --flavor kubernetes --synchronous=true --kubernetes_namespace=zenml-workloads
  zenml orchestrator connect aks-demo-cluster --connector azure-service-principal
  ```
- Connect ACR Container Registry:
  ```bash
  zenml container-registry register acr-demo-registry --flavor azure --uri=demozenmlcontainerregistry.azurecr.io
  zenml container-registry connect acr-demo-registry --connector azure-service-principal
  ```

#### Example Pipeline
```python
from zenml import pipeline, step

@step
def step_1() -> str:
    return "world"

@step(enable_cache=False)
def step_2(input_one: str, input_two: str) -> None:
    print(f"{input_one} {input_two}")

@pipeline
def my_pipeline():
    output_step_one = step_1()
    step_2(input_one="hello", input_two=output_step_one)

if __name__ == "__main__":
    my_pipeline()
```

### Summary
The Azure Service Connector in ZenML allows seamless integration with Azure resources, enabling efficient management of cloud services through a unified interface. Proper authentication and resource configuration are crucial for optimal functionality.

================================================================================

### Docker Service Connector Overview
The ZenML Docker Service Connector enables authentication with Docker/OCI container registries and manages Docker clients. It provides pre-authenticated `python-docker` clients for linked Stack Components.

#### Command to List Connector Types
```shell
zenml service-connector list-types --type docker
```

#### Supported Resource Types
- **Resource Type**: `docker-registry`
- **Registry Formats**:
  - DockerHub: `docker.io` or `https://index.docker.io/v1/<repository-name>`
  - OCI registry: `https://host:port/<repository-name>`

#### Authentication Methods
Authentication is via username/password or access token, with a preference for API tokens.

#### Registering a DockerHub Connector
```sh
zenml service-connector register dockerhub --type docker -in
```

#### Example Command Output
```
Please enter a name for the service connector [dockerhub]:
Please enter a description for the service connector []:
...
Successfully registered service connector `dockerhub` with access to:
┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃   RESOURCE TYPE    │ RESOURCE NAMES ┃
┠────────────────────┼────────────────┨
┃ 🐳 docker-registry │ docker.io      ┃
┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

**Note**: Credentials are distributed directly to clients; short-lived credentials are not supported.

#### Auto-Configuration
The connector does not auto-discover authentication credentials from local Docker clients. Feedback can be provided via [Slack](https://zenml.io/slack) or [GitHub](https://github.com/zenml-io/zenml/issues).

#### Local Client Provisioning
To configure the local Docker client:
```sh
zenml service-connector login dockerhub
```

#### Example Command Output
```
Attempting to configure local client using service connector 'dockerhub'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
```

#### Stack Components Usage
The Docker Service Connector allows Container Registry stack components to authenticate to remote registries, enabling image building and publishing without explicit Docker credentials in the environment.

**Warning**: ZenML does not currently support automatic Docker credential configuration in container runtimes like Kubernetes. This feature will be added in a future release.

================================================================================

# HyperAI Service Connector

The ZenML HyperAI Service Connector enables authentication with HyperAI instances for pipeline deployment. It provides pre-authenticated Paramiko SSH clients to linked Stack Components.

## Command to List Connector Types
```shell
$ zenml service-connector list-types --type hyperai
```

## Connector Overview
| Name                     | Type      | Resource Types      | Auth Methods   | Local | Remote |
|--------------------------|-----------|---------------------|----------------|-------|--------|
| HyperAI Service Connector | 🤖 hyperai | 🤖 hyperai-instance | rsa-key        | ✅    | ✅     |
|                          |           |                     | dsa-key        |       |        |
|                          |           |                     | ecdsa-key      |       |        |
|                          |           |                     | ed25519-key    |       |        |

## Prerequisites
Install the HyperAI integration:
```shell
$ zenml integration install hyperai
```

## Resource Types
Supports HyperAI instances.

## Authentication Methods
ZenML establishes an SSH connection to HyperAI instances, supporting:
1. RSA key
2. DSA (DSS) key
3. ECDSA key
4. ED25519 key

**Warning:** SSH keys are long-lived credentials granting unrestricted access to HyperAI instances. They will be shared across clients using the connector.

### Configuration Requirements
- Provide at least one `hostname` and `username`.
- Optionally, include an `ssh_passphrase`.

### Usage Options
1. One connector per HyperAI instance with unique SSH keys.
2. Reuse a single SSH key across multiple instances.

## Auto-configuration
This connector does not support auto-discovery of authentication credentials. Feedback can be provided via [Slack](https://zenml.io/slack) or [GitHub](https://github.com/zenml-io/zenml/issues).

## Stack Components
The HyperAI Service Connector is utilized by the HyperAI Orchestrator for deploying pipeline runs to HyperAI instances.

================================================================================

# Configuring ZenML for Data Visualizations

## Visualizing Artifacts
ZenML saves visualizations of common data types for display in the ZenML dashboard and Jupyter notebooks using `artifact.visualize()`. Supported visualization types include:
- **HTML:** Embedded HTML visualizations (e.g., data validation reports)
- **Image:** Visualizations of image data
- **CSV:** Tables (e.g., pandas DataFrame `.describe()`)
- **Markdown:** Markdown strings

## Server Access to Visualizations
To display visualizations on the dashboard, the ZenML server must access the artifact store. This requires configuring a service connector. For details, refer to the [service connector documentation](../auth-management/) and the [AWS S3 artifact store documentation](../../component-guide/artifact-stores/s3.md). 

**Note:** With the default/local artifact store, the server cannot access local files, and visualizations won't display. Use a remote artifact store with a service connector for visualization.

## Configuring Artifact Stores
If visualizations are missing, check if the ZenML server has the necessary dependencies and permissions for the artifact store. Refer to the [custom artifact store documentation](../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores).

## Creating Custom Visualizations
You can add custom visualizations in two ways:
1. **Using Special Return Types:** Return HTML, Markdown, or CSV data by casting to specific types:
   - `zenml.types.HTMLString`
   - `zenml.types.MarkdownString`
   - `zenml.types.CSVString`

   **Example:**
   ```python
   from zenml.types import CSVString

   @step
   def my_step() -> CSVString:
       return CSVString("a,b,c\n1,2,3")
   ```

2. **Using Materializers:** Override `save_visualizations()` in a custom materializer to extract visualizations for specific data types. 

### Custom Return Type and Materializer
To visualize custom data:
1. Create a custom class for the data.
2. Build a custom materializer with visualization logic.
3. Return the custom class from a ZenML step.

**Example: Facets Data Skew Visualization**
1. **Custom Class:**
   ```python
   class FacetsComparison(BaseModel):
       datasets: List[Dict[str, Union[str, pd.DataFrame]]]
   ```

2. **Materializer:**
   ```python
   class FacetsMaterializer(BaseMaterializer):
       ASSOCIATED_TYPES = (FacetsComparison,)
       ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS

       def save_visualizations(self, data: FacetsComparison) -> Dict[str, VisualizationType]:
           html = ...  # Create visualization
           with fileio.open(os.path.join(self.uri, VISUALIZATION_FILENAME), "w") as f:
               f.write(html)
           return {visualization_path: VisualizationType.HTML}
   ```

3. **Step:**
   ```python
   @step
   def facets_visualization_step(reference: pd.DataFrame, comparison: pd.DataFrame) -> FacetsComparison:
       return FacetsComparison(datasets=[{"name": "reference", "table": reference}, {"name": "comparison", "table": comparison}])
   ```

## Disabling Visualizations
To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level:
```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

================================================================================

# Minimal GCP Stack Setup Guide

This guide provides steps to set up a minimal production stack on Google Cloud Platform (GCP) for ZenML.

## Steps to Set Up

### 1. Choose a GCP Project
Select or create a GCP project in the console. Ensure a billing account is attached.

```bash
gcloud projects create <PROJECT_ID> --billing-project=<BILLING_PROJECT>
```

### 2. Enable GCloud APIs
Enable the following APIs in your GCP project:
- Cloud Functions API
- Cloud Run Admin API
- Cloud Build API
- Artifact Registry API
- Cloud Logging API

### 3. Create a Dedicated Service Account
Assign the following roles to the service account:
- AI Platform Service Agent
- Storage Object Admin

### 4. Create a JSON Key for the Service Account
Download the JSON key file for authentication.

```bash
export JSON_KEY_FILE_PATH=<JSON_KEY_FILE_PATH>
```

### 5. Create a Service Connector in ZenML
Authenticate ZenML with GCP.

```bash
zenml integration install gcp \
&& zenml service-connector register gcp_connector \
--type gcp \
--auth-method service-account \
--service_account_json=@${JSON_KEY_FILE_PATH} \
--project_id=<GCP_PROJECT_ID>
```

### 6. Create Stack Components

#### Artifact Store
Create a GCS bucket and register it as an artifact store.

```bash
export ARTIFACT_STORE_NAME=gcp_artifact_store
zenml artifact-store register ${ARTIFACT_STORE_NAME} --flavor gcp --path=gs://<YOUR_BUCKET_NAME>
zenml artifact-store connect ${ARTIFACT_STORE_NAME} -i
```

#### Orchestrator
Use Vertex AI as the orchestrator.

```bash
export ORCHESTRATOR_NAME=gcp_vertex_orchestrator
zenml orchestrator register ${ORCHESTRATOR_NAME} --flavor=vertex --project=<PROJECT_NAME> --location=europe-west2
zenml orchestrator connect ${ORCHESTRATOR_NAME} -i
```

#### Container Registry
Register the container registry.

```bash
export CONTAINER_REGISTRY_NAME=gcp_container_registry
zenml container-registry register ${CONTAINER_REGISTRY_NAME} --flavor=gcp --uri=<GCR-URI>
zenml container-registry connect ${CONTAINER_REGISTRY_NAME} -i
```

### 7. Create Stack

```bash
export STACK_NAME=gcp_stack
zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

## Cleanup
To remove created resources, delete the project.

```bash
gcloud project delete <PROJECT_ID_OR_NUMBER>
```

## Best Practices

- **Use IAM and Least Privilege Principle:** Grant only necessary permissions and regularly review IAM roles.
- **Leverage GCP Resource Labeling:** Implement a labeling strategy for resource management.

```bash
gcloud storage buckets update gs://your-bucket-name --update-labels=project=zenml,environment=production
```

- **Implement Cost Management Strategies:** Use GCP's cost management tools to monitor spending.

```bash
gcloud billing budgets create --billing-account=BILLING_ACCOUNT_ID --display-name="ZenML Monthly Budget" --budget-amount=1000 --threshold-rule=percent=90
```

- **Implement a Robust Backup Strategy:** Regularly back up data and configurations.

```bash
gsutil versioning set on gs://your-bucket-name
```

By following these steps and best practices, you can efficiently set up and manage a GCP stack for ZenML projects.

================================================================================

# Quick Guide to Set Up Azure Stack for ZenML Pipelines

## Prerequisites
- Active Azure account
- ZenML installed
- ZenML Azure integration: `zenml integration install azure`

## 1. Set Up Credentials
1. Create a service principal via Azure App Registrations:
   - Go to Azure portal > App Registrations > `+ New registration`.
   - Note Application ID and Tenant ID.
2. Create a client secret under `Certificates & secrets` and note the secret value.

## 2. Create Resource Group and AzureML Instance
- Create a resource group in Azure portal > `Resource Groups` > `+ Create`.
- In the new resource group, click `+ Create` to add an Azure Machine Learning workspace.

## 3. Create Role Assignments
- In the resource group, go to `Access control (IAM)` > `+ Add` a role assignment.
- Assign the following roles to your registered app:
  - AzureML Compute Operator
  - AzureML Data Scientist
  - AzureML Registry User

## 4. Create Service Connector
Register the ZenML Azure Service Connector:
```bash
zenml service-connector register azure_connector --type azure \
  --auth-method service-principal \
  --client_secret=<CLIENT_SECRET> \
  --tenant_id=<TENANT_ID> \
  --client_id=<APPLICATION_ID>
```

## 5. Create Stack Components
### Artifact Store (Azure Blob Storage)
1. Create a container in the AzureML workspace storage account.
2. Register the artifact store:
```bash
zenml artifact-store register azure_artifact_store -f azure \
  --path=<PATH_TO_YOUR_CONTAINER> \
  --connector azure_connector
```

### Orchestrator (AzureML)
Register the orchestrator:
```bash
zenml orchestrator register azure_orchestrator -f azureml \
  --subscription_id=<YOUR_AZUREML_SUBSCRIPTION_ID> \
  --resource_group=<NAME_OF_YOUR_RESOURCE_GROUP> \
  --workspace=<NAME_OF_YOUR_AZUREML_WORKSPACE> \
  --connector azure_connector
```

### Container Registry (Azure Container Registry)
Register the container registry:
```bash
zenml container-registry register azure_container_registry -f azure \
  --uri=<URI_TO_YOUR_AZURE_CONTAINER_REGISTRY> \
  --connector azure_connector
```

## 6. Create a Stack
Create the Azure ZenML stack:
```shell
zenml stack register azure_stack \
  -o azure_orchestrator \
  -a azure_artifact_store \
  -c azure_container_registry \
  --set
```

## 7. Run Your Pipeline
Define and run a simple ZenML pipeline:
```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from Azure!"

@pipeline
def azure_pipeline():
    hello_world()

if __name__ == "__main__":
    azure_pipeline()
```
Save as `run.py` and execute:
```shell
python run.py
```

## Next Steps
- Explore ZenML's [production guide](../../user-guide/production-guide/README.md).
- Check ZenML's [integrations](../../component-guide/README.md).
- Join the [ZenML community](https://zenml.io/slack) for support.

================================================================================

### Summary: Using SkyPilot with ZenML

**SkyPilot Overview**  
The ZenML SkyPilot VM Orchestrator enables provisioning and management of VMs across cloud providers (AWS, GCP, Azure, Lambda Labs) for ML pipelines, offering cost savings and high GPU availability.

**Prerequisites**  
- Install ZenML SkyPilot integration for your cloud provider:  
  ```bash
  zenml integration install <PROVIDER> skypilot_<PROVIDER>
  ```
- Docker must be installed and running.
- A remote artifact store and container registry in your ZenML stack.
- A remote ZenML deployment.
- Permissions to provision VMs on your cloud provider.
- Service connector configured for authentication (not needed for Lambda Labs).

**Configuration Steps**  
*For AWS, GCP, Azure:*  
1. Install SkyPilot integration and connectors.
2. Register a service connector with required permissions.
3. Register the orchestrator and connect it to the service connector.
4. Register and activate a stack with the new orchestrator.

```bash
zenml service-connector register <PROVIDER>-skypilot-vm -t <PROVIDER> --auto-configure
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_<PROVIDER>  
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <PROVIDER>-skypilot-vm
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

*For Lambda Labs:*  
1. Install SkyPilot Lambda integration.
2. Register a secret with your API key.
3. Register the orchestrator with the API key secret.
4. Register and activate a stack with the new orchestrator.

```bash
zenml secret create lambda_api_key --scope user --api_key=<KEY>
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_lambda --api_key={{lambda_api_key.api_key}}
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

**Running a Pipeline**  
Once configured, run any ZenML pipeline using the SkyPilot VM Orchestrator. Each step runs in a Docker container on a provisioned VM.

**Additional Configuration**  
Further configure the orchestrator with cloud-specific `Settings` objects:

```python
from zenml.integrations.skypilot_<PROVIDER>.flavors.skypilot_orchestrator_<PROVIDER>_vm_flavor import Skypilot<PROVIDER>OrchestratorSettings

skypilot_settings = Skypilot<PROVIDER>OrchestratorSettings(
   cpus="2",
   memory="16", 
   accelerators="V100:2",
   use_spot=True,
   region=<REGION>,
)

@pipeline(settings={"orchestrator": skypilot_settings})
```

Configure resources per step:

```python
@step(settings={"orchestrator": high_resource_settings})  
def resource_intensive_step():
   ...
```

For detailed options, refer to the [full SkyPilot VM Orchestrator documentation](../../component-guide/orchestrators/skypilot-vm.md).

================================================================================

# MLflow Experiment Tracker with ZenML

## Overview
The ZenML MLflow Experiment Tracker integration allows logging and visualization of pipeline step information using MLflow without additional code.

## Prerequisites
- Install ZenML MLflow integration: 
  ```bash
  zenml integration install mlflow -y
  ```
- MLflow deployment: local or remote with proxied artifact storage.

## Configuring the Experiment Tracker
### 1. Local Deployment
No extra configuration needed. Register the tracker:
```bash
zenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow
zenml stack register custom_stack -e mlflow_experiment_tracker ... --set
```

### 2. Remote Deployment
Requires authentication:
- Basic authentication (not recommended)
- ZenML secrets (recommended)
  
Create ZenML secret:
```bash
zenml secret create mlflow_secret --username=<USERNAME> --password=<PASSWORD>
```
Register the tracker:
```bash
zenml experiment-tracker register mlflow --flavor=mlflow --tracking_username={{mlflow_secret.username}} --tracking_password={{mlflow_secret.password}} ...
```

## Using the Experiment Tracker
To log information in a pipeline step:
1. Enable the tracker with the `@step` decorator.
2. Use MLflow logging as usual.
```python
import mlflow

@step(experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>")
def train_step(...):
   mlflow.tensorflow.autolog()
   mlflow.log_param(...)
   mlflow.log_metric(...)
   mlflow.log_artifact(...)
```

## Viewing Results
Get the MLflow experiment URL for a ZenML run:
```python
last_run = client.get_pipeline("<PIPELINE_NAME>").last_run
tracking_url = last_run.get_step("<STEP_NAME>").run_metadata["experiment_tracker_url"].value
```

## Additional Configuration
Further configure the tracker using `MLFlowExperimentTrackerSettings`:
```python
from zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import MLFlowExperimentTrackerSettings

mlflow_settings = MLFlowExperimentTrackerSettings(nested=True, tags={"key": "value"})

@step(experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>", settings={"experiment_tracker": mlflow_settings})
```

For more details, refer to the [full MLflow Experiment Tracker documentation](../../component-guide/experiment-trackers/mlflow.md).

================================================================================

--- icon: puzzle-piece description: Integrate ZenML with your favorite tools. --- # Popular Integrations ZenML seamlessly integrates with popular data science and machine learning tools. This guide outlines the integration process for these tools. <figure><img src="https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc" alt="ZenML Scarf"></figure>

================================================================================

# Deploying ZenML Pipelines on Kubernetes

## Overview
The ZenML Kubernetes Orchestrator enables running ML pipelines on a Kubernetes cluster without needing to write Kubernetes code, serving as a lightweight alternative to orchestrators like Airflow or Kubeflow.

## Prerequisites
To use the Kubernetes Orchestrator, ensure you have:
- ZenML `kubernetes` integration: `zenml integration install kubernetes`
- Docker installed and running
- `kubectl` installed
- A remote artifact store and container registry in your ZenML stack
- A deployed Kubernetes cluster
- (Optional) Configured `kubectl` context for the cluster

## Deploying the Orchestrator
You need a Kubernetes cluster to run the orchestrator. Various deployment methods exist; refer to the [cloud guide](../../user-guide/cloud-guide/cloud-guide.md) for options.

## Configuring the Orchestrator
You can configure the orchestrator in two ways:

1. **Using a Service Connector** (recommended for cloud-managed clusters):
   ```bash
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubernetes
   zenml service-connector list-resources --resource-type kubernetes-cluster -e
   zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
   zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
   ```

2. **Using `kubectl` context**:
   ```bash
   zenml orchestrator register <ORCHESTRATOR_NAME> --flavor=kubernetes --kubernetes_context=<KUBERNETES_CONTEXT>
   zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
   ```

## Running a Pipeline
To run a ZenML pipeline with the Kubernetes Orchestrator:
```bash
python your_pipeline.py
```
This command creates a Kubernetes pod for each pipeline step. Use `kubectl` commands to interact with the pods. For more details, refer to the [full Kubernetes Orchestrator documentation](../../component-guide/orchestrators/kubernetes.md).

================================================================================

# AWS Stack Setup for ZenML Pipelines

## Overview
This guide provides steps to set up a minimal production stack on AWS for running ZenML pipelines, including IAM role creation and resource configuration.

## Prerequisites
- Active AWS account with permissions for S3, SageMaker, ECR, and ECS.
- ZenML installed.
- AWS CLI installed and configured.

## Steps

### 1. Set Up Credentials and Local Environment
1. **Choose AWS Region**: Select your desired region in the AWS console (e.g., `us-east-1`).
2. **Create IAM Role**:
   - Get your AWS account ID:
     ```shell
     aws sts get-caller-identity --query Account --output text
     ```
   - Create `assume-role-policy.json`:
     ```json
     {
       "Version": "2012-10-17",
       "Statement": [
         {
           "Effect": "Allow",
           "Principal": {
             "AWS": "arn:aws:iam::<YOUR_ACCOUNT_ID>:root",
             "Service": "sagemaker.amazonaws.com"
           },
           "Action": "sts:AssumeRole"
         }
       ]
     }
     ```
   - Create the IAM role:
     ```shell
     aws iam create-role --role-name zenml-role --assume-role-policy-document file://assume-role-policy.json
     ```
   - Attach necessary policies:
     ```shell
     aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
     aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
     aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
     ```
3. **Install ZenML Integrations**:
   ```shell
   zenml integration install aws s3 -y
   ```

### 2. Create a ZenML Service Connector
Register an AWS Service Connector:
```shell
zenml service-connector register aws_connector \
  --type aws \
  --auth-method iam-role \
  --role_arn=<ROLE_ARN> \
  --region=<YOUR_REGION> \
  --aws_access_key_id=<YOUR_ACCESS_KEY_ID> \
  --aws_secret_access_key=<YOUR_SECRET_ACCESS_KEY>
```

### 3. Create Stack Components
#### Artifact Store (S3)
1. Create an S3 bucket:
   ```shell
   aws s3api create-bucket --bucket your-bucket-name
   ```
2. Register the S3 Artifact Store:
   ```shell
   zenml artifact-store register cloud_artifact_store -f s3 --path=s3://your-bucket-name --connector aws_connector
   ```

#### Orchestrator (SageMaker Pipelines)
1. Create a SageMaker domain (if not already created).
2. Register the SageMaker orchestrator:
   ```shell
   zenml orchestrator register sagemaker-orchestrator --flavor=sagemaker --region=<YOUR_REGION> --execution_role=<ROLE_ARN>
   ```

#### Container Registry (ECR)
1. Create an ECR repository:
   ```shell
   aws ecr create-repository --repository-name zenml --region <YOUR_REGION>
   ```
2. Register the ECR container registry:
   ```shell
   zenml container-registry register ecr-registry --flavor=aws --uri=<ACCOUNT_ID>.dkr.ecr.<YOUR_REGION>.amazonaws.com --connector aws_connector
   ```

### 4. Create Stack
```shell
export STACK_NAME=aws_stack
zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

### 5. Run a Pipeline
Define and run a simple ZenML pipeline:
```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from SageMaker!"

@pipeline
def aws_sagemaker_pipeline():
    hello_world()

if __name__ == "__main__":
    aws_sagemaker_pipeline()
```
Execute:
```shell
python run.py
```

## Cleanup
To avoid charges, delete resources:
```shell
# Delete S3 bucket
aws s3 rm s3://your-bucket-name --recursive
aws s3api delete-bucket --bucket your-bucket-name

# Delete SageMaker domain
aws sagemaker delete-domain --domain-id <DOMAIN_ID>

# Delete ECR repository
aws ecr delete-repository --repository-name zenml --force

# Detach policies and delete IAM role
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
aws iam delete-role --role-name zenml-role
```

## Conclusion
This guide covered setting up an AWS stack with ZenML for scalable machine learning pipelines, including IAM role creation, service connector setup, and stack component registration. For best practices, consider IAM roles, resource tagging, cost management, and backup strategies.

================================================================================

# Kubeflow Orchestrator Overview

The ZenML Kubeflow Orchestrator enables running ML pipelines on Kubeflow without writing Kubeflow code.

## Prerequisites
- Install ZenML `kubeflow` integration: `zenml integration install kubeflow`
- Docker installed and running
- (Optional) `kubectl` installed
- Kubernetes cluster with Kubeflow Pipelines
- Remote artifact store and container registry in ZenML stack
- Remote ZenML server deployed
- (Optional) Kubernetes context name for the remote cluster

## Configuring the Orchestrator
### Method 1: Using Service Connector (Recommended)
```bash
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubeflow
zenml service-connector list-resources --resource-type kubernetes-cluster -e  
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
zenml stack update -o <ORCHESTRATOR_NAME>
```

### Method 2: Using `kubectl` Context
```bash  
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor=kubeflow --kubernetes_context=<KUBERNETES_CONTEXT>
zenml stack update -o <ORCHESTRATOR_NAME>
```

## Running a Pipeline
Run your ZenML pipeline with:
```bash
python your_pipeline.py
```
This creates a Kubernetes pod for each pipeline step, viewable in the Kubeflow UI.

## Additional Configuration
Configure the orchestrator with `KubeflowOrchestratorSettings`:
```python
from zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import KubeflowOrchestratorSettings

kubeflow_settings = KubeflowOrchestratorSettings(
   client_args={},  
   user_namespace="my_namespace",
   pod_settings={
       "affinity": {...},
       "tolerations": [...]
   }
)

@pipeline(settings={"orchestrator": kubeflow_settings})
```

## Multi-Tenancy Deployments
Register the orchestrator with the `kubeflow_hostname`:
```bash
zenml orchestrator register <NAME> --flavor=kubeflow --kubeflow_hostname=<KUBEFLOW_HOSTNAME>
```
Provide namespace, username, and password:
```python
kubeflow_settings = KubeflowOrchestratorSettings(
   client_username="admin",
   client_password="abc123", 
   user_namespace="namespace_name"
)

@pipeline(settings={"orchestrator": kubeflow_settings})
```

For more details, refer to the full [Kubeflow Orchestrator documentation](../../component-guide/orchestrators/kubeflow.md).

================================================================================

# Interact with Secrets

## What is a ZenML Secret?
ZenML secrets are **key-value pairs** securely stored in the ZenML secrets store, identified by a **name** for easy reference in pipelines and stacks.

## Creating a Secret

### CLI
To create a secret with a name `<SECRET_NAME>` and key-value pairs:

```shell
zenml secret create <SECRET_NAME> --<KEY_1>=<VALUE_1> --<KEY_2>=<VALUE_2>
```

Alternatively, use JSON or YAML format:

```shell
zenml secret create <SECRET_NAME> --values='{"key1":"value1","key2":"value2"}'
```

For interactive creation:

```shell
zenml secret create <SECRET_NAME> -i
```

For large values or special characters, read from a file:

```bash
zenml secret create <SECRET_NAME> --key=@path/to/file.txt
zenml secret create <SECRET_NAME> --values=@path/to/file.txt
```

Use the CLI to list, update, and delete secrets. For interactive registration of missing secrets in a stack:

```shell
zenml stack register-secrets [<STACK_NAME>]
```

### Python SDK
Using the ZenML client API:

```python
from zenml.client import Client

client = Client()
client.create_secret(name="my_secret", values={"username": "admin", "password": "abc123"})
```

Other methods include `get_secret`, `update_secret`, `list_secrets`, and `delete_secret`. Full API reference available [here](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/).

## Set Scope for Secrets
Secrets can be scoped to a user. To create a user-scoped secret:

```shell
zenml secret create <SECRET_NAME> --scope user --<KEY_1>=<VALUE_1>
```

## Accessing Registered Secrets

### Referencing Secrets
To reference secrets in stack components, use the syntax: `{{<SECRET_NAME>.<SECRET_KEY>}}`.

Example:

```shell
zenml secret create mlflow_secret --username=admin --password=abc123
zenml experiment-tracker register mlflow --flavor=mlflow --tracking_username={{mlflow_secret.username}} --tracking_password={{mlflow_secret.password}}
```

ZenML validates the existence of referenced secrets before running a pipeline. Control validation with `ZENML_SECRET_VALIDATION_LEVEL`:

- `NONE`: disables validation.
- `SECRET_EXISTS`: checks for secret existence.
- `SECRET_AND_KEY_EXISTS`: (default) checks both secret and key existence.

### Fetching Secret Values in a Step
To access secrets in steps:

```python
from zenml import step
from zenml.client import Client

@step
def secret_loader() -> None:
    secret = Client().get_secret(<SECRET_NAME>)
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
```

This allows secure access to secrets without hard-coding credentials.

================================================================================

# Project Setup and Management

This section outlines the setup and management of ZenML projects, covering essential processes and configurations.

================================================================================

# Organizing Stacks, Pipelines, Models, and Artifacts in ZenML

This guide provides an overview of organizing stacks, pipelines, models, and artifacts in ZenML, which are essential for effective MLOps.

## Key Concepts

- **Stacks**: Configuration of tools and infrastructure for running pipelines, including components like orchestrators and artifact stores. Stacks allow for consistent environments across local, staging, and production setups.
  
- **Pipelines**: Sequences of steps representing tasks in the ML workflow, automating processes and providing visibility. It’s advisable to separate pipelines for different tasks (e.g., training vs. inference) for better modularity.

- **Models**: Collections of related pipelines, artifacts, and metadata, acting as a project workspace. Models facilitate data transfer between pipelines.

- **Artifacts**: Outputs of pipeline steps that can be reused across pipelines, such as datasets or trained models. Proper naming and versioning ensure traceability.

## Stack Management

- A single stack can support multiple pipelines, reducing configuration overhead and promoting reproducibility.
- Refer to the [Managing Stacks and Components](../../infrastructure-deployment/stack-deployment/README.md) guide for more details.

## Organizing Pipelines, Models, and Artifacts

### Pipelines
- Modularize workflows by separating tasks into distinct pipelines.
- Benefits include independent execution, easier code management, and better organization of runs.

### Models
- Use models to connect related pipelines and manage data flow.
- The Model Control Plane helps manage model versions and stages.

### Artifacts
- Track and reuse outputs from pipeline steps, ensuring clear history and traceability.
- Artifacts can be linked to models for better organization.

## Example Workflow

1. Team members create pipelines for feature engineering, training, and inference.
2. They use a shared `default` stack for local testing.
3. Ensure consistent preprocessing steps across pipelines.
4. Use ZenML Models to manage artifacts and facilitate collaboration.
5. Track model versions with the Model Control Plane for easy comparisons and promotions.

## Guidelines for Organization

### Models
- One model per ML use case.
- Group related pipelines and artifacts.
- Manage versions and stages effectively.

### Stacks
- Separate stacks for different environments.
- Share production and staging stacks for consistency.
- Keep local stacks simple.

### Naming and Organization
- Use consistent naming conventions.
- Leverage tags for resource organization.
- Document configurations and dependencies.
- Keep code modular and reusable.

Following these guidelines will help maintain a clean and scalable MLOps workflow as your project evolves.

================================================================================

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

================================================================================

# Shared Libraries and Logic for Teams

## Overview
Sharing code libraries enhances collaboration, robustness, and standardization across projects. This guide focuses on what can be shared and how to distribute shared components using ZenML.

## What Can Be Shared
ZenML supports sharing several custom components:

### Custom Flavors
1. Create a custom flavor in a shared repository.
2. Implement the custom stack component as per the ZenML documentation.
3. Register the component using the ZenML CLI:
   ```bash
   zenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>
   ```

### Custom Steps
Custom steps can be created in a separate repository and referenced like Python modules.

### Custom Materializers
1. Create the materializer in a shared repository.
2. Implement it as described in the ZenML documentation.
3. Team members can import and use the shared materializer.

## How to Distribute Shared Components

### Shared Private Wheels
1. Create a private PyPI server (e.g., AWS CodeArtifact).
2. Build your code into wheel format.
3. Upload the wheel to the private PyPI server.
4. Configure pip to use the private server.
5. Install packages using pip.

### Using Shared Libraries with `DockerSettings`
To include shared libraries in a Docker image:
- Specify requirements:
   ```python
   import os
   from zenml.config import DockerSettings
   from zenml import pipeline

   docker_settings = DockerSettings(
       requirements=["my-simple-package==0.1.0"],
       environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ.get('PYPI_TOKEN', '')}@my-private-pypi-server.com/{os.environ.get('PYPI_USERNAME', '')}/"}
   )

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

- Use a requirements file:
   ```python
   docker_settings = DockerSettings(requirements="/path/to/requirements.txt")

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

The `requirements.txt` should include:
```
--extra-index-url https://YOURTOKEN@my-private-pypi-server.com/YOURUSERNAME/
my-simple-package==0.1.0
```

## Best Practices
- **Version Control**: Use systems like Git for collaboration.
- **Access Controls**: Implement security measures for private repositories.
- **Documentation**: Maintain clear and comprehensive documentation.
- **Regular Updates**: Keep shared libraries updated and communicate changes.
- **Continuous Integration**: Set up CI for quality assurance of shared components.

By following these guidelines, teams can enhance collaboration and streamline development within the ZenML framework.

================================================================================

# Access Management and Roles in ZenML

Effective access management is essential for security and efficiency in ZenML projects. This guide outlines user roles and access management strategies.

## Typical Roles in an ML Project
- **Data Scientists**: Develop and run pipelines.
- **MLOps Platform Engineers**: Manage infrastructure and stack components.
- **Project Owners**: Oversee ZenML deployment and user access.

Roles may vary, but responsibilities are generally consistent.

{% hint style="info" %}
You can create [Roles in ZenML Pro](../../../getting-started/zenml-pro/roles.md) with specific permissions for Users or Teams. Sign up for a free trial: https://cloud.zenml.io/
{% endhint %}

## Service Connectors
Service connectors integrate cloud services with ZenML, abstracting credentials and configurations. Only MLOps Platform Engineers should manage these connectors, while Data Scientists can use them without access to credentials.

**Data Scientist Permissions**:
- Use connectors to create stack components and run pipelines.
- No permissions to create, update, or delete connectors.

**MLOps Platform Engineer Permissions**:
- Create, update, delete connectors, and read secret values.

{% hint style="info" %}
RBAC features are available in ZenML Pro. Learn more [here](../../../getting-started/zenml-pro/roles.md).
{% endhint %}

## Upgrade Responsibilities
Project Owners decide when to upgrade the ZenML server, consulting all teams to avoid conflicts. MLOps Platform Engineers handle the upgrade process, ensuring data backup and no service disruption.

{% hint style="info" %}
Consider using separate servers for different teams to ease upgrade pressures. ZenML Pro supports [multi-tenancy](../../../getting-started/zenml-pro/tenants.md). Sign up for a free trial: https://cloud.zenml.io/
{% endhint %}

## Pipeline Migration and Maintenance
Data Scientists own pipeline code but must collaborate with Platform Engineers to test compatibility with new ZenML versions. Both should review release notes and migration guides.

## Best Practices for Access Management
- **Regular Audits**: Periodically review user access and permissions.
- **RBAC**: Implement Role-Based Access Control for streamlined permission management.
- **Least Privilege**: Grant minimal necessary permissions.
- **Documentation**: Maintain clear records of roles and access policies.

{% hint style="info" %}
RBAC and permission assignment are exclusive to ZenML Pro users.
{% endhint %}

By adhering to these practices, you can maintain a secure and collaborative ZenML environment.

================================================================================

### Creating Your Own ZenML Template

To standardize and share ML workflows, you can create a ZenML template using Copier. Follow these steps:

1. **Create a Repository**: Store your template's code and configuration files in a new repository.

2. **Define Workflows**: Use existing ZenML templates (e.g., [starter template](https://github.com/zenml-io/template-starter)) as a base to define your ML workflows with ZenML steps and pipelines.

3. **Create `copier.yml`**: This file defines your template's parameters and default values. Refer to the [Copier docs](https://copier.readthedocs.io/en/stable/creating/) for details.

4. **Test Your Template**: Use the command below to generate a new project from your template:

   ```bash
   copier copy https://github.com/your-username/your-template.git your-project
   ```

5. **Initialize with ZenML**: Use the following command to set up your project with your template:

   ```bash
   zenml init --template https://github.com/your-username/your-template.git
   ```

   For a specific version, add the `--template-tag` option:

   ```bash
   zenml init --template https://github.com/your-username/your-template.git --template-tag v1.0.0
   ```

6. **Keep Updated**: Regularly update your template to align with best practices.

For practical examples, install the `e2e_batch` template using:

```bash
mkdir e2e_batch
cd e2e_batch
zenml init --template e2e_batch --template-with-defaults
```

Now you can efficiently set up new ML projects using your ZenML template.

================================================================================

# ZenML Project Templates Overview

## Introduction
ZenML project templates provide a quick way to understand the ZenML framework and build ML pipelines, featuring a collection of steps, pipelines, and a CLI.

## Available Project Templates

| Project Template [Short name] | Tags | Description |
|-------------------------------|------|-------------|
| [Starter template](https://github.com/zenml-io/template-starter) [code: starter] | code: basic, code: scikit-learn | Basic ML setup with parameterized steps, model training pipeline, and a simple CLI using scikit-learn. |
| [E2E Training with Batch Predictions](https://github.com/zenml-io/template-e2e-batch) [code: e2e_batch] | code: etl, code: hp-tuning, code: model-promotion, code: drift-detection, code: batch-prediction, code: scikit-learn | Two pipelines covering data loading, HP tuning, model training, evaluation, promotion, drift detection, and batch inference. |
| [NLP Training Pipeline](https://github.com/zenml-io/template-nlp) [code: nlp] | code: nlp, code: hp-tuning, code: model-promotion, code: training, code: pytorch, code: gradio, code: huggingface | Simple NLP pipeline for tokenization, training, HP tuning, evaluation, and deployment of BERT or GPT-2 models, tested locally with Gradio. |

## Collaboration
ZenML seeks design partnerships for real-world MLOps scenarios. Interested users can [join our Slack](https://zenml.io/slack/) to share their projects.

## Using a Project Template
To use templates, install ZenML with templates:

```bash
pip install zenml[templates]
```

**Note:** These templates differ from 'Run Templates' used for triggering pipelines. More information on Run Templates can be found [here](https://docs.zenml.io/how-to/trigger-pipelines).

To generate a project from a template:

```bash
zenml init --template <short_name_of_template>
# Example: zenml init --template e2e_batch
```

For default values, use:

```bash
zenml init --template <short_name_of_template> --template-with-defaults
# Example: zenml init --template e2e_batch --template-with-defaults
```

================================================================================

### Connecting Your Git Repository in ZenML

**Overview**: Connecting a code repository (e.g., GitHub, GitLab) allows ZenML to track code versions and speeds up Docker image builds by avoiding unnecessary rebuilds.

#### Registering a Code Repository

1. **Install Integration**:
   ```shell
   zenml integration install <INTEGRATION_NAME>
   ```

2. **Register Repository**:
   ```shell
   zenml code-repository register <NAME> --type=<TYPE> [--CODE_REPOSITORY_OPTIONS]
   ```

#### Available Implementations

- **GitHub**:
  - Install:
    ```shell
    zenml integration install github
    ```
  - Register:
    ```shell
    zenml code-repository register <NAME> --type=github \
    --url=<GITHUB_URL> --owner=<OWNER> --repository=<REPOSITORY> \
    --token=<GITHUB_TOKEN>
    ```
  - **Token Generation**: Go to GitHub settings > Developer settings > Personal access tokens > Generate new token.

- **GitLab**:
  - Install:
    ```shell
    zenml integration install gitlab
    ```
  - Register:
    ```shell
    zenml code-repository register <NAME> --type=gitlab \
    --url=<GITLAB_URL> --group=<GROUP> --project=<PROJECT> \
    --token=<GITLAB_TOKEN>
    ```
  - **Token Generation**: Go to GitLab settings > Access Tokens > Create personal access token.

#### Developing a Custom Code Repository

To create a custom repository, subclass `zenml.code_repositories.BaseCodeRepository` and implement the required methods:

```python
class BaseCodeRepository(ABC):
    @abstractmethod
    def login(self) -> None:
        pass

    @abstractmethod
    def download_files(self, commit: str, directory: str, repo_sub_directory: Optional[str]) -> None:
        pass

    @abstractmethod
    def get_local_context(self, path: str) -> Optional["LocalRepositoryContext"]:
        pass
```

Register the custom repository:
```shell
zenml code-repository register <NAME> --type=custom --source=my_module.MyRepositoryClass [--CODE_REPOSITORY_OPTIONS]
``` 

This setup allows you to integrate various code repositories into ZenML for efficient pipeline management.

================================================================================

# Setting up a Well-Architected ZenML Project

This guide outlines best practices for structuring ZenML projects to enhance scalability, maintainability, and team collaboration.

## Importance of a Well-Architected Project
A well-architected ZenML project is essential for effective MLOps, providing a foundation for efficient development, deployment, and maintenance of ML models.

## Key Components

### Repository Structure
- Organize folders for pipelines, steps, and configurations.
- Maintain clear separation of concerns and consistent naming conventions.

### Version Control and Collaboration
- Integrate with Git for code management and collaboration.
- Enables faster pipeline builds by reusing images and code.

### Stacks, Pipelines, Models, and Artifacts
- **Stacks**: Define infrastructure and tool configurations.
- **Models**: Represent ML models and metadata.
- **Pipelines**: Encapsulate ML workflows.
- **Artifacts**: Track data and model outputs.

### Access Management and Roles
- Define roles (e.g., data scientists, MLOps engineers).
- Set up service connectors and manage authorizations.
- Use ZenML Pro Teams for role assignment.

### Shared Components and Libraries
- Promote code reuse with custom flavors, steps, and materializers.
- Share private wheels and manage library authentication.

### Project Templates
- Utilize pre-made or custom templates for consistency.

### Migration and Maintenance
- Strategies for migrating legacy code and upgrading ZenML servers.

## Getting Started
Explore the guides in this section for detailed information on project setup and management. Regularly review and refine your project structure to meet evolving team needs. Following these guidelines will help create a robust and collaborative MLOps environment.

================================================================================

### Recommended Repository Structure and Best Practices

#### Project Structure
A recommended structure for ZenML projects is as follows:

```markdown
.
├── .dockerignore
├── Dockerfile
├── steps
│   ├── loader_step
│   │   ├── loader_step.py
│   │   └── requirements.txt (optional)
│   └── training_step
├── pipelines
│   ├── training_pipeline
│   │   ├── training_pipeline.py
│   │   └── requirements.txt (optional)
│   └── deployment_pipeline
├── notebooks
│   └── *.ipynb
├── requirements.txt
├── .zen
└── run.py
```

- **Steps and Pipelines**: Store steps and pipelines in separate Python files for better organization.
- **Code Repository**: Register your repository to track code versions and speed up Docker image builds.

#### Steps
- Keep steps in separate Python files.
- Use the `logging` module for logging, which will be recorded in the ZenML dashboard.

```python
from zenml.logger import get_logger

logger = get_logger(__name__)

@step
def training_data_loader():
    logger.info("My logs")
```

#### Pipelines
- Store pipelines in separate Python files.
- Separate pipeline execution from definition to avoid immediate execution upon import.
- Avoid naming pipelines "pipeline" to prevent conflicts.

#### .dockerignore
Exclude unnecessary files (e.g., data, virtual environments) in `.dockerignore` to optimize Docker image size and build speed.

#### Dockerfile
ZenML uses a default Docker image. You can provide your own `Dockerfile` if needed.

#### Notebooks
Organize all notebooks in a dedicated folder.

#### .zen
Run `zenml init` at the project root to define the project's scope, which is especially important for Jupyter notebooks.

#### run.py
Place pipeline runners in the root directory to ensure correct import resolution. If no `.zen` file is defined, it implicitly sets the source's root.

================================================================================

# How to Use a Private PyPI Repository

To use a private PyPI repository for packages requiring authentication, follow these steps:

1. Store credentials securely using environment variables.
2. Configure pip or poetry to utilize these credentials for package installation.
3. Optionally, use custom Docker images with the necessary authentication.

### Example Code for Authentication Setup

```python
import os
from my_simple_package import important_function
from zenml.config import DockerSettings
from zenml import step, pipeline

docker_settings = DockerSettings(
    requirements=["my-simple-package==0.1.0"],
    environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ['PYPI_TOKEN']}@my-private-pypi-server.com/{os.environ['PYPI_USERNAME']}/"}
)

@step
def my_step():
    return important_function()

@pipeline(settings={"docker": docker_settings})
def my_pipeline():
    my_step()

if __name__ == "__main__":
    my_pipeline()
```

**Note:** Handle credentials with care and use secure methods for managing and distributing authentication information within your team.

================================================================================

# Customize Docker Builds

ZenML executes pipeline steps sequentially in the local Python environment. For remote orchestrators or step operators, it builds Docker images to run pipelines in an isolated environment. This section covers controlling the dockerization process. 

For more details, refer to the [Docker](https://www.docker.com/) documentation.

================================================================================

### Docker Settings on a Step

By default, all steps in a pipeline use the same Docker image defined at the pipeline level. To customize the Docker image for specific steps, use the `DockerSettings` in the step decorator or within the configuration file.

**Using Step Decorator:**
```python
from zenml import step
from zenml.config import DockerSettings

@step(
  settings={
    "docker": DockerSettings(
      parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime"
    )
  }
)
def training(...):
    ...
```

**Using Configuration File:**
```yaml
steps:
  training:
    settings:
      docker:
        parent_image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
        required_integrations:
          - gcp
          - github
        requirements:
          - zenml
          - numpy
```

This allows for tailored Docker settings per step based on specific requirements.

================================================================================

# Specifying Pip Dependencies and Apt Packages

**Note:** Configuration for pip and apt dependencies applies only to remote pipelines, not local ones.

When using a remote orchestrator, a Dockerfile is generated at runtime to build the Docker image. You can import `DockerSettings` with `from zenml.config import DockerSettings`. By default, ZenML installs all required packages for your active stack, but you can specify additional packages in several ways:

1. **Replicate Local Environment:**
   ```python
   docker_settings = DockerSettings(replicate_local_python_environment="pip_freeze")

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

2. **Custom Command for Requirements:**
   ```python
   docker_settings = DockerSettings(replicate_local_python_environment=[
       "poetry", "export", "--extras=train", "--format=requirements.txt"
   ])

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

3. **Specify Requirements in Code:**
   ```python
   docker_settings = DockerSettings(requirements=["torch==1.12.0", "torchvision"])

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

4. **Use a Requirements File:**
   ```python
   docker_settings = DockerSettings(requirements="/path/to/requirements.txt")

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

5. **Specify ZenML Integrations:**
   ```python
   from zenml.integrations.constants import PYTORCH, EVIDENTLY

   docker_settings = DockerSettings(required_integrations=[PYTORCH, EVIDENTLY])

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

6. **Specify Apt Packages:**
   ```python
   docker_settings = DockerSettings(apt_packages=["git"])

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

7. **Disable Automatic Stack Requirement Installation:**
   ```python
   docker_settings = DockerSettings(install_stack_requirements=False)

   @pipeline(settings={"docker": docker_settings})
   def my_pipeline(...):
       ...
   ```

8. **Custom Docker Settings for Steps:**
   ```python
   docker_settings = DockerSettings(requirements=["tensorflow"])

   @step(settings={"docker": docker_settings})
   def my_training_step(...):
       ...
   ```

**Note:** You can combine methods, ensuring no overlap in requirements.

**Installation Order:**
1. Local Python environment packages
2. Stack requirements (unless disabled)
3. Required integrations
4. Specified requirements

**Additional Installer Arguments:**
```python
docker_settings = DockerSettings(python_package_installer_args={"timeout": 1000})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**Experimental:** Use `uv` for faster package installation:
```python
docker_settings = DockerSettings(python_package_installer="uv")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```
*Note:* `uv` is less stable than `pip`. If errors occur, switch back to `pip`. For more on `uv` with PyTorch, refer to [Astral Docs](https://docs.astral.sh/uv/guides/integration/pytorch/).

================================================================================

### Reusing Builds in ZenML

#### Overview
ZenML optimizes pipeline runs by reusing existing builds. A build encapsulates a pipeline and its stack, including Docker images and optionally the pipeline code.

#### What is a Build?
A pipeline build contains:
- Docker images with stack requirements and integrations.
- Optionally, the pipeline code.

**List Builds:**
```bash
zenml pipeline builds list --pipeline_id='startswith:ab53ca'
```

**Create a Build:**
```bash
zenml pipeline build --stack vertex-stack my_module.my_pipeline_instance
```

#### Reusing Builds
ZenML automatically reuses builds that match your pipeline and stack. You can specify a build ID to force the use of a specific build. Note that reusing a build executes the code in the Docker image, not local changes. To include local changes, disconnect your code from the build by registering a code repository or using the artifact store.

#### Using the Artifact Store
If no code repository is detected, ZenML uploads your code to the artifact store by default unless `allow_download_from_artifact_store` is set to `False` in `DockerSettings`.

#### Connecting Code Repositories
Connecting a Git repository speeds up Docker builds and allows code iteration without rebuilding images. ZenML reuses images built by colleagues for the same stack automatically.

**Install Git Integration:**
```sh
zenml integration install github
```

#### Detecting Local Code Repositories
ZenML checks if the files used in a pipeline are tracked in registered repositories by computing the source root and verifying its inclusion in a local checkout.

#### Tracking Code Versions
If a local code repository is detected, ZenML stores the current commit reference for the pipeline run, ensuring reproducibility. This only occurs if the local checkout is clean.

#### Best Practices
- Ensure the local checkout is clean and the latest commit is pushed for file downloads to succeed.
- For options to disable or enforce file downloads, refer to the [Docker settings documentation](./docker-settings-on-a-pipeline.md).

================================================================================

# ZenML Image File Management

ZenML determines the root directory of source files in this order:
1. If `zenml init` was executed in the current or parent directory, that directory is used.
2. If not, the parent directory of the executing Python file is used.

You can control file handling in the root directory using the following attributes in `DockerSettings`:

- **`allow_download_from_code_repository`**: If `True`, files from a registered code repository without local changes will be downloaded instead of included in the image.
- **`allow_download_from_artifact_store`**: If the previous option is `False`, and a code repository without local changes doesn't exist, files will be archived and uploaded to the artifact store if set to `True`.
- **`allow_including_files_in_images`**: If both previous options are `False`, files will be included in the Docker image if this option is enabled. Modifications to code files will require a new Docker image build.

> **Warning**: Setting all attributes to `False` is not recommended, as it may lead to unintended behavior. You must ensure all files are correctly located in the Docker images used for pipeline execution.

## File Management

- **Excluding Files**: To exclude files when downloading from a code repository, use a `.gitignore` file.
- **Including Files**: To exclude files from the Docker image and reduce size, use a `.dockerignore` file:
  - Place a `.dockerignore` file in the source root directory.
  - Alternatively, specify a `.dockerignore` file in the build config:

```python
docker_settings = DockerSettings(build_config={"dockerignore": "/path/to/.dockerignore"})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```


================================================================================

### Skip Building an Image for ZenML Pipeline

#### Overview
When executing a ZenML pipeline on a remote Stack, ZenML typically builds a Docker image with a base ZenML image and project dependencies. This process can be time-consuming due to dependency size, system performance, and internet speed. To optimize time and costs, you can use a prebuilt image instead of building one each time.

**Important Note:** Using a prebuilt image means updates to your code or dependencies won't be reflected unless included in the image.

#### Using Prebuilt Images
To use a prebuilt image, configure the `DockerSettings` class:

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

Ensure the image is pushed to a registry accessible by the orchestrator and other components.

#### Requirements for the Parent Image
The specified `parent_image` must include:
- All dependencies required for the pipeline.
- Any code files if no code repository is registered and `allow_download_from_artifact_store` is `False`.

If using an image built in a previous run for the same stack, it can be reused without modifications.

#### Stack and Integration Requirements
1. **Stack Requirements**: Retrieve stack requirements with:
   ```python
   from zenml.client import Client

   Client().set_active_stack(<YOUR_STACK>)
   stack_requirements = Client().active_stack.requirements()
   ```

2. **Integration Requirements**: Gather integration dependencies:
   ```python
   from zenml.integrations.registry import integration_registry
   from zenml.integrations.constants import HUGGINGFACE, PYTORCH
   import itertools

   required_integrations = [PYTORCH, HUGGINGFACE]
   integration_requirements = set(
       itertools.chain.from_iterable(
           integration_registry.select_integration_requirements(
               integration_name=integration,
               target_os=OperatingSystemType.LINUX,
           )
           for integration in required_integrations
       )
   )
   ```

3. **Project-Specific Requirements**: Install dependencies via Dockerfile:
   ```Dockerfile
   RUN pip install <ANY_ARGS> -r FILE
   ```

4. **System Packages**: Include necessary `apt` packages:
   ```Dockerfile
   RUN apt-get update && apt-get install -y --no-install-recommends YOUR_APT_PACKAGES
   ```

5. **Project Code Files**: Ensure your pipeline code is accessible:
   - If a code repository is registered, ZenML will handle code retrieval.
   - If `allow_download_from_artifact_store` is `True`, ZenML uploads code to the artifact store.
   - If both options are disabled, include code files in the image (not recommended).

Ensure your code is in the `/app` directory and that Python, `pip`, and `zenml` are installed in the image.

================================================================================

### Summary: Using Docker Images to Run Your Pipeline

#### Docker Settings for a Pipeline
When running a pipeline with a remote orchestrator, a Dockerfile is generated at runtime to build a Docker image using the ZenML image builder. The Dockerfile includes:

1. **Parent Image**: Starts from the official ZenML image for the active Python environment. For custom images, refer to the guide on using a custom parent image.
2. **Pip Dependencies**: ZenML detects and installs required integrations. For additional requirements, see the guide on custom dependencies.
3. **Source Files**: Source files must be accessible in the Docker container. Customize handling of source files as needed.
4. **Environment Variables**: User-defined variables can be set.

For a complete list of configuration options, refer to the [DockerSettings object](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings).

#### Configuring Docker Settings
You can customize Docker builds using the `DockerSettings` class:

```python
from zenml.config import DockerSettings
```

**Apply settings to a pipeline:**

```python
docker_settings = DockerSettings()

@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()
```

**Apply settings to a step:**

```python
@step(settings={"docker": docker_settings})
def my_step() -> None:
    pass
```

**Using a YAML configuration file:**

```yaml
settings:
    docker:
        ...
steps:
  step_name:
    settings:
        docker:
            ...
```

Refer to the configuration hierarchy for precedence details.

#### Specifying Docker Build Options
To specify build options for the image builder:

```python
docker_settings = DockerSettings(build_config={"build_options": {...}})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**For MacOS ARM architecture:**

```python
docker_settings = DockerSettings(build_config={"build_options": {"platform": "linux/amd64"}})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

#### Using a Custom Parent Image
To use a custom parent image, ensure it has Python, pip, and ZenML installed. You can specify it in Docker settings:

**Using a pre-built parent image:**

```python
docker_settings = DockerSettings(parent_image="my_registry.io/image_name:tag")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**Skip Docker builds:**

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

**Warning**: This advanced feature may lead to unintended behavior. Ensure your code files are included in the specified image. For more details, refer to the guide on using a prebuilt image.

================================================================================

# Using Custom Docker Files in ZenML

ZenML allows you to build a parent Docker image dynamically during pipeline execution by specifying a custom Dockerfile, build context, and build options. The build process is as follows:

- **No Dockerfile**: If requirements or environment settings necessitate an image build, ZenML creates one; otherwise, it uses the `parent_image`.
- **Dockerfile specified**: ZenML builds an image from the specified Dockerfile. If additional requirements need another image, ZenML builds a second image; otherwise, it uses the first image for the pipeline.

The order of package installation in the Docker image, based on `DockerSettings`, is:
1. Local Python environment packages.
2. Packages from the `requirements` attribute.
3. Packages from `required_integrations` and stack requirements.

*Note*: The intermediate image may also be used directly for executing pipeline steps.

### Example Code

```python
docker_settings = DockerSettings(
    dockerfile="/path/to/dockerfile",
    build_context_root="/path/to/build/context",
    parent_image_build_config={
        "build_options": ...,
        "dockerignore": ...
    }
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

================================================================================

### Image Builder Definition

ZenML executes pipeline steps sequentially in the active Python environment locally. For remote orchestrators or step operators, ZenML builds Docker images to run pipelines in isolated environments. By default, execution environments are created using the local Docker client, which requires Docker installation and permissions.

ZenML provides image builders, a stack component that allows building and pushing Docker images in a specialized environment. If no image builder is configured, ZenML defaults to the local image builder for consistency across builds, using the client environment.

You do not need to interact directly with the image builder in your code; it will be automatically used by any component that requires container image building, as long as it is part of your active ZenML stack.

================================================================================

# Manage Your ZenML Server

This section provides best practices for upgrading your ZenML server, using it in production, and troubleshooting. It includes recommended upgrade steps and migration guides for version transitions. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

# ZenML Server Upgrade Guide

## Overview
Upgrading your ZenML server varies based on deployment method. Refer to the [best practices for upgrading ZenML](./best-practices-upgrading-zenml.md) before proceeding. Upgrade promptly after a new version release to benefit from improvements and fixes.

## Upgrade Methods

### Docker
1. **Ensure Data Persistence**: Confirm data is stored on persistent storage or an external MySQL instance. Consider backing up data before upgrading.
2. **Delete Existing Container**:
   ```bash
   docker ps  # Find your container ID
   docker stop <CONTAINER_ID>
   docker rm <CONTAINER_ID>
   ```
3. **Deploy New Version**:
   ```bash
   docker run -it -d -p 8080:8080 --name <CONTAINER_NAME> zenmldocker/zenml-server:<VERSION>
   ```

### Kubernetes with Helm
1. **Update Helm Chart**:
   ```bash
   git clone https://github.com/zenml-io/zenml.git
   git pull
   cd src/zenml/zen_server/deploy/helm/
   ```
2. **Reuse or Extract Values**:
   ```bash
   helm -n <namespace> get values zenml-server > custom-values.yaml  # If needed
   ```
3. **Upgrade Release**:
   ```bash
   helm -n <namespace> upgrade zenml-server . -f custom-values.yaml
   ```

> **Note**: Avoid changing the container image tag in the Helm chart unless necessary, as compatibility is not guaranteed.

## Important Notes
- **Downgrading**: Not supported; may cause unexpected behavior.
- **Python Client Version**: Should match the server version.

For further details, consult the respective sections in the documentation.

================================================================================

# Best Practices for Using ZenML Server in Production

## Overview
This guide outlines best practices for setting up a ZenML server in production environments, focusing on autoscaling, performance optimization, database management, ingress/load balancing, monitoring, and backup strategies.

## Autoscaling Replicas
To handle larger pipelines and high traffic, configure autoscaling based on your deployment environment:

### Kubernetes with Helm
Enable autoscaling using the following configuration:
```yaml
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
```

### ECS (AWS)
1. Go to the ECS console and select your ZenML service.
2. Click "Update Service" and enable autoscaling in the "Service auto scaling - optional" section.

### Cloud Run (GCP)
1. Access the Cloud Run console and select your service.
2. Click "Edit & Deploy new Revision" and set minimum and maximum instances in the "Revision auto-scaling" section.

### Docker Compose
Scale your service with:
```bash
docker compose up --scale zenml-server=N
```

## High Connection Pool Values
Increase server performance by adjusting thread pool size:
```yaml
zenml:
  threadPoolSize: 100
```
Set `ZENML_SERVER_THREAD_POOL_SIZE` for other deployments. Adjust `zenml.database.poolSize` and `zenml.database.maxOverflow` accordingly.

## Scaling the Backing Database
Monitor and scale your database based on:
- **CPU Utilization**: Scale if consistently above 50%.
- **Freeable Memory**: Scale if below 100-200 MB.

## Setting Up Ingress/Load Balancer
Securely expose your ZenML server:

### Kubernetes with Helm
Enable ingress:
```yaml
zenml:
  ingress:
    enabled: true
    className: "nginx"
```

### ECS
Use Application Load Balancers for traffic routing. Refer to [AWS documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html).

### Cloud Run
Utilize Cloud Load Balancing. See [GCP documentation](https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless).

### Docker Compose
Set up an NGINX server as a reverse proxy.

## Monitoring
Implement monitoring tools based on your deployment:

### Kubernetes with Helm
Use Prometheus and Grafana. Monitor with:
```
sum by(namespace) (rate(container_cpu_usage_seconds_total{namespace=~"zenml.*"}[5m]))
```

### ECS
Utilize CloudWatch for metrics like CPU and Memory utilization.

### Cloud Run
Use Cloud Monitoring for metrics in the Cloud Run console.

## Backups
Establish a backup strategy to protect critical data:
- Automate backups with a retention period (e.g., 30 days).
- Periodically export data to external storage (e.g., S3, GCS).
- Perform manual backups before upgrades.

================================================================================

# ZenML Deployment Troubleshooting Guide

## Viewing Logs
To debug issues, analyze logs based on your deployment type.

### Kubernetes
1. Check running pods:
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> get pods
   ```
2. If pods aren't running, view logs for all pods:
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml
   ```
3. For specific container logs:
   ```bash
   kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml -c <CONTAINER_NAME>
   ```
   - Use `zenml-db-init` for `Init` state errors, otherwise use `zenml`.

### Docker
- For Docker CLI deployment:
   ```shell
   zenml logs -f
   ```
- For `docker run`:
   ```shell
   docker logs zenml -f
   ```
- For `docker compose`:
   ```shell
   docker compose -p zenml logs -f
   ```

## Fixing Database Connection Problems
Common MySQL connection issues:
- **Access Denied**:
   - Error: `ERROR 1045 (28000): Access denied for user <USER> using password YES`
   - Solution: Verify username and password.
  
- **Can't Connect to MySQL**:
   - Error: `ERROR 2003 (HY000): Can't connect to MySQL server on <HOST> (<IP>)`
   - Solution: Check host settings. Test connection:
   ```bash
   mysql -h <HOST> -u <USER> -p
   ```
   - For Kubernetes, use `kubectl port-forward` to connect locally.

## Fixing Database Initialization Problems
If migrating from a newer to an older ZenML version results in `Revision not found` errors:
1. Log in to MySQL:
   ```bash
   mysql -h <HOST> -u <NAME> -p
   ```
2. Drop the existing database:
   ```sql
   drop database <NAME>;
   ```
3. Create a new database:
   ```sql
   create database <NAME>;
   ```
4. Restart your Kubernetes pods or Docker container to reinitialize the database.

================================================================================

# Best Practices for Upgrading ZenML

## Upgrading Your Server

### Data Backups
- **Database Backup**: Create a backup of your MySQL database before upgrading to allow rollback if needed.
- **Automated Backups**: Set up daily automated backups using services like AWS RDS or Google Cloud SQL.

### Upgrade Strategies
- **Staged Upgrade**: Use two ZenML server instances (old and new) for gradual migration.
- **Team Coordination**: Align upgrade timing among teams to reduce disruption.
- **Separate ZenML Servers**: Consider dedicated servers for teams requiring different upgrade schedules.

### Minimizing Downtime
- **Upgrade Timing**: Schedule upgrades during low-activity periods.
- **Avoid Mid-Pipeline Upgrades**: Prevent interruptions to long-running pipelines.

## Upgrading Your Code

### Testing and Compatibility
- **Local Testing**: Test locally after upgrading (`pip install zenml --upgrade`) and run old pipelines for compatibility.
- **End-to-End Testing**: Develop simple tests to ensure compatibility with your pipeline code.
- **Artifact Compatibility**: Be cautious with pickle-based materializers; use version-agnostic methods when possible. Load older artifacts as follows:

```python
from zenml.client import Client

artifact = Client().get_artifact_version('YOUR_ARTIFACT_ID')
loaded_artifact = artifact.load()
```

### Dependency Management
- **Python Version**: Ensure compatibility with the ZenML version; check the [installation guide](../../getting-started/installation.md).
- **External Dependencies**: Watch for incompatible external dependencies; refer to the [release notes](https://github.com/zenml-io/zenml/releases).

### Handling API Changes
- **Changelog Review**: Always check the [changelog](https://github.com/zenml-io/zenml/releases) for breaking changes.
- **Migration Scripts**: Use available [migration scripts](migration-guide/migration-guide.md) for database schema changes.

By following these best practices, you can minimize risks and ensure a smoother upgrade process for your ZenML server. Adapt these guidelines to your specific environment.

================================================================================

# User Authentication with ZenML

Authenticate clients with the ZenML Server using the ZenML CLI and web-based login via:

```bash
zenml login https://...
```

This command initiates a browser validation process. You can choose to trust your device, which issues a 30-day token, or not, which issues a 24-hour token. To view authorized devices:

```bash
zenml authorized-device list
```

To inspect a specific device:

```bash
zenml authorized-device describe <DEVICE_ID>
```

For added security, invalidate a token with:

```bash
zenml authorized-device lock <DEVICE_ID>
```

### Summary Steps:
1. Run `zenml login <URL>` to connect.
2. Decide to trust the device.
3. List devices with `zenml devices list`.
4. Lock a device with `zenml device lock ...`.

### Important Notice
Use the ZenML CLI securely. Regularly manage device trust levels and lock devices if necessary, as every token is a potential access point to your data and infrastructure.

================================================================================

# Connecting to ZenML

Once [ZenML is deployed](../../../user-guide/production-guide/deploying-zenml.md), you can connect to it through various methods. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

================================================================================

# Connecting with a Service Account

To authenticate to a ZenML server in non-interactive environments (e.g., CI/CD, serverless functions), create a service account and use its API key.

## Create a Service Account
```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```
The API key will be displayed and cannot be retrieved later.

## Authenticate Using API Key
You can authenticate via:
- **CLI Prompt**:
  ```bash
  zenml login https://... --api-key
  ```
- **Environment Variables** (suitable for CI/CD):
  ```bash
  export ZENML_STORE_URL=https://...
  export ZENML_STORE_API_KEY=<API_KEY>
  ```
  No need to run `zenml login` after setting these variables.

## List Service Accounts and API Keys
```bash
zenml service-account list
zenml service-account api-key <SERVICE_ACCOUNT_NAME> list
```

## Describe Service Account or API Key
```bash
zenml service-account describe <SERVICE_ACCOUNT_NAME>
zenml service-account api-key <SERVICE_ACCOUNT_NAME> describe <API_KEY_NAME>
```

## Rotate API Keys
API keys do not expire, but should be rotated regularly for security:
```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME>
```
To retain the old key for a specified time (e.g., 60 minutes):
```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME> --retain 60
```

## Deactivate Service Accounts or API Keys
```bash
zenml service-account update <SERVICE_ACCOUNT_NAME> --active false
zenml service-account api-key <SERVICE_ACCOUNT_NAME> update <API_KEY_NAME> --active false
```
Deactivation takes immediate effect.

## Summary of Steps
1. Create a service account: `zenml service-account create`.
2. Authenticate: `zenml login <url> --api-key` or set environment variables.
3. List accounts: `zenml service-account list`.
4. List API keys: `zenml service-account api-key <SERVICE_ACCOUNT_NAME> list`.
5. Rotate API keys: `zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate`.
6. Deactivate accounts/keys: `zenml service-account update` or `zenml service-account api-key <SERVICE_ACCOUNT_NAME> update`.

### Important Notice
Regularly rotate API keys and deactivate/delete unused service accounts and keys to secure your data and infrastructure.

================================================================================

### ZenML Migration Guide: Version 0.58.2 to 0.60.0 (Pydantic 2)

#### Overview
ZenML has upgraded to Pydantic v2, introducing stricter validation and performance improvements. Users may encounter new validation errors due to these changes. For issues, contact us on [GitHub](https://github.com/zenml-io/zenml) or [Slack](https://zenml.io/slack-invite).

#### Dependency Updates
- **SQLModel**: Upgraded from `0.0.8` to `0.0.18` for Pydantic v2 compatibility.
- **SQLAlchemy**: Upgraded from v1 to v2. If using SQLAlchemy, refer to [their migration guide](https://docs.sqlalchemy.org/en/20/changelog/migration_20.html).

#### Pydantic v2 Features
Pydantic v2 introduces performance enhancements and new features in model design, validation, and serialization. For detailed changes, see the [Pydantic migration guide](https://docs.pydantic.dev/2.7/migration/).

#### Integration Changes
- **Airflow**: Removed dependencies due to Airflow's use of SQLAlchemy v1. Use ZenML for pipeline creation in a separate environment.
- **AWS**: Updated `sagemaker` to version `2.172.0` for `protobuf` 4 compatibility.
- **Evidently**: Updated to support Pydantic v2 (versions `0.4.16` to `0.4.22`).
- **Feast**: Removed incompatible `redis` dependency.
- **GCP & Kubeflow**: Upgraded `kfp` dependency to v2, eliminating Pydantic dependency.
- **Great Expectations**: Updated to `great-expectations>=0.17.15,<1.0` for Pydantic v2 support.
- **MLflow**: Compatible with both Pydantic versions; manual requirement added to prevent downgrades.
- **Label Studio**: Updated to support Pydantic v2 with the new `label-studio-sdk` 1.0.
- **Skypilot**: Integration deactivated due to `azurecli` incompatibility; stay on the previous ZenML version until resolved.
- **TensorFlow**: Requires `tensorflow>=2.12.0` due to dependency changes; higher Python versions recommended for compatibility.
- **Tekton**: Updated to use `kfp` v2, with documentation revised accordingly.

#### Warning
Upgrading to ZenML 0.60.0 may lead to dependency issues, especially with integrations that did not support Pydantic v2. It is advisable to set up a fresh Python environment for the upgrade.

================================================================================

### Migration Guide: ZenML 0.20.0-0.23.0 to 0.30.0-0.39.1

**Warning:** Migrating to `0.30.0` involves irreversible database changes; downgrading to `<=0.23.0` is not possible. If using an older version, refer to the [0.20.0 Migration Guide](migration-zero-twenty.md) first.

**Changes in ZenML 0.30.0:**
- Removed `ml-pipelines-sdk` dependency.
- Pipeline runs and artifacts are now stored natively in the ZenML database.

**Migration Steps:**
Run the following commands after installing the new version:

```bash
pip install zenml==0.30.0
zenml version  # Should output 0.30.0
```

================================================================================

# Migration Guide: ZenML 0.13.2 to 0.20.0

**Last updated: 2023-07-24**

ZenML 0.20.0 introduces significant architectural changes that may not be backwards compatible. This guide outlines the migration process for existing ZenML stacks and pipelines.

## Key Changes
- **Metadata Store**: ZenML now manages its own Metadata Store, eliminating the need for separate components. Migrate to a ZenML server if using remote stores.
- **ZenML Dashboard**: A new dashboard is included for managing deployments.
- **Profiles Removed**: ZenML Profiles are replaced by Projects. Existing Profiles must be manually migrated.
- **Decoupled Configuration**: Stack component configuration is now separate from implementation, requiring updates for custom components.
- **Collaborative Features**: Users can share stacks and components through the ZenML server.

## Migration Steps

### 1. Update ZenML
To revert to the previous version if issues arise:
```bash
pip install zenml==0.13.2
```

### 2. Migrate Pipeline Runs
Use the `zenml pipeline runs migrate` command:
- Backup metadata stores before upgrading.
- Connect to your ZenML server:
```bash
zenml connect
```
- Migrate runs:
```bash
zenml pipeline runs migrate PATH/TO/LOCAL/STORE/metadata.db
```
For MySQL:
```bash
zenml pipeline runs migrate DATABASE_NAME --database_type=mysql --mysql_host=URL/TO/MYSQL --mysql_username=MYSQL_USERNAME --mysql_password=MYSQL_PASSWORD
```

### 3. Deploy ZenML Server
To deploy a local server:
```bash
zenml up
```
To connect to a pre-existing server:
```bash
zenml connect
```

### 4. Migrate Profiles
1. Update ZenML to 0.20.0.
2. Connect to your ZenML server:
```bash
zenml connect
```
3. Migrate profiles:
```bash
zenml profile migrate /path/to/profile
```

### 5. Configuration Changes
- **Rename Classes**: Update `Repository` to `Client` and `BaseStepConfig` to `BaseParameters`.
- **New Settings**: Use `BaseSettings` for configuration, removing deprecated decorators.

Example of new step configuration:
```python
@step(
    experiment_tracker="mlflow_stack_comp_name",
    settings={"experiment_tracker.mlflow": {"experiment_name": "name", "nested": False}}
)
```

### 6. Post-Execution Changes
Update post-execution workflows:
```python
from zenml.post_execution import get_pipelines, get_pipeline
```

## Future Changes
- Potential removal of the secrets manager from the stack.
- Deprecation of `StepContext`.

## Reporting Bugs
For issues or feature requests, join the [Slack community](https://zenml.io/slack) or submit a [GitHub Issue](https://github.com/zenml-io/zenml/issues/new/choose). 

This guide ensures a smooth transition to ZenML 0.20.0, maintaining the integrity of your existing workflows.

================================================================================

# Migration Guide: ZenML 0.39.1 to 0.41.0

ZenML versions 0.40.0 to 0.41.0 introduced a new syntax for defining steps and pipelines. The old syntax is deprecated and will be removed in future releases.

## Overview

### Old Syntax
```python
from typing import Optional
from zenml.steps import BaseParameters, Output, StepContext, step
from zenml.pipelines import pipeline

class MyStepParameters(BaseParameters):
    param_1: int
    param_2: Optional[float] = None

@step
def my_step(params: MyStepParameters, context: StepContext) -> Output(int_output=int, str_output=str):
    result = int(params.param_1 * (params.param_2 or 1))
    result_uri = context.get_output_artifact_uri()
    return result, result_uri

@pipeline
def my_pipeline(my_step):
    my_step()

step_instance = my_step(params=MyStepParameters(param_1=17))
pipeline_instance = my_pipeline(my_step=step_instance)
pipeline_instance.run(schedule=Schedule(...))
```

### New Syntax
```python
from typing import Optional, Tuple
from zenml import get_step_context, pipeline, step

@step
def my_step(param_1: int, param_2: Optional[float] = None) -> Tuple[int, str]:
    result = int(param_1 * (param_2 or 1))
    result_uri = get_step_context().get_output_artifact_uri()
    return result, result_uri

@pipeline
def my_pipeline():
    my_step(param_1=17)

my_pipeline = my_pipeline.with_options(enable_cache=False, schedule=Schedule(...))
my_pipeline()
```

## Defining Steps

### Old Syntax
```python
from zenml.steps import step, BaseParameters

class MyStepParameters(BaseParameters):
    param_1: int
    param_2: Optional[float] = None

@step
def my_step(params: MyStepParameters) -> None:
    ...

@pipeline
def my_pipeline(my_step):
    my_step()
```

### New Syntax
```python
from zenml import pipeline, step

@step
def my_step(param_1: int, param_2: Optional[float] = None) -> None:
    ...

@pipeline
def my_pipeline():
    my_step(param_1=17)
```

## Running Steps and Pipelines

### Calling a Step
- **Old:** `my_step.entrypoint()`
- **New:** `my_step()`

### Defining a Pipeline
- **Old:** `@pipeline def my_pipeline(my_step):`
- **New:** `@pipeline def my_pipeline():`

### Configuring Pipelines
- **Old:** `pipeline_instance.configure(enable_cache=False)`
- **New:** `my_pipeline = my_pipeline.with_options(enable_cache=False)`

### Running Pipelines
- **Old:** `pipeline_instance.run(...)`
- **New:** `my_pipeline()`

### Scheduling Pipelines
- **Old:** `pipeline_instance.run(schedule=schedule)`
- **New:** `my_pipeline = my_pipeline.with_options(schedule=schedule)`

## Fetching Pipeline Information

### Old Syntax
```python
pipeline: PipelineView = zenml.post_execution.get_pipeline("first_pipeline")
last_run: PipelineRunView = pipeline.runs[0]
model_trainer_step: StepView = last_run.get_step("model_trainer")
loaded_model = model_trainer_step.output.read()
```

### New Syntax
```python
pipeline: PipelineResponseModel = zenml.client.Client().get_pipeline("first_pipeline")
last_run: PipelineRunResponseModel = pipeline.last_run
model_trainer_step: StepRunResponseModel = last_run.steps["model_trainer"]
loaded_model = model_trainer_step.output.load()
```

## Controlling Step Execution Order
### Old Syntax
```python
@pipeline
def my_pipeline(step_1, step_2, step_3):
    step_3.after(step_1)
    step_3.after(step_2)
```

### New Syntax
```python
@pipeline
def my_pipeline():
    step_3(after=["step_1", "step_2"])
```

## Defining Steps with Multiple Outputs

### Old Syntax
```python
from zenml.steps import step, Output

@step
def my_step() -> Output(int_output=int, str_output=str):
    ...
```

### New Syntax
```python
from typing import Tuple
from zenml import step

@step
def my_step() -> Tuple[int, str]:
    ...
```

## Accessing Run Information Inside Steps

### Old Syntax
```python
from zenml.steps import StepContext, step

@step
def my_step(context: StepContext) -> Any:
    ...
```

### New Syntax
```python
from zenml import get_step_context, step

@step
def my_step() -> Any:
    context = get_step_context()
    ...
```

For more detailed information, refer to the relevant sections in the ZenML documentation.

================================================================================

# ZenML Migration Guide

Migrations are required for ZenML releases with breaking changes, specifically for minor version increments (e.g., `0.X` to `0.Y`) and major version increments (first non-zero digit).

## Release Type Examples
- `0.40.2` to `0.40.3`: No breaking changes, no migration needed.
- `0.40.3` to `0.41.0`: Minor breaking changes, migration required.
- `0.39.1` to `0.40.0`: Major breaking changes, significant code adjustments needed.

## Major Migration Guides
Follow these guides sequentially for major version migrations:
- [0.13.2 → 0.20.0](migration-zero-twenty.md)
- [0.23.0 → 0.30.0](migration-zero-thirty.md)
- [0.39.1 → 0.41.0](migration-zero-forty.md)
- [0.58.2 → 0.60.0](migration-zero-sixty.md)

## Release Notes
For minor breaking changes (e.g., `0.40.3` to `0.41.0`), refer to the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases).

================================================================================

