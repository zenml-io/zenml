# docs/book/how-to/debug-and-solve-issues.md

# Debugging and Issue Resolution in ZenML

This guide provides best practices for debugging issues in ZenML and obtaining help efficiently.

### When to Seek Help
Before reaching out for assistance, follow this checklist:
- Use the Slack search function to find relevant discussions.
- Check [GitHub issues](https://github.com/zenml-io/zenml/issues) for similar problems.
- Search the [ZenML documentation](https://docs.zenml.io) using the search bar.
- Review the [common errors](debug-and-solve-issues.md#most-common-errors) section.
- Analyze [additional logs](debug-and-solve-issues.md#41-additional-logs) and [client/server logs](debug-and-solve-issues.md#client-and-server-logs) for insights.

If you still need help, post your question on [Slack](https://zenml.io/slack).

### How to Post on Slack
When posting, include the following information to facilitate quicker assistance:
1. **System Information**: Provide relevant details about your system by running specific commands in your terminal and sharing the output.

```shell
zenml info -a -s
```

To troubleshoot issues with specific packages in ZenML, you can use the `-p` option followed by the package name. For instance, to address problems with the `tensorflow` package, execute the command as follows: 

```bash
zenml command -p tensorflow
```

This allows for targeted diagnostics, helping streamline the debugging process in your ZenML projects.

```shell
zenml info -p tensorflow
```

Sure, please provide the documentation text you would like summarized.

```yaml
ZENML_LOCAL_VERSION: 0.40.2
ZENML_SERVER_VERSION: 0.40.2
ZENML_SERVER_DATABASE: mysql
ZENML_SERVER_DEPLOYMENT_TYPE: alpha
ZENML_CONFIG_DIR: /Users/my_username/Library/Application Support/zenml
ZENML_LOCAL_STORE_DIR: /Users/my_username/Library/Application Support/zenml/local_stores
ZENML_SERVER_URL: https://someserver.zenml.io
ZENML_ACTIVE_REPOSITORY_ROOT: /Users/my_username/coding/zenml/repos/zenml
PYTHON_VERSION: 3.9.13
ENVIRONMENT: native
SYSTEM_INFO: {'os': 'mac', 'mac_version': '13.2'}
ACTIVE_STACK: default
ACTIVE_USER: some_user
TELEMETRY_STATUS: disabled
ANALYTICS_CLIENT_ID: xxxxxxx-xxxxxxx-xxxxxxx
ANALYTICS_USER_ID: xxxxxxx-xxxxxxx-xxxxxxx
ANALYTICS_SERVER_ID: xxxxxxx-xxxxxxx-xxxxxxx
INTEGRATIONS: ['airflow', 'aws', 'azure', 'dash', 'evidently', 'facets', 'feast', 'gcp', 'github',
'graphviz', 'huggingface', 'kaniko', 'kubeflow', 'kubernetes', 'lightgbm', 'mlflow',
'neptune', 'neural_prophet', 'pillow', 'plotly', 'pytorch', 'pytorch_lightning', 's3', 'scipy',
'sklearn', 'slack', 'spark', 'tensorboard', 'tensorflow', 'vault', 'wandb', 'whylogs', 'xgboost']
```

### ZenML Documentation Summary

**System Information**: Providing system information enhances issue context and reduces follow-up questions, facilitating quicker resolutions.

**Issue Reporting**:
1. **Describe the Issue**:
   - What were you trying to achieve?
   - What did you expect to happen?
   - What actually happened?

2. **Reproduction Steps**: Clearly outline the steps to reproduce the error. Use text or video for clarity.

3. **Log Outputs**: Always include relevant log outputs and the full error traceback. If lengthy, attach it via services like [Pastebin](https://pastebin.com/) or [Github's Gist](https://gist.github.com/). Additionally, provide outputs for:
   - `zenml status`
   - `zenml stack describe`
   - Orchestrator logs (e.g., Kubeflow pod logs for failed steps).

4. **Additional Logs**: If default logs are insufficient, adjust the `ZENML_LOGGING_VERBOSITY` environment variable to access more detailed logs. The default setting can be modified to enhance troubleshooting.

This structured approach aids in efficient problem-solving within ZenML projects.

```
ZENML_LOGGING_VERBOSITY=INFO
```

To customize logging levels in ZenML, you can set the log level to values like `WARN`, `ERROR`, `CRITICAL`, or `DEBUG`. This is done by exporting the desired log level as an environment variable in your terminal. For instance, in a Linux environment, you would use the following command to set the log level.

```shell
export ZENML_LOGGING_VERBOSITY=DEBUG
```

### Setting Environment Variables for ZenML

To configure ZenML, you need to set environment variables. Instructions for different operating systems are available:

- **Linux**: [How to set and list environment variables](https://linuxize.com/post/how-to-set-and-list-environment-variables-in-linux/)
- **macOS**: [Setting up environment variables](https://youngstone89.medium.com/setting-up-environment-variables-in-mac-os-28e5941c771c)
- **Windows**: [Environment variables guide](https://www.computerhope.com/issues/ch000549.htm)

### Viewing Client and Server Logs

For troubleshooting ZenML Server issues, you can access the server logs. To view these logs, execute the appropriate command in your terminal.

```shell
zenml logs
```

ZenML is an open-source framework designed to streamline the process of building and deploying machine learning (ML) pipelines. It provides a standardized way to manage the entire ML lifecycle, from data ingestion to model deployment.

Key Features:
- **Pipeline Orchestration**: ZenML allows users to define, manage, and execute ML pipelines with ease.
- **Integration**: It supports various tools and platforms, enabling seamless integration with existing workflows.
- **Versioning**: ZenML provides built-in version control for data, models, and pipelines, ensuring reproducibility.
- **Experiment Tracking**: Users can track experiments and monitor performance metrics effectively.

Getting Started:
1. **Installation**: ZenML can be installed via pip: `pip install zenml`.
2. **Creating a Pipeline**: Define a pipeline using decorators and specify components for data processing, model training, and evaluation.
3. **Running Pipelines**: Execute pipelines locally or on cloud platforms, leveraging ZenML's orchestration capabilities.

Best Practices:
- Maintain modular components for reusability.
- Use versioning to manage changes in data and models.
- Regularly monitor logs for server health and performance metrics.

Logs from a healthy server should display expected operational messages, indicating successful execution of tasks and no errors. 

For more detailed usage and advanced features, refer to the official ZenML documentation.

```shell
INFO:asyncio:Syncing pipeline runs...
2022-10-19 09:09:18,195 - zenml.zen_stores.metadata_store - DEBUG - Fetched 4 steps for pipeline run '13'. (metadata_store.py:315)
2022-10-19 09:09:18,359 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)
2022-10-19 09:09:18,461 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)
2022-10-19 09:09:18,516 - zenml.zen_stores.metadata_store - DEBUG - Fetched 2 inputs and 2 outputs for step 'normalizer'. (metadata_store.py:427)
2022-10-19 09:09:18,606 - zenml.zen_stores.metadata_store - DEBUG - Fetched 0 inputs and 4 outputs for step 'importer'. (metadata_store.py:427)
```

### Common Errors in ZenML

#### Error Initializing REST Store
This error typically occurs during the setup phase. Users may encounter issues related to configuration or connectivity. To resolve this, ensure that the REST store is correctly configured in your ZenML settings and that all necessary dependencies are installed. Check network connectivity and permissions if the problem persists.

```bash
RuntimeError: Error initializing rest store with URL 'http://127.0.0.1:8237': HTTPConnectionPool(host='127.0.0.1', port=8237): Max retries exceeded with url: /api/v1/login (Caused by 
NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f9abb198550>: Failed to establish a new connection: [Errno 61] Connection refused'))
```

ZenML requires re-login after a machine restart. If you started the local ZenML server using `zenml login --local`, you must execute the command again after each restart, as local deployments do not persist through reboots. 

Additionally, ensure that the 'step_configuration' column is not null, as this may lead to errors in your workflows.

```bash
sqlalchemy.exc.IntegrityError: (pymysql.err.IntegrityError) (1048, "Column 'step_configuration' cannot be null")
```

### ZenML Error Handling Summary

1. **Step Configuration Length**: 
   - The maximum allowed length for step configurations has been increased from 4K to 65K characters. However, excessively long strings may still cause issues.

2. **Common Error - 'NoneType' Object**:
   - This error occurs when required stack components are not registered. Ensure all necessary components are included in your stack configuration to avoid this error. 

This information is crucial for troubleshooting common issues when using ZenML in your projects.

```shell
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ /home/dnth/Documents/zenml-projects/nba-pipeline/run_pipeline.py:24 in <module>                  │
│                                                                                                  │
│    21 │   reference_data_splitter,                                                               │
│    22 │   TrainingSplitConfig,                                                                   │
│    23 )                                                                                          │
│ ❱  24 from steps.trainer import random_forest_trainer                                            │
│    25 from steps.encoder import encode_columns_and_clean                                         │
│    26 from steps.importer import (                                                               │
│    27 │   import_season_schedule,                                                                │
│                                                                                                  │
│ /home/dnth/Documents/zenml-projects/nba-pipeline/steps/trainer.py:24 in <module>                 │
│                                                                                                  │
│   21 │   max_depth: int = 10000                                                                  │
│   22 │   target_col: str = "FG3M"                                                                │
│   23                                                                                             │
│ ❱ 24 @step(enable_cache=False, experiment_tracker=experiment_tracker.name)                       │
│   25 def random_forest_trainer(                                                                  │
│   26 │   train_df_x: pd.DataFrame,                                                               │
│   27 │   train_df_y: pd.DataFrame,                                                               │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
AttributeError: 'NoneType' object has no attribute 'name'
```

In the error snippet, the `step` on line 24 requires an experiment tracker but cannot locate one in the stack. To resolve this issue, register a suitable experiment tracker in the stack.

```shell
zenml experiment-tracker register mlflow_tracker --flavor=mlflow
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow by providing a standardized way to manage ML pipelines. It enables reproducibility, collaboration, and scalability in ML projects.

To integrate an experiment tracker into your ZenML stack, follow these steps:

1. **Install the Experiment Tracker**: Use the package manager to install the desired experiment tracking library compatible with ZenML (e.g., MLflow, Weights & Biases).

2. **Update Your Stack**: Modify your ZenML stack configuration to include the experiment tracker. This can be done using the ZenML CLI or by editing the stack configuration file directly.

3. **Configure Tracking**: Set up the necessary configurations for the experiment tracker, including API keys or connection settings, to ensure proper integration.

4. **Run Experiments**: Utilize the integrated experiment tracker to log and monitor your experiments, capturing metrics, parameters, and artifacts for analysis.

By following these steps, you can enhance your ML workflow with robust experiment tracking capabilities, making it easier to manage and analyze your experiments within ZenML.

```shell
zenml stack update -e mlflow_tracker
```

ZenML is a framework designed to streamline the development and deployment of machine learning (ML) workflows. It integrates various stack components, allowing users to build reproducible and scalable ML pipelines. Key features include:

- **Modular Architecture**: ZenML's stack components can be easily customized and extended to fit specific project needs.
- **Reproducibility**: Ensures consistent results across different environments by managing dependencies and configurations.
- **Scalability**: Supports scaling ML workflows from local development to production environments.

For detailed guidance on using ZenML and its components, refer to the [component guide](../component-guide/README.md).



================================================================================

# docs/book/how-to/advanced-topics/README.md

# Advanced Topics in ZenML

This section delves into advanced features and configurations of ZenML, aimed at enhancing user understanding and application in projects. Key points include:

- **Custom Pipelines**: Users can create tailored pipelines to suit specific workflows, allowing for greater flexibility and efficiency.
- **Integrations**: ZenML supports various integrations with tools and platforms, enabling seamless data flow and process automation.
- **Versioning**: Implement version control for pipelines and artifacts, ensuring reproducibility and traceability in machine learning projects.
- **Secrets Management**: Securely manage sensitive information, such as API keys and credentials, within ZenML pipelines.
- **Custom Components**: Users can develop and integrate custom components, extending ZenML’s functionality to meet unique project requirements.

This section is essential for users looking to leverage ZenML's full potential in their machine learning workflows.



================================================================================

# docs/book/how-to/manage-the-zenml-server/migration-guide/README.md

# ZenML Migration Guide

Migrations are required for ZenML releases with breaking changes, specifically for minor version increments (e.g., `0.X` to `0.Y`). Major version increments indicate significant changes and are detailed in separate migration guides.

## Release Type Examples
- **No Breaking Changes**: `0.40.2` to `0.40.3` - No migration needed.
- **Minor Breaking Changes**: `0.40.3` to `0.41.0` - Migration required.
- **Major Breaking Changes**: `0.39.1` to `0.40.0` - Significant shifts in code usage.

## Major Migration Guides
Follow these guides sequentially for major version migrations:
- [0.13.2 → 0.20.0](migration-zero-twenty.md)
- [0.23.0 → 0.30.0](migration-zero-thirty.md)
- [0.39.1 → 0.41.0](migration-zero-forty.md)

## Release Notes
For minor breaking changes, refer to the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases) for details on changes introduced.



================================================================================

# docs/book/how-to/pipeline-development/README.md

# Pipeline Development in ZenML

This section provides a comprehensive overview of pipeline development using ZenML, a framework designed to streamline the creation and management of machine learning workflows. Key components include:

- **Pipeline Structure**: ZenML pipelines consist of steps that define the flow of data and operations. Each step can be a component like data ingestion, preprocessing, model training, or evaluation.

- **Steps and Components**: Steps are modular and can be reused across different pipelines. ZenML supports various component types, including custom and pre-built components.

- **Orchestration**: ZenML integrates with orchestration tools to manage the execution of pipelines, ensuring that steps run in the correct order and handle dependencies effectively.

- **Versioning**: ZenML allows for version control of pipelines and components, facilitating reproducibility and collaboration.

- **Integration**: The framework supports integration with popular machine learning libraries and cloud platforms, making it versatile for different project requirements.

- **Configuration**: Users can configure pipelines through YAML files or programmatically, enabling flexibility in defining parameters and settings.

This section is essential for understanding how to leverage ZenML for efficient pipeline development in machine learning projects.



================================================================================

# docs/book/how-to/pipeline-development/run-remote-notebooks/limitations-of-defining-steps-in-notebook-cells.md

# Limitations of Defining Steps in Notebook Cells

To run ZenML steps defined in notebook cells remotely (using a remote orchestrator or step operator), the following conditions must be met:

- The cell must contain only Python code; Jupyter magic commands or shell commands (starting with `%` or `!`) are not allowed.
- The cell must not call code from other notebook cells; however, functions or classes imported from Python files are permitted.
- The cell must handle all necessary imports independently, including ZenML imports (e.g., `from zenml import step`), without relying on imports from previous cells.



================================================================================

# docs/book/how-to/pipeline-development/run-remote-notebooks/README.md

### Run Remote Pipelines from Notebooks

ZenML allows you to define and execute steps and pipelines directly from Jupyter notebooks. The code from your notebook cells is extracted and run as Python modules within Docker containers for remote execution. 

**Key Points:**
- Ensure that the notebook cells defining your steps adhere to specific conditions for successful execution.
- For detailed guidance, refer to the following resources:
  - [Limitations of Defining Steps in Notebook Cells](limitations-of-defining-steps-in-notebook-cells.md)
  - [Run a Single Step from a Notebook](run-a-single-step-from-a-notebook.md)

This functionality enhances the integration of ZenML into your data science workflows, leveraging the interactive capabilities of Jupyter notebooks.



================================================================================

# docs/book/how-to/pipeline-development/run-remote-notebooks/run-a-single-step-from-a-notebook.md

### Running a Single Step from a Notebook in ZenML

To execute a single step remotely from a notebook, call the step like a regular Python function. ZenML will automatically create a pipeline containing only that step and execute it on the active stack. 

**Important Note:** Be aware of the [limitations](limitations-of-defining-steps-in-notebook-cells.md) associated with defining steps in notebook cells.

```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC

# Configure the step to use a step operator. If you're not using
# a step operator, you can remove this and the step will run on
# your orchestrator instead.
@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    gamma: float = 0.001,
) -> Tuple[
    Annotated[ClassifierMixin, "trained_model"],
    Annotated[float, "training_acc"],
]:
    """Train a sklearn SVC classifier."""

    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())

    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
    print(f"Train accuracy: {train_acc}")

    return model, train_acc


X_train = pd.DataFrame(...)
y_train = pd.Series(...)

# Call the step directly. This will internally create a
# pipeline with just this step, which will be executed on
# the active stack.
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow by providing a standardized way to create, manage, and deploy ML pipelines. It emphasizes reproducibility, collaboration, and scalability, making it easier for teams to work on ML projects.

### Key Features:
- **Pipeline Abstraction**: ZenML allows users to define pipelines that encapsulate the entire ML workflow, from data ingestion to model deployment.
- **Integration with Tools**: It integrates seamlessly with popular ML tools and platforms, enabling users to leverage existing infrastructure.
- **Version Control**: ZenML supports versioning of pipelines and artifacts, ensuring reproducibility and traceability in ML experiments.
- **Modular Components**: Users can create reusable components for various stages of the ML lifecycle, promoting best practices and reducing redundancy.

### Getting Started:
1. **Installation**: ZenML can be installed via pip. Use the command `pip install zenml` to get started.
2. **Creating a Pipeline**: Define a pipeline using decorators to specify each step, such as data preprocessing, model training, and evaluation.
3. **Running Pipelines**: Execute pipelines locally or in the cloud, depending on the project's requirements.
4. **Monitoring and Logging**: ZenML provides tools for monitoring pipeline execution and logging results for analysis.

### Use Cases:
- **Collaborative Projects**: Teams can work together on ML projects with clear version control and reproducibility.
- **Experiment Tracking**: Keep track of different model versions and their performance metrics.
- **Deployment**: Simplify the deployment process of ML models to production environments.

ZenML is ideal for data scientists and ML engineers looking to enhance their workflow efficiency and maintain high standards in their projects.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/what-can-be-configured.md

### ZenML Configuration Overview

This section provides an example of a YAML configuration file for ZenML, highlighting key configuration options. For a comprehensive list of all possible keys, refer to the detailed guide on generating a template YAML file. 

Key points to note:
- The YAML file is essential for configuring ZenML pipelines.
- Important configurations include specifying components, parameters, and settings relevant to your project.

For further details and a complete list of configuration options, consult the linked documentation.

```yaml
# Build ID (i.e. which Docker image to use)
build: dcd6fafb-c200-4e85-8328-428bef98d804

# Enable flags (boolean flags that control behavior)
enable_artifact_metadata: True
enable_artifact_visualization: False
enable_cache: False
enable_step_logs: True

# Extra dictionary to pass in arbitrary values
extra: 
  any_param: 1
  another_random_key: "some_string"

# Specify the "ZenML Model"
model:
  name: "classification_model"
  version: production

  audience: "Data scientists"
  description: "This classifies hotdogs and not hotdogs"
  ethics: "No ethical implications"
  license: "Apache 2.0"
  limitations: "Only works for hotdogs"
  tags: ["sklearn", "hotdog", "classification"]

# Parameters of the pipeline 
parameters: 
  dataset_name: "another_dataset"

# Name of the run
run_name: "my_great_run"

# Schedule, if supported on the orchestrator
schedule:
  catchup: true
  cron_expression: "* * * * *"

# Real-time settings for Docker and resources
settings:
  # Controls Docker building
  docker:
    apt_packages: ["curl"]
    copy_files: True
    dockerfile: "Dockerfile"
    dockerignore: ".dockerignore"
    environment:
      ZENML_LOGGING_VERBOSITY: DEBUG
    parent_image: "zenml-io/zenml-cuda"
    requirements: ["torch"]
    skip_build: False
  
  # Control resources for the entire pipeline
  resources:
    cpu_count: 2
    gpu_count: 1
    memory: "4Gb"
  
# Per step configuration
steps:
  # Top-level key should be the name of the step invocation ID
  train_model:
    # Parameters of the step
    parameters:
      data_source: "best_dataset"

    # Step-only configuration
    experiment_tracker: "mlflow_production"
    step_operator: "vertex_gpu"
    outputs: {}
    failure_hook_source: {}
    success_hook_source: {}

    # Same as pipeline level configuration, if specified overrides for this step
    enable_artifact_metadata: True
    enable_artifact_visualization: True
    enable_cache: False
    enable_step_logs: True

    # Same as pipeline level configuration, if specified overrides for this step
    extra: {}

    # Same as pipeline level configuration, if specified overrides for this step
    model: {}
      
    # Same as pipeline level configuration, if specified overrides for this step
    settings:
      docker: {}
      resources: {}

      # Stack component specific settings
      step_operator.sagemaker:
        estimator_args:
          instance_type: m7g.medium
```

## Deep-dive: `enable_XXX` Parameters

The `enable_XXX` parameters are boolean flags for configuring ZenML functionalities:

- **`enable_artifact_metadata`**: Determines if metadata should be associated with artifacts.
- **`enable_artifact_visualization`**: Controls the attachment of visualizations to artifacts.
- **`enable_cache`**: Enables or disables caching mechanisms.
- **`enable_step_logs`**: Activates tracking of step logs. 

These parameters allow users to customize their ZenML experience based on project needs.

```yaml
enable_artifact_metadata: True
enable_artifact_visualization: True
enable_cache: True
enable_step_logs: True
```

### `build` ID

The `build` ID is the UUID of the specific [`build`](../../infrastructure-deployment/customize-docker-builds/README.md) to utilize for a pipeline. When provided, it bypasses Docker image building for remote orchestrators, using the specified Docker image from this build instead.

```yaml
build: <INSERT-BUILD-ID-HERE>
```

### Configuring the `model`

In ZenML, the `model` configuration specifies the machine learning model to be utilized within a pipeline. For detailed guidance on tracking ML models, refer to the ZenML [Model documentation](../../../user-guide/starter-guide/track-ml-models.md).

```yaml
model:
  name: "ModelName"
  version: "production"
  description: An example model
  tags: ["classifier"]
```

### Pipeline and Step Parameters

In ZenML, parameters are defined as a dictionary of JSON-serializable values at both the pipeline and step levels. These parameters allow for dynamic configuration of pipelines and steps, enabling customization and flexibility in your workflows. For detailed usage, refer to the [parameters documentation](../../pipeline-development/build-pipelines/use-pipeline-step-parameters.md).

```yaml
parameters:
    gamma: 0.01

steps:
    trainer:
        parameters:
            gamma: 0.001
```

Sure! Please provide the documentation text you would like me to summarize.

```python
from zenml import step, pipeline

@step
def trainer(gamma: float):
    # Use gamma as normal
    print(gamma)

@pipeline
def my_pipeline(gamma: float):
    # use gamma or pass it into the step
    print(0.01)
    trainer(gamma=gamma)
```

ZenML allows users to define pipeline parameters and configurations through YAML files. Notably, parameters specified in the YAML configuration take precedence over those passed in code. Typically, pipeline-level parameters are utilized across multiple steps, while step-level configurations are less common.

It's important to differentiate between parameters and artifacts: 
- **Parameters** are JSON-serializable values used in the runtime configuration of a pipeline.
- **Artifacts** represent the inputs and outputs of a step and may not be JSON-serializable; their persistence is managed by materializers in the artifact store.

To customize the name of a run, use the `run_name` parameter, which can also accept dynamic values. For more detailed information, refer to the section on configuration hierarchy.

```python
run_name: <INSERT_RUN_NAME_HERE>  
```

### ZenML Documentation Summary

**Warning:** Avoid using the same `run_name` twice, especially when scheduling runs. Incorporate auto-incrementation or timestamps in the name.

### Stack Component Runtime Settings
Runtime settings are specific configurations for a pipeline or step, outlined in a dedicated section. They define execution configurations, including Docker building and resource settings.

### Docker Settings
Docker settings can be specified as objects or as dictionary representations. Configuration files can include these settings directly for streamlined integration.

```yaml
settings:
  docker:
    requirements:
      - pandas
    
```

### ZenML Resource Settings

ZenML provides options for configuring resource settings within certain stacks. For a comprehensive overview of Docker settings, refer to the complete list [here](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings). To understand pipeline containerization, consult the documentation [here](../../infrastructure-deployment/customize-docker-builds/README.md).

```yaml
resources:
  cpu_count: 2
  gpu_count: 1
  memory: "4Gb"
```

### ZenML Configuration Overview

ZenML allows for both pipeline-level and step-specific configurations. 

#### Hooks
- **Failure and Success Hooks**: The `source` for [failure and success hooks](../../pipeline-development/build-pipelines/use-failure-success-hooks.md) can be specified. 

#### Step-Specific Configuration
Certain configurations are exclusive to individual steps:
- **`experiment_tracker`**: Specify the name of the [experiment tracker](../../../component-guide/experiment-trackers/experiment-trackers.md) to enable for the step. This must match a defined tracker in the active stack.
- **`step_operator`**: Specify the name of the [step operator](../../../component-guide/step-operators/step-operators.md) for the step, which should also be defined in the active stack.
- **`outputs`**: Configure output artifacts for the step, keyed by output name (default is `output`). Notably, the `materializer_source` specifies the UDF path for the materializer to use for this output (e.g., `materializers.some_data.materializer.materializer_class`). More details on this can be found [here](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md).

For detailed component compatibility, refer to the specific orchestrator documentation.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/README.md

### ZenML Configuration Files

ZenML simplifies pipeline configuration and execution using YAML files. These files allow users to set parameters, control caching behavior, and configure stack components at runtime.

#### Key Configuration Areas:
- **What Can Be Configured**: Details on configurable elements in ZenML pipelines. [Learn more](what-can-be-configured.md).
- **Configuration Hierarchy**: Understanding the structure of configuration files. [Learn more](configuration-hierarchy.md).
- **Autogenerate a Template YAML File**: Instructions for creating a template YAML file automatically. [Learn more](autogenerate-a-template-yaml-file.md).

This streamlined approach enables efficient management of pipeline settings, making ZenML a powerful tool for data workflows.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/autogenerate-a-template-yaml-file.md

### ZenML Configuration File Template Generation

To assist in creating a configuration file for your pipeline, ZenML allows you to autogenerate a template YAML file. Use the `.write_run_configuration_template()` method to generate this file, which will include all available options commented out. This enables you to selectively enable the settings that are relevant to your project.

```python
from zenml import pipeline
...

@pipeline(enable_cache=True) # set cache behavior at step level
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter=parameter)
    train_model(dataset)

simple_ml_pipeline.write_run_configuration_template(path="<Insert_path_here>")
```

### ZenML YAML Configuration Template Example

This section provides an example of a generated YAML configuration template for ZenML. The template outlines the structure and key components necessary for setting up a ZenML pipeline.

#### Key Components:
- **Pipeline Definition**: Specifies the sequence of steps in the pipeline.
- **Steps**: Individual tasks within the pipeline, each defined with parameters and configurations.
- **Artifacts**: Outputs generated by each step, which can be used as inputs for subsequent steps.
- **Parameters**: Customizable settings that allow users to adjust the behavior of the pipeline.

#### Usage:
To utilize the YAML template, users can modify the components according to their project requirements. This enables easy configuration and management of machine learning workflows in ZenML.

This template serves as a foundational guide for users to effectively implement and customize their ZenML pipelines.

```yaml
build: Union[PipelineBuildBase, UUID, NoneType]
enable_artifact_metadata: Optional[bool]
enable_artifact_visualization: Optional[bool]
enable_cache: Optional[bool]
enable_step_logs: Optional[bool]
extra: Mapping[str, Any]
model:
  audience: Optional[str]
  description: Optional[str]
  ethics: Optional[str]
  license: Optional[str]
  limitations: Optional[str]
  name: str
  save_models_to_registry: bool
  suppress_class_validation_warnings: bool
  tags: Optional[List[str]]
  trade_offs: Optional[str]
  use_cases: Optional[str]
  version: Union[ModelStages, int, str, NoneType]
parameters: Optional[Mapping[str, Any]]
run_name: Optional[str]
schedule:
  catchup: bool
  cron_expression: Optional[str]
  end_time: Optional[datetime]
  interval_second: Optional[timedelta]
  name: Optional[str]
  run_once_start_time: Optional[datetime]
  start_time: Optional[datetime]
settings:
  docker:
    apt_packages: List[str]
    build_context_root: Optional[str]
    build_options: Mapping[str, Any]
    copy_files: bool
    copy_global_config: bool
    dockerfile: Optional[str]
    dockerignore: Optional[str]
    environment: Mapping[str, Any]
    install_stack_requirements: bool
    parent_image: Optional[str]
    python_package_installer: PythonPackageInstaller
    replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,
     NoneType]
    required_integrations: List[str]
    requirements: Union[NoneType, str, List[str]]
    skip_build: bool
    prevent_build_reuse: bool
    allow_including_files_in_images: bool
    allow_download_from_code_repository: bool
    allow_download_from_artifact_store: bool
    target_repository: str
    user: Optional[str]
  resources:
    cpu_count: Optional[PositiveFloat]
    gpu_count: Optional[NonNegativeInt]
    memory: Optional[ConstrainedStrValue]
steps:
  load_data:
    enable_artifact_metadata: Optional[bool]
    enable_artifact_visualization: Optional[bool]
    enable_cache: Optional[bool]
    enable_step_logs: Optional[bool]
    experiment_tracker: Optional[str]
    extra: Mapping[str, Any]
    failure_hook_source:
      attribute: Optional[str]
      module: str
      type: SourceType
    model:
      audience: Optional[str]
      description: Optional[str]
      ethics: Optional[str]
      license: Optional[str]
      limitations: Optional[str]
      name: str
      save_models_to_registry: bool
      suppress_class_validation_warnings: bool
      tags: Optional[List[str]]
      trade_offs: Optional[str]
      use_cases: Optional[str]
      version: Union[ModelStages, int, str, NoneType]
    name: Optional[str]
    outputs:
      output:
        default_materializer_source:
          attribute: Optional[str]
          module: str
          type: SourceType
      materializer_source: Optional[Tuple[Source, ...]]
    parameters: {}
    settings:
      docker:
        apt_packages: List[str]
        build_context_root: Optional[str]
        build_options: Mapping[str, Any]
        copy_files: bool
        copy_global_config: bool
        dockerfile: Optional[str]
        dockerignore: Optional[str]
        environment: Mapping[str, Any]
        install_stack_requirements: bool
        parent_image: Optional[str]
        python_package_installer: PythonPackageInstaller
        replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,
         NoneType]
        required_integrations: List[str]
        requirements: Union[NoneType, str, List[str]]
        skip_build: bool
        prevent_build_reuse: bool
        allow_including_files_in_images: bool
        allow_download_from_code_repository: bool
        allow_download_from_artifact_store: bool
        target_repository: str
        user: Optional[str]
      resources:
        cpu_count: Optional[PositiveFloat]
        gpu_count: Optional[NonNegativeInt]
        memory: Optional[ConstrainedStrValue]
    step_operator: Optional[str]
    success_hook_source:
      attribute: Optional[str]
      module: str
      type: SourceType
  train_model:
    enable_artifact_metadata: Optional[bool]
    enable_artifact_visualization: Optional[bool]
    enable_cache: Optional[bool]
    enable_step_logs: Optional[bool]
    experiment_tracker: Optional[str]
    extra: Mapping[str, Any]
    failure_hook_source:
      attribute: Optional[str]
      module: str
      type: SourceType
    model:
      audience: Optional[str]
      description: Optional[str]
      ethics: Optional[str]
      license: Optional[str]
      limitations: Optional[str]
      name: str
      save_models_to_registry: bool
      suppress_class_validation_warnings: bool
      tags: Optional[List[str]]
      trade_offs: Optional[str]
      use_cases: Optional[str]
      version: Union[ModelStages, int, str, NoneType]
    name: Optional[str]
    outputs: {}
    parameters: {}
    settings:
      docker:
        apt_packages: List[str]
        build_context_root: Optional[str]
        build_options: Mapping[str, Any]
        copy_files: bool
        copy_global_config: bool
        dockerfile: Optional[str]
        dockerignore: Optional[str]
        environment: Mapping[str, Any]
        install_stack_requirements: bool
        parent_image: Optional[str]
        python_package_installer: PythonPackageInstaller
        replicate_local_python_environment: Union[List[str], PythonEnvironmentExportMethod,
         NoneType]
        required_integrations: List[str]
        requirements: Union[NoneType, str, List[str]]
        skip_build: bool
        prevent_build_reuse: bool
        allow_including_files_in_images: bool
        allow_download_from_code_repository: bool
        allow_download_from_artifact_store: bool
        target_repository: str
        user: Optional[str]
      resources:
        cpu_count: Optional[PositiveFloat]
        gpu_count: Optional[NonNegativeInt]
        memory: Optional[ConstrainedStrValue]
    step_operator: Optional[str]
    success_hook_source:
      attribute: Optional[str]
      module: str
      type: SourceType

```

To configure your ZenML pipeline with a specific stack, use the command: `...write_run_configuration_template(stack=<Insert_stack_here>)`. This allows you to tailor your pipeline to the desired stack environment.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/runtime-configuration.md

### ZenML Runtime Configuration Settings

ZenML allows users to configure runtime settings for pipelines and stack components through a central concept known as `BaseSettings`. These settings enable customization of various aspects of the pipeline, including:

- **Resource Requirements**: Specify the resources needed for each step.
- **Containerization**: Define requirements for Docker image builds.
- **Component-Specific Configurations**: Pass parameters like experiment names at runtime.

#### Types of Settings

1. **General Settings**: Applicable across all ZenML pipelines.
   - Examples:
     - [`DockerSettings`](../customize-docker-builds/README.md)
     - [`ResourceSettings`](../training-with-gpus/training-with-gpus.md)

2. **Stack-Component-Specific Settings**: Provide runtime configurations for specific stack components. The key format is `<COMPONENT_CATEGORY>` or `<COMPONENT_CATEGORY>.<COMPONENT_FLAVOR>`. Settings for inactive components are ignored.
   - Examples:
     - [`SkypilotAWSOrchestratorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-skypilot_aws/#zenml.integrations.skypilot_aws.flavors.skypilot_orchestrator_aws_vm_flavor.SkypilotAWSOrchestratorSettings)
     - [`KubeflowOrchestratorSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-kubeflow/#zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor.KubeflowOrchestratorSettings)
     - [`MLflowExperimentTrackerSettings`](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-mlflow/#zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor.MLFlowExperimentTrackerSettings)
     - Additional settings for W&B, Whylogs, AWS Sagemaker, GCP Vertex, and AzureML.

#### Registration-Time vs. Real-Time Settings

- **Registration-Time Settings**: Static configurations set during component registration (e.g., `tracking_url` for MLflow).
- **Real-Time Settings**: Dynamic configurations that can change with each pipeline run (e.g., `experiment_name`).

Default values for settings can be specified during registration, which will apply unless overridden at runtime.

#### Key Specification for Settings

When defining stack-component-specific settings, use the correct key format. If only the category (e.g., `step_operator`) is specified, ZenML applies those settings to any flavor of the component in the stack. If the settings do not match the component flavor, they will be ignored. For instance, to specify `estimator_args` for the SagemakerStepOperator, use the key `step_operator`. 

This structured approach to settings allows for flexible and powerful configuration of ZenML pipelines, enabling users to tailor their machine learning workflows effectively.

```python
@step(step_operator="nameofstepoperator", settings= {"step_operator": {"estimator_args": {"instance_type": "m7g.medium"}}})
def my_step():
  ...

# Using the class
@step(step_operator="nameofstepoperator", settings= {"step_operator": SagemakerStepOperatorSettings(instance_type="m7g.medium")})
def my_step():
  ...
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow. It provides a standardized way to create reproducible ML pipelines, enabling users to focus on model development rather than infrastructure concerns. Key features include:

- **Pipeline Abstraction**: ZenML allows users to define pipelines in a modular way, promoting reusability and collaboration.
- **Integrations**: It supports various tools and platforms, such as TensorFlow, PyTorch, and cloud services, facilitating seamless integration into existing workflows.
- **Versioning**: ZenML automatically tracks versions of data, code, and models, ensuring reproducibility and traceability.
- **Environment Management**: Users can manage different environments for experimentation and production, simplifying the transition between them.

To use ZenML in projects, follow these steps:

1. **Installation**: Install ZenML via pip: `pip install zenml`.
2. **Initialize a Repository**: Use `zenml init` to set up a new ZenML repository.
3. **Create a Pipeline**: Define your pipeline components (steps) and connect them using decorators.
4. **Run Pipelines**: Execute the pipeline using the ZenML CLI or programmatically.
5. **Monitor and Manage**: Utilize ZenML's dashboard to monitor pipeline runs and manage artifacts.

For detailed usage, refer to the official ZenML documentation, which covers advanced features, best practices, and examples.

```yaml
steps:
  my_step:
    step_operator: "nameofstepoperator"
    settings:
      step_operator:
        estimator_args:
          instance_type: m7g.medium
```

ZenML is an open-source framework designed to streamline the development and deployment of machine learning (ML) workflows. It provides a standardized way to manage the entire ML lifecycle, from data ingestion to model deployment. Key features include:

- **Pipeline Abstraction**: ZenML allows users to define reusable pipelines that encapsulate various stages of ML processes, promoting modularity and collaboration.
- **Integration with Tools**: It integrates seamlessly with popular ML and data engineering tools, enabling users to leverage existing infrastructure and services.
- **Version Control**: ZenML supports versioning of data, models, and pipelines, ensuring reproducibility and traceability in ML projects.
- **Experiment Tracking**: Users can track experiments and their results, facilitating better decision-making and optimization of ML models.
- **Deployment Flexibility**: The framework supports multiple deployment environments, allowing models to be deployed in various settings, from local to cloud infrastructures.

To get started with ZenML, users can install it via pip, create a new pipeline, and integrate it with their preferred tools. The documentation provides comprehensive guides and examples to assist users in implementing ZenML in their projects effectively.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/retrieve-used-configuration-of-a-run.md

## Extracting Configuration from a Pipeline Run in ZenML

To retrieve the configuration used for a completed pipeline run, you can load the pipeline run and access its `config` attribute. This can also be done for individual steps within the pipeline by accessing their respective `config` attributes. This feature allows users to analyze the configurations applied during previous runs for better understanding and reproducibility.

```python
from zenml.client import Client

pipeline_run = Client().get_pipeline_run(<PIPELINE_RUN_NAME>)

# General configuration for the pipeline
pipeline_run.config

# Configuration for a specific step
pipeline_run.steps[<STEP_NAME>].config
```

ZenML is an open-source framework designed to streamline the process of building and managing machine learning (ML) pipelines. It emphasizes reproducibility, collaboration, and ease of use, making it suitable for both beginners and experienced practitioners.

Key Features:
- **Pipeline Abstraction**: ZenML allows users to define ML workflows as pipelines, which can be easily versioned and reused.
- **Integration with Tools**: It supports integration with various ML tools and cloud platforms, enhancing flexibility in tool selection.
- **Artifact Management**: ZenML manages artifacts generated during the pipeline execution, ensuring that results are reproducible.
- **Version Control**: It provides built-in version control for pipelines, enabling tracking of changes and facilitating collaboration among team members.

Getting Started:
1. **Installation**: ZenML can be installed via pip, making it accessible for quick setup.
2. **Creating Pipelines**: Users can define their pipelines using simple Python code, specifying components like data ingestion, model training, and evaluation.
3. **Execution**: Pipelines can be executed locally or in the cloud, with support for orchestration tools to manage workflows.

ZenML aims to simplify the ML lifecycle, making it easier for teams to collaborate and maintain high-quality standards in their projects.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/how-to-use-config.md

### ZenML Configuration Files

ZenML allows configuration through YAML files, promoting best practices by separating configuration from code. While all configurations can be specified in code, using a YAML file is recommended for clarity and maintainability.

To apply your configuration to a pipeline, use the `with_options(config_path=<PATH_TO_CONFIG>)` pattern. 

#### Example
A minimal example of using a file-based configuration in YAML can be implemented as follows:

```yaml
# Example YAML configuration
```

This approach helps streamline project setup and enhances readability.

```yaml
enable_cache: False

# Configure the pipeline parameters
parameters:
  dataset_name: "best_dataset"  
  
steps:
  load_data:  # Use the step name here
    enable_cache: False  # same as @step(enable_cache=False)
```

```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline  # This function combines steps together 
def simple_ml_pipeline(dataset_name: str):
    load_data(dataset_name)
    
if __name__=="__main__":
    simple_ml_pipeline.with_options(config_path=<INSERT_PATH_TO_CONFIG_YAML>)()
```

To run the `simple_ml_pipeline` in ZenML with caching disabled for the `load_data` step and the `dataset_name` parameter set to `best_dataset`, use the following configuration. This allows for efficient data handling while ensuring the pipeline operates with the specified dataset. 

For visual reference, see the ZenML Scarf image provided. 

This setup is essential for users looking to optimize their machine learning workflows using ZenML.



================================================================================

# docs/book/how-to/pipeline-development/use-configuration-files/configuration-hierarchy.md

### ZenML Configuration Hierarchy

In ZenML, configuration settings can be applied at both the pipeline and step levels, with specific rules governing their precedence:

- **Code vs. YAML**: Configurations defined in code take precedence over those specified in the YAML file.
- **Step vs. Pipeline**: Step-level configurations override pipeline-level configurations.
- **Attribute Merging**: When dealing with attributes, dictionaries are merged.

Understanding this hierarchy is crucial for effectively managing configurations in your ZenML projects.

```python
from zenml import pipeline, step
from zenml.config import ResourceSettings


@step
def load_data(parameter: int) -> dict:
    ...

@step(settings={"resources": ResourceSettings(gpu_count=1, memory="2GB")})
def train_model(data: dict) -> None:
    ...


@pipeline(settings={"resources": ResourceSettings(cpu_count=2, memory="1GB")}) 
def simple_ml_pipeline(parameter: int):
    ...
    
# ZenMl merges the two configurations and uses the step configuration to override 
# values defined on the pipeline level

train_model.configuration.settings["resources"]
# -> cpu_count: 2, gpu_count=1, memory="2GB"

simple_ml_pipeline.configuration.settings["resources"]
# -> cpu_count: 2, memory="1GB"
```

ZenML is an open-source framework designed to streamline the creation and management of reproducible machine learning (ML) pipelines. It facilitates the integration of various tools and platforms, enabling data scientists and ML engineers to focus on developing models rather than managing infrastructure.

### Key Features:
- **Pipeline Abstraction**: ZenML provides a high-level abstraction for defining ML workflows, allowing users to create modular and reusable components.
- **Integration**: It supports integration with popular ML tools and cloud services, enhancing flexibility and scalability.
- **Reproducibility**: ZenML ensures that pipelines can be easily reproduced, which is crucial for experimentation and production deployment.
- **Version Control**: The framework includes built-in versioning for datasets, models, and pipelines, promoting better collaboration and tracking.

### Getting Started:
1. **Installation**: ZenML can be installed via pip:
   ```bash
   pip install zenml
   ```
2. **Creating a Pipeline**: Users can define a pipeline by creating steps that encapsulate data processing, training, and evaluation tasks.
3. **Running Pipelines**: Pipelines can be executed locally or deployed to cloud environments, depending on project requirements.

### Use Cases:
- **Experiment Tracking**: ZenML helps in tracking experiments and comparing results efficiently.
- **Productionization**: It simplifies the transition from development to production, ensuring smooth deployment of ML models.

ZenML is ideal for teams looking to enhance their ML workflow efficiency while maintaining high standards of reproducibility and collaboration.



================================================================================

# docs/book/how-to/pipeline-development/develop-locally/local-prod-pipeline-variants.md

### Creating Pipeline Variants for Local Development and Production in ZenML

When developing ZenML pipelines, it's useful to create different variants for local development and production environments. This enables rapid iteration during development while ensuring a robust setup for production. You can achieve this through:

1. **Configuration Files**: Use YAML files to specify pipeline and step configurations.
2. **Code Implementation**: Directly implement variants within your code.
3. **Environment Variables**: Utilize environment variables to manage configurations.

These methods provide flexibility in managing your pipeline setups effectively.

```yaml
enable_cache: False
parameters:
    dataset_name: "small_dataset"
steps:
    load_data:
        enable_cache: False
```

The config file configures a development variant of ZenML by utilizing a smaller dataset and disabling caching. To implement this configuration in your pipeline, use the `with_options(config_path=<PATH_TO_CONFIG>)` method.

```python
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    ...

@pipeline
def ml_pipeline(dataset_name: str):
    load_data(dataset_name)

if __name__ == "__main__":
    ml_pipeline.with_options(config_path="path/to/config.yaml")()
```

ZenML allows for the creation of separate configuration files for different environments. Use `config_dev.yaml` for local development and `config_prod.yaml` for production settings. Additionally, you can implement pipeline variants directly within your code, enabling flexibility and customization in your workflows.

```python
import os
from zenml import step, pipeline

@step
def load_data(dataset_name: str) -> dict:
    # Load data based on the dataset name
    ...

@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "small_dataset" if is_dev else "full_dataset"
    load_data(dataset)

if __name__ == "__main__":
    is_dev = os.environ.get("ZENML_ENVIRONMENT") == "dev"
    ml_pipeline(is_dev=is_dev)
```

ZenML allows users to easily switch between development and production variants of their projects using a boolean flag. Additionally, environment variables can be utilized to specify which variant to execute, providing flexibility in managing different environments.

```python
import os

if os.environ.get("ZENML_ENVIRONMENT") == "dev":
    config_path = "config_dev.yaml"
else:
    config_path = "config_prod.yaml"

ml_pipeline.with_options(config_path=config_path)()
```

To run your ZenML pipeline, use the command: `ZENML_ENVIRONMENT=dev python run.py` for development or `ZENML_ENVIRONMENT=prod python run.py` for production. 

### Development Variant Considerations
When creating a development variant of your pipeline, optimize for faster iteration and debugging by:

- Using smaller datasets
- Specifying a local stack for execution
- Reducing the number of training epochs
- Decreasing batch size
- Utilizing a smaller base model

These adjustments can significantly enhance the efficiency of your development process.

```yaml
parameters:
    dataset_path: "data/small_dataset.csv"
epochs: 1
batch_size: 16
stack: local_stack
```

Sure! Please provide the documentation text you would like me to summarize.

```python
@pipeline
def ml_pipeline(is_dev: bool = False):
    dataset = "data/small_dataset.csv" if is_dev else "data/full_dataset.csv"
    epochs = 1 if is_dev else 100
    batch_size = 16 if is_dev else 64
    
    load_data(dataset)
    train_model(epochs=epochs, batch_size=batch_size)
```

ZenML allows you to create different variants of your pipeline, enabling quick local testing and debugging with a lightweight setup while preserving a full-scale configuration for production. This approach enhances your development workflow and facilitates efficient iteration without affecting the production pipeline.



================================================================================

# docs/book/how-to/pipeline-development/develop-locally/README.md

# Develop Locally with ZenML

This section outlines best practices for developing pipelines locally, allowing for faster iteration and cost-effective testing. Users often work with a smaller subset of data or synthetic data during local development. ZenML supports this workflow, enabling users to develop locally and then transition to running pipelines on more powerful remote hardware when necessary.



================================================================================

# docs/book/how-to/pipeline-development/develop-locally/keep-your-dashboard-server-clean.md

### Keeping Your ZenML Pipeline Runs Clean

During pipeline development, frequent runs can clutter your server and dashboard. ZenML offers strategies to maintain a clean environment:

- **Run Locally**: Disconnect from the remote server and initiate a local server to prevent cluttering the shared environment. This allows for efficient debugging without affecting the main dashboard. 

Utilizing these methods helps streamline your development process and keeps your workspace organized.

```bash
zenml login --local
```

ZenML allows for local runs without the need for remote infrastructure, providing a clean and efficient way to manage your workflows. However, there are limitations when using remote infrastructure. To reconnect to the server for shared runs, use the command `zenml login <remote-url>`. 

### Pipeline Runs
You can create pipeline runs that are not explicitly linked to a pipeline by using the `unlisted` parameter during execution.

```python
pipeline_instance.run(unlisted=True)
```

### ZenML Documentation Summary

**Unlisted Runs**: Unlisted runs are not shown on the pipeline's dashboard page but can be found in the pipeline run section. This feature helps maintain a clean and focused history for important pipelines.

**Deleting Pipeline Runs**: To delete a specific pipeline run, utilize a script designed for this purpose. 

This functionality supports better management of pipeline histories in ZenML projects.

```bash
zenml pipeline runs delete <PIPELINE_RUN_NAME_OR_ID>
```

To delete all pipeline runs from the last 24 hours in ZenML, you can execute the following script. This operation allows for efficient management of your pipeline runs by clearing out recent executions that may no longer be needed. 

Ensure you have the necessary permissions and context set up before running the script to avoid unintended data loss. 

For detailed usage and further customization options, refer to the ZenML documentation.

```
#!/usr/bin/env python3

import datetime
from zenml.client import Client

def delete_recent_pipeline_runs():
    # Initialize ZenML client
    zc = Client()
    
    # Calculate the timestamp for 24 hours ago
    twenty_four_hours_ago = datetime.datetime.utcnow() - datetime.timedelta(hours=24)
    
    # Format the timestamp as required by ZenML
    time_filter = twenty_four_hours_ago.strftime("%Y-%m-%d %H:%M:%S")
    
    # Get the list of pipeline runs created in the last 24 hours
    recent_runs = zc.list_pipeline_runs(created=f"gt:{time_filter}")
    
    # Delete each run
    for run in recent_runs:
        print(f"Deleting run: {run.id} (Created: {run.body.created})")
        zc.delete_pipeline_run(run.id)
    
    print(f"Deleted {len(recent_runs)} pipeline runs.")

if __name__ == "__main__":
    delete_recent_pipeline_runs()
```

### ZenML Documentation Summary

**Pipelines: Deleting Pipelines**  
To delete pipelines that are no longer needed, use the following command: 

*Insert command here*

This allows for efficient management of your pipeline resources within ZenML. Adjust the command as necessary for different time ranges or specific pipeline contexts.

```bash
zenml pipeline delete <PIPELINE_ID_OR_NAME>
```

ZenML enables users to start with a clean slate by deleting a pipeline and all its associated runs, which can be beneficial for maintaining a tidy development environment. Each pipeline can be assigned a unique name for identification, particularly useful during multiple iterations. By default, ZenML auto-generates names based on the current date and time, but users can specify a custom `run_name` when defining the pipeline.

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name"
)
training_pipeline()
```

### ZenML Documentation Summary

#### Pipeline Naming
- Pipeline names must be unique. For details, refer to the [naming pipeline runs documentation](../../pipeline-development/build-pipelines/name-your-pipeline-and-runs.md).

#### Models
- Models must be explicitly registered or passed when defining a pipeline. 
- To run a pipeline without attaching a model, avoid actions outlined in the [model registration documentation](../../model-management-metrics/model-control-plane/register-a-model.md).
- Models and specific versions can be deleted using the CLI or Python SDK. 
- To delete all versions of a model, specific commands can be utilized (details not provided in the excerpt). 

This summary provides essential information on naming conventions for pipelines and model management within ZenML, aiding users in effectively utilizing the framework in their projects.

```bash
zenml model delete <MODEL_NAME>
```

### ZenML: Deleting Models and Pruning Artifacts

To delete models in ZenML, refer to the detailed documentation [here](../../model-management-metrics/model-control-plane/delete-a-model.md).

#### Pruning Artifacts
To delete artifacts that are not referenced by any pipeline runs, utilize the following CLI command. This helps maintain a clean workspace by removing unused artifacts. 

For further details, consult the full documentation.

```bash
zenml artifact prune
```

In ZenML, the default behavior for deleting artifacts removes them from both the artifact store and the database. This can be modified using the `--only-artifact` and `--only-metadata` flags. For further details, refer to the documentation on artifact pruning.

To clean your environment, the `zenml clean` command can be executed to remove all pipelines, pipeline runs, and associated metadata, as well as all artifacts. The `--local` flag can be used to delete local files related to the active stack. Note that `zenml clean` only affects local data and does not delete server-side artifacts or pipelines. Utilizing these options helps maintain a clean and organized pipeline dashboard, allowing you to focus on relevant runs for your project.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/schedule-a-pipeline.md

### Scheduling Pipelines in ZenML

ZenML allows you to set, pause, and stop schedules for pipelines. However, scheduling support varies by orchestrator. Below is a summary of orchestrators and their scheduling capabilities:

| Orchestrator | Scheduling Support |
|--------------|--------------------|
| [AirflowOrchestrator](../../../component-guide/orchestrators/airflow.md) | ✅ |
| [AzureMLOrchestrator](../../../component-guide/orchestrators/azureml.md) | ✅ |
| [DatabricksOrchestrator](../../../component-guide/orchestrators/databricks.md) | ✅ |
| [HyperAIOrchestrator](../../component-guide/orchestrators/hyperai.md) | ✅ |
| [KubeflowOrchestrator](../../../component-guide/orchestrators/kubeflow.md) | ✅ |
| [KubernetesOrchestrator](../../../component-guide/orchestrators/kubernetes.md) | ✅ |
| [LocalOrchestrator](../../../component-guide/orchestrators/local.md) | ⛔️ |
| [LocalDockerOrchestrator](../../../component-guide/orchestrators/local-docker.md) | ⛔️ |
| [SagemakerOrchestrator](../../../component-guide/orchestrators/sagemaker.md) | ⛔️ |
| [SkypilotAWSOrchestrator](../../../component-guide/orchestrators/skypilot-vm.md) | ⛔️ |
| [SkypilotAzureOrchestrator](../../../component-guide/orchestrators/skypilot-vm.md) | ⛔️ |
| [SkypilotGCPOrchestrator](../../../component-guide/orchestrators/skypilot-vm.md) | ⛔️ |
| [SkypilotLambdaOrchestrator](../../../component-guide/orchestrators/skypilot-vm.md) | ⛔️ |
| [TektonOrchestrator](../../../component-guide/orchestrators/tekton.md) | ⛔️ |
| [VertexOrchestrator](../../../component-guide/orchestrators/vertex.md) | ✅ |

For a successful implementation, ensure you choose an orchestrator that supports scheduling.

```python
from zenml.config.schedule import Schedule
from zenml import pipeline
from datetime import datetime

@pipeline()
def my_pipeline(...):
    ...

# Use cron expressions
schedule = Schedule(cron_expression="5 14 * * 3")
# or alternatively use human-readable notations
schedule = Schedule(start_time=datetime.now(), interval_second=1800)

my_pipeline = my_pipeline.with_options(schedule=schedule)
my_pipeline()
```

### ZenML Scheduling Overview

ZenML allows users to schedule pipelines, with the method of scheduling dependent on the orchestrator in use. For instance, if using Kubeflow, users can manage scheduled runs via the Kubeflow UI. However, the specific steps for pausing or stopping a schedule will vary by orchestrator, so it's essential to consult the relevant documentation for detailed instructions.

**Key Points:**
- ZenML facilitates scheduling, but users are responsible for managing the lifecycle of these schedules.
- Running a pipeline with a schedule multiple times results in the creation of multiple scheduled pipelines, each with unique names.

For more information on scheduling options, refer to the [SDK docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.schedule.Schedule).

**Related Resources:**
- Learn about remote orchestrators [here](../../../component-guide/orchestrators/orchestrators.md).



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/delete-a-pipeline.md

### Deleting a Pipeline in ZenML

To delete a pipeline in ZenML, you can use either the Command Line Interface (CLI) or the Python SDK.

#### Using the CLI
- **Command**: Use the appropriate command in the CLI to remove the desired pipeline.

#### Using the Python SDK
- **Method**: Utilize the relevant function in the Python SDK to delete the pipeline programmatically.

This functionality allows users to manage their pipelines effectively within ZenML.

```shell
zenml pipeline delete <PIPELINE_NAME>
```

ZenML is a framework designed to streamline the machine learning (ML) workflow, enabling reproducibility and collaboration. The Python SDK is a core component, providing tools to build and manage ML pipelines efficiently.

Key Features:
- **Pipeline Creation**: Easily define and manage ML pipelines using decorators and context managers.
- **Integration**: Supports various ML libraries and tools, allowing seamless integration into existing workflows.
- **Reproducibility**: Ensures consistent results through versioning and tracking of pipeline components.
- **Modularity**: Encourages the use of reusable components, promoting best practices in ML development.

Usage:
1. **Installation**: Install ZenML via pip.
2. **Pipeline Definition**: Use `@pipeline` decorator to define a pipeline, and `@step` decorator for individual steps.
3. **Execution**: Run pipelines using the ZenML CLI or Python API.
4. **Artifact Management**: Automatically track and manage artifacts generated during pipeline execution.

ZenML is ideal for teams looking to enhance their ML processes with a focus on collaboration, reproducibility, and efficiency.

```python
from zenml.client import Client

Client().delete_pipeline(<PIPELINE_NAME>)
```

To delete a pipeline in ZenML, be aware that this action does not remove associated runs or artifacts. For bulk deletion of multiple pipelines, the Python SDK is recommended. If your pipelines share the same prefix, you must provide the `id` for each pipeline to ensure proper identification. You can utilize a script to facilitate this process.

```python
from zenml.client import Client

client = Client()

# Get the list of pipelines that start with "test_pipeline"
# use a large size to ensure we get all of them
pipelines_list = client.list_pipelines(name="startswith:test_pipeline", size=100)

target_pipeline_ids = [p.id for p in pipelines_list.items]

print(f"Found {len(target_pipeline_ids)} pipelines to delete")

confirmation = input("Do you really want to delete these pipelines? (y/n): ").lower()

if confirmation == 'y':
    print(f"Deleting {len(target_pipeline_ids)} pipelines")
    for pid in target_pipeline_ids:
        client.delete_pipeline(pid)
    print("Deletion complete")
else:
    print("Deletion cancelled")
```

## Deleting a Pipeline Run in ZenML

To delete a pipeline run, utilize the following methods:

### CLI Command
You can execute a specific command in the CLI to remove a pipeline run.

### Client Method
Alternatively, you can use the ZenML client to delete a pipeline run programmatically.

Ensure you have the necessary permissions and confirm the run you wish to delete, as this action is irreversible.

```shell
zenml pipeline runs delete <RUN_NAME_OR_ID>
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow by providing a standardized way to build, manage, and deploy ML pipelines. The Python SDK is a core component that allows users to create and manage these pipelines efficiently.

### Key Features of ZenML Python SDK:
- **Pipeline Creation**: Easily define ML pipelines using decorators and functions.
- **Integration**: Supports various tools and platforms, enabling seamless integration with existing workflows.
- **Versioning**: Automatically tracks and manages versions of pipelines and components for reproducibility.
- **Modularity**: Encourages modular design, allowing users to reuse components across different projects.
- **Extensibility**: Users can extend the SDK with custom components and integrations.

### Getting Started:
1. **Installation**: Install the ZenML Python SDK via pip:
   ```bash
   pip install zenml
   ```
2. **Initialize a Repository**: Create a new ZenML repository to manage your pipelines:
   ```bash
   zenml init
   ```
3. **Define a Pipeline**: Use decorators to define your pipeline and its steps:
   ```python
   @pipeline
   def my_pipeline():
       step1 = step1_function()
       step2 = step2_function(step1)
   ```
4. **Run the Pipeline**: Execute your pipeline using the command line or programmatically.

### Best Practices:
- Organize your code into reusable components.
- Use version control for your ZenML configurations.
- Leverage built-in integrations for data ingestion, model training, and deployment.

ZenML simplifies the ML lifecycle, making it easier for teams to collaborate and iterate on their models. For detailed usage and advanced features, refer to the full documentation.

```python
from zenml.client import Client

Client().delete_pipeline_run(<RUN_NAME_OR_ID>)
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow by providing a standardized way to build and manage ML pipelines. It emphasizes reproducibility, collaboration, and scalability in ML projects.

Key Features:
- **Pipeline Abstraction**: ZenML allows users to define pipelines that encapsulate the entire ML workflow, from data ingestion to model deployment.
- **Integrations**: It supports various tools and platforms, enabling seamless integration with popular ML libraries, cloud services, and orchestration tools.
- **Versioning**: ZenML automatically tracks changes in data, code, and configurations, ensuring reproducibility and traceability of experiments.
- **Modularity**: Users can create reusable components (steps) within pipelines, promoting code reuse and simplifying maintenance.

Getting Started:
1. **Installation**: ZenML can be installed via pip, making it easy to set up in any Python environment.
2. **Creating a Pipeline**: Users can define their pipeline using decorators to specify steps and their dependencies.
3. **Running Pipelines**: Pipelines can be executed locally or on cloud platforms, with built-in support for different orchestration tools.

ZenML is ideal for data scientists and ML engineers looking to enhance their workflow efficiency and maintain high standards of project organization.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/configuring-a-pipeline-at-runtime.md

### Runtime Configuration of a Pipeline in ZenML

ZenML allows for dynamic configuration of pipelines at runtime. You can configure a pipeline using the `pipeline.with_options` method in two ways:

1. **Explicit Configuration**: Specify options directly, e.g., `with_options(steps="trainer": {"parameters": {"param1": 1}})`.
2. **YAML Configuration**: Pass a YAML file with `with_options(config_file="path_to_yaml_file")`.

For triggering a pipeline from a client or another pipeline, use the `PipelineRunConfiguration` object. 

For more details on configuration options, refer to the [configuration files documentation](../../pipeline-development/use-configuration-files/README.md).



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/compose-pipelines.md

### ZenML: Reusing Steps Between Pipelines

ZenML enables the composition of pipelines, allowing users to extract common functionality into separate functions to reduce code duplication. This feature is essential for creating modular and maintainable workflows in machine learning projects. By reusing steps, developers can streamline their pipelines and enhance efficiency.

```python
from zenml import pipeline

@pipeline
def data_loading_pipeline(mode: str):
    if mode == "train":
        data = training_data_loader_step()
    else:
        data = test_data_loader_step()
    
    processed_data = preprocessing_step(data)
    return processed_data


@pipeline
def training_pipeline():
    training_data = data_loading_pipeline(mode="train")
    model = training_step(data=training_data)
    test_data = data_loading_pipeline(mode="test")
    evaluation_step(model=model, data=test_data)
```

ZenML allows users to call one pipeline from within another, effectively integrating the steps of a child pipeline (e.g., `data_loading_pipeline`) into a parent pipeline (e.g., `training_pipeline`). Only the parent pipeline will be displayed in the dashboard. For instructions on triggering a pipeline from another, refer to the advanced usage section [here](../../pipeline-development/trigger-pipelines/use-templates-python.md#advanced-usage-run-a-template-from-another-pipeline). 

For more information on orchestrators, visit the [orchestrators documentation](../../../component-guide/orchestrators/orchestrators.md).



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/README.md

ZenML simplifies pipeline creation by using the `@step` and `@pipeline` decorators. This allows users to easily define and organize their workflows in a straightforward manner.

```python
from zenml import pipeline, step


@step  # Just add this decorator
def load_data() -> dict:
    training_data = [[1, 2], [3, 4], [5, 6]]
    labels = [0, 1, 0]
    return {'features': training_data, 'labels': labels}


@step
def train_model(data: dict) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])

    # Train some model here

    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}")


@pipeline  # This function combines steps together 
def simple_ml_pipeline():
    dataset = load_data()
    train_model(dataset)
```

To run the ZenML pipeline, invoke the function directly. This streamlined approach simplifies the execution process, making it easier for users to integrate ZenML into their projects.

```python
simple_ml_pipeline()
```

When a ZenML pipeline is executed, its run is logged in the ZenML dashboard, where users can view the Directed Acyclic Graph (DAG) and associated metadata. To access the dashboard, a ZenML server must be running either locally or remotely. For setup instructions, refer to the [deployment documentation](../../../getting-started/deploying-zenml/README.md).

### Advanced Pipeline Features
- **Configure Pipeline/Step Parameters:** [Documentation](use-pipeline-step-parameters.md)
- **Name and Annotate Step Outputs:** [Documentation](step-output-typing-and-annotation.md)
- **Control Caching Behavior:** [Documentation](control-caching-behavior.md)
- **Run Pipeline from Another Pipeline:** [Documentation](trigger-a-pipeline-from-another.md)
- **Control Execution Order of Steps:** [Documentation](control-execution-order-of-steps.md)
- **Customize Step Invocation IDs:** [Documentation](using-a-custom-step-invocation-id.md)
- **Name Your Pipeline Runs:** [Documentation](name-your-pipeline-and-runs.md)
- **Use Failure/Success Hooks:** [Documentation](use-failure-success-hooks.md)
- **Hyperparameter Tuning:** [Documentation](hyper-parameter-tuning.md)
- **Attach Metadata to a Step:** [Documentation](../track-metrics-metadata/attach-metadata-to-a-step.md)
- **Fetch Metadata Within Steps:** [Documentation](../../model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md)
- **Fetch Metadata During Pipeline Composition:** [Documentation](../../model-management-metrics/track-metrics-metadata/fetch-metadata-within-pipeline.md)
- **Enable/Disable Logs Storing:** [Documentation](../../advanced-topics/control-logging/enable-or-disable-logs-storing.md)
- **Special Metadata Types:** [Documentation](../../model-management-metrics/track-metrics-metadata/logging-metadata.md)
- **Access Secrets in a Step:** [Documentation](access-secrets-in-a-step.md)

This summary provides a concise overview of ZenML's capabilities for managing and monitoring pipelines, making it easier for users to leverage its features in their projects.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/use-pipeline-step-parameters.md

### ZenML: Parameterizing Steps and Pipelines

In ZenML, steps and pipelines can be parameterized similarly to standard Python functions. 

#### Step Parameters
When invoking a step in a pipeline, inputs can be either:
- **Artifacts**: Outputs from previous steps within the same pipeline, facilitating data sharing.
- **Parameters**: Explicitly provided values that configure the step's behavior independently of other steps.

**Important Note**: Only values that can be serialized to JSON using Pydantic are allowed as parameters for configuration files. For non-JSON-serializable objects, such as NumPy arrays, use [External Artifacts](../../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline). 

This functionality enhances the flexibility and configurability of your pipelines in ZenML.

```python
from zenml import step, pipeline

@step
def my_step(input_1: int, input_2: int) -> None:
    pass


@pipeline
def my_pipeline():
    int_artifact = some_other_step()
    # We supply the value of `input_1` as an artifact and
    # `input_2` as a parameter
    my_step(input_1=int_artifact, input_2=42)
    # We could also call the step with two artifacts or two
    # parameters instead:
    # my_step(input_1=int_artifact, input_2=int_artifact)
    # my_step(input_1=1, input_2=2)
```

ZenML allows the use of YAML configuration files to pass parameters for steps and pipelines, enabling easier updates without modifying the Python code. This integration provides flexibility in managing configurations, streamlining the development process.

```yaml
# config.yaml

# these are parameters of the pipeline
parameters:
  environment: production

steps:
  my_step:
    # these are parameters of the step `my_step`
    parameters:
      input_2: 42
```

```python
from zenml import step, pipeline
@step
def my_step(input_1: int, input_2: int) -> None:
    ...

# input `environment` will come from the configuration file,
# and it is evaluated to `production`
@pipeline
def my_pipeline(environment: str):
    ...

if __name__=="__main__":
    my_pipeline.with_options(config_paths="config.yaml")()
```

### ZenML Configuration Conflicts

When using YAML configuration files in ZenML, be aware that conflicts may arise between step or pipeline inputs. This occurs if a parameter is defined in the YAML file and then overridden in the code. In the event of a conflict, ZenML will notify you with specific details and instructions for resolution.

**Example of Conflict:**
- A parameter defined in the YAML file is later modified in the code, leading to a conflict that ZenML will flag.

This feature ensures that users are informed of any discrepancies, allowing for easier debugging and correction in their projects.

```yaml
# config.yaml
parameters:
    some_param: 24

steps:
  my_step:
    parameters:
      input_2: 42
```

```python
# run.py
from zenml import step, pipeline

@step
def my_step(input_1: int, input_2: int) -> None:
    pass

@pipeline
def my_pipeline(some_param: int):
    # here an error will be raised since `input_2` is
    # `42` in config, but `43` was provided in the code
    my_step(input_1=42, input_2=43)

if __name__=="__main__":
    # here an error will be raised since `some_param` is
    # `24` in config, but `23` was provided in the code
    my_pipeline(23)
```

### ZenML Caching Overview

**Parameters and Caching**: A step will be cached only if all input parameter values match those from previous executions.

**Artifacts and Caching**: A step will be cached only if all input artifacts are identical to those from prior executions. If any upstream steps producing the input artifacts were not cached, the step will execute again.

### Related Documentation
- [Use configuration files to set parameters](use-pipeline-step-parameters.md)
- [How caching works and how to control it](control-caching-behavior.md)



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/reference-environment-variables-in-configurations.md

# Reference Environment Variables in ZenML Configurations

ZenML enables flexible configurations by allowing the use of environment variables. You can reference these variables in your code and configuration files using the placeholder syntax: `${ENV_VARIABLE_NAME}`. This feature enhances the adaptability of your configurations in various environments.

```python
from zenml import step

@step(extra={"value_from_environment": "${ENV_VAR}"})
def my_step() -> None:
    ...
```

**ZenML Configuration File Overview**

ZenML utilizes configuration files to streamline the setup and management of machine learning workflows. These files define various parameters and settings essential for project execution. Key elements include:

- **Pipeline Definitions**: Specify the steps in your ML workflow, including data ingestion, preprocessing, model training, and evaluation.
- **Artifact Management**: Configure how and where to store artifacts generated during the pipeline execution, such as models and datasets.
- **Environment Settings**: Define the execution environment, including dependencies and resource allocation, to ensure consistent performance across different setups.
- **Integration Points**: Set up connections to external services and tools, such as cloud storage, databases, and ML platforms, to enhance functionality and scalability.

To effectively use ZenML, users should familiarize themselves with the structure and syntax of the configuration file, ensuring all necessary components are accurately defined for optimal workflow execution.

```yaml
extra:
  value_from_environment: ${ENV_VAR}
  combined_value: prefix_${ENV_VAR}_suffix
```

ZenML is an open-source framework designed to streamline the development and deployment of machine learning (ML) pipelines. It provides a standardized way to create reproducible and maintainable ML workflows, making it easier for data scientists and engineers to collaborate on projects.

Key Features:
- **Pipeline Abstraction**: ZenML allows users to define pipelines as code, facilitating version control and collaboration.
- **Integration**: It supports integration with various tools and platforms, including cloud services, data orchestration tools, and ML libraries, enhancing flexibility in ML workflows.
- **Reproducibility**: ZenML ensures that experiments can be reproduced by tracking metadata and artifacts associated with pipeline runs.
- **Modular Components**: Users can create custom components for data ingestion, preprocessing, training, and deployment, promoting reusability.

Getting Started:
1. **Installation**: Install ZenML via pip with the command `pip install zenml`.
2. **Create a Pipeline**: Define a pipeline using decorators to specify steps and their dependencies.
3. **Run the Pipeline**: Execute the pipeline locally or on a cloud platform, leveraging ZenML's orchestration capabilities.

ZenML is ideal for teams looking to enhance their ML workflow efficiency and maintainability, making it a valuable tool for modern data science projects.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/name-your-pipeline-runs.md

# Naming Pipeline Runs in ZenML

In ZenML, each pipeline run is assigned a unique name that appears in the output logs. This naming convention helps in identifying and tracking individual runs, making it easier to manage and analyze the results of different executions. Properly naming your pipeline runs is essential for effective monitoring and debugging within your projects.

```bash
Pipeline run training_pipeline-2023_05_24-12_41_04_576473 has finished in 3.742s.
```

In ZenML, the run name is automatically generated using the current date and time. To customize the run name, use the `run_name` parameter with the `with_options()` method.

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name"
)
training_pipeline()
```

In ZenML, pipeline run names must be unique. To manage multiple runs or scheduled executions, compute run names dynamically or use placeholders that ZenML will replace. Custom placeholders, such as `experiment_name`, can be set in the `@pipeline` decorator or via the `pipeline.with_options` function, applying to all steps in the pipeline. Standard substitutions available for all steps include:

- `{date}`: current date (e.g., `2024_11_27`)
- `{time}`: current time in UTC format (e.g., `11_07_09_326492`) 

This ensures consistent naming across pipeline runs.

```python
training_pipeline = training_pipeline.with_options(
    run_name="custom_pipeline_run_name_{experiment_name}_{date}_{time}"
)
training_pipeline()
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow. It provides a standardized way to build, manage, and deploy ML pipelines, enabling teams to focus on developing models rather than dealing with infrastructure complexities.

Key Features:
- **Pipeline Abstraction**: ZenML allows users to define ML pipelines in a modular fashion, promoting reusability and collaboration.
- **Integration**: It supports integration with various tools and platforms, facilitating seamless data processing, model training, and deployment.
- **Version Control**: ZenML tracks changes in data and models, ensuring reproducibility and traceability throughout the ML lifecycle.
- **Extensibility**: Users can extend ZenML's functionality by creating custom components and integrations tailored to their specific needs.

Getting Started:
1. **Installation**: Install ZenML via pip with `pip install zenml`.
2. **Initialize a Project**: Use `zenml init` to set up a new ZenML project.
3. **Create Pipelines**: Define your ML workflows using ZenML's pipeline decorators.
4. **Run Pipelines**: Execute pipelines locally or in the cloud, leveraging ZenML's orchestration capabilities.

ZenML is ideal for data scientists and ML engineers looking to enhance their workflow efficiency and collaboration in ML projects.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/run-pipelines-asynchronously.md

### Running Pipelines Asynchronously in ZenML

By default, ZenML pipelines run synchronously, allowing users to view logs in real-time via the terminal. To enable asynchronous execution, you have two options:

1. **Global Configuration**: Set the orchestrator to always run asynchronously by configuring `synchronous=False`.
2. **Runtime Configuration**: Temporarily set the pipeline to run asynchronously at the configuration level during execution.

This flexibility allows for better management of pipeline runs, especially in larger projects.

```python
from zenml import pipeline

@pipeline(settings = {"orchestrator": {"synchronous": False}})
def my_pipeline():
  ...
```

ZenML is an open-source framework designed to streamline the machine learning (ML) workflow. It enables users to create reproducible, production-ready ML pipelines with minimal effort. Key features include:

- **Pipeline Abstraction**: ZenML allows users to define pipelines that encapsulate the entire ML workflow, from data ingestion to model deployment.
- **Integrations**: It supports various tools and platforms, such as TensorFlow, PyTorch, and cloud services, making it versatile for different ML projects.
- **Versioning**: ZenML automatically tracks changes in data, code, and configurations, ensuring reproducibility and traceability.
- **Configuration Management**: Users can configure pipelines through code or YAML files, providing flexibility in how they set up their projects.

To get started with ZenML, users can install it via pip and follow the documentation for creating their first pipeline, integrating with existing tools, and managing configurations effectively.

```yaml
settings:
  orchestrator.<STACK_NAME>:
    synchronous: false
```

ZenML is a framework designed to streamline the machine learning (ML) workflow by providing a structured approach to building and managing ML pipelines. It integrates various components, including orchestrators, which are essential for managing the execution of these pipelines.

For more detailed information on orchestrators, refer to the [orchestrators documentation](../../../component-guide/orchestrators/orchestrators.md).

ZenML aims to simplify the ML process, making it easier for developers to implement and scale their projects effectively.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/hyper-parameter-tuning.md

### Hyperparameter Tuning with ZenML

**Overview**: Hyperparameter tuning is currently not a primary feature in ZenML but is planned for future support. Users can implement basic hyperparameter tuning in their ZenML runs using a simple pipeline.

**Key Points**:
- Hyperparameter tuning is on ZenML's roadmap for future enhancements.
- Users can manually implement hyperparameter tuning by iterating through hyperparameters in a pipeline.

For detailed implementation examples, refer to the ZenML documentation.

```python
@pipeline
def my_pipeline(step_count: int) -> None:
    data = load_data_step()
    after = []
    for i in range(step_count):
        train_step(data, learning_rate=i * 0.0001, name=f"train_step_{i}")
        after.append(f"train_step_{i}")
    model = select_model_step(..., after=after)
```

ZenML provides a basic grid search implementation for hyperparameter tuning, specifically for varying learning rates within the same `train_step`. After executing the training with different learning rates, the `select_model_step` identifies the hyperparameters that yield the best performance.

To see this in action, refer to the E2E example. Set up your local environment by following the guidelines in the [Project templates](../../project-setup-and-management/setting-up-a-project-repository/using-project-templates.md). In the file [`pipelines/training.py`](../../../../examples/e2e/pipelines/training.py), you will find a training pipeline featuring a `Hyperparameter tuning stage`. This section includes a `for` loop that runs `hp_tuning_single_search` across the defined model search spaces, followed by `hp_tuning_select_best_model` to determine the `best_model_config` for subsequent model training.

```python
...
########## Hyperparameter tuning stage ##########
after = []
search_steps_prefix = "hp_tuning_search_"
for i, model_search_configuration in enumerate(
    MetaConfig.model_search_space
):
    step_name = f"{search_steps_prefix}{i}"
    hp_tuning_single_search(
        model_metadata=ExternalArtifact(
            value=model_search_configuration,
        ),
        id=step_name,
        dataset_trn=dataset_trn,
        dataset_tst=dataset_tst,
        target=target,
    )
    after.append(step_name)
best_model_config = hp_tuning_select_best_model(
    search_steps_prefix=search_steps_prefix, after=after
)
...
```

ZenML currently faces a limitation where a variable number of artifacts cannot be passed into a step programmatically. As a workaround, the `select_model_step` must retrieve all artifacts generated by prior steps using the ZenML Client. This approach ensures that the necessary artifacts are accessible for subsequent processing.

```python
from zenml import step, get_step_context
from zenml.client import Client

@step
def select_model_step():
    run_name = get_step_context().pipeline_run.name
    run = Client().get_pipeline_run(run_name)

    # Fetch all models trained by a 'train_step' before
    trained_models_by_lr = {}
    for step_name, step in run.steps.items():
        if step_name.startswith("train_step"):
            for output_name, output in step.outputs.items():
                if output_name == "<NAME_OF_MODEL_OUTPUT_IN_TRAIN_STEP>":
                    model = output.load()
                    lr = step.config.parameters["learning_rate"]
                    trained_models_by_lr[lr] = model
    
    # Evaluate the models to find the best one
    for lr, model in trained_models_by_lr.items():
        ...
```

### ZenML Hyperparameter Tuning Overview

To set up a local environment for ZenML, refer to the [Project templates](../../project-setup-and-management/setting-up-a-project-repository/using-project-templates.md). Within the `steps/hp_tuning` directory, two key step files are available for hyperparameter search:

1. **`hp_tuning_single_search(...)`**: Conducts a randomized search for optimal model hyperparameters within a specified space.
2. **`hp_tuning_select_best_model(...)`**: Evaluates results from previous random searches to identify the best model based on a defined metric.

These files serve as a foundation for customizing hyperparameter tuning to fit specific project needs.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/control-caching-behavior.md

ZenML automatically caches steps in pipelines when the code and parameters remain unchanged. This feature enhances performance by avoiding redundant computations. Users can control caching behavior to optimize their workflows.

```python
@step(enable_cache=True) # set cache behavior at step level
def load_data(parameter: int) -> dict:
    ...

@step(enable_cache=False) # settings at step level override pipeline level
def train_model(data: dict) -> None:
    ...

@pipeline(enable_cache=True) # set cache behavior at step level
def simple_ml_pipeline(parameter: int):
    ...
```

ZenML is a framework designed to streamline the machine learning (ML) workflow by providing a structured approach to building and managing ML pipelines. It emphasizes reproducibility, collaboration, and scalability.

### Key Features:
- **Caching**: ZenML caches results only when the code and parameters remain unchanged, enhancing efficiency by avoiding redundant computations.
- **Modifiable Settings**: Users can alter step and pipeline configurations post-creation, allowing for flexibility and adaptability in ML projects.

This documentation serves as a guide for users to understand ZenML's functionalities and how to effectively implement it in their ML workflows.

```python
# Same as passing it in the step decorator
my_step.configure(enable_cache=...)

# Same as passing it in the pipeline decorator
my_pipeline.configure(enable_cache=...)
```

ZenML is a framework designed to streamline the machine learning (ML) pipeline development process. It allows users to configure their projects using YAML files, which enhances reproducibility and collaboration. For detailed instructions on configuring ZenML in a YAML file, refer to the [use-configuration-files](../../pipeline-development/use-configuration-files/) documentation. 

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/run-an-individual-step.md

## Running an Individual Step in ZenML

To execute a single step in your ZenML stack, call the step like a standard Python function. ZenML will automatically create and run a pipeline containing only that step on the active stack. Note that this pipeline run will be `unlisted`, meaning it won't be linked to any specific pipeline, but it will still be visible in the "Runs" tab of the dashboard.

```python
from zenml import step
import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.svm import SVC

# Configure the step to use a step operator. If you're not using
# a step operator, you can remove this and the step will run on
# your orchestrator instead.
@step(step_operator="<STEP_OPERATOR_NAME>")
def svc_trainer(
    X_train: pd.DataFrame,
    y_train: pd.Series,
    gamma: float = 0.001,
) -> Tuple[
    Annotated[ClassifierMixin, "trained_model"],
    Annotated[float, "training_acc"],
]:
    """Train a sklearn SVC classifier."""

    model = SVC(gamma=gamma)
    model.fit(X_train.to_numpy(), y_train.to_numpy())

    train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
    print(f"Train accuracy: {train_acc}")

    return model, train_acc


X_train = pd.DataFrame(...)
y_train = pd.Series(...)

# Call the step directly. This will internally create a
# pipeline with just this step, which will be executed on
# the active stack.
model, train_acc = svc_trainer(X_train=X_train, y_train=y_train)
```

## Running Step Functions Directly in ZenML

To execute a step function without ZenML's involvement, utilize the `entrypoint(...)` method of the step. This allows for direct execution of the underlying function, bypassing the ZenML framework.

```python
X_train = pd.DataFrame(...)
y_train = pd.Series(...)

model, train_acc = svc_trainer.entrypoint(X_train=X_train, y_train=y_train)
```

ZenML allows users to customize the behavior of their steps. To make a step call default to executing without the ZenML stack, set the environment variable `ZENML_RUN_SINGLE_STEPS_WITHOUT_STACK` to `True`. This configuration enables direct function calls, bypassing the ZenML stack.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/control-execution-order-of-steps.md

# Control Execution Order of Steps in ZenML

ZenML determines the execution order of pipeline steps based on data dependencies. For instance, if `step_3` relies on the outputs of `step_1` and `step_2`, ZenML can execute `step_1` and `step_2` in parallel. However, `step_3` will only start once both preceding steps are completed. This dependency management allows for efficient pipeline execution.

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1()
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

In ZenML, you can manage the execution order of steps by specifying non-data dependencies using the `after` argument. To indicate that a step should run after another, use `my_step(after="other_step")`. For multiple upstream steps, provide a list: `my_step(after=["other_step", "other_step_2"])`. For more details on invocation IDs and custom usage, refer to the [documentation here](using-a-custom-step-invocation-id.md).

```python
from zenml import pipeline

@pipeline
def example_pipeline():
    step_1_output = step_1(after="step_2")
    step_2_output = step_2()
    step_3(step_1_output, step_2_output)
```

ZenML enables the orchestration of machine learning workflows by managing the execution order of pipeline steps. In this example, ZenML ensures that `step_1` only begins after the completion of `step_2`. This functionality helps maintain the integrity of the workflow and ensures dependencies are respected. 

For visual reference, see the accompanying image of the ZenML architecture.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/fetching-pipelines.md

### Inspecting Finished Pipeline Runs in ZenML

Once a pipeline run is completed, users can access its information programmatically, allowing for:

- **Loading Artifacts**: Retrieve models or datasets saved from previous runs.
- **Accessing Metadata**: Obtain configurations and metadata from earlier runs.
- **Inspecting Lineage**: Analyze the lineage of pipeline runs and their associated artifacts.

The structure of ZenML consists of a hierarchy that includes pipelines, runs, steps, and artifacts, facilitating organized access to these components.

```mermaid
flowchart LR
    pipelines -->|1:N| runs
    runs -->|1:N| steps
    steps -->|1:N| artifacts
```

ZenML provides a structured approach to managing machine learning workflows through a layered hierarchy of 1-to-N relationships. To interact with pipelines, users can retrieve a previously executed pipeline using the [`Client.get_pipeline()`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client.get_pipeline) method. This functionality allows for efficient navigation and management of pipelines within the ZenML framework.

```python
from zenml.client import Client

pipeline_model = Client().get_pipeline("first_pipeline")
```

### ZenML Overview

ZenML is a framework designed to streamline the machine learning workflow by managing pipelines efficiently. Users can discover and list all registered pipelines through the ZenML dashboard or programmatically using the ZenML Client or CLI.

### Listing Pipelines

To retrieve a list of all registered pipelines in ZenML, utilize the `Client.list_pipelines()` method. For further details on the `Client` class and its functionalities, refer to the [ZenML Client Documentation](../../../reference/python-client.md).

```python
from zenml.client import Client

pipelines = Client().list_pipelines()
```

### ZenML CLI Overview

To list pipelines in ZenML, you can use the following CLI command:

```bash
zenml pipeline list
```

This command provides a straightforward way to view all available pipelines within your ZenML environment.

```shell
zenml pipeline list
```

## Runs in ZenML

Each pipeline in ZenML can be executed multiple times, generating several **Runs**. 

### Retrieving Pipeline Runs
To obtain a list of all runs associated with a specific pipeline, utilize the `runs` property of the pipeline.

```python
runs = pipeline_model.runs
```

To retrieve the most recent runs of a pipeline in ZenML, you can use the `pipeline_model.get_runs()` method, which provides options for filtering and pagination. For the latest run, utilize the `last_run` property or access it via the `runs` list. For further details, refer to the [ZenML SDK Docs](../../../reference/python-client.md#list-of-resources).

```
last_run = pipeline_model.last_run  # OR: pipeline_model.runs[0]
```

To retrieve the latest run from a ZenML pipeline, simply call the pipeline, which will execute it and return the response of the most recent run. If your recent runs have failed and you need to identify the last successful run, utilize the `last_successful_run` property.

```python
run = training_pipeline()
```

**ZenML Pipeline Run Initialization**

When you initiate a pipeline run in ZenML, the returned model represents the state stored in the ZenML database at the time of the method call. It's important to note that the pipeline run is still in the initialization phase, and no steps have been executed yet. To obtain the most current state of the pipeline run, you can retrieve a refreshed version from the client.

```python
from zenml.client import Client

Client().get_pipeline_run(run.id) # to get a refreshed version
```

### Fetching a Pipeline Run with ZenML

To retrieve a specific pipeline run in ZenML, use the [`Client.get_pipeline_run()`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client.get_pipeline_run) method. This allows you to directly access the run if you already know its details, such as from the dashboard, without needing to query the pipeline first.

```python
from zenml.client import Client

pipeline_run = Client().get_pipeline_run("first_pipeline-2023_06_20-16_20_13_274466")
```

### ZenML Run Information

In ZenML, you can query pipeline runs using their ID, name, or name prefix. Discover runs through the Client or CLI with the [`Client.list_pipeline_runs()`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client.list_pipeline_runs) or the `zenml pipeline runs list` command.

#### Key Pipeline Run Information
Each run contains critical information for reproduction, including:

- **Status**: Indicates the state of a pipeline run, which can be one of the following: initialized, failed, completed, running, or cached.

For a comprehensive list of available information, refer to the [`PipelineRunResponse`](https://sdkdocs.zenml.io/latest/core_code_docs/core-models/#zenml.models.v2.core.pipeline_run.PipelineRunResponse) definition.

```python
status = run.status
```

### Configuration Overview

The `pipeline_configuration` object encapsulates all configurations related to the pipeline and its execution. This includes essential pipeline-level settings, which are detailed in the production guide. Understanding this configuration is crucial for effectively utilizing ZenML in your projects.

```python
pipeline_config = run.config
pipeline_settings = run.config.settings
```

### Component-Specific Metadata in ZenML

ZenML allows for the inclusion of component-specific metadata based on the stack components utilized in your project. This metadata may include details like the URL to the UI of a remote orchestrator. You can access this information through the `run_metadata` attribute.

````python
run_metadata = run.run_metadata
# The following only works for runs on certain remote orchestrators
orchestrator_url = run_metadata["orchestrator_url"].value

## Steps

Within a given pipeline run you can now further zoom in on individual steps using the `steps` attribute:

```

ZenML allows users to manage and interact with pipeline runs effectively. To retrieve all steps of a specific pipeline run, use the command `steps = run.steps`. For accessing a particular step, reference it by its invocation ID, such as `step = run.steps["first_step"]`. This functionality is essential for tracking and manipulating individual steps within a pipeline.

````

{% hint style="info" %}
If you're only calling each step once inside your pipeline, the **invocation ID** will be the same as the name of your step. For more complex pipelines, check out [this page](../../pipeline-development/build-pipelines/using-a-custom-step-invocation-id.md) to learn more about the invocation ID.
{% endhint %}

### Inspect pipeline runs with our VS Code extension

![GIF of our VS code extension, showing some of the uses of the sidebar](../../../.gitbook/assets/zenml-extension-shortened.gif)

If you are using [our VS Code extension](https://marketplace.visualstudio.com/items?itemName=ZenML.zenml-vscode), you can easily view your pipeline runs by opening the sidebar (click on the ZenML icon). You can then click on any particular pipeline run to see its status and some other metadata. If you want to delete a run, you can also do so from the same sidebar view.

### Step information

Similar to the run, you can use the `step` object to access a variety of useful information:

* The parameters used to run the step via `step.config.parameters`,
* The step-level settings via `step.config.settings`,
* Component-specific step metadata, such as the URL of an experiment tracker or model deployer, via `step.run_metadata`

See the [`StepRunResponse`](https://github.com/zenml-io/zenml/blob/main/src/zenml/models/v2/core/step_run.py) definition for a comprehensive list of available information.

## Artifacts

Each step of a pipeline run can have multiple output and input artifacts that we can inspect via the `outputs` and `inputs` properties.

To inspect the output artifacts of a step, you can use the `outputs` attribute, which is a dictionary that can be indexed using the name of an output. Alternatively, if your step only has a single output, you can use the `output` property as a shortcut directly:

```

In ZenML, the outputs of a step can be accessed by their designated names using `step.outputs["output_name"]`. If a step has only one output, it can be accessed directly with the `.output` property. To load the artifact into memory, use the `.load()` method, as shown: `my_pytorch_model = output.load()`.

```

Similarly, you can use the `inputs` and `input` properties to get the input artifacts of a step instead.

{% hint style="info" %}
Check out [this page](../../../user-guide/starter-guide/manage-artifacts.md#giving-names-to-your-artifacts) to see what the output names of your steps are and how to customize them.
{% endhint %}

Note that the output of a step corresponds to a specific artifact version.

### Fetching artifacts directly

If you'd like to fetch an artifact or an artifact version directly, it is easy to do so with the `Client`:

```

To use ZenML for managing artifacts, you can retrieve a specific artifact and its versions using the following code:

```python
from zenml.client import Client

# Get the artifact
artifact = Client().get_artifact('iris_dataset')

# Access all versions of the artifact
artifact.versions

# Retrieve a specific version by name
output = artifact.versions['2022']

# Alternatively, get the artifact version directly:
# By version name
output = Client().get_artifact_version('iris_dataset', '2022')

# By UUID
output = Client().get_artifact_version('f429f94c-fb15-43b5-961d-dbea287507c5')

# Load the artifact
loaded_artifact = output.load()
```

This allows users to manage and load different versions of artifacts effectively within their ZenML projects.

```

### Artifact information

Regardless of how one fetches it, each artifact contains a lot of general information about the artifact as well as datatype-specific metadata and visualizations.

#### Metadata

All output artifacts saved through ZenML will automatically have certain datatype-specific metadata saved with them. NumPy Arrays, for instance, always have their storage size, `shape`, `dtype`, and some statistical properties saved with them. You can access such metadata via the `run_metadata` attribute of an output, e.g.:

```

In ZenML, you can access the metadata of an output using the `run_metadata` attribute. To retrieve the storage size in bytes of the output, use the following code:

```python
output_metadata = output.run_metadata 
storage_size_in_bytes = output_metadata["storage_size"].value
```

This allows users to obtain important information about the output's storage characteristics, which can be useful for managing resources in their projects.

```

We will talk more about metadata [in the next section](../../../user-guide/starter-guide/manage-artifacts.md#logging-metadata-for-an-artifact).

#### Visualizations

ZenML automatically saves visualizations for many common data types. Using the `visualize()` method you can programmatically show these visualizations in Jupyter notebooks:

```

### ZenML Output Visualization

The `output.visualize()` function in ZenML is used to generate visual representations of outputs from pipelines. This function aids in understanding and analyzing the results of machine learning workflows.

#### Key Features:
- **Visualization of Outputs**: Provides graphical insights into the data produced by pipeline steps.
- **Integration with ZenML Pipelines**: Seamlessly integrates with existing ZenML pipelines, allowing users to visualize outputs at various stages.
- **Customizable**: Users can customize visualizations to suit specific needs, enhancing interpretability.

#### Usage:
To utilize the `output.visualize()` function, ensure that it is called on the output object of a pipeline step. This will render the visual representation based on the data type and content.

#### Example:
```python
output.visualize()
```

This command will display the visualization corresponding to the output generated by the preceding steps in the pipeline.

#### Conclusion:
The `output.visualize()` function is a powerful tool in ZenML for visualizing outputs, facilitating better understanding and communication of results in machine learning projects.

```

![output.visualize() Output](../../../.gitbook/assets/artifact\_visualization\_evidently.png)

{% hint style="info" %}
If you're not in a Jupyter notebook, you can simply view the visualizations in the ZenML dashboard by running `zenml login --local` and clicking on the respective artifact in the pipeline run DAG instead. Check out the [artifact visualization page](../../handle-data-artifacts/visualize-artifacts.md) to learn more about how to build and view artifact visualizations in ZenML!
{% endhint %}

## Fetching information during run execution

While most of this document has focused on fetching objects after a pipeline run has been completed, the same logic can also be used within the context of a running pipeline.

This is often desirable in cases where a pipeline is running continuously over time and decisions have to be made according to older runs.

For example, this is how we can fetch the last pipeline run of the same pipeline from within a ZenML step:

```

ZenML is a framework designed to streamline the machine learning workflow. The following code snippet demonstrates how to access pipeline run information within a ZenML step:

```python
from zenml import get_step_context
from zenml.client import Client

@step
def my_step():
    # Get the name of the current pipeline run
    current_run_name = get_step_context().pipeline_run.name
    
    # Fetch the current pipeline run
    current_run = Client().get_pipeline_run(current_run_name)
    
    # Fetch the previous run of the same pipeline
    previous_run = current_run.pipeline.runs[1]  # index 0 is the current run
```

Key Points:
- Use `get_step_context()` to retrieve the current pipeline run's name.
- Access the current run using `Client().get_pipeline_run()`.
- Previous runs can be accessed via the `runs` attribute of the pipeline, with the current run at index 0. 

This functionality is essential for tracking and comparing different runs in a ZenML pipeline.

```

{% hint style="info" %}
As shown in the example, we can get additional information about the current run using the `StepContext`, which is explained in more detail in the [advanced docs](../../model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md).
{% endhint %}

## Code example

This section combines all the code from this section into one simple script that you can use to see the concepts discussed above:

<details>

<summary>Code Example of this Section</summary>

Putting it all together, this is how we can load the model trained by the `svc_trainer` step of our example pipeline from the previous sections:

```

### ZenML Overview and Usage

ZenML is a framework designed to streamline the machine learning workflow. Below is a concise guide on how to use ZenML for training a Support Vector Classifier (SVC) with the Iris dataset.

#### Key Components

1. **Data Loading Step**:
   - **Function**: `training_data_loader`
   - **Purpose**: Loads the Iris dataset and splits it into training and testing sets.
   - **Returns**: Tuple of training and testing data (features and labels).
   ```python
   @step
   def training_data_loader() -> Tuple[Annotated[pd.DataFrame, "X_train"], Annotated[pd.DataFrame, "X_test"], Annotated[pd.Series, "y_train"], Annotated[pd.Series, "y_test"]]:
       iris = load_iris(as_frame=True)
       X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, shuffle=True, random_state=42)
       return X_train, X_test, y_train, y_test
   ```

2. **Model Training Step**:
   - **Function**: `svc_trainer`
   - **Purpose**: Trains an SVC classifier and logs the training accuracy.
   - **Parameters**: `X_train`, `y_train`, `gamma` (default: 0.001).
   - **Returns**: Tuple of the trained model and training accuracy.
   ```python
   @step
   def svc_trainer(X_train: pd.DataFrame, y_train: pd.Series, gamma: float = 0.001) -> Tuple[Annotated[ClassifierMixin, "trained_model"], Annotated[float, "training_acc"]]:
       model = SVC(gamma=gamma)
       model.fit(X_train.to_numpy(), y_train.to_numpy())
       train_acc = model.score(X_train.to_numpy(), y_train.to_numpy())
       return model, train_acc
   ```

3. **Pipeline Definition**:
   - **Function**: `training_pipeline`
   - **Purpose**: Defines the workflow for loading data and training the model.
   - **Parameters**: `gamma` (default: 0.002).
   ```python
   @pipeline
   def training_pipeline(gamma: float = 0.002):
       X_train, X_test, y_train, y_test = training_data_loader()
       svc_trainer(gamma=gamma, X_train=X_train, y_train=y_train)
   ```

#### Running the Pipeline

- To execute the pipeline and retrieve the last run object:
  ```python
  if __name__ == "__main__":
      last_run = training_pipeline()
      print(last_run.id)
  ```

- Accessing the model after execution:
  ```python
  last_run = training_pipeline.model.last_run
  print(last_run.id)
  ```

- Fetching the last run from an existing pipeline:
  ```python
  pipeline = Client().get_pipeline("training_pipeline")
  last_run = pipeline.last_run
  print(last_run.id)
  ```

- Loading the trained model:
  ```python
  trainer_step = last_run.steps["svc_trainer"]
  model = trainer_step.outputs["trained_model"].load()
  ```

This documentation provides a foundational understanding of how to implement a machine learning pipeline using ZenML, focusing on data loading, model training, and execution.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/access-secrets-in-a-step.md

# Accessing Secrets in ZenML

## Fetching Secret Values in a Step

ZenML secrets are collections of **key-value pairs** securely stored in the ZenML secrets store, each identified by a unique **name** for easy reference in pipelines and stacks. To configure and create secrets, refer to the [platform guide on secrets](../../../getting-started/deploying-zenml/secret-management.md). 

You can access secrets within your steps using the ZenML `Client` API, enabling you to query APIs without hard-coding access keys.

```python
from zenml import step
from zenml.client import Client

from somewhere import authenticate_to_some_api


@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    # Fetch the secret from ZenML.
    secret = Client().get_secret("<SECRET_NAME>")

    # `secret.secret_values` will contain a dictionary with all key-value
    # pairs within your secret.
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
    ...
```

### ZenML Overview

ZenML is a framework designed to streamline the machine learning (ML) workflow by providing tools for managing pipelines, secrets, and integrations. 

#### Key Features:
- **Secrets Management**: ZenML allows users to create and manage secrets securely, essential for handling sensitive information in ML projects.
- **Backend Support**: It supports various secrets backends, ensuring flexibility in how secrets are stored and accessed.

#### Resources:
- **Creating and Managing Secrets**: Learn how to effectively handle secrets in your ZenML projects. [Interact with Secrets](../../interact-with-secrets.md)
- **Secrets Backend Information**: Explore the different secrets backend options available in ZenML. [Secrets Management](../../../getting-started/deploying-zenml/secret-management.md)

For further insights, refer to the provided links for detailed instructions and guidance on utilizing ZenML in your projects.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/get-past-pipeline-step-runs.md

# Retrieving Past Pipeline/Step Runs in ZenML

To access past pipeline or step runs in ZenML, utilize the `get_pipeline` method along with the `last_run` property, or access runs by indexing. Here’s how to do it:

```python
from zenml.client import Client

client = Client()

# Retrieve a pipeline by its name
p = client.get_pipeline("mlflow_train_deploy_pipeline")

# Get the latest run of this pipeline
latest_run = p.last_run

# Alternatively, access runs by index or name
first_run = p[0]
```

This allows users to efficiently track and manage their pipeline executions.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/step-output-typing-and-annotation.md

### ZenML Step Output Typing and Annotation

Step outputs in ZenML are stored in an artifact store. It’s important to annotate and name these outputs for clarity.

#### Type Annotations
While ZenML steps can function without type annotations, adding them provides significant advantages:

- **Type Validation**: Ensures that step functions receive the correct input types from upstream steps.
- **Improved Serialization**: With type annotations, ZenML can select the most appropriate materializer for output serialization. If built-in materializers are inadequate, users can create custom materializers.

**Warning**: ZenML includes a built-in `CloudpickleMaterializer` for handling any object serialization. However, it is not production-ready due to compatibility issues across different Python versions. Additionally, it poses security risks, as it may allow the upload of malicious files that could execute arbitrary code. For robust and secure serialization, consider developing a custom materializer.

```python
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> float:
    return number ** 0.5

# To define a step with multiple outputs, use a `Tuple` type annotation
@step
def divide(a: int, b: int) -> Tuple[int, int]:
    return a // b, a % b
```

To ensure type annotations are enforced in ZenML, set the environment variable `ZENML_ENFORCE_TYPE_ANNOTATIONS` to `True`. This will trigger an exception if any step lacks a type annotation.

### Tuple vs Multiple Outputs
ZenML differentiates between a single output artifact of type `Tuple` and multiple output artifacts based on the return statement. If the return statement uses a tuple literal (e.g., `return 1, 2` or `return (value_1, value_2)`), it is treated as multiple outputs. Any other return cases are considered a single output of type `Tuple`.

```python
from zenml import step
from typing_extensions import Annotated
from typing import Tuple

# Single output artifact
@step
def my_step() -> Tuple[int, int]:
    output_value = (0, 1)
    return output_value

# Single output artifact with variable length
@step
def my_step(condition) -> Tuple[int, ...]:
    if condition:
        output_value = (0, 1)
    else:
        output_value = (0, 1, 2)

    return output_value

# Single output artifact using the `Annotated` annotation
@step
def my_step() -> Annotated[Tuple[int, ...], "my_output"]:
    return 0, 1


# Multiple output artifacts
@step
def my_step() -> Tuple[int, int]:
    return 0, 1


# Not allowed: Variable length tuple annotation when using
# multiple output artifacts
@step
def my_step() -> Tuple[int, ...]:
    return 0, 1
```

## Step Output Names in ZenML

ZenML defaults to using `output` for single-output steps and `output_0`, `output_1`, etc., for multi-output steps. These names are utilized for displaying outputs in the dashboard and for fetching them post-pipeline execution. To customize output names, use the `Annotated` type annotation.

```python
from typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+
from typing import Tuple
from zenml import step

@step
def square_root(number: int) -> Annotated[float, "custom_output_name"]:
    return number ** 0.5

@step
def divide(a: int, b: int) -> Tuple[
    Annotated[int, "quotient"],
    Annotated[int, "remainder"]
]:
    return a // b, a % b
```

### ZenML Output Naming and Artifact Management

When outputs are not given custom names, ZenML automatically names the created artifacts in the format `{pipeline_name}::{step_name}::output` or `{pipeline_name}::{step_name}::output_{i}`. For detailed information on artifact versioning and configuration, refer to the [artifact management documentation](../../../user-guide/starter-guide/manage-artifacts.md).

### Additional Resources
- Learn about output annotation: [Return Multiple Outputs from a Step](../../data-artifact-management/handle-data-artifacts/return-multiple-outputs-from-a-step.md)
- Handling custom data types: [Handle Custom Data Types](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md)



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/use-failure-success-hooks.md

### ZenML: Using Failure and Success Hooks

**Overview**: Hooks in ZenML allow users to perform actions after the execution of a step, useful for notifications, logging, or resource cleanup. They run in the same environment as the step, providing access to all dependencies.

**Types of Hooks**:
- **`on_failure`**: Executes when a step fails.
- **`on_success`**: Executes when a step succeeds.

**Defining Hooks**: Hooks are defined as callback functions and must be accessible within the repository containing the pipeline and steps. For failure hooks, you can include a `BaseException` argument to access the specific exception that caused the failure.

**Demo**: A short demonstration of hooks in ZenML can be found [here](https://www.youtube.com/watch?v=KUW2G3EsqF8).

```python
from zenml import step

def on_failure(exception: BaseException):
    print(f"Step failed: {str(exception)}")


def on_success():
    print("Step succeeded!")


@step(on_failure=on_failure)
def my_failing_step() -> int:
    """Returns an integer."""
    raise ValueError("Error")


@step(on_success=on_success)
def my_successful_step() -> int:
    """Returns an integer."""
    return 1
```

In ZenML, hooks can be defined to execute specific actions on step outcomes. Two types of hooks are demonstrated: `on_failure`, which activates when a step fails (e.g., `my_failing_step` raises a `ValueError`), and `on_success`, which activates when a step succeeds (e.g., `my_successful_step` returns an integer). Steps can also be defined as local user-defined functions using the format `mymodule.myfile.my_function`, which is useful for YAML configuration. Additionally, hooks can be defined at the pipeline level to apply to all steps, simplifying the process of managing hooks across multiple steps.

```python
@pipeline(on_failure=on_failure, on_success=on_success)
def my_pipeline(...):
    ...
```

### ZenML Documentation Summary

**Hooks in ZenML:**
- **Step-level hooks** take precedence over **pipeline-level hooks**.

**Example Setup:**
- To set up the local environment, refer to the [Project templates](../../project-setup-and-management/setting-up-a-project-repository/using-project-templates.md).
- In the file [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), a step is defined to notify users of success and a function to notify on step failure using the Alerter from the active stack.
- The `@step` decorator is used for success notifications to indicate a fully successful pipeline run, rather than notifying for each successful step.
- In [`pipelines/training.py`](../../../../examples/e2e/pipelines/training.py), the notification step is utilized, and the `notify_on_failure` function is attached directly to the pipeline definition. 

This structure allows for effective user notifications during pipeline execution.

```python
from zenml import pipeline
@pipeline(
    ...
    on_failure=notify_on_failure,
    ...
)
```

In ZenML, the `notify_on_success` step is executed at the end of the training pipeline, contingent upon the completion of all preceding steps. This is managed using the `after` statement, ensuring that notifications are sent only after successful execution of the entire pipeline.

```python
...
last_step_name = "promote_metric_compare_promoter"

notify_on_success(after=[last_step_name])
...
```

## Accessing Step Information in a Hook

In ZenML, you can utilize the [StepContext](../../model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md) to retrieve details about the current pipeline run or step within your hook function. This allows for enhanced interaction and data handling during the execution of your pipelines.

```python
from zenml import step, get_step_context

def on_failure(exception: BaseException):
    context = get_step_context()
    print(context.step_run.name)  # Output will be `my_step`
    print(context.step_run.config.parameters)  # Print parameters of the step
    print(type(exception))  # Of type value error
    print("Step failed!")


@step(on_failure=on_failure)
def my_step(some_parameter: int = 1)
    raise ValueError("My exception")
```

### ZenML E2E Example Overview

To set up the local environment for the ZenML E2E example, refer to the guidelines in the [Project templates](../../project-setup-and-management/setting-up-a-project-repository/using-project-templates.md). 

In the file [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), there is a step designed to notify users of pipeline success and a function to alert users of step failures using the [Alerter](../../../component-guide/alerters/alerters.md) from the active stack. The `@step` decorator is utilized for success notifications to ensure users are informed only after a complete successful pipeline run, rather than after each successful step.

The helper function `build_message()` demonstrates how to use [StepContext](../../model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md) for crafting appropriate notifications.

```python
from zenml import get_step_context

def build_message(status: str) -> str:
    """Builds a message to post.

    Args:
        status: Status to be set in text.

    Returns:
        str: Prepared message.
    """
    step_context = get_step_context()
    run_url = get_run_url(step_context.pipeline_run)

    return (
        f"Pipeline `{step_context.pipeline.name}` [{str(step_context.pipeline.id)}] {status}!\n"
        f"Run `{step_context.pipeline_run.name}` [{str(step_context.pipeline_run.id)}]\n"
        f"URL: {run_url}"
    )

@step(enable_cache=False)
def notify_on_success() -> None:
    """Notifies user on pipeline success."""
    step_context = get_step_context()
    if alerter and step_context.pipeline_run.config.extra["notify_on_success"]:
        alerter.post(message=build_message(status="succeeded"))
```

## Linking to the Alerter Stack Component

The Alerter component in ZenML can be integrated into failure or success hooks to notify relevant stakeholders. This integration is straightforward and enhances communication regarding pipeline outcomes. For detailed instructions, refer to the Alerter component guide.

```python
from zenml import get_step_context
from zenml.client import Client

def on_failure():
    step_name = get_step_context().step_run.name
    Client().active_stack.alerter.post(f"{step_name} just failed!")
```

ZenML offers standard failure and success hooks that integrate with the configured alerter in your stack. These hooks can be utilized in your pipelines to manage notifications effectively.

```python
from zenml.hooks import alerter_success_hook, alerter_failure_hook


@step(on_failure=alerter_failure_hook, on_success=alerter_success_hook)
def my_step(...):
    ...
```

### ZenML E2E Example Overview

To set up the local environment for ZenML, refer to the [Project templates documentation](../../project-setup-and-management/setting-up-a-project-repository/using-project-templates.md). 

In the file [`steps/alerts/notify_on.py`](../../../../examples/e2e/steps/alerts/notify_on.py), a step is implemented to notify users of pipeline success and a function for notifying about step failures using the [Alerter component](../../../component-guide/alerters/alerters.md) from the active stack. The `@step` decorator is utilized for success notifications to ensure that users are only notified of a fully successful pipeline run, rather than every successful step. This file demonstrates how developers can leverage the Alerter component to send notification messages across configured channels.

```python
from zenml.client import Client
from zenml import get_step_context

alerter = Client().active_stack.alerter

def notify_on_failure() -> None:
    """Notifies user on step failure. Used in Hook."""
    step_context = get_step_context()
    if alerter and step_context.pipeline_run.config.extra["notify_on_failure"]:
        alerter.post(message=build_message(status="failed"))
```

In ZenML, if the AI component is absent from the Stack, notifications are suppressed. However, you can log this event as an error by using the appropriate logging function.

```python
from zenml.client import Client
from zenml.logger import get_logger
from zenml import get_step_context

logger = get_logger(__name__)
alerter = Client().active_stack.alerter

def notify_on_failure() -> None:
    """Notifies user on step failure. Used in Hook."""
    step_context = get_step_context()
    if step_context.pipeline_run.config.extra["notify_on_failure"]:
        if alerter:
            alerter.post(message=build_message(status="failed"))
        else:
            logger.error(message=build_message(status="failed"))
```

## Using the OpenAI ChatGPT Failure Hook

The OpenAI ChatGPT failure hook in ZenML allows users to generate potential fixes for exceptions that cause step failures. To use this feature, you need a valid OpenAI API key with billing set up. 

**Important Notes:**
- Using the OpenAI integration will incur charges on your OpenAI account.
- Ensure the OpenAI integration is installed and your API key is stored as a ZenML secret. 

This hook simplifies troubleshooting by leveraging AI to suggest solutions for encountered errors.

```shell
zenml integration install openai
zenml secret create openai --api_key=<YOUR_API_KEY>
```

To use a hook in your ZenML pipeline, follow these steps:

1. **Define the Hook**: Create a hook by implementing the necessary methods that will interact with your pipeline components.

2. **Integrate the Hook**: Add the hook to your pipeline configuration, ensuring it is properly connected to the relevant pipeline steps.

3. **Execute the Pipeline**: Run your pipeline, and the hook will automatically trigger at the designated points, allowing for custom actions or modifications during execution.

This integration enhances the functionality of your ZenML pipelines, enabling more flexible and powerful workflows.

```python
from zenml.integration.openai.hooks import openai_chatgpt_alerter_failure_hook
from zenml import step

@step(on_failure=openai_chatgpt_alerter_failure_hook)
def my_step(...):
    ...
```

In ZenML, if you set up a Slack alerter, you will receive failure notifications that provide suggestions to help troubleshoot issues in your code. For users with GPT-4 enabled, the `openai_gpt4_alerter_failure_hook` can be utilized as an alternative to the standard Slack alerter. This integration enhances the debugging process by leveraging AI-driven insights.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/retry-steps.md

### Step Retry Configuration in ZenML

ZenML includes a built-in retry mechanism for steps, allowing automatic retries in case of failures, which is particularly useful for handling intermittent issues or transient errors. This feature is beneficial when working with GPU-backed hardware where resource availability may fluctuate.

You can configure the following parameters for step retries:

- **max_retries:** Maximum number of retry attempts for a failed step.
- **delay:** Initial delay (in seconds) before the first retry.
- **backoff:** Multiplier for the delay after each retry attempt.

To implement the retry configuration, use the `@step` decorator in your step definition.

```python
from zenml.config.retry_config import StepRetryConfig

@step(
    retry=StepRetryConfig(
        max_retries=3, 
        delay=10, 
        backoff=2
    )
)
def my_step() -> None:
    raise Exception("This is a test exception")
steps:
  my_step:
    retry:
      max_retries: 3
      delay: 10
      backoff: 2
```

### ZenML Documentation Summary

**Retries Management**: ZenML does not support infinite retries. When setting `max_retries`, specify a reasonable value to avoid infinite loops, as ZenML enforces an internal maximum regardless of the value provided. This is crucial for managing transient failures effectively.

**Related Topics**:
- [Failure/Success Hooks](use-failure-success-hooks.md)
- [Configure Pipelines](../../pipeline-development/use-configuration-files/how-to-use-config.md)

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/tag-your-pipeline-runs.md

# Tagging Pipeline Runs in ZenML

In ZenML, you can tag your pipeline runs to enhance organization and tracking. Tags can be specified in the configuration file, allowing for better categorization and filtering of runs. This feature is essential for managing multiple experiments and improving the clarity of your project’s workflow.

```yaml
# config.yaml
tags:
  - tag_in_config_file
```

ZenML allows users to define pipelines using the `@pipeline` decorator or the `with_options` method. The `@pipeline` decorator is used to annotate a function, marking it as a pipeline, while `with_options` provides a way to configure pipeline options dynamically. Both methods enable users to create modular and reusable components in their machine learning workflows, facilitating better organization and management of data processing and model training tasks.

```python
@pipeline(tags=["tag_on_decorator"])
def my_pipeline():
  ...

my_pipeline = my_pipeline.with_options(tags=["tag_on_with_options"])
```

ZenML allows users to run pipelines where tags from various sources are merged and applied to the pipeline run. This feature enhances the organization and tracking of pipeline executions. For visual reference, a diagram illustrating this process is available.



================================================================================

# docs/book/how-to/pipeline-development/build-pipelines/using-a-custom-step-invocation-id.md

# Using a Custom Step Invocation ID in ZenML

When invoking a ZenML step within a pipeline, it is assigned a unique **invocation ID**. This ID is essential for:

- **Defining Execution Order**: Use the invocation ID to specify the order of pipeline steps.
- **Fetching Information**: Retrieve details about the step invocation after the pipeline execution is complete.

This feature enhances the management and tracking of pipeline executions in ZenML.

```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def example_pipeline():
    # When calling a step for the first time inside a pipeline,
    # the invocation ID will be equal to the step name -> `my_step`.
    my_step()
    # When calling the same step again, the suffix `_2`, `_3`, ... will
    # be appended to the step name to generate a unique invocation ID.
    # For this call, the invocation ID would be `my_step_2`.
    my_step()
    # If you want to use a custom invocation ID when calling a step, you can
    # do so by passing it like this. If you pass a custom ID, it needs to be
    # unique for all the step invocations that happen as part of this pipeline.
    my_step(id="my_custom_invocation_id")
```

ZenML is an open-source framework designed to streamline the development and deployment of machine learning (ML) workflows. It provides a structured approach to building reproducible and maintainable ML pipelines, enabling data scientists and ML engineers to focus on model development rather than infrastructure.

Key Features:
- **Pipeline Abstraction**: ZenML allows users to define ML workflows as pipelines, encapsulating data processing, model training, and evaluation steps.
- **Integration with Tools**: It integrates seamlessly with popular ML tools and libraries, such as TensorFlow, PyTorch, and Scikit-learn, as well as data orchestration tools like Apache Airflow and Kubeflow.
- **Version Control**: ZenML supports versioning of pipelines and artifacts, ensuring reproducibility and traceability of experiments.
- **Modular Components**: Users can create reusable components for data ingestion, preprocessing, training, and deployment, promoting code reuse and collaboration.

Getting Started:
1. **Installation**: ZenML can be installed via pip with the command `pip install zenml`.
2. **Creating a Pipeline**: Users can define a pipeline using decorators, specifying each step and its dependencies.
3. **Running Pipelines**: Pipelines can be executed locally or deployed to cloud environments, with support for monitoring and logging.

ZenML is ideal for teams looking to enhance their ML workflow efficiency and maintainability, making it a valuable addition to any ML project.



================================================================================

# docs/book/how-to/pipeline-development/training-with-gpus/README.md

### ZenML: Utilizing GPU-Backed Hardware for Machine Learning Pipelines

ZenML allows you to scale machine learning pipelines to the cloud, enabling the use of powerful hardware and task distribution across multiple nodes. To run your steps on GPU-backed hardware, you need to configure `ResourceSettings` to allocate additional resources on an orchestrator node and adjust the container environment as necessary.

#### Specifying Resource Requirements for Steps
For resource-intensive steps in your pipeline, you can specify the required hardware resources to ensure optimal execution.

```python
from zenml.config import ResourceSettings
from zenml import step

@step(settings={"resources": ResourceSettings(cpu_count=8, gpu_count=2, memory="8GB")})
def training_step(...) -> ...:
    # train a model
```

In ZenML, if your stack's orchestrator supports resource specification, you can configure resource settings to secure these resources. Note that some orchestrators, such as the Skypilot orchestrator, do not directly support `ResourceSettings`. Instead, they utilize orchestrator-specific settings to manage resources effectively.

```python
from zenml import step
from zenml.integrations.skypilot.flavors.skypilot_orchestrator_aws_vm_flavor import SkypilotAWSOrchestratorSettings

skypilot_settings = SkypilotAWSOrchestratorSettings(
    cpus="2",
    memory="16",
    accelerators="V100:2",
)


@step(settings={"orchestrator": skypilot_settings)
def training_step(...) -> ...:
    # train a model
```

### ZenML GPU Configuration Guide

To utilize GPU capabilities in ZenML, ensure your container is CUDA-enabled by following these steps:

1. **Orchestrator Resource Specification**: Check the source code and documentation of your chosen orchestrator to understand how to specify resources. If your orchestrator does not support this feature, consider using [step operators](../../component-guide/step-operators/step-operators.md) to execute pipeline steps in independent environments.

2. **CUDA Tools Installation**: Install the necessary CUDA tools in your environment. This is essential for leveraging GPU hardware effectively. Without these changes, your steps may run but won't benefit from performance enhancements.

3. **Containerized Environment**: All GPU-backed steps will run in a containerized environment, whether using local Docker or cloud-based Kubeflow. 

4. **Docker Settings Amendments**: Update your Docker settings to specify a CUDA-enabled parent image in your `DockerSettings`. For detailed instructions, refer to the [containerization page](../../infrastructure-deployment/customize-docker-builds/README.md). For example, to use the latest CUDA-enabled official PyTorch image, include the appropriate code in your settings.

By following these guidelines, you can effectively configure ZenML to utilize GPU resources in your projects.

```python
from zenml import pipeline
from zenml.config import DockerSettings

docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To use ZenML with TensorFlow, you can utilize the `tensorflow/tensorflow:latest-gpu` Docker image, as outlined in the official TensorFlow documentation. 

### Installation of ZenML
ZenML must be explicitly included as a pip requirement for the containers executing your pipelines and steps. Ensure that ZenML is installed by specifying it in your project dependencies. 

This concise approach will help you integrate ZenML into your TensorFlow projects effectively.

```python
from zenml.config import DockerSettings
from zenml import pipeline

docker_settings = DockerSettings(
    parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
    requirements=["zenml==0.39.1", "torchvision"]
)


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To enable GPU acceleration in ZenML, ensure that CUDA is configured for specific steps requiring it. Be cautious when selecting Docker images, as local and remote environments may have different CUDA versions. Core cloud operators provide prebuilt Docker images tailored to their hardware, available for AWS, GCP, and Azure. Note that not all images are on DockerHub; ensure your orchestrator environment has permission to pull from the necessary registries.

Consider resetting the CUDA cache between steps to prevent issues, especially if your training jobs are intensive. This can be easily done using a helper function at the start of any GPU-enabled step.

```python
import gc
import torch

def cleanup_memory() -> None:
    while gc.collect():
        torch.cuda.empty_cache()
```

To initiate GPU-enabled steps in ZenML, call the designated function at the start of your workflow. This ensures that the necessary GPU resources are allocated for optimal performance in your machine learning projects.

```python
from zenml import step

@step
def training_step(...):
    cleanup_memory()
    # train a model
```

### ZenML Multi-GPU Training

ZenML allows for training models across multiple GPUs on a single node, which is beneficial for handling large datasets in parallel. Key considerations include:

- **Preventing Multiple Instances**: Ensure that multiple ZenML instances are not spawned when distributing work across GPUs.
- **Implementation Steps**:
  - Create a script or Python function for model training that supports parallel execution on multiple GPUs.
  - Call this script or function within your ZenML step, potentially using a wrapper to configure it dynamically.

ZenML is actively working on improving support for multi-GPU training. For assistance with implementation, users are encouraged to connect via [Slack](https://zenml.io/slack). 

**Note**: Resetting the memory cache may impact others using the same GPU, so it should be done cautiously.



================================================================================

# docs/book/how-to/pipeline-development/training-with-gpus/accelerate-distributed-training.md

### Distributed Training with Hugging Face's Accelerate in ZenML

ZenML integrates with [Hugging Face's Accelerate library](https://github.com/huggingface/accelerate) to facilitate distributed training in machine learning pipelines. This integration allows users to efficiently leverage multiple GPUs or nodes for training.

#### Key Features:
- **Seamless Integration**: Utilize the Accelerate library within ZenML pipelines for distributed training.
- **Enhanced Training Steps**: Apply the `run_with_accelerate` decorator to specific steps in your pipeline, particularly those related to training, to enable distributed execution.

This functionality enhances the scalability of machine learning projects, making it easier to handle larger datasets and complex models.

```python
from zenml import step, pipeline
from zenml.integrations.huggingface.steps import run_with_accelerate

@run_with_accelerate(num_processes=4, multi_gpu=True)
@step
def training_step(some_param: int, ...):
    # your training code is below
    ...

@pipeline
def training_pipeline(some_param: int, ...):
    training_step(some_param, ...)
```

The `run_with_accelerate` decorator in ZenML enables steps to utilize Accelerate's distributed training capabilities. It accepts arguments similar to those used in the `accelerate launch` CLI command. For a comprehensive list of arguments, refer to the [Accelerate CLI documentation](https://huggingface.co/docs/accelerate/en/package_reference/cli#accelerate-launch).

### Configuration
Key arguments for the `run_with_accelerate` decorator include:
- `num_processes`: Number of processes for distributed training.
- `cpu`: Forces training on CPU.
- `multi_gpu`: Enables distributed GPU training.
- `mixed_precision`: Sets mixed precision training mode ('no', 'fp16', or 'bf16').

### Important Usage Notes
1. Use the `run_with_accelerate` decorator directly on steps with the '@' syntax; it cannot be used as a function in the pipeline definition.
2. Accelerated steps require keyword arguments; positional arguments are not supported.
3. Misuse of the decorator will raise a `RuntimeError` with guidance on correct usage.

For a practical example of using Accelerate in a ZenML pipeline, refer to the [llm-lora-finetuning](https://github.com/zenml-io/zenml-projects/blob/main/llm-lora-finetuning/README.md) project.

### Ensure Your Container is Accelerate-Ready
To effectively run steps with Accelerate, ensure your environment has the necessary dependencies. Configuration changes are mandatory for proper functionality; without them, steps may run but will not utilize distributed training.

All steps using Accelerate must be executed in a containerized environment. You need to:
1. Specify a CUDA-enabled parent image in your `DockerSettings`. For more details, see the [containerization page](../../infrastructure-deployment/customize-docker-builds/README.md). An example is provided using a CUDA-enabled PyTorch image.

```python
from zenml import pipeline
from zenml.config import DockerSettings

docker_settings = DockerSettings(parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

### 2. Add Accelerate as a Pip Requirement

To ensure that the Accelerate library is available in your container, explicitly include it in your pip requirements. This step is crucial for projects utilizing ZenML that depend on Accelerate for performance optimization.

```python
from zenml.config import DockerSettings
from zenml import pipeline

docker_settings = DockerSettings(
    parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime",
    requirements=["accelerate", "torchvision"]
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

## Train Across Multiple GPUs with ZenML

ZenML's Accelerate integration enables training models using multiple GPUs, either on a single node or across multiple nodes. This is ideal for handling large datasets or complex models that benefit from parallel processing. Key steps for using Accelerate with multiple GPUs include:

- Wrapping your training step with the `run_with_accelerate` function in your pipeline.
- Configuring Accelerate arguments such as `num_processes` and `multi_gpu`.
- Ensuring compatibility of your training code with distributed training (most compatibility is handled automatically by Accelerate).

For assistance with distributed training or troubleshooting, connect with the ZenML community on [Slack](https://zenml.io/slack). By utilizing the Accelerate integration, you can effectively scale your training processes while leveraging your hardware resources within ZenML's structured pipeline framework.



================================================================================

# docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-cli.md

### Creating a Template with ZenML CLI

**Note:** This feature is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

To create a run template, utilize the ZenML CLI. This functionality allows users to streamline their workflows by defining reusable configurations for experiments and pipelines.

```bash
# The <PIPELINE_SOURCE_PATH> will be `run.my_pipeline` if you defined a
# pipeline with name `my_pipeline` in a file called `run.py`
zenml pipeline create-run-template <PIPELINE_SOURCE_PATH> --name=<TEMPLATE_NAME>
```

### ZenML Overview

ZenML is a framework designed to streamline the machine learning workflow by providing a structured approach to building reproducible pipelines. 

### Important Note
- Ensure you have an **active remote stack** when executing commands. Alternatively, you can specify a stack using the `--stack` option.

### Visual Reference
![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)

This documentation is part of a larger guide aimed at helping users effectively implement ZenML in their projects.



================================================================================

# docs/book/how-to/pipeline-development/trigger-pipelines/README.md

### Triggering a Pipeline in ZenML

In ZenML, the most straightforward method to execute a pipeline is by calling your pipeline function directly. This allows users to initiate a run efficiently. There are various other methods to trigger a pipeline, providing flexibility in how you can integrate ZenML into your projects.

```python
from zenml import step, pipeline


@step  # Just add this decorator
def load_data() -> dict:
    training_data = [[1, 2], [3, 4], [5, 6]]
    labels = [0, 1, 0]
    return {'features': training_data, 'labels': labels}


@step
def train_model(data: dict) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])

    # Train some model here...

    print(
        f"Trained model using {len(data['features'])} data points. "
        f"Feature sum is {total_features}, label sum is {total_labels}."
    )


@pipeline  # This function combines steps together 
def simple_ml_pipeline():
    dataset = load_data()
    train_model(dataset)


if __name__ == "__main__":
    simple_ml_pipeline()
```

### ZenML Pipeline Triggering and Run Templates

ZenML allows for various methods to trigger pipelines, especially those utilizing a remote stack (including remote orchestrators, artifact stores, and container registries).

#### Run Templates
**Run Templates** are parameterized configurations for ZenML pipelines that can be executed from the ZenML dashboard or through the Client/REST API. They serve as customizable blueprints for pipeline runs.

- **Note**: Run Templates are a feature exclusive to ZenML Pro users. [Sign up here](https://cloud.zenml.io) for access.

#### Usage
Run Templates can be utilized in different ways:
- **Python SDK**: [Use templates: Python SDK](use-templates-python.md)
- **CLI**: [Use templates: CLI](use-templates-cli.md)
- **Dashboard**: [Use templates: Dashboard](use-templates-dashboard.md)
- **REST API**: [Use templates: Rest API](use-templates-rest-api.md)

This feature enhances the flexibility and efficiency of managing pipeline executions in ZenML.



================================================================================

# docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-python.md

### ZenML: Creating and Running a Template with the Python SDK

**Note:** This feature is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Creating a Template
Utilize the ZenML client to create a run template. This allows for streamlined execution of workflows within your projects. 

For detailed instructions and examples, refer to the ZenML documentation.

```python
from zenml.client import Client

run = Client().get_pipeline_run(<RUN_NAME_OR_ID>)

Client().create_run_template(
    name=<TEMPLATE_NAME>,
    deployment_id=run.deployment_id
)
```

To create a template from a pipeline definition in ZenML, ensure that you have selected a pipeline run executed on a remote stack, which includes a remote orchestrator, artifact store, and container registry. You can generate the template by executing the appropriate code while a remote stack is active.

```python
from zenml import pipeline

@pipeline
def my_pipeline():
    ...

template = my_pipeline.create_run_template(name=<TEMPLATE_NAME>)
```

## Running a Template in ZenML

To execute a template using the ZenML client, follow these steps:

1. **Initialize ZenML Client**: Ensure you have the ZenML client set up in your environment.
2. **Select a Template**: Choose the desired template from the available options.
3. **Run the Template**: Use the appropriate command to execute the selected template.

This process allows you to quickly implement predefined workflows in your projects, facilitating streamlined development and deployment.

```python
from zenml.client import Client

template = Client().get_run_template(<TEMPLATE_NAME>)

config = template.config_template

# [OPTIONAL] ---- modify the config here ----

Client().trigger_pipeline(
    template_id=template.id,
    run_configuration=config,
)
```

ZenML allows users to trigger a new run based on an existing template, executing it on the same stack as the original run. Additionally, users can run a pipeline within another pipeline, leveraging the same logic for advanced usage scenarios. This functionality enhances the flexibility and modularity of workflows in ZenML projects.

```python
import pandas as pd

from zenml import pipeline, step
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml.artifacts.utils import load_artifact
from zenml.client import Client
from zenml.config.pipeline_run_configuration import PipelineRunConfiguration


@step
def trainer(data_artifact_id: str):
    df = load_artifact(data_artifact_id)


@pipeline
def training_pipeline():
    trainer()


@step
def load_data() -> pd.Dataframe:
    ...


@step
def trigger_pipeline(df: UnmaterializedArtifact):
    # By using UnmaterializedArtifact we can get the ID of the artifact
    run_config = PipelineRunConfiguration(
        steps={"trainer": {"parameters": {"data_artifact_id": df.id}}}
    )

    Client().trigger_pipeline("training_pipeline", run_configuration=run_config)


@pipeline
def loads_data_and_triggers_training():
    df = load_data()
    trigger_pipeline(df)  # Will trigger the other pipeline
```

ZenML is a framework designed to streamline the machine learning workflow. Key components include the `PipelineRunConfiguration`, which manages the configuration of pipeline runs, and the `trigger_pipeline` function, which initiates these runs. For detailed information on these components, refer to the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) and [`trigger_pipeline`](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/#zenml.client.Client) documentation.

Additionally, ZenML addresses the concept of Unmaterialized Artifacts, which can be explored further [here](../../data-artifact-management/complex-usecases/unmaterialized-artifacts.md). 

For visual reference, see the ZenML Scarf image below:

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)



================================================================================

# docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-dashboard.md

### ZenML Dashboard: Creating and Running Templates

**Feature Access**: This functionality is exclusive to [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Creating a Template
1. Navigate to a pipeline run executed on a remote stack (requires a remote orchestrator, artifact store, and container registry).
2. Click `+ New Template`, provide a name, and click `Create`.

#### Running a Template
1. To run a template, either:
   - Click `Run a Pipeline` on the main `Pipelines` page, or
   - Go to a specific template page and select `Run Template`.
2. You will be directed to the `Run Details` page, where you can upload a `.yaml` configuration file or modify settings using the editor.
3. Running the template will execute a new run on the same stack as the original. 

This process allows users to efficiently create and execute pipeline templates directly from the ZenML Dashboard.



================================================================================

# docs/book/how-to/pipeline-development/trigger-pipelines/use-templates-rest-api.md

### ZenML REST API: Running a Template

**Note:** This feature is available only in [ZenML Pro](https://zenml.io/pro). [Sign up here](https://cloud.zenml.io) for access.

#### Triggering a Pipeline via REST API

To trigger a pipeline, you must have created at least one run template for that pipeline. Follow these steps:

1. **Get Pipeline ID:**
   - Call `GET /pipelines?name=<PIPELINE_NAME>` to retrieve the `<PIPELINE_ID>`.

2. **Get Template ID:**
   - Call `GET /run_templates?pipeline_id=<PIPELINE_ID>` to obtain a list of templates and select a `<TEMPLATE_ID>`.

3. **Run the Pipeline:**
   - Execute `POST /run_templates/<TEMPLATE_ID>/runs` to trigger the pipeline. You can include the [PipelineRunConfiguration](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.pipeline_run_configuration.PipelineRunConfiguration) in the request body.

#### Example

To re-run a pipeline named `training`, start by querying the `/pipelines` endpoint.

**Additional Information:** For details on obtaining a bearer token for API access, refer to the [API Reference](../../../reference/api-reference.md#using-a-bearer-token-to-access-the-api-programmatically).

```shell
curl -X 'GET' \
  '<YOUR_ZENML_SERVER_URL>/api/v1/pipelines?hydrate=false&name=training' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer <YOUR_TOKEN>'
```

To use ZenML, you can identify the pipeline ID from the response list of objects. For example, the pipeline ID is `c953985e-650a-4cbf-a03a-e49463f58473`. Once you have the pipeline ID, you can call the API endpoint `/run_templates?pipeline_id=<PIPELINE_ID>` to proceed with your operations.

```shell
curl -X 'GET' \
  '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates?hydrate=false&logical_operator=and&page=1&size=20&pipeline_id=b826b714-a9b3-461c-9a6e-1bde3df3241d' \
  -H 'accept: application/json' \
  -H 'Authorization: Bearer <YOUR_TOKEN>'
```

To trigger a pipeline in ZenML, first obtain the template ID from the response. For example, the template ID is `b826b714-a9b3-461c-9a6e-1bde3df3241d`. This ID can then be used to initiate the pipeline with a new configuration.

```shell
curl -X 'POST' \
  '<YOUR_ZENML_SERVER_URL>/api/v1/run_templates/b826b714-a9b3-461c-9a6e-1bde3df3241d/runs' \
  -H 'accept: application/json' \
  -H 'Content-Type: application/json' \
  -H 'Authorization: Bearer <YOUR_TOKEN>' \
  -d '{
  "steps": {"model_trainer": {"parameters": {"model_type": "rf"}}}
}'
```

ZenML is a framework designed to streamline the machine learning (ML) workflow by enabling reproducibility and collaboration. It allows users to create pipelines that can be easily re-triggered with different configurations. This flexibility is essential for experimenting with various settings and improving model performance.

Key Features:
- **Pipeline Management**: ZenML facilitates the creation and management of ML pipelines.
- **Re-triggering Pipelines**: Users can re-trigger pipelines with altered configurations to test different scenarios.

For visual reference, ZenML includes graphical elements, such as the ZenML Scarf image, which enhances user understanding of the framework's components.

In summary, ZenML is a powerful tool for managing ML workflows, allowing for easy adjustments and re-execution of pipelines to optimize results.



================================================================================

# docs/book/how-to/pipeline-development/configure-python-environments/handling-dependencies.md

### Handling Dependencies in ZenML

ZenML is designed to be stack- and integration-agnostic, allowing users to run pipelines with various tools. However, this flexibility can lead to conflicting dependencies when integrating with other libraries.

#### Installing Dependencies
Use the command `zenml integration install ...` to install dependencies for specific integrations. After installing additional dependencies, check if ZenML requirements are met by running `zenml integration list`. A green tick indicates that all requirements are satisfied.

#### Suggestions for Resolving Dependency Conflicts

1. **Use `pip-compile` for Reproducibility**: 
   - Utilize `pip-compile` from the `pip-tools` package to create a static `requirements.txt` file for consistent environments. For more details, refer to the [gitflow repository](https://github.com/zenml-io/zenml-gitflow#-software-requirements-management).

2. **Run `pip check`**: 
   - Execute `pip check` to identify any dependency conflicts in your environment. This command will list incompatible dependencies, which may affect your project.

3. **Known Dependency Issues**: 
   - Some integrations have strict dependency requirements. For example, ZenML requires `click~=8.0.3` for its CLI. Using a version greater than 8.0.3 may lead to unexpected behaviors.

4. **Manual Dependency Installation**: 
   - While not recommended, you can manually install dependencies instead of using ZenML's integration installation. The command `zenml integration install ...` executes a `pip install ...` for the specified integration's dependencies. To find these dependencies, run the relevant command.

By following these guidelines, you can effectively manage and resolve dependency conflicts while using ZenML in your projects.

```bash
# to have the requirements exported to a file
zenml integration export-requirements --output-file integration-requirements.txt INTEGRATION_NAME

# to have the requirements printed to the console
zenml integration export-requirements INTEGRATION_NAME
```

In ZenML, you can customize your project dependencies as needed. If using a remote orchestrator, update the dependency versions in a `DockerSettings` object to ensure proper functionality. For detailed instructions on configuring Docker builds, refer to the relevant documentation section.


# docs/book/how-to/pipeline-development/configure-python-environments/configure-the-server-environment.md

### Configure the Server Environment

The ZenML server environment is configured using environment variables that must be set before deploying your server instance. For a complete list of available environment variables, refer to the [full list here](../../../reference/environment-variables.md).



================================================================================

# docs/book/how-to/control-logging/disable-colorful-logging.md

To disable colorful logging in ZenML, set the environment variable as follows:

```bash
ZENML_LOGGING_COLORS_DISABLED=true
```

Setting the `ZENML_LOGGING_COLORS_DISABLED` environment variable on the client environment (e.g., local machine) will disable colorful logging for remote pipeline runs. To disable it only locally while enabling it for remote runs, configure the environment variable in the pipeline runs environment.

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_COLORS_DISABLED": "false"})

# Either add it to the decorator
@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure the pipelines options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

The documentation includes an image of the ZenML Scarf, which is referenced with a specific URL. The image has an alt text "ZenML Scarf" and uses a referrer policy of "no-referrer-when-downgrade."



================================================================================

# docs/book/how-to/control-logging/disable-rich-traceback.md

To disable rich traceback output in ZenML, which uses the `rich` library for enhanced debugging, set the following environment variable: [insert variable name here].

```bash
export ZENML_ENABLE_RICH_TRACEBACK=false
```

To see only plain text traceback output, set the `ZENML_ENABLE_RICH_TRACEBACK` environment variable. Note that this setting affects only local pipeline runs and does not automatically disable rich tracebacks for remote runs. To disable rich tracebacks for remote pipeline runs, set the `ZENML_ENABLE_RICH_TRACEBACK` variable in the remote pipeline runs environment.

```python
docker_settings = DockerSettings(environment={"ZENML_ENABLE_RICH_TRACEBACK": "false"})

# Either add it to the decorator
@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure the pipelines options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and a referrer policy of "no-referrer-when-downgrade." The image source is a URL that includes a unique identifier.



================================================================================

# docs/book/how-to/control-logging/view-logs-on-the-dasbhoard.md

# Viewing Logs on the Dashboard

ZenML captures logs during step execution using a logging handler. Users can utilize the default Python logging module or print statements, which ZenML will capture and store.

```python
import logging

from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")  # You can use the regular `logging` module.
    print("World.")  # You can utilize `print` statements as well. 
```

Logs are stored in the artifact store of your ZenML stack and can be viewed in the dashboard only if the ZenML server has direct access to it. Access conditions are as follows:

1. **Local ZenML Server**: Both local and remote artifact stores may be accessible based on client configuration.
2. **Deployed ZenML Server**:
   - Logs from a local artifact store are not accessible.
   - Logs from a remote artifact store may be accessible if configured with a service connector. Refer to the production guide for configuration details.

If configured correctly, logs will display in the dashboard. To disable log storage due to performance or storage concerns, follow the provided instructions.



================================================================================

# docs/book/how-to/control-logging/set-logging-verbosity.md

To change the logging verbosity in ZenML, set the environment variable to your desired level. By default, the verbosity is set to `INFO`.

```bash
export ZENML_LOGGING_VERBOSITY=INFO
```

You can choose a logging level from `INFO`, `WARN`, `ERROR`, `CRITICAL`, or `DEBUG`. Setting this on the client environment (e.g., your local machine) will not affect the logging verbosity for remote pipeline runs. To control logging for remote runs, set the `ZENML_LOGGING_VERBOSITY` environment variable in the pipeline runs environment.

```python
docker_settings = DockerSettings(environment={"ZENML_LOGGING_VERBOSITY": "DEBUG"})

# Either add it to the decorator
@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure the pipelines options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and referrer policy. The image source is a URL that includes a unique identifier.



================================================================================

# docs/book/how-to/control-logging/enable-or-disable-logs-storing.md

ZenML captures logs during step execution using a logging handler. Users can utilize the default Python logging module or print statements, which ZenML will store.

```python
import logging

from zenml import step

@step 
def my_step() -> None:
    logging.warning("`Hello`")  # You can use the regular `logging` module.
    print("World.")  # You can utilize `print` statements as well. 
```

Logs are stored in your stack's artifact store and can be displayed on the dashboard. However, if you are not connected to a cloud artifact store with a service connector, you won't be able to view the logs. For more details, refer to the documentation on viewing logs. To prevent logs from being stored in the artifact store, disable it using the `enable_step_logs` parameter with either the `@pipeline` or `@step` decorator.

```python
    from zenml import pipeline, step

    @step(enable_step_logs=False)  # disables logging for this step
    def my_step() -> None:
        ...

    @pipeline(enable_step_logs=False)  # disables logging for the entire pipeline
    def my_pipeline():
        ...
    ```

To disable step logs storage, set the environmental variable `ZENML_DISABLE_STEP_LOGS_STORAGE` to `true`. This variable overrides the previously mentioned parameters and must be configured in the execution environment at the orchestrator level.

```python
docker_settings = DockerSettings(environment={"ZENML_DISABLE_STEP_LOGS_STORAGE": "true"})

# Either add it to the decorator
@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure the pipelines options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text. The image is hosted on Scarf's server and has a referrer policy of "no-referrer-when-downgrade."



================================================================================

# docs/book/how-to/configuring-zenml/configuring-zenml.md

### Configuring ZenML

This guide outlines methods to customize ZenML's default behavior. Users can adapt specific aspects of ZenML to suit their needs.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/grouping-metadata.md

### Grouping Metadata in the Dashboard

To group key-value pairs in the ZenML dashboard, pass a dictionary of dictionaries in the `metadata` parameter. This organizes metadata into cards, enhancing visualization and comprehension. 

![Metadata in the dashboard](../../../.gitbook/assets/metadata-in-dashboard.png) 

Example of grouping metadata into cards is provided in the documentation.

```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize

log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="my_artifact_version",
)
```

In the ZenML dashboard, "model_metrics" and "data_details" are displayed as separate cards, each containing relevant key-value pairs.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/fetch-metadata-within-pipeline.md

### Fetching Metadata During Pipeline Composition

To access pipeline configuration information during composition, utilize the `zenml.get_pipeline_context()` function to retrieve the `PipelineContext` of your pipeline.

```python
from zenml import get_pipeline_context, pipeline

...

@pipeline(
    extra={
        "complex_parameter": [
            ("sklearn.tree", "DecisionTreeClassifier"),
            ("sklearn.ensemble", "RandomForestClassifier"),
        ]
    }
)
def my_pipeline():
    context = get_pipeline_context()

    after = []
    search_steps_prefix = "hp_tuning_search_"
    for i, model_search_configuration in enumerate(
        context.extra["complex_parameter"]
    ):
        step_name = f"{search_steps_prefix}{i}"
        cross_validation(
            model_package=model_search_configuration[0],
            model_class=model_search_configuration[1],
            id=step_name
        )
        after.append(step_name)
    select_best_model(
        search_steps_prefix=search_steps_prefix, 
        after=after,
    )
```

Refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.pipelines.pipeline_context.PipelineContext) for detailed information on the attributes and methods available in the `PipelineContext`.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-an-artifact.md

### Attach Metadata to an Artifact

In ZenML, metadata enhances artifacts by providing context and details such as size, structure, and performance metrics. This information is accessible in the ZenML dashboard for easier inspection and comparison of artifacts across pipeline runs.

#### Logging Metadata for Artifacts

Artifacts are outputs from pipeline steps (e.g., datasets, models). To log metadata, use the `log_metadata` function with the artifact's name, version, or ID. The metadata can be any JSON-serializable value, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`. For more details on these types, refer to the logging metadata documentation.

Example of logging metadata for an artifact:

```python
import pandas as pd

from zenml import step, log_metadata
from zenml.metadata.metadata_types import StorageSize


@step
def process_data_step(dataframe: pd.DataFrame) -> pd.DataFrame:
    """Process a dataframe and log metadata about the result."""
    processed_dataframe = ...

    # Log metadata about the processed dataframe
    log_metadata(
        metadata={
            "row_count": len(processed_dataframe),
            "columns": list(processed_dataframe.columns),
            "storage_size": StorageSize(
                processed_dataframe.memory_usage().sum())
        },
        infer_artifact=True,
    )
    return processed_dataframe
```

### Selecting the Artifact for Metadata Logging

When using `log_metadata` with an artifact name, ZenML offers several methods to attach metadata:

1. **Using `infer_artifact`**: Within a step, ZenML infers output artifacts from the step context. If there's a single output, that artifact is selected. If an `artifact_name` is provided, ZenML searches for it among the step's outputs, which is useful for steps with multiple outputs.

2. **Name and Version Provided**: If both an artifact name and version are supplied, ZenML identifies and attaches metadata to the specified artifact version.

3. **Artifact Version ID Provided**: If an artifact version ID is given, ZenML uses it to fetch and attach metadata to that specific version.

### Fetching Logged Metadata

Once metadata is logged to an artifact or step, it can be easily retrieved using the ZenML Client.

```python
from zenml.client import Client

client = Client()
artifact = client.get_artifact_version("my_artifact", "my_version")

print(artifact.run_metadata["metadata_key"])
```

When fetching metadata with a specific key, the returned value reflects the latest entry. 

## Grouping Metadata in the Dashboard
To group metadata in the ZenML dashboard, pass a dictionary of dictionaries in the `metadata` parameter. This organizes metadata into cards, enhancing visualization and comprehension.

```python
from zenml import log_metadata

from zenml.metadata.metadata_types import StorageSize

log_metadata(
    metadata={
        "model_metrics": {
            "accuracy": 0.95,
            "precision": 0.92,
            "recall": 0.90
        },
        "data_details": {
            "dataset_size": StorageSize(1500000),
            "feature_columns": ["age", "income", "score"]
        }
    },
    artifact_name="my_artifact",
    artifact_version="version",
)
```

In the ZenML dashboard, `model_metrics` and `data_details` are displayed as separate cards, each containing relevant key-value pairs.



================================================================================

TODO SOME READMEs will be repeated

....



================================================================================


# docs/book/how-to/pipeline-development/configure-python-environments/README.md

# Configure Python Environments

ZenML deployments involve multiple environments for managing dependencies and configurations. Below is an overview of these environments:

## Client Environment (Runner Environment)
The client environment is where ZenML pipelines are compiled, typically in a `run.py` script. Types of client environments include:
- Local development
- CI runner in production
- [ZenML Pro](https://zenml.io/pro) runner
- `runner` image orchestrated by the ZenML server

Use a package manager (e.g., `pip`, `poetry`) to manage dependencies, including the ZenML package and required integrations. Key steps for starting a pipeline:
1. Compile an intermediate pipeline representation via the `@pipeline` function.
2. Create or trigger pipeline and step build environments if running remotely.
3. Trigger a run in the orchestrator.

The `@pipeline` function is only called in this environment, focusing on compile time rather than execution time.

## ZenML Server Environment
The ZenML server environment is a FastAPI application that manages pipelines and metadata, including the ZenML Dashboard. Manage dependencies during [ZenML deployment](../../../getting-started/deploying-zenml/README.md), especially for custom integrations. More details can be found in [configuring the server environment](./configure-the-server-environment.md).

## Execution Environments
When running locally, the client, server, and execution environments are the same. For remote pipeline execution, ZenML transfers code and environment to the remote orchestrator by building Docker images (execution environments). ZenML configures these images starting from a [base image](https://hub.docker.com/r/zenmldocker/zenml) with ZenML and Python, adding pipeline dependencies. Follow the [containerize your pipeline](../../infrastructure-deployment/customize-docker-builds/README.md) guide for Docker image configuration.

## Image Builder Environment
Execution environments are typically created locally using the local Docker client, which requires Docker installation and permissions. ZenML provides [image builders](../../../component-guide/image-builders/image-builders.md) to build and push Docker images in a specialized image builder environment. If no image builder is configured, ZenML defaults to the local image builder, ensuring consistency across builds.



================================================================================

# docs/book/how-to/pipeline-development/configure-python-environments/configure-the-server-environment.md

### Configure the Server Environment

The ZenML server environment is configured using environment variables, which must be set before deploying your server instance. For a complete list of available environment variables, refer to [the full list here](../../../reference/environment-variables.md).



================================================================================

# docs/book/how-to/control-logging/README.md

# Configuring ZenML's Default Logging Behavior

ZenML generates different types of logs across various environments:

- **ZenML Server**: Produces server logs similar to any FastAPI server.
- **Client or Runner Environment**: Logs events related to pipeline execution, including pre, post, and during pipeline run activities.
- **Execution Environment**: Logs are generated at the orchestrator level during the execution of each pipeline step, typically using Python's `logging` module.

This section outlines how users can manage logging behavior across these environments.



================================================================================

# docs/book/how-to/model-management-metrics/README.md

# Model Management and Metrics

This section addresses managing models and tracking metrics in ZenML.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/README.md

# Track Metrics and Metadata

ZenML offers a unified method for logging and managing metrics and metadata via the `log_metadata` function. This function enables logging across different entities such as models, artifacts, steps, and runs through a single interface. Users can also choose to automatically log the same metadata for related entities.

### Basic Use-Case
The `log_metadata` function can be utilized within a step.

```python
from zenml import step, log_metadata

@step
def my_step() -> ...:
    log_metadata(metadata={"accuracy": 0.91})
    ...
```

The `log_metadata` function logs the `accuracy` for a step, its pipeline run, and optionally its model version. It supports various use-cases by allowing specification of the target entity (model, artifact, step, or run) with flexible parameters. For more details, refer to the following pages: 
- [Log metadata to a step](attach-metadata-to-a-step.md) 
- [Log metadata to a run](attach-metadata-to-a-run.md) 
- [Log metadata to an artifact](attach-metadata-to-an-artifact.md) 
- [Log metadata to a model](attach-metadata-to-a-model.md) 

**Note:** The older methods (`log_model_metadata`, `log_artifact_metadata`, `log_step_metadata`) are deprecated. Use `log_metadata` for all future implementations.

================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/logging-metadata.md

**Tracking Your Metadata with ZenML**

ZenML supports special metadata types to capture specific information. Key types include:

- **Uri**: Represents a uniform resource identifier.
- **Path**: Denotes a file system path.
- **DType**: Specifies data types.
- **StorageSize**: Indicates the size of storage used.

These types facilitate effective metadata tracking in your workflows.

```python
from zenml import log_metadata
from zenml.metadata.metadata_types import StorageSize, DType, Uri, Path

log_metadata(
    metadata={
        "dataset_source": Uri("gs://my-bucket/datasets/source.csv"),
        "preprocessing_script": Path("/scripts/preprocess.py"),
        "column_types": {
            "age": DType("int"),
            "income": DType("float"),
            "score": DType("int")
        },
        "processed_data_size": StorageSize(2500000)
    },
)
```

In this example, the following special types are defined: 
- `Uri`: indicates the dataset source URI.
- `Path`: specifies the filesystem path to a preprocessing script.
- `DType`: describes the data types of specific columns.
- `StorageSize`: indicates the size of the processed data in bytes.

These types standardize metadata format and ensure consistent logging.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-run.md

### Attach Metadata to a Run

In ZenML, you can log metadata to a pipeline run using the `log_metadata` function, which accepts a dictionary of key-value pairs. Values can be any JSON-serializable type, including ZenML custom types like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Run

When logging metadata from a step in a pipeline run, `log_metadata` attaches the metadata with the key format `step_name::metadata_key`, allowing for consistent use of metadata keys across different steps during execution.

```python
from typing import Annotated

import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier

from zenml import step, log_metadata, ArtifactConfig


@step
def train_model(dataset: pd.DataFrame) -> Annotated[
    ClassifierMixin,
    ArtifactConfig(name="sklearn_classifier", is_model_artifact=True)
]:
    """Train a model and log run-level metadata."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    # Log metadata at the run level
    log_metadata(
        metadata={
            "run_metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall
            }
        }
    )
    return classifier
```

## Manually Logging Metadata to a Pipeline Run

You can attach metadata to a specific pipeline run using identifiers such as the run ID, without requiring a step. This is beneficial for logging information or metrics calculated after execution.

```python
from zenml import log_metadata

log_metadata(
    metadata={"post_run_info": {"some_metric": 5.0}},
    run_id_name_or_prefix="run_id_name_or_prefix"
)
```

## Fetching Logged Metadata

Once metadata is logged in a pipeline run, it can be retrieved using the ZenML Client.

```python
from zenml.client import Client

client = Client()
run = client.get_pipeline_run("run_id_name_or_prefix")

print(run.run_metadata["metadata_key"])
```

When fetching metadata with a specific key, the returned value will always be the latest entry.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-step.md

### Attach Metadata to a Step

In ZenML, you can log metadata for a specific step using the `log_metadata` function, which allows you to attach a dictionary of key-value pairs as metadata. The metadata can include any JSON-serializable values, such as custom classes like `Uri`, `Path`, `DType`, and `StorageSize`.

#### Logging Metadata Within a Step

When called within a step, `log_metadata` automatically attaches the metadata to the currently executing step and its associated pipeline run, making it suitable for logging metrics or information available during execution.

```python
from typing import Annotated

import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier

from zenml import step, log_metadata, ArtifactConfig


@step
def train_model(dataset: pd.DataFrame) -> Annotated[
    ClassifierMixin,
    ArtifactConfig(name="sklearn_classifier")
]:
    """Train a model and log evaluation metrics."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...

    # Log metadata at the step level
    log_metadata(
        metadata={
            "evaluation_metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall
            }
        }
    )
    return classifier
```

{% hint style="info" %} When executing a cached pipeline step, the cached run will replicate the original step's metadata. However, any manually generated metadata after the original execution will not be included. {% endhint %} 

## Manually Logging Metadata for a Step Run
You can log metadata for a specific step after execution by using identifiers for the pipeline, step, and run. This is beneficial for logging metadata post-execution.

```python
from zenml import log_metadata

log_metadata(
    metadata={
        "additional_info": {"a_number": 3}
    },
    step_name="step_name",
    run_id_name_or_prefix="run_id_name_or_prefix"
)

# or 

log_metadata(
    metadata={
        "additional_info": {"a_number": 3}
    },
    step_id="step_id",
)
```

## Fetching Logged Metadata

After logging metadata in a step, it can be retrieved using the ZenML Client.

```python
from zenml.client import Client

client = Client()
step = client.get_pipeline_run("pipeline_id").steps["step_name"]

print(step.run_metadata["metadata_key"])
```

When fetching metadata with a specific key, the returned value will always show the latest entry.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/attach-metadata-to-a-model.md

### Attach Metadata to a Model

ZenML enables logging metadata for models, providing context beyond individual artifact details. This metadata can include evaluation results, deployment information, and customer-specific details, aiding in the management and interpretation of model usage and performance across versions.

#### Logging Metadata for Models

To log metadata, use the `log_metadata` function to attach key-value pairs, including metrics and JSON-serializable values like custom ZenML types (`Uri`, `Path`, `StorageSize`). 

Example of logging metadata for a model:

```python
from typing import Annotated

import pandas as pd
from sklearn.base import ClassifierMixin
from sklearn.ensemble import RandomForestClassifier

from zenml import step, log_metadata, ArtifactConfig, get_step_context


@step
def train_model(dataset: pd.DataFrame) -> Annotated[
    ClassifierMixin, ArtifactConfig(name="sklearn_classifier")
]:
    """Train a model and log model metadata."""
    classifier = RandomForestClassifier().fit(dataset)
    accuracy, precision, recall = ...
    
    log_metadata(
        metadata={
            "evaluation_metrics": {
                "accuracy": accuracy,
                "precision": precision,
                "recall": recall
            }
        },
        infer_model=True,
    )

    return classifier
```

The metadata in this example is linked to the model rather than a specific classifier artifact, which is beneficial for summarizing various pipeline steps and artifacts. 

### Selecting Models with `log_metadata`
ZenML offers flexible options for attaching metadata to model versions:
1. **Using `infer_model`**: Attaches metadata based on the model inferred from the step context.
2. **Model Name and Version Provided**: Attaches metadata to a specific model version when both are provided.
3. **Model Version ID Provided**: Attaches metadata to a model version using a directly provided ID.

### Fetching Logged Metadata
Once attached, metadata can be retrieved for inspection or analysis via the ZenML Client.

```python
from zenml.client import Client

client = Client()
model = client.get_model_version("my_model", "my_version")

print(model.run_metadata["metadata_key"])
```

When fetching metadata with a specific key, the returned value will always reflect the latest entry.



================================================================================

# docs/book/how-to/model-management-metrics/track-metrics-metadata/fetch-metadata-within-steps.md

**Accessing Meta Information in Real-Time**

To fetch metadata during pipeline execution, utilize the `zenml.get_step_context()` function to access the current `StepContext`. This allows you to retrieve information about the running pipeline or step.

```python
from zenml import step, get_step_context


@step
def my_step():
    step_context = get_step_context()
    pipeline_name = step_context.pipeline.name
    run_name = step_context.pipeline_run.name
    step_name = step_context.step_run.name
```

You can use the `StepContext` to determine where the outputs of your current step will be stored and identify the corresponding [Materializer](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md) class for saving them.

```python
from zenml import step, get_step_context


@step
def my_step():
    step_context = get_step_context()
    # Get the URI where the output will be saved.
    uri = step_context.get_output_artifact_uri()

    # Get the materializer that will be used to save the output.
    materializer = step_context.get_output_materializer() 
```

Refer to the [SDK Docs](https://sdkdocs.zenml.io/latest/core_code_docs/core-new/#zenml.steps.step_context.StepContext) for detailed information on the attributes and methods available in the `StepContext`.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/model-versions.md

# Model Versions

Model versions allow tracking of different training iterations, supporting the full ML lifecycle with dashboard and API functionalities. You can associate model versions with stages based on business rules and promote them to production. An interface is available to link versions with non-technical artifacts, such as business data and datasets. Model versions are created automatically during training, but you can explicitly name them using the `version` argument in the `Model` object; otherwise, ZenML generates a version number automatically.

```python
from zenml import Model, step, pipeline

model= Model(
    name="my_model",
    version="1.0.5"
)

# The step configuration will take precedence over the pipeline
@step(model=model)
def svc_trainer(...) -> ...:
    ...

# This configures it for all steps within the pipeline
@pipeline(model=model)
def training_pipeline( ... ):
    # training happens here
```

This documentation outlines how to configure model settings for a specific step or an entire pipeline. If a model version exists, it automatically associates with the pipeline and becomes active, so users should be cautious about whether to create a new pipeline or fetch an existing one. 

To manage model versions effectively, users can utilize name templates in the `version` and/or `name` arguments of the `Model` object. This approach allows for unique, semantically meaningful names for each run, enhancing searchability and readability for the team.

```python
from zenml import Model, step, pipeline

model= Model(
    name="{team}_my_model",
    version="experiment_with_phi_3_{date}_{time}"
)

# The step configuration will take precedence over the pipeline
@step(model=model)
def llm_trainer(...) -> ...:
    ...

# This configures it for all steps within the pipeline
@pipeline(model=model, substitutions={"team": "Team_A"})
def training_pipeline( ... ):
    # training happens here
```

This documentation outlines the configuration of model versions within a pipeline. When executed, the pipeline generates a model version name based on runtime evaluations, such as `experiment_with_phi_3_2024_08_30_12_42_53`. Subsequent runs will retain the same model name and version, as runtime substitutions like `time` and `date` apply to the entire pipeline. A custom substitution, `{team}`, can be set to `Team_A` in the `pipeline` decorator.

Custom placeholders can be defined in various scopes:
- `@pipeline` decorator: applies to all steps in the pipeline.
- `pipeline.with_options`: applies to all steps in the current run.
- `@step` decorator: applies only to the specific step (overrides pipeline settings).
- `step.with_options`: applies only to the specific step run (overrides pipeline settings).

Standard substitutions available in all pipeline steps include:
- `{date}`: current date (e.g., `2024_11_27`)
- `{time}`: current UTC time (e.g., `11_07_09_326492`)

Additionally, model versions can be assigned a specific `stage` (e.g., `production`, `staging`, `development`) for easier retrieval, either via the dashboard or through a CLI command.

```shell
zenml model version update MODEL_NAME --stage=STAGE
```

Stages can be specified as a `version` to retrieve the appropriate model version later.

```python
from zenml import Model, step, pipeline

model= Model(
    name="my_model",
    version="production"
)

# The step configuration will take precedence over the pipeline
@step(model=model)
def svc_trainer(...) -> ...:
    ...

# This configures it for all steps within the pipeline
@pipeline(model=model)
def training_pipeline( ... ):
    # training happens here
```

## Autonumbering of Versions

ZenML automatically assigns version numbers to your models. If no version number is specified or `None` is passed to the `version` argument of the `Model` object, ZenML generates a new version number. For instance, if you have a model version `really_good_version` for `my_model`, you can create a new version easily.

```python
from zenml import Model, step

model = Model(
    name="my_model",
    version="even_better_version"
)

@step(model=model)
def svc_trainer(...) -> ...:
    ...
```

A new model version will be created, and ZenML will track it in the iteration sequence using the `number` property. For example, if `really_good_version` is the 5th version of `my_model`, then `even_better_version` will be the 6th version.

```python
from zenml import Model

earlier_version = Model(
    name="my_model",
    version="really_good_version"
).number # == 5

updated_version = Model(
    name="my_model",
    version="even_better_version"
).number # == 6
```

The documentation features an image of the "ZenML Scarf," which is referenced by a URL. The image has an alt text description and includes a referrer policy of "no-referrer-when-downgrade."



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/README.md

# Use the Model Control Plane

A `Model` in ZenML is an entity that consolidates pipelines, artifacts, metadata, and essential business data, encapsulating your ML product's business logic. It can be viewed as a "project" or "workspace." 

**Key Points:**
- The technical model (model file/files with weights and parameters) is a common artifact associated with a ZenML Model, but other relevant artifacts include training data and production predictions.
- Models are first-class entities in ZenML, accessible through the ZenML API, client, and the ZenML Pro dashboard.
- Each Model captures lineage information and supports version staging, allowing for predictions at specific stages (e.g., `Production`) and decision-making based on business rules.
- The Model Control Plane provides a unified interface to manage models, integrating pipeline logic, artifacts, and the technical model.

For a complete example, refer to the [starter guide](../../../user-guide/starter-guide/track-ml-models.md).



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/associate-a-pipeline-with-a-model.md

# Associate a Pipeline with a Model

To associate a pipeline with a model in ZenML, use the following code:

```python
from zenml import pipeline
from zenml import Model

@pipeline(
    model=Model(
        name="ClassificationModel",  # Unique model name
        tags=["MVP", "Tabular"]      # Tags for filtering
    )
)
def my_pipeline():
    ...
```

This code associates the pipeline with the specified model. If the model already exists, a new version will be created. To attach the pipeline to an existing model version, specify it accordingly.

```python
from zenml import pipeline
from zenml import Model
from zenml.enums import ModelStages

@pipeline(
    model=Model(
        name="ClassificationModel",  # Give your models unique names
        tags=["MVP", "Tabular"],  # Use tags for future filtering
        version=ModelStages.LATEST  # Alternatively use a stage: [STAGING, PRODUCTION]]
    )
)
def my_pipeline():
    ...
```

You can incorporate Model configuration into your configuration files for better organization and management.

```yaml
...

model:
  name: text_classifier
  description: A breast cancer classifier
  tags: ["classifier","sgd"]

...
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and referrer policy. The image is sourced from a URL with a unique identifier.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/connecting-artifacts-via-a-model.md

### Structuring an MLOps Project

In MLOps, artifacts, models, and pipelines are interconnected. For an effective project structure, refer to the [best practices](../../project-setup-and-management/setting-up-a-project-repository/README.md).

An MLOps project typically consists of multiple pipelines, including:

- **Feature Engineering Pipeline**: Prepares raw data for training.
- **Training Pipeline**: Trains models using data from the feature engineering pipeline.
- **Inference Pipeline**: Runs batch predictions on the trained model, often using pre-processed data from the training pipeline.
- **Deployment Pipeline**: Deploys the trained model to a production endpoint.

The structure of these pipelines may vary based on project requirements, with some projects merging pipelines or breaking them into smaller components. Regardless of design, sharing information (artifacts, models, and metadata) between pipelines is essential.

#### Pattern 1: Artifact Exchange via `Client`

For example, in a feature engineering pipeline that generates multiple datasets, only selected datasets should be sent to the training pipeline. The [ZenML Client](../../../reference/python-client.md#client-methods) can facilitate this artifact exchange.

```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    dataset = load_data()
    # This returns artifacts called "iris_training_dataset" and "iris_testing_dataset"
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    # Fetch by name alone - uses the latest version of this artifact
    train_data = client.get_artifact_version(name="iris_training_dataset")
    # For test, we want a particular version
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")

    # We can now send these directly into ZenML steps
    sklearn_classifier = model_trainer(train_data)
    model_evaluator(model, sklearn_classifier)
```

**Important Note:** In the example, `train_data` and `test_data` are not materialized in memory within the `@pipeline` function; they are references to data stored in the artifact store. Logic regarding the data's nature cannot be applied during compilation time in the `@pipeline` function.

## Pattern 2: Artifact Exchange Between Pipelines via a Model

Instead of using artifact IDs or names, it's often preferable to reference the ZenML Model. For instance, the `train_and_promote` pipeline generates multiple model artifacts, which are collected in a ZenML Model. A new `iris_classifier` is created with each run, but it is only promoted to production if it meets a specified accuracy threshold, which can be automated or manually set.

The `do_predictions` pipeline retrieves the latest promoted model for batch inference without needing to know the IDs or names of artifacts from the training pipeline. This allows both pipelines to operate independently while relying on each other's outputs.

In code, once the pipelines are configured to use a specific model, `get_step_context` can be used to access the configured model within a step. For example, in the `do_predictions` pipeline's `predict` step, the `production` model can be fetched easily.

```python
from zenml import step, get_step_context

# IMPORTANT: Cache needs to be disabled to avoid unexpected behavior
@step(enable_cache=False)
def predict(
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    # model name and version are derived from pipeline context
    model = get_step_context().model

    # Fetch the model directly from the model control plane
    model = model.get_model_artifact("trained_model")

    # Make predictions
    predictions = pd.Series(model.predict(data))
    return predictions
```

Caching steps can lead to unexpected results. To mitigate this, you can disable the cache for the specific step or the entire pipeline. Alternatively, you can resolve the artifact at the pipeline level.

```python
from typing_extensions import Annotated
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd
from sklearn.base import ClassifierMixin


@step
def predict(
    model: ClassifierMixin,
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    predictions = pd.Series(model.predict(data))
    return predictions

@pipeline(
    model=Model(
        name="iris_classifier",
        # Using the production stage
        version=ModelStages.PRODUCTION,
    ),
)
def do_predictions():
    # model name and version are derived from pipeline context
    model = get_pipeline_context().model
    inference_data = load_data()
    predict(
        # Here, we load in the `trained_model` from a trainer step
        model=model.get_model_artifact("trained_model"),  
        data=inference_data,
    )


if __name__ == "__main__":
    do_predictions()
```

Both approaches are acceptable; choose based on your preferences.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/linking-model-binaries-data-to-models.md

# Linking Model Binaries/Data to Models

Models and artifacts generated during pipeline runs can be linked in ZenML for lineage tracking and transparency in data and model usage during training, evaluation, and inference. 

## Configuring the Model at a Pipeline Level

The simplest method to link artifacts is by configuring the `model` parameter in the `@pipeline` or `@step` decorator.

```python
from zenml import Model, pipeline

model = Model(
    name="my_model",
    version="1.0.0"
)

@pipeline(model=model)
def my_pipeline():
    ...
```

This documentation outlines the automatic linking of all artifacts from a pipeline run to a specified model configuration. To save intermediate artifacts during processes like epoch-based training, use the `save_artifact` utility function to save data assets as ZenML artifacts. If the Model context is configured in the `@pipeline` or `@step` decorator, the artifacts will be automatically linked, allowing easy access through Model Control Plane features.

```python
from zenml import step, Model
from zenml.artifacts.utils import save_artifact
import pandas as pd
from typing_extensions import Annotated
from zenml.artifacts.artifact_config import ArtifactConfig

@step(model=Model(name="MyModel", version="1.2.42"))
def trainer(
    trn_dataset: pd.DataFrame,
) -> Annotated[
    ClassifierMixin, ArtifactConfig("trained_model")
]:  # this configuration will be applied to `model` output
    """Step running slow training."""
    ...

    for epoch in epochs:
        checkpoint = model.train(epoch)
        # this will save each checkpoint in `training_checkpoint` artifact
        # with distinct version e.g. `1.2.42_0`, `1.2.42_1`, etc.
        # Checkpoint artifacts will be linked to `MyModel` version `1.2.42`
        # implicitly.
        save_artifact(
            data=checkpoint,
            name="training_checkpoint",
            version=f"1.2.42_{epoch}",
        )

    ...

    return model
```

## Link Artifacts Explicitly

To link an artifact to a model outside the step context, use the `link_artifact_to_model` function. You need a ready-to-link artifact and the model's configuration.

```python
from zenml import step, Model, link_artifact_to_model, save_artifact
from zenml.client import Client


@step
def f_() -> None:
    # produce new artifact
    new_artifact = save_artifact(data="Hello, World!", name="manual_artifact")
    # and link it inside a step
    link_artifact_to_model(
        artifact_version_id=new_artifact.id,
        model=Model(name="MyModel", version="0.0.42"),
    )


# use existing artifact
existing_artifact = Client().get_artifact_version(name_id_or_prefix="existing_artifact")
# and link it even outside a step
link_artifact_to_model(
    artifact_version_id=existing_artifact.id,
    model=Model(name="MyModel", version="0.2.42"),
)
```

The documentation includes an image of the "ZenML Scarf." The image is referenced with a specific URL and includes a referrer policy of "no-referrer-when-downgrade."



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/promote-a-model.md

# Promote a Model

## Stages and Promotion
Model promotion stages represent the lifecycle progress of different model versions. A ZenML model version can be promoted through the Dashboard, ZenML CLI, or code, adding metadata to indicate its state. The available stages are:

- **staging**: Prepared for production.
- **production**: Actively running in production.
- **latest**: Represents the most recent version (non-promotable).
- **archived**: No longer relevant, moved from any other stage.

Promotion decisions depend on your specific business logic. 

### Promotion via CLI
CLI promotion is less common but useful for certain use cases, such as CI systems. Use the appropriate CLI subcommand for promotion.

```bash
zenml model version update iris_logistic_regression --stage=...
```

### Promotion via Cloud Dashboard
This feature is not yet available, but will soon allow model version promotion directly from the ZenML Pro dashboard.

### Promotion via Python SDK
This is the primary method for promoting models. Detailed instructions can be found here.

```python
from zenml import Model

MODEL_NAME = "iris_logistic_regression"
from zenml.enums import ModelStages

model = Model(name=MODEL_NAME, version="1.2.3")
model.set_stage(stage=ModelStages.PRODUCTION)

# get latest model and set it as Staging
# (if there is current Staging version it will get Archived)
latest_model = Model(name=MODEL_NAME, version=ModelStages.LATEST)
latest_model.set_stage(stage=ModelStages.STAGING)
```

In a pipeline context, the model is retrieved from the step context, while the method for setting the stage remains consistent.

```python
from zenml import get_step_context, step, pipeline
from zenml.enums import ModelStages

@step
def promote_to_staging():
    model = get_step_context().model
    model.set_stage(ModelStages.STAGING, force=True)

@pipeline(
    ...
)
def train_and_promote_model():
    ...
    promote_to_staging(after=["train_and_evaluate"])
```

## Fetching Model Versions by Stage

To load the appropriate model version, specify the desired stage by passing it as a `version`.

```python
from zenml import Model, step, pipeline

model= Model(
    name="my_model",
    version="production"
)

# The step configuration will take precedence over the pipeline
@step(model=model)
def svc_trainer(...) -> ...:
    ...

# This configures it for all steps within the pipeline
@pipeline(model=model)
def training_pipeline( ... ):
    # training happens here
```

The documentation includes an image of the "ZenML Scarf" with the specified alt text and a referrer policy of "no-referrer-when-downgrade." The image source URL is provided for reference.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/register-a-model.md

# Registering Models

Models can be registered in several ways: explicitly via the CLI or Python SDK, or implicitly during a pipeline run. 

**Note:** ZenML Pro users have access to a dashboard interface for model registration.

## Explicit CLI Registration

To register models using the CLI, use the following command:

```bash
zenml model register iris_logistic_regression --license=... --description=...
```

To view available options for the `zenml model register` command, run `zenml model register --help`. Note that when using the CLI outside a pipeline, only non-runtime arguments can be passed. You can also associate tags with models using the `--tag` option.

### Explicit Dashboard Registration
Users of [ZenML Pro](https://zenml.io/pro) can register models directly through the cloud dashboard.

### Explicit Python SDK Registration
Models can be registered using the Python SDK.

```python
from zenml import Model
from zenml.client import Client

Client().create_model(
    name="iris_logistic_regression",
    license="Copyright (c) ZenML GmbH 2023",
    description="Logistic regression model trained on the Iris dataset.",
    tags=["regression", "sklearn", "iris"],
)
```

## Implicit Registration by ZenML

Implicit model registration occurs during a pipeline run by using a `Model` object in the `model` argument of the `@pipeline` decorator. For instance, a training pipeline can orchestrate model training, storing datasets and the model as links within a new Model version. This integration is configured within a Model Context using `Model`, where the name is required and other fields are optional.

```python
from zenml import pipeline
from zenml import Model

@pipeline(
    enable_cache=False,
    model=Model(
        name="demo",
        license="Apache",
        description="Show case Model Control Plane.",
    ),
)
def train_and_promote_model():
    ...
```

Running the training pipeline generates a new model version while preserving the connection to the artifacts.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/load-a-model-in-code.md

# Loading a ZenML Model in Code

There are several methods to load a ZenML Model in code:

## Load the Active Model in a Pipeline
You can access the active model to retrieve model metadata and associated artifacts, as detailed in the [starter guide](../../../user-guide/starter-guide/track-ml-models.md).

```python
from zenml import step, pipeline, get_step_context, pipeline, Model

@pipeline(model=Model(name="my_model"))
def my_pipeline():
    ...

@step
def my_step():
    # Get model from active step context
    mv = get_step_context().model

    # Get metadata
    print(mv.run_metadata["metadata_key"].value)

    # Directly fetch an artifact that is attached to the model
    output = mv.get_artifact("my_dataset", "my_version")
    output.run_metadata["accuracy"].value
```

## Load Any Model via the Client

You can load models using the `Client` interface.

```python
from zenml import step
from zenml.client import Client
from zenml.enums import ModelStages

@step
def model_evaluator_step()
    ...
    # Get staging model version 
    try:
        staging_zenml_model = Client().get_model_version(
            model_name_or_id="<INSERT_MODEL_NAME>",
            model_version_name_or_number_or_id=ModelStages.STAGING,
        )
    except KeyError:
        staging_zenml_model = None
    ...
```

The documentation features an image of the "ZenML Scarf." The image is referenced with a specific URL and includes a referrer policy of "no-referrer-when-downgrade."



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/load-artifacts-from-model.md

# Loading Artifacts from Model

A common use case for a Model is to transfer artifacts between pipelines. Understanding when and how to load these artifacts is crucial. For instance, consider a two-pipeline project: the first pipeline executes training logic, while the second performs batch inference using the trained model artifacts.

```python
from typing_extensions import Annotated
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd
from sklearn.base import ClassifierMixin


@step
def predict(
    model: ClassifierMixin,
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    predictions = pd.Series(model.predict(data))
    return predictions

@pipeline(
    model=Model(
        name="iris_classifier",
        # Using the production stage
        version=ModelStages.PRODUCTION,
    ),
)
def do_predictions():
    # model name and version are derived from pipeline context
    model = get_pipeline_context().model
    inference_data = load_data()
    predict(
        # Here, we load in the `trained_model` from a trainer step
        model=model.get_model_artifact("trained_model"),  
        data=inference_data,
    )


if __name__ == "__main__":
    do_predictions()
```

In the example, the `get_pipeline_context().model` property is used to obtain the model context for the pipeline. During compilation, this context is not evaluated since the `Production` model version may change before execution. Similarly, `model.get_model_artifact("trained_model")` is stored in the step configuration for delayed materialization, occurring during the step run. Alternatively, the same functionality can be achieved using `Client` methods by modifying the pipeline code.

```python
from zenml.client import Client

@pipeline
def do_predictions():
    # model name and version are directly passed into client method
    model = Client().get_model_version("iris_classifier", ModelStages.PRODUCTION)
    inference_data = load_data()
    predict(
        # Here, we load in the `trained_model` from a trainer step
        model=model.get_model_artifact("trained_model"),  
        data=inference_data,
    )
```

The evaluation of the actual artifact occurs only during the execution of the step.



================================================================================

# docs/book/how-to/model-management-metrics/model-control-plane/delete-a-model.md

**Deleting a Model**

To delete a model or a specific version, you remove all links between the Model entity and its artifacts and pipeline runs, along with all associated metadata.

### Deleting All Versions of a Model

**CLI Instructions:** (Further details would follow in the complete documentation.)

```shell
zenml model delete <MODEL_NAME>
```

The provided text appears to be a fragment of documentation related to a "Python SDK." However, it does not contain any specific technical information or key points to summarize. Please provide the complete documentation text for an accurate summary.

```python
from zenml.client import Client

Client().delete_model(<MODEL_NAME>)
```

## Delete a Specific Version of a Model

### CLI

To delete a specific version of a model, use the appropriate command in the command-line interface (CLI). Ensure that you specify the model identifier and the version number you wish to delete. Confirm the action as it may be irreversible.

```shell
zenml model version delete <MODEL_VERSION_NAME>
```

It appears that the provided text is incomplete. Please provide the full documentation text for the Python SDK, and I will summarize it for you.

```python
from zenml.client import Client

Client().delete_model_version(<MODEL_VERSION_ID>)
```

The provided text appears to be a fragment of documentation that includes a closing tag for tabs and a figure element with an image. It does not contain any specific technical information or key points to summarize. If you have additional content or context to include, please provide it for a more comprehensive summary.



================================================================================

# docs/book/how-to/contribute-to-zenml/README.md

# Contribute to ZenML

Thank you for considering contributing to ZenML! We welcome contributions such as new features, documentation improvements, integrations, and bug reports. For detailed guidelines on contributing, including best practices and conventions, please refer to the [ZenML contribution guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).



================================================================================

# docs/book/how-to/contribute-to-zenml/implement-a-custom-integration.md

# Creating an External Integration and Contributing to ZenML

ZenML aims to bring order to the MLOps landscape by offering numerous integrations with popular tools. If you want to contribute your integration to ZenML's main codebase, follow this guide.

### Step 1: Plan Your Integration
Identify the categories your integration fits into by referring to the categories defined by ZenML. A single integration may belong to multiple categories, such as cloud integrations (AWS/GCP/Azure) that include container registries and artifact stores.

### Step 2: Create Stack Component Flavors
Each selected category corresponds to a stack component type. Develop individual stack component flavors according to the detailed instructions provided for each type. Before packaging your components, you can test them as a custom flavor. For example, if developing a custom orchestrator, register your flavor class using the appropriate method.

```shell
zenml orchestrator flavor register flavors.my_flavor.MyOrchestratorFlavor
```

{% hint style="warning" %} ZenML resolves the flavor class starting from the path where you initialized ZenML using `zenml init`. It is recommended to initialize ZenML at the root of your repository to avoid relying on the default mechanism, which uses the current working directory if no initialized repository is found in parent directories. Following this best practice ensures proper functionality. After initialization, the new flavor will appear in the list of available flavors. {% endhint %}

```shell
zenml orchestrator flavor list
```

For detailed information on component extensibility, refer to the documentation [here](../../component-guide/README.md) or explore existing integrations like the [MLflow experiment tracker](../../component-guide/experiment-trackers/mlflow.md).

### Step 3: Create an Integration Class

After implementing your custom flavors, proceed to package them into your integration and the base ZenML package. Follow this checklist:

**1. Clone Repo**  
Clone the [main ZenML repository](https://github.com/zenml-io/zenml) and set up your local development environment by following the [contributing guide](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).

**2. Create the Integration Directory**  
All integrations are located in [`src/zenml/integrations/`](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations) within their own sub-folder. Create a new folder named after your integration.

```
/src/zenml/integrations/                        <- ZenML integration directory
    <example-integration>                       <- Root integration directory
        |
        ├── artifact-stores                     <- Separated directory for  
        |      ├── __init_.py                      every type
        |      └── <example-artifact-store>     <- Implementation class for the  
        |                                          artifact store flavor
        ├── flavors 
        |      ├── __init_.py 
        |      └── <example-artifact-store-flavor>  <- Config class and flavor
        |
        └── __init_.py                          <- Integration class 
```

To define the name of your integration, add the integration name in the `zenml/integrations/constants.py` file.

```python
EXAMPLE_INTEGRATION = "<name-of-integration>"
```

The name of the integration will be displayed during execution.

```shell
 zenml integration install <name-of-integration>
```

**4. Create the integration class \_\_init\_\_.py**  
In `src/zenml/integrations/<YOUR_INTEGRATION>/init__.py`, create a subclass of the `Integration` class. Set the attributes `NAME` and `REQUIREMENTS`, and override the `flavors` class method.

```python
from zenml.integrations.constants import <EXAMPLE_INTEGRATION>
from zenml.integrations.integration import Integration
from zenml.stack import Flavor

# This is the flavor that will be used when registering this stack component
#  `zenml <type-of-stack-component> register ... -f example-orchestrator-flavor`
EXAMPLE_ORCHESTRATOR_FLAVOR = <"example-orchestrator-flavor">

# Create a Subclass of the Integration Class
class ExampleIntegration(Integration):
    """Definition of Example Integration for ZenML."""

    NAME = <EXAMPLE_INTEGRATION>
    REQUIREMENTS = ["<INSERT PYTHON REQUIREMENTS HERE>"]

    @classmethod
    def flavors(cls) -> List[Type[Flavor]]:
        """Declare the stack component flavors for the <EXAMPLE> integration."""
        from zenml.integrations.<example_flavor> import <ExampleFlavor>
        
        return [<ExampleFlavor>]
        
ExampleIntegration.check_installation() # this checks if the requirements are installed
```

To integrate with ZenML, refer to the [MLflow Integration](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/mlflow/__init__.py) for guidance. 

**5. Import in the right places**: Ensure the integration is imported in [`src/zenml/integrations/__init__.py`](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/__init__.py).

### Step 4: Create a PR
You can now [create a PR](https://github.com/zenml-io/zenml/compare) for ZenML. Wait for core maintainers to review your contribution. Thank you for your support!



================================================================================

# docs/book/how-to/data-artifact-management/README.md

### Data and Artifact Management

This section addresses the management of data and artifacts in ZenML, detailing essential practices and tools for effective handling.



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/unmaterialized-artifacts.md

### Unmaterialized Artifacts in ZenML

In ZenML, a pipeline is structured around data, with each step defined by its inputs and outputs, which interact with the artifact store. **Materializers** manage how artifacts are stored and retrieved, handling serialization and deserialization. When artifacts are passed between steps, their materializers dictate the process.

However, there are scenarios where you may want to **skip materialization** and use a reference to the artifact instead. This can be useful for obtaining the exact storage path of an artifact. 

**Warning:** Skipping materialization may lead to issues for downstream tasks that depend on materialized artifacts. It should only be done when absolutely necessary.

### How to Skip Materialization

To utilize an unmaterialized artifact, use the `zenml.materializers.UnmaterializedArtifact` class, which includes a `uri` property that indicates the artifact's unique storage path. Specify `UnmaterializedArtifact` as the type in the step to implement this.

```python
from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import step

@step
def my_step(my_artifact: UnmaterializedArtifact):  # rather than pd.DataFrame
    pass
```

## Code Example

This section demonstrates the use of unmaterialized artifacts in a pipeline. The defined pipeline will include the following steps:

```shell
s1 -> s3 
s2 -> s4
```

`s1` and `s2` generate identical artifacts. In contrast, `s3` uses materialized artifacts, while `s4` utilizes unmaterialized artifacts. `s4` can directly access `dict_.uri` and `list_.uri` paths instead of their materialized versions.

```python
from typing_extensions import Annotated  # or `from typing import Annotated on Python 3.9+
from typing import Dict, List, Tuple

from zenml.artifacts.unmaterialized_artifact import UnmaterializedArtifact
from zenml import pipeline, step


@step
def step_1() -> Tuple[
    Annotated[Dict[str, str], "dict_"],
    Annotated[List[str], "list_"],
]:
    return {"some": "data"}, []


@step
def step_2() -> Tuple[
    Annotated[Dict[str, str], "dict_"],
    Annotated[List[str], "list_"],
]:
    return {"some": "data"}, []


@step
def step_3(dict_: Dict, list_: List) -> None:
    assert isinstance(dict_, dict)
    assert isinstance(list_, list)


@step
def step_4(
        dict_: UnmaterializedArtifact,
        list_: UnmaterializedArtifact,
) -> None:
    print(dict_.uri)
    print(list_.uri)


@pipeline
def example_pipeline():
    step_3(*step_1())
    step_4(*step_2())


example_pipeline()
```

An example of using an `UnmaterializedArtifact` is provided when triggering a [pipeline from another](../../pipeline-development/trigger-pipelines/use-templates-python.md#advanced-usage-run-a-template-from-another-pipeline).



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/README.md

It seems that the text you provided is incomplete or missing. Please provide the full documentation text you would like summarized, and I'll be happy to help!



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/registering-existing-data.md

### Register Existing Data as a ZenML Artifact

This documentation explains how to register external data as a ZenML artifact for future use. Many Machine Learning frameworks generate data during model training, which can be registered directly in ZenML without needing to materialize them.

#### Register Existing Folder as a ZenML Artifact

If the external data is in a folder, you can register the entire folder as a ZenML Artifact for use in subsequent steps or other pipelines.

```python
import os
from uuid import uuid4
from pathlib import Path

from zenml.client import Client
from zenml import register_artifact

prefix = Client().active_stack.artifact_store.path
test_file_name = "test_file.txt"
preexisting_folder = os.path.join(prefix,f"my_test_folder_{uuid4()}")
preexisting_file = os.path.join(preexisting_folder,test_file_name)

# produce a folder with a file inside artifact store boundaries
os.mkdir(preexisting_folder)
with open(preexisting_file,"w") as f:
    f.write("test")

# create artifact from the preexisting folder
register_artifact(
    folder_or_file_uri=preexisting_folder,
    name="my_folder_artifact"
)

# consume artifact as a folder
temp_artifact_folder_path = Client().get_artifact_version(name_id_or_prefix="my_folder_artifact").load()
assert isinstance(temp_artifact_folder_path, Path)
assert os.path.isdir(temp_artifact_folder_path)
with open(os.path.join(temp_artifact_folder_path,test_file_name),"r") as f:
    assert f.read() == "test"
```

The artifact generated from preexisting data will be of `pathlib.Path` type, pointing to a temporary location in the executing environment. It can be used like a standard local `Path` in functions such as `from_pretrained` or `open`. 

To register an externally created file as a ZenML Artifact, follow the appropriate steps to utilize it in future steps or pipelines.

```python
import os
from uuid import uuid4
from pathlib import Path

from zenml.client import Client
from zenml import register_artifact

prefix = Client().active_stack.artifact_store.path
test_file_name = "test_file.txt"
preexisting_folder = os.path.join(prefix,f"my_test_folder_{uuid4()}")
preexisting_file = os.path.join(preexisting_folder,test_file_name)

# produce a file inside artifact store boundaries
os.mkdir(preexisting_folder)
with open(preexisting_file,"w") as f:
    f.write("test")

# create artifact from the preexisting file
register_artifact(
    folder_or_file_uri=preexisting_file,
    name="my_file_artifact"
)

# consume artifact as a file
temp_artifact_file_path = Client().get_artifact_version(name_id_or_prefix="my_file_artifact").load()
assert isinstance(temp_artifact_file_path, Path)
assert not os.path.isdir(temp_artifact_file_path)
with open(temp_artifact_file_path,"r") as f:
    assert f.read() == "test"
```

## Register All Checkpoints of a PyTorch Lightning Training Run

This documentation outlines how to fit a model using PyTorch Lightning and store the checkpoints in a remote location. It provides a step-by-step guide to ensure that all checkpoints are registered during the training process.

```python
import os
from zenml.client import Client
from zenml import register_artifact
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from uuid import uuid4

# Define where the model data should be saved
# use active ArtifactStore
prefix = Client().active_stack.artifact_store.path
# keep data separable for future runs with uuid4 folder
default_root_dir = os.path.join(prefix, uuid4().hex)

# Define the model and fit it
model = ...
trainer = Trainer(
    default_root_dir=default_root_dir,
    callbacks=[
        ModelCheckpoint(
            every_n_epochs=1, save_top_k=-1, filename="checkpoint-{epoch:02d}"
        )
    ],
)
try:
    trainer.fit(model)
finally:
    # We now link those checkpoints in ZenML as an artifact
    # This will create a new artifact version
    register_artifact(default_root_dir, name="all_my_model_checkpoints")
```

Artifacts created externally can be managed like any other ZenML artifacts. To version checkpoints from a PyTorch Lightning training run, extend the `ModelCheckpoint` callback. For instance, modify the `on_train_epoch_end` method to register each checkpoint as a separate Artifact Version in ZenML. Note that to retain all checkpoint files, set `save_top_k=-1`; otherwise, older checkpoints will be deleted, rendering registered artifact versions unusable.

```python
import os

from zenml.client import Client
from zenml import register_artifact
from zenml import get_step_context
from zenml.exceptions import StepContextError
from zenml.logger import get_logger

from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import Trainer, LightningModule

logger = get_logger(__name__)


class ZenMLModelCheckpoint(ModelCheckpoint):
    """A ModelCheckpoint that can be used with ZenML.

    Used to store model checkpoints in ZenML as artifacts.
    Supports `default_root_dir` to pass into `Trainer`.
    """

    def __init__(
        self,
        artifact_name: str,
        every_n_epochs: int = 1,
        save_top_k: int = -1,
        *args,
        **kwargs,
    ):
        # get all needed info for the ZenML logic
        try:
            zenml_model = get_step_context().model
        except StepContextError:
            raise RuntimeError(
                "`ZenMLModelCheckpoint` can only be called from within a step."
            )
        model_name = zenml_model.name
        filename = model_name + "_{epoch:02d}"
        self.filename_format = model_name + "_epoch={epoch:02d}.ckpt"
        self.artifact_name = artifact_name

        prefix = Client().active_stack.artifact_store.path
        self.default_root_dir = os.path.join(prefix, str(zenml_model.version))
        logger.info(f"Model data will be stored in {self.default_root_dir}")

        super().__init__(
            every_n_epochs=every_n_epochs,
            save_top_k=save_top_k,
            filename=filename,
            *args,
            **kwargs,
        )

    def on_train_epoch_end(
        self, trainer: "Trainer", pl_module: "LightningModule"
    ) -> None:
        super().on_train_epoch_end(trainer, pl_module)

        # We now link those checkpoints in ZenML as an artifact
        # This will create a new artifact version
        register_artifact(
            os.path.join(
                self.dirpath, self.filename_format.format(epoch=trainer.current_epoch)
            ),
            self.artifact_name,
        )
```

This documentation presents an advanced example of a PyTorch Lightning training pipeline that incorporates artifact linkage for checkpoint management via an extended Callback. The example demonstrates how to effectively manage checkpoints during the training process.

```python
import os
from typing import Annotated
from pathlib import Path

import numpy as np
from zenml.client import Client
from zenml import register_artifact
from zenml import step, pipeline, get_step_context, Model
from zenml.exceptions import StepContextError
from zenml.logger import get_logger

from torch.utils.data import DataLoader
from torch.nn import ReLU, Linear, Sequential
from torch.nn.functional import mse_loss
from torch.optim import Adam
from torch import rand
from torchvision.datasets import MNIST
from torchvision.transforms import ToTensor
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning import Trainer, LightningModule

from zenml.new.pipelines.pipeline_context import get_pipeline_context

logger = get_logger(__name__)


class ZenMLModelCheckpoint(ModelCheckpoint):
    """A ModelCheckpoint that can be used with ZenML.

    Used to store model checkpoints in ZenML as artifacts.
    Supports `default_root_dir` to pass into `Trainer`.
    """

    def __init__(
        self,
        artifact_name: str,
        every_n_epochs: int = 1,
        save_top_k: int = -1,
        *args,
        **kwargs,
    ):
        # get all needed info for the ZenML logic
        try:
            zenml_model = get_step_context().model
        except StepContextError:
            raise RuntimeError(
                "`ZenMLModelCheckpoint` can only be called from within a step."
            )
        model_name = zenml_model.name
        filename = model_name + "_{epoch:02d}"
        self.filename_format = model_name + "_epoch={epoch:02d}.ckpt"
        self.artifact_name = artifact_name

        prefix = Client().active_stack.artifact_store.path
        self.default_root_dir = os.path.join(prefix, str(zenml_model.version))
        logger.info(f"Model data will be stored in {self.default_root_dir}")

        super().__init__(
            every_n_epochs=every_n_epochs,
            save_top_k=save_top_k,
            filename=filename,
            *args,
            **kwargs,
        )

    def on_train_epoch_end(
        self, trainer: "Trainer", pl_module: "LightningModule"
    ) -> None:
        super().on_train_epoch_end(trainer, pl_module)

        # We now link those checkpoints in ZenML as an artifact
        # This will create a new artifact version
        register_artifact(
            os.path.join(
                self.dirpath, self.filename_format.format(epoch=trainer.current_epoch)
            ),
            self.artifact_name,
        )


# define the LightningModule toy model
class LitAutoEncoder(LightningModule):
    def __init__(self, encoder, decoder):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder

    def training_step(self, batch, batch_idx):
        # training_step defines the train loop.
        # it is independent of forward
        x, _ = batch
        x = x.view(x.size(0), -1)
        z = self.encoder(x)
        x_hat = self.decoder(z)
        loss = mse_loss(x_hat, x)
        # Logging to TensorBoard (if installed) by default
        self.log("train_loss", loss)
        return loss

    def configure_optimizers(self):
        optimizer = Adam(self.parameters(), lr=1e-3)
        return optimizer


@step
def get_data() -> DataLoader:
    """Get the training data."""
    dataset = MNIST(os.getcwd(), download=True, transform=ToTensor())
    train_loader = DataLoader(dataset)

    return train_loader


@step
def get_model() -> LightningModule:
    """Get the model to train."""
    encoder = Sequential(Linear(28 * 28, 64), ReLU(), Linear(64, 3))
    decoder = Sequential(Linear(3, 64), ReLU(), Linear(64, 28 * 28))
    model = LitAutoEncoder(encoder, decoder)
    return model


@step
def train_model(
    model: LightningModule,
    train_loader: DataLoader,
    epochs: int = 1,
    artifact_name: str = "my_model_ckpts",
) -> None:
    """Run the training loop."""
    # configure checkpointing
    chkpt_cb = ZenMLModelCheckpoint(artifact_name=artifact_name)

    trainer = Trainer(
        # pass default_root_dir from ZenML checkpoint to
        # ensure that the data is accessible for the artifact
        # store
        default_root_dir=chkpt_cb.default_root_dir,
        limit_train_batches=100,
        max_epochs=epochs,
        callbacks=[chkpt_cb],
    )
    trainer.fit(model, train_loader)


@step
def predict(
    checkpoint_file: Path,
) -> Annotated[np.ndarray, "predictions"]:
    # load the model from the checkpoint
    encoder = Sequential(Linear(28 * 28, 64), ReLU(), Linear(64, 3))
    decoder = Sequential(Linear(3, 64), ReLU(), Linear(64, 28 * 28))
    autoencoder = LitAutoEncoder.load_from_checkpoint(
        checkpoint_file, encoder=encoder, decoder=decoder
    )
    encoder = autoencoder.encoder
    encoder.eval()

    # predict on fake batch
    fake_image_batch = rand(4, 28 * 28, device=autoencoder.device)
    embeddings = encoder(fake_image_batch)
    if embeddings.device.type == "cpu":
        return embeddings.detach().numpy()
    else:
        return embeddings.detach().cpu().numpy()


@pipeline(model=Model(name="LightningDemo"))
def train_pipeline(artifact_name: str = "my_model_ckpts"):
    train_loader = get_data()
    model = get_model()
    train_model(model, train_loader, 10, artifact_name)
    # pass in the latest checkpoint for predictions
    predict(
        get_pipeline_context().model.get_artifact(artifact_name), after=["train_model"]
    )


if __name__ == "__main__":
    train_pipeline()
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and a referrer policy. The image is hosted at a specific URL. No additional technical information or key points are provided beyond this description.



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/datasets.md

### Custom Dataset Classes and Complex Data Flows in ZenML

As machine learning projects become more complex, managing various data sources and intricate data flows is essential. This chapter discusses using custom Dataset classes and Materializers in ZenML to address these challenges effectively. For scaling data processing for larger datasets, see [scaling strategies for big data](manage-big-data.md).

#### Introduction to Custom Dataset Classes

Custom Dataset classes in ZenML encapsulate data loading, processing, and saving logic for different data sources. They are particularly beneficial when:

1. Working with multiple data sources (e.g., CSV files, databases, cloud storage)
2. Handling complex data structures requiring special processing
3. Implementing custom data processing or transformation logic

#### Implementing Dataset Classes for Different Data Sources

This section will demonstrate creating a base Dataset class and implementing it for CSV and BigQuery data sources.

```python
from abc import ABC, abstractmethod
import pandas as pd
from google.cloud import bigquery
from typing import Optional

class Dataset(ABC):
    @abstractmethod
    def read_data(self) -> pd.DataFrame:
        pass

class CSVDataset(Dataset):
    def __init__(self, data_path: str, df: Optional[pd.DataFrame] = None):
        self.data_path = data_path
        self.df = df

    def read_data(self) -> pd.DataFrame:
        if self.df is None:
            self.df = pd.read_csv(self.data_path)
        return self.df

class BigQueryDataset(Dataset):
    def __init__(
        self,
        table_id: str,
        df: Optional[pd.DataFrame] = None,
        project: Optional[str] = None,
    ):
        self.table_id = table_id
        self.project = project
        self.df = df
        self.client = bigquery.Client(project=self.project)

    def read_data(self) -> pd.DataFrame:
        query = f"SELECT * FROM `{self.table_id}`"
        self.df = self.client.query(query).to_dataframe()
        return self.df

    def write_data(self) -> None:
        job_config = bigquery.LoadJobConfig(write_disposition="WRITE_TRUNCATE")
        job = self.client.load_table_from_dataframe(self.df, self.table_id, job_config=job_config)
        job.result()
```

## Creating Custom Materializers

Materializers in ZenML manage the serialization and deserialization of artifacts. Custom Materializers are crucial for handling custom Dataset classes.

```python
from typing import Type
from zenml.materializers import BaseMaterializer
from zenml.io import fileio
from zenml.enums import ArtifactType
import json
import os
import tempfile
import pandas as pd


class CSVDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (CSVDataset,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[CSVDataset]) -> CSVDataset:
        # Create a temporary file to store the CSV data
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            # Copy the CSV file from the artifact store to the temporary location
            with fileio.open(os.path.join(self.uri, "data.csv"), "rb") as source_file:
                temp_file.write(source_file.read())
            
            temp_path = temp_file.name

        # Create and return the CSVDataset
        dataset = CSVDataset(temp_path)
        dataset.read_data()
        return dataset

    def save(self, dataset: CSVDataset) -> None:
        # Ensure we have data to save
        df = dataset.read_data()

        # Save the dataframe to a temporary CSV file
        with tempfile.NamedTemporaryFile(delete=False, suffix='.csv') as temp_file:
            df.to_csv(temp_file.name, index=False)
            temp_path = temp_file.name

        # Copy the temporary file to the artifact store
        with open(temp_path, "rb") as source_file:
            with fileio.open(os.path.join(self.uri, "data.csv"), "wb") as target_file:
                target_file.write(source_file.read())

        # Clean up the temporary file
        os.remove(temp_path)

class BigQueryDatasetMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (BigQueryDataset,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[BigQueryDataset]) -> BigQueryDataset:
        with fileio.open(os.path.join(self.uri, "metadata.json"), "r") as f:
            metadata = json.load(f)
        dataset = BigQueryDataset(
            table_id=metadata["table_id"],
            project=metadata["project"],
        )
        dataset.read_data()
        return dataset

    def save(self, bq_dataset: BigQueryDataset) -> None:
        metadata = {
            "table_id": bq_dataset.table_id,
            "project": bq_dataset.project,
        }
        with fileio.open(os.path.join(self.uri, "metadata.json"), "w") as f:
            json.dump(metadata, f)
        if bq_dataset.df is not None:
            bq_dataset.write_data()
```

## Managing Complexity in Pipelines with Multiple Data Sources

When handling multiple data sources, it's essential to design flexible pipelines. For instance, a pipeline can be structured to accommodate both CSV and BigQuery datasets effectively.

```python
from zenml import step, pipeline
from typing_extensions import Annotated

@step(output_materializer=CSVDatasetMaterializer)
def extract_data_local(data_path: str = "data/raw_data.csv") -> CSVDataset:
    return CSVDataset(data_path)

@step(output_materializer=BigQueryDatasetMaterializer)
def extract_data_remote(table_id: str) -> BigQueryDataset:
    return BigQueryDataset(table_id)

@step
def transform(dataset: Dataset) -> pd.DataFrame
    df = dataset.read_data()
    # Transform data
    transformed_df = df.copy()  # Apply transformations here
    return transformed_df

@pipeline
def etl_pipeline(mode: str = "develop"):
    if mode == "develop":
        raw_data = extract_data_local()
    else:
        raw_data = extract_data_remote(table_id="project.dataset.raw_table")

    transformed_data = transform(raw_data)
```

## Best Practices for Designing Flexible and Maintainable Pipelines

When working with custom Dataset classes in ZenML pipelines, follow these best practices for flexibility and maintainability:

1. **Use a Common Base Class**: Implement the `Dataset` base class for consistent handling of various data sources in your pipeline steps, allowing for easy data source swaps without altering the pipeline structure.

```python
@step
def process_data(dataset: Dataset) -> pd.DataFrame:
    data = dataset.read_data()
    # Process data...
    return processed_data
```

**Create Specialized Steps for Dataset Loading**: Implement distinct steps for loading various datasets, ensuring that the underlying processes remain standardized.

```python
@step
def load_csv_data() -> CSVDataset:
    # CSV-specific processing
    pass

@step
def load_bigquery_data() -> BigQueryDataset:
    # BigQuery-specific processing
    pass

@step
def common_processing_step(dataset: Dataset) -> pd.DataFrame:
    # Loads the base dataset, does not know concrete type
    pass
```

**Implement Flexible Pipelines**: Design pipelines to adapt to various data sources and processing needs using configuration parameters or conditional logic to control execution steps.

```python
@pipeline
def flexible_data_pipeline(data_source: str):
    if data_source == "csv":
        dataset = load_csv_data()
    elif data_source == "bigquery":
        dataset = load_bigquery_data()
    
    final_result = common_processing_step(dataset)
    return final_result
```

4. **Modular Step Design**: Develop steps for specific tasks (e.g., data loading, transformation, analysis) that are compatible with various dataset types, enhancing code reuse and maintenance.

```python
@step
def transform_data(dataset: Dataset) -> pd.DataFrame:
    data = dataset.read_data()
    # Common transformation logic
    return transformed_data

@step
def analyze_data(data: pd.DataFrame) -> pd.DataFrame:
    # Common analysis logic
    return analysis_result
```

To create efficient ZenML pipelines that manage complex data flows and multiple sources, adopt practices that ensure adaptability to changing requirements. Utilize custom Dataset classes to maintain consistency and flexibility in your machine learning workflows. For scaling data processing with larger datasets, consult the section on [scaling strategies for big data](manage-big-data.md).



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/manage-big-data.md

### Scaling Strategies for Big Data in ZenML

As machine learning projects expand, managing large datasets can strain existing data processing pipelines. This section outlines strategies for scaling ZenML pipelines to accommodate larger datasets. For creating custom Dataset classes and managing complex data flows, refer to [custom dataset classes](datasets.md).

#### Dataset Size Thresholds
Understanding dataset size thresholds is crucial for selecting appropriate processing strategies:
1. **Small datasets (up to a few GB)**: Handled in-memory with standard pandas operations.
2. **Medium datasets (up to tens of GB)**: Require chunking or out-of-core processing.
3. **Large datasets (hundreds of GB or more)**: Necessitate distributed processing frameworks.

#### Strategies for Datasets up to a Few Gigabytes
For datasets fitting in memory but becoming unwieldy, consider the following optimizations:
1. **Use efficient data formats**: Transition from CSV to more efficient formats like Parquet.

```python
import pyarrow.parquet as pq

class ParquetDataset(Dataset):
    def __init__(self, data_path: str):
        self.data_path = data_path

    def read_data(self) -> pd.DataFrame:
        return pq.read_table(self.data_path).to_pandas()

    def write_data(self, df: pd.DataFrame):
        table = pa.Table.from_pandas(df)
        pq.write_table(table, self.data_path)
```

**Implement Basic Data Sampling**: Integrate sampling methods into your Dataset classes.

```python
import random

class SampleableDataset(Dataset):
    def sample_data(self, fraction: float = 0.1) -> pd.DataFrame:
        df = self.read_data()
        return df.sample(frac=fraction)

@step
def analyze_sample(dataset: SampleableDataset) -> Dict[str, float]:
    sample = dataset.sample_data(fraction=0.1)
    # Perform analysis on the sample
    return {"mean": sample["value"].mean(), "std": sample["value"].std()}
```

**Optimize pandas operations**: Utilize efficient pandas and numpy functions to reduce memory consumption.

```python
@step
def optimize_processing(df: pd.DataFrame) -> pd.DataFrame:
    # Use inplace operations where possible
    df['new_column'] = df['column1'] + df['column2']
    
    # Use numpy operations for speed
    df['mean_normalized'] = df['value'] - np.mean(df['value'])
    
    return df
```

## Handling Datasets up to Tens of Gigabytes

When data exceeds memory capacity, use the following strategies:

### Chunking for CSV Datasets
Implement chunking in your Dataset classes to process large files in manageable pieces.

```python
class ChunkedCSVDataset(Dataset):
    def __init__(self, data_path: str, chunk_size: int = 10000):
        self.data_path = data_path
        self.chunk_size = chunk_size

    def read_data(self):
        for chunk in pd.read_csv(self.data_path, chunksize=self.chunk_size):
            yield chunk

@step
def process_chunked_csv(dataset: ChunkedCSVDataset) -> pd.DataFrame:
    processed_chunks = []
    for chunk in dataset.read_data():
        processed_chunks.append(process_chunk(chunk))
    return pd.concat(processed_chunks)

def process_chunk(chunk: pd.DataFrame) -> pd.DataFrame:
    # Process each chunk here
    return chunk
```

### Leveraging Data Warehouses for Large Datasets

Utilize data warehouses such as [Google BigQuery](https://cloud.google.com/bigquery) for their distributed processing capabilities, which are essential for handling large datasets efficiently.

```python
@step
def process_big_query_data(dataset: BigQueryDataset) -> BigQueryDataset:
    client = bigquery.Client()
    query = f"""
    SELECT 
        column1, 
        AVG(column2) as avg_column2
    FROM 
        `{dataset.table_id}`
    GROUP BY 
        column1
    """
    result_table_id = f"{dataset.project}.{dataset.dataset}.processed_data"
    job_config = bigquery.QueryJobConfig(destination=result_table_id)
    query_job = client.query(query, job_config=job_config)
    query_job.result()  # Wait for the job to complete
    
    return BigQueryDataset(table_id=result_table_id)
```

## Approaches for Very Large Datasets: Using Distributed Computing Frameworks in ZenML

For handling very large datasets (hundreds of gigabytes or more), distributed computing frameworks like Apache Spark or Ray can be utilized. Although ZenML lacks built-in integrations for these frameworks, they can be directly incorporated into your pipeline steps.

### Using Apache Spark in ZenML

To integrate Spark into a ZenML pipeline, initialize and use Spark within your step function.

```python
from pyspark.sql import SparkSession
from zenml import step, pipeline

@step
def process_with_spark(input_data: str) -> None:
    # Initialize Spark
    spark = SparkSession.builder.appName("ZenMLSparkStep").getOrCreate()
    
    # Read data
    df = spark.read.format("csv").option("header", "true").load(input_data)
    
    # Process data using Spark
    result = df.groupBy("column1").agg({"column2": "mean"})
    
    # Write results
    result.write.csv("output_path", header=True, mode="overwrite")
    
    # Stop the Spark session
    spark.stop()

@pipeline
def spark_pipeline(input_data: str):
    process_with_spark(input_data)

# Run the pipeline
spark_pipeline(input_data="path/to/your/data.csv")
```

To use Ray in a ZenML pipeline, ensure Spark is installed and its dependencies are available. You can initialize and use Ray directly within your pipeline step.

```python
import ray
from zenml import step, pipeline

@step
def process_with_ray(input_data: str) -> None:
    ray.init()

    @ray.remote
    def process_partition(partition):
        # Process a partition of the data
        return processed_partition

    # Load and split your data
    data = load_data(input_data)
    partitions = split_data(data)

    # Distribute processing across Ray cluster
    results = ray.get([process_partition.remote(part) for part in partitions])

    # Combine and save results
    combined_results = combine_results(results)
    save_results(combined_results, "output_path")

    ray.shutdown()

@pipeline
def ray_pipeline(input_data: str):
    process_with_ray(input_data)

# Run the pipeline
ray_pipeline(input_data="path/to/your/data.csv")
```

To use Dask in ZenML, ensure that Ray is installed in your environment along with its necessary dependencies. Dask is a flexible library for parallel computing in Python that can be integrated into ZenML pipelines to manage large datasets and parallelize computations.

```python
from zenml import step, pipeline
import dask.dataframe as dd
from zenml.materializers.base_materializer import BaseMaterializer
import os

class DaskDataFrameMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (dd.DataFrame,)
    ASSOCIATED_ARTIFACT_TYPE = "dask_dataframe"

    def load(self, data_type):
        return dd.read_parquet(os.path.join(self.uri, "data.parquet"))

    def save(self, data):
        data.to_parquet(os.path.join(self.uri, "data.parquet"))

@step(output_materializers=DaskDataFrameMaterializer)
def create_dask_dataframe():
    df = dd.from_pandas(pd.DataFrame({'A': range(1000), 'B': range(1000, 2000)}), npartitions=4)
    return df

@step
def process_dask_dataframe(df: dd.DataFrame) -> dd.DataFrame:
    result = df.map_partitions(lambda x: x ** 2)
    return result

@step
def compute_result(df: dd.DataFrame) -> pd.DataFrame:
    return df.compute()

@pipeline
def dask_pipeline():
    df = create_dask_dataframe()
    processed = process_dask_dataframe(df)
    result = compute_result(processed)

# Run the pipeline
dask_pipeline()

```

This documentation describes the creation of a custom `DaskDataFrameMaterializer` for processing Dask DataFrames within a pipeline. The pipeline utilizes Dask's distributed computing to create and compute the final Dask DataFrame result. Additionally, it mentions integrating [Numba](https://numba.pydata.org/), a just-in-time compiler for Python, to enhance the performance of numerical Python code in a ZenML pipeline.

```python
from zenml import step, pipeline
import numpy as np
from numba import jit
import os

@jit(nopython=True)
def numba_function(x):
    return x * x + 2 * x - 1

@step
def load_data() -> np.ndarray:
    return np.arange(1000000)

@step
def apply_numba_function(data: np.ndarray) -> np.ndarray:
    return numba_function(data)

@pipeline
def numba_pipeline():
    data = load_data()
    result = apply_numba_function(data)

# Run the pipeline
numba_pipeline()
```

The pipeline creates a Numba-accelerated function, applies it to a large NumPy array, and returns the result. 

### Important Considerations
1. **Environment Setup**: Ensure Spark or Ray frameworks are installed in your execution environment.
2. **Resource Management**: Coordinate resource allocation between these frameworks and ZenML's orchestration.
3. **Error Handling**: Implement error handling and cleanup for Spark sessions or Ray runtime.
4. **Data I/O**: Use intermediate storage (e.g., cloud storage) for large datasets during data transfer.
5. **Scaling**: Ensure your infrastructure supports the scale of computation required.

Incorporating Spark or Ray into ZenML steps allows for efficient distributed processing of large datasets while utilizing ZenML's pipeline management and versioning.

### Choosing the Right Scaling Strategy
1. **Dataset Size**: Start with simpler strategies for smaller datasets.
2. **Processing Complexity**: Use BigQuery for simple aggregations; Spark or Ray for complex ML preprocessing.
3. **Infrastructure and Resources**: Ensure sufficient compute resources for distributed processing.
4. **Update Frequency**: Consider data change frequency and reprocessing needs.
5. **Team Expertise**: Choose familiar technologies for your team.

Start simple and scale as needed. ZenML's architecture supports evolving data processing strategies. For custom Dataset classes and complex data flows, refer to [custom dataset classes](datasets.md).



================================================================================

# docs/book/how-to/data-artifact-management/complex-usecases/passing-artifacts-between-pipelines.md

### Structuring an MLOps Project

An MLOps project typically consists of multiple pipelines, including:

- **Feature Engineering Pipeline**: Prepares raw data for training.
- **Training Pipeline**: Trains models using data from the feature engineering pipeline.
- **Inference Pipeline**: Runs batch predictions on the trained model, often utilizing pre-processing from the training pipeline.
- **Deployment Pipeline**: Deploys the trained model to a production endpoint.

The structure of these pipelines can vary based on project requirements; they may be merged into a single pipeline or divided into smaller components. Regardless of the structure, transferring artifacts, models, and metadata between pipelines is essential.

#### Artifact Exchange Pattern

**Pattern 1: Artifact Exchange via Client**  
In a scenario with a feature engineering pipeline producing various datasets, only selected datasets are sent to the training pipeline for model training. The [ZenML Client](../../../reference/python-client.md#client-methods) can facilitate this exchange effectively. 

![Artifact Exchange](../../.gitbook/assets/artifact_exchange.png)  
*Figure: A simple artifact exchange between two pipelines*

```python
from zenml import pipeline
from zenml.client import Client

@pipeline
def feature_engineering_pipeline():
    dataset = load_data()
    # This returns artifacts called "iris_training_dataset" and "iris_testing_dataset"
    train_data, test_data = prepare_data()

@pipeline
def training_pipeline():
    client = Client()
    # Fetch by name alone - uses the latest version of this artifact
    train_data = client.get_artifact_version(name="iris_training_dataset")
    # For test, we want a particular version
    test_data = client.get_artifact_version(name="iris_testing_dataset", version="raw_2023")

    # We can now send these directly into ZenML steps
    sklearn_classifier = model_trainer(train_data)
    model_evaluator(model, sklearn_classifier)
```

### Summary

In the example provided, `train_data` and `test_data` in the `@pipeline` function are references to data stored in the artifact store and are not materialized in memory. This means that logic regarding the data's nature cannot be applied during compilation time.

#### Pattern 2: Artifact Exchange via Model

Instead of using artifact IDs or names, it is often preferable to reference the ZenML Model. For instance, the `train_and_promote` pipeline generates multiple model artifacts, collected in a ZenML Model, and promotes the `iris_classifier` to production based on an accuracy threshold. Promotion can be automated or manual. The `do_predictions` pipeline then uses the latest promoted model for batch inference without needing to know the artifact IDs or names, allowing both pipelines to operate independently while relying on each other's outputs.

To implement this, once pipelines are configured to use a specific model, `get_step_context` can be used to access the configured model within a step. For example, in the `do_predictions` pipeline's `predict` step, the production model can be fetched directly.

```python
from zenml import step, get_step_context

# IMPORTANT: Cache needs to be disabled to avoid unexpected behavior
@step(enable_cache=False)
def predict(
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    # model name and version are derived from pipeline context
    model = get_step_context().model

    # Fetch the model directly from the model control plane
    model = model.get_model_artifact("trained_model")

    # Make predictions
    predictions = pd.Series(model.predict(data))
    return predictions
```

Caching steps can lead to unexpected results. To mitigate this, you can disable the cache for the specific step or the entire pipeline. Alternatively, you can resolve the artifact at the pipeline level.

```python
from typing_extensions import Annotated
from zenml import get_pipeline_context, pipeline, Model
from zenml.enums import ModelStages
import pandas as pd
from sklearn.base import ClassifierMixin


@step
def predict(
    model: ClassifierMixin,
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    predictions = pd.Series(model.predict(data))
    return predictions

@pipeline(
    model=Model(
        name="iris_classifier",
        # Using the production stage
        version=ModelStages.PRODUCTION,
    ),
)
def do_predictions():
    # model name and version are derived from pipeline context
    model = get_pipeline_context().model
    inference_data = load_data()
    predict(
        # Here, we load in the `trained_model` from a trainer step
        model=model.get_model_artifact("trained_model"),  
        data=inference_data,
    )


if __name__ == "__main__":
    do_predictions()
```

Both approaches are valid; choose based on your preferences.



================================================================================

# docs/book/how-to/data-artifact-management/visualize-artifacts/types-of-visualizations.md

### Types of Visualizations in ZenML

ZenML automatically saves visualizations for various data types, accessible via the ZenML dashboard or in Jupyter notebooks using the `artifact.visualize()` method. 

**Default Visualizations Include:**
- Statistical representations of Pandas DataFrames as PNG images.
- Drift detection reports from Evidently, Great Expectations, and whylogs.
- A Hugging Face datasets viewer embedded as an HTML iframe.

Visualizations enhance data insights and can be easily integrated into workflows.



================================================================================

# docs/book/how-to/data-artifact-management/visualize-artifacts/README.md

# Visualize Artifacts in ZenML

ZenML allows easy configuration for displaying data visualizations in the dashboard. Users can associate visualizations with data and artifacts seamlessly. 

![ZenML Artifact Visualizations](../../../.gitbook/assets/artifact_visualization_dashboard.png) 

For more information, refer to the ZenML documentation.



================================================================================

# docs/book/how-to/data-artifact-management/visualize-artifacts/creating-custom-visualizations.md

### Creating Custom Visualizations

You can associate a custom visualization with an artifact in ZenML if it is one of the supported types: 

- **HTML:** Embedded HTML visualizations (e.g., data validation reports)
- **Image:** Visualizations of image data (e.g., Pillow images)
- **CSV:** Tables (e.g., pandas DataFrame `.describe()` output)
- **Markdown:** Markdown strings or pages
- **JSON:** JSON strings or objects

#### Methods to Add Custom Visualizations:

1. **Direct Casting:** If you have HTML, Markdown, CSV, or JSON data in your steps, cast them to a special class to visualize with minimal code.
2. **Custom Materializer:** Define type-specific visualization logic to automatically extract visualizations for artifacts of a certain data type.
3. **Custom Return Type Class:** Create a custom return type class with a corresponding materializer and return this type from your steps.

#### Visualization via Special Return Types:

For existing HTML, Markdown, CSV, or JSON data as strings, cast and return them using:

- `zenml.types.HTMLString` for HTML strings (e.g., `"<h1>Header</h1>Some text"`)
- `zenml.types.MarkdownString` for Markdown strings (e.g., `"# Header\nSome text"`)
- `zenml.types.CSVString` for CSV strings (e.g., `"a,b,c\n1,2,3"`)
- `zenml.types.JSONString` for JSON strings (e.g., `{"key": "value"}`) 

This allows for straightforward visualization integration in your ZenML workflow.

```python
from zenml.types import CSVString

@step
def my_step() -> CSVString:
    some_csv = "a,b,c\n1,2,3"
    return CSVString(some_csv)
```

This documentation outlines how to create visualizations in the ZenML dashboard, specifically through materializers. 

### Key Points:

- To automatically extract visualizations for specific data types, override the `save_visualizations()` method in the relevant materializer. Refer to the [materializer documentation](../../data-artifact-management/handle-data-artifacts/handle-custom-data-types.md#optional-how-to-visualize-the-artifact) for details on creating custom materializers. A code example for visualizing Hugging Face datasets is available on [GitHub](https://github.com/zenml-io/zenml/blob/main/src/zenml/integrations/huggingface/materializers/huggingface_datasets_materializer.py).

### Steps to Create Custom Visualizations:

1. **Create a Custom Class**: This class will hold the data for visualization.
2. **Build a Custom Materializer**: Implement the visualization logic in the `save_visualizations()` method.
3. **Return the Custom Class**: Use this class in any ZenML steps.

### Example: Facets Data Skew Visualization

The [Facets Integration](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-facets) demonstrates visualizing data skew between multiple Pandas DataFrames. The custom class used is [FacetsComparison](https://sdkdocs.zenml.io/0.42.0/integration_code_docs/integrations-facets/#zenml.integrations.facets.models.FacetsComparison), which holds the necessary data for visualization. 

![CSV Visualization Example](../../.gitbook/assets/artifact_visualization_csv.png)  
![Facets Visualization](../../.gitbook/assets/facets-visualization.png)

```python
class FacetsComparison(BaseModel):
    datasets: List[Dict[str, Union[str, pd.DataFrame]]]
```

**2. Materializer** The [FacetsMaterializer](https://sdkdocs.zenml.io/0.42.0/integration_code_docs/integrations-facets/#zenml.integrations.facets.materializers.facets_materializer.FacetsMaterializer) is a custom materializer designed specifically for a custom class, incorporating the necessary visualization logic.

```python
class FacetsMaterializer(BaseMaterializer):

    ASSOCIATED_TYPES = (FacetsComparison,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS

    def save_visualizations(
        self, data: FacetsComparison
    ) -> Dict[str, VisualizationType]:
        html = ...  # Create a visualization for the custom type 
        visualization_path = os.path.join(self.uri, VISUALIZATION_FILENAME)
        with fileio.open(visualization_path, "w") as f:
            f.write(html)
        return {visualization_path: VisualizationType.HTML}
```

**3. Step** The `facets` integration involves three steps to create `FacetsComparison`s for various input sets. For example, the `facets_visualization_step` accepts two DataFrames and constructs a `FacetsComparison` object from them.

```python
@step
def facets_visualization_step(
    reference: pd.DataFrame, comparison: pd.DataFrame
) -> FacetsComparison:  # Return the custom type from your step
    return FacetsComparison(
        datasets=[
            {"name": "reference", "table": reference},
            {"name": "comparison", "table": comparison},
        ]
    )
```

When the `facets_visualization_step` is added to your pipeline, the following occurs:

1. A `FacetsComparison` is created and returned.
2. Upon completion, ZenML locates the `FacetsMaterializer`, which then executes the `save_visualizations()` method to generate and save the visualization as an HTML file in the artifact store.
3. The visualization HTML file can be accessed and displayed by clicking on the artifact in the run DAG on your dashboard.



================================================================================

# docs/book/how-to/data-artifact-management/visualize-artifacts/disabling-visualizations.md

To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level.

```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

The provided documentation text includes an image of "ZenML Scarf" but lacks any specific technical information or key points to summarize. Please provide additional text or details for a meaningful summary.



================================================================================

# docs/book/how-to/data-artifact-management/visualize-artifacts/visualizations-in-dashboard.md

### Displaying Visualizations in the Dashboard

To display visualizations on the ZenML dashboard, the following steps are necessary:

#### Configuring a Service Connector
- Visualizations are stored in the artifact store. Users must configure a service connector to allow the ZenML server to access this store. Detailed guidance is available in the [service connector documentation](../../infrastructure-deployment/auth-management/README.md) and for specific configurations, refer to the [AWS S3 artifact store documentation](../../../component-guide/artifact-stores/s3.md).
- **Note:** When using the default/local artifact store with a deployed ZenML, the server cannot access local files, resulting in visualizations not being displayed. Use a service connector with a remote artifact store to view visualizations.

#### Configuring Artifact Stores
- If visualizations from a pipeline run are missing, it may indicate that the ZenML server lacks the necessary dependencies or permissions for the artifact store. Refer to the [custom artifact store documentation](../../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores) for further details.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/README.md

Step outputs in ZenML are stored in the artifact store, facilitating caching, lineage, and auditability. Using type annotations for outputs enhances transparency, aids in data transfer between steps, and allows ZenML to serialize and deserialize data (termed 'materialize').

```python
@step
def load_data(parameter: int) -> Dict[str, Any]:

    # do something with the parameter here

    training_data = [[1, 2], [3, 4], [5, 6]]
    labels = [0, 1, 0]
    return {'features': training_data, 'labels': labels}

@step
def train_model(data: Dict[str, Any]) -> None:
    total_features = sum(map(sum, data['features']))
    total_labels = sum(data['labels'])
    
    # Train some model here
    
    print(f"Trained model using {len(data['features'])} data points. "
          f"Feature sum is {total_features}, label sum is {total_labels}")


@pipeline  
def simple_ml_pipeline(parameter: int):
    dataset = load_data(parameter=parameter)  # Get the output 
    train_model(dataset)  # Pipe the previous step output into the downstream step
```

The code defines two steps in a ZenML pipeline: `load_data` and `train_model`. The `load_data` step takes an integer parameter and returns a dictionary with training data and labels. The `train_model` step receives this dictionary, extracts features and labels, and trains a model. The pipeline, `simple_ml_pipeline`, connects these steps, allowing data to flow from `load_data` to `train_model`.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/artifacts-naming.md

### How Artifact Naming Works in ZenML

In ZenML pipelines, reusing steps with different inputs can lead to multiple artifacts, making it difficult to track outputs due to the default naming convention. ZenML allows for both static and dynamic naming of output artifacts to address this issue.

Key Points:
- ZenML uses type annotations in function definitions to determine artifact names.
- Artifacts with the same name are saved with incremented version numbers.
- Naming options include:
  - Dynamic generation at runtime
  - Support for string templates (standard and custom placeholders)
  - Compatibility with single and multiple output scenarios
- Static names are defined directly as string literals.

```python
@step
def static_single() -> Annotated[str, "static_output_name"]:
    return "null"
```

### Dynamic Naming

Dynamic names can be generated using string templates with standard placeholders. ZenML automatically replaces the following placeholders:

- `{date}`: resolves to the current date (e.g., `2024_11_18`)
- `{time}`: resolves to the current time (e.g., `11_07_09_326492`)

```python
@step
def dynamic_single_string() -> Annotated[str, "name_{date}_{time}"]:
    return "null"
```

### String Templates Using Custom Placeholders

Utilize placeholders in ZenML that can be replaced during a step execution by using the `substitutions` parameter.

```python
@step(substitutions={"custom_placeholder": "some_substitute"})
def dynamic_single_string() -> Annotated[str, "name_{custom_placeholder}_{time}"]:
    return "null"
```

You can use `with_options` to dynamically redefine the placeholder.

```python
@step
def extract_data(source: str) -> Annotated[str, "{stage}_dataset"]:
    ...
    return "my data"

@pipeline
def extraction_pipeline():
    extract_data.with_options(substitutions={"stage": "train"})(source="s3://train")
    extract_data.with_options(substitutions={"stage": "test"})(source="s3://test")
```

The custom placeholders, such as `stage`, can be set in various ways:

- `@pipeline` decorator: Applies to all steps in the pipeline.
- `pipeline.with_options` function: Applies to all steps in the pipeline run.
- `@step` decorator: Applies to the specific step (overrides pipeline settings).
- `step.with_options` function: Applies to the specific step run (overrides pipeline settings).

Standard substitutions available in all steps include:
- `{date}`: Current date (e.g., `2024_11_27`).
- `{time}`: Current time in UTC format (e.g., `11_07_09_326492`).

For returning multiple artifacts from a ZenML step, you can combine the naming options mentioned above.

```python
@step
def mixed_tuple() -> Tuple[
    Annotated[str, "static_output_name"],
    Annotated[str, "name_{date}_{time}"],
]:
    return "static_namer", "str_namer"
```

## Naming in Cached Runs
When a ZenML step with caching enabled uses the cache, the names of the output artifacts (both static and dynamic) will remain unchanged from the original run.

```python
from typing_extensions import Annotated
from typing import Tuple

from zenml import step, pipeline
from zenml.models import PipelineRunResponse


@step(substitutions={"custom_placeholder": "resolution"})
def demo() -> Tuple[
    Annotated[int, "name_{date}_{time}"],
    Annotated[int, "name_{custom_placeholder}"],
]:
    return 42, 43


@pipeline
def my_pipeline():
    demo()


if __name__ == "__main__":
    run_without_cache: PipelineRunResponse = my_pipeline.with_options(
        enable_cache=False
    )()
    run_with_cache: PipelineRunResponse = my_pipeline.with_options(enable_cache=True)()

    assert set(run_without_cache.steps["demo"].outputs.keys()) == set(
        run_with_cache.steps["demo"].outputs.keys()
    )
    print(list(run_without_cache.steps["demo"].outputs.keys()))
```

The two runs will generate output similar to the example provided below:

```
Initiating a new run for the pipeline: my_pipeline.
Caching is disabled by default for my_pipeline.
Using user: default
Using stack: default
  orchestrator: default
  artifact_store: default
You can visualize your pipeline runs in the ZenML Dashboard. In order to try it locally, please run zenml login --local.
Step demo has started.
Step demo has finished in 0.038s.
Pipeline run has finished in 0.064s.
Initiating a new run for the pipeline: my_pipeline.
Using user: default
Using stack: default
  orchestrator: default
  artifact_store: default
You can visualize your pipeline runs in the ZenML Dashboard. In order to try it locally, please run zenml login --local.
Using cached version of step demo.
All steps of the pipeline run were cached.
['name_2024_11_21_14_27_33_750134', 'name_resolution']
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and referrer policy. The image source is provided via a URL.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/load-artifacts-into-memory.md

# Loading Artifacts into Memory

ZenML pipeline steps typically consume artifacts produced by other steps, but external data may also need to be incorporated. For artifacts from non-ZenML sources, use [ExternalArtifact](../../../user-guide/starter-guide/manage-artifacts.md#consuming-external-artifacts-within-a-pipeline). When exchanging data between ZenML pipelines, late materialization is essential. This allows for the passing of not-yet-existing artifacts and their metadata as step inputs during the compilation phase.

### Use Cases for Exchanging Artifacts
1. Grouping data products using ZenML Models.
2. Utilizing [ZenML Client](../../../reference/python-client.md#client-methods) to integrate components.

**Recommendation:** Use models to group and access artifacts across pipelines. For details on loading artifacts from a ZenML Model, refer to [here](../../model-management-metrics/model-control-plane/load-artifacts-from-model.md).

## Using Client Methods to Exchange Artifacts
If not using the Model Control Plane, data can still be exchanged between pipelines through late materialization. Adjust the `do_predictions` pipeline code accordingly.

```python
from typing import Annotated
from zenml import step, pipeline
from zenml.client import Client
import pandas as pd
from sklearn.base import ClassifierMixin


@step
def predict(
    model1: ClassifierMixin,
    model2: ClassifierMixin,
    model1_metric: float,
    model2_metric: float,
    data: pd.DataFrame,
) -> Annotated[pd.Series, "predictions"]:
    # compare which model performs better on the fly
    if model1_metric < model2_metric:
        predictions = pd.Series(model1.predict(data))
    else:
        predictions = pd.Series(model2.predict(data))
    return predictions

@step
def load_data() -> pd.DataFrame:
    # load inference data
    ...

@pipeline
def do_predictions():
    # get specific artifact version
    model_42 = Client().get_artifact_version("trained_model", version="42")
    metric_42 = model_42.run_metadata["MSE"].value

    # get latest artifact version
    model_latest = Client().get_artifact_version("trained_model")
    metric_latest = model_latest.run_metadata["MSE"].value

    inference_data = load_data()
    predict(
        model1=model_42,
        model2=model_latest,
        model1_metric=metric_42,
        model2_metric=metric_latest,
        data=inference_data,
    )

if __name__ == "__main__":
    do_predictions()
```

The `predict` step logic has been enhanced to include a metric comparison using the MSE metric, ensuring predictions are made with the best model. A new `load_data` step has been introduced to load inference data. Calls like `Client().get_artifact_version("trained_model", version="42")` and `model_latest.run_metadata["MSE"].value` evaluate the actual objects only during step execution, not at pipeline compilation. This approach guarantees that the latest version is current at execution time, rather than at compilation.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/artifact-versioning.md

### How ZenML Stores Data

ZenML integrates data versioning and lineage tracking into its core functionality. Each pipeline run generates automatically tracked artifacts, which can be viewed and interacted with through a dashboard. This facilitates insights, streamlines experimentation, and ensures reproducibility in machine learning workflows.

#### Artifact Creation and Caching

During a pipeline run, ZenML checks for changes in inputs, outputs, parameters, or configurations. Each step generates a new directory in the artifact store. If a step is new or modified, a unique directory structure is created with a unique ID. If unchanged, ZenML may cache the step, saving time and computational resources. This allows users to focus on experimenting without rerunning unchanged parts. ZenML provides traceability of artifacts, enabling users to understand the sequence of executions leading to their creation, ensuring reproducibility and reliability, especially in team environments.

For more on managing artifact names, versions, and properties, refer to the [artifact versioning and configuration documentation](../../../user-guide/starter-guide/manage-artifacts.md).

#### Saving and Loading Artifacts with Materializers

Materializers are essential for artifact management, handling serialization and deserialization of artifacts in the artifact store. Each materializer saves data in unique directories. ZenML offers built-in materializers for common data types and uses `cloudpickle` for objects without a default materializer. Custom materializers can be created by extending the `BaseMaterializer` class.

**Warning:** The built-in `CloudpickleMaterializer` can handle any object but is not production-ready due to compatibility issues with different Python versions and potential security risks from malicious file uploads. For robust serialization, consider building a custom materializer.

ZenML uses materializers to save and load artifacts via its `fileio` system, simplifying interactions with various data formats and enabling artifact caching and lineage tracking. An example of a default materializer, the `numpy` materializer, can be found [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy_materializer.py).



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/tagging.md

### Organizing Data with Tags in ZenML

Tags are used in ZenML to organize and categorize machine learning artifacts and models, improving workflow and discoverability. This guide explains how to assign tags to artifacts and models.

#### Assigning Tags to Artifacts

To tag artifact versions from repeatedly executed steps or pipelines, use the `tags` property of `ArtifactConfig` to assign multiple tags to created artifacts. 

![Tags are visible in the ZenML Dashboard](../../../.gitbook/assets/tags-in-dashboard.png)

```python
from zenml import step, ArtifactConfig

@step
def training_data_loader() -> (
    Annotated[pd.DataFrame, ArtifactConfig(tags=["sklearn", "pre-training"])]
):
    ...
```

The `zenml artifacts` CLI allows you to add tags to artifacts.

```shell
# Tag the artifact
zenml artifacts update iris_dataset -t sklearn

# Tag the artifact version
zenml artifacts versions update iris_dataset raw_2023 -t sklearn
```

This documentation explains how to assign tags to artifacts and models in ZenML for better organization. Users can tag artifacts with keywords like `sklearn` and `pre-training`, which can be used for filtering. ZenML Pro users can also tag artifacts directly in the cloud dashboard.

For models, tags can be added as key-value pairs when creating a model version using the `Model` object. Note that if a model is implicitly created during a pipeline run, it will not inherit tags from the `Model` class. Users can manage model tags using the SDK or the ZenML Pro UI.

```python
from zenml.models import Model

# Define tags to be added to the model version
tags = ["experiment", "v1", "classification-task"]

# Create a model version with tags
model = Model(
    name="iris_classifier",
    version="1.0.0",
    tags=tags,
)

# Use this tagged model in your steps and pipelines as needed
@pipeline(model=model)
def my_pipeline(...):
    ...
```

You can assign tags during the creation or updating of models using the Python SDK.

```python
from zenml.models import Model
from zenml.client import Client

# Create or register a new model with tags
Client().create_model(
    name="iris_logistic_regression",
    tags=["classification", "iris-dataset"],
)

# Create or register a new model version also with tags
Client().create_model_version(
    model_name_or_id="iris_logistic_regression",
    name="2",
    tags=["version-1", "experiment-42"],
)
```

To add tags to existing models and their versions with the ZenML CLI, use the following commands:

```shell
# Tag an existing model
zenml model update iris_logistic_regression --tag "classification"

# Tag a specific model version
zenml model version update iris_logistic_regression 2 --tag "experiment3"
```

The provided text includes an image of "ZenML Scarf" but lacks any additional technical information or context. Therefore, there are no key points or details to summarize. Please provide more content for a comprehensive summary.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/get-arbitrary-artifacts-in-a-step.md

Artifacts do not have to originate solely from direct upstream steps. According to the metadata guide, metadata can be retrieved using the client, enabling the fetching of artifacts from other upstream steps or entirely different pipelines within a step.

```python
from zenml.client import Client
from zenml import step

@step
def my_step():
    client = Client()
    # Directly fetch an artifact
    output = client.get_artifact_version("my_dataset", "my_version")
    output.run_metadata["accuracy"].value
```

You can access previously created artifacts stored in the artifact store, which is useful for utilizing artifacts from other pipelines or non-upstream steps. For more information, refer to the section on [Managing artifacts](../../../user-guide/starter-guide/manage-artifacts.md) to learn about the `ExternalArtifact` type and artifact transfer between steps.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/handle-custom-data-types.md

### Summary: Using Materializers for Custom Data Types in ZenML Pipelines

ZenML pipelines are structured around data flow, where the inputs and outputs of steps determine their connections and execution order. Each step operates independently, reading from and writing to the artifact store, facilitated by **materializers**. Materializers manage how artifacts are serialized for storage and deserialized for use in subsequent steps.

#### Built-In Materializers
ZenML provides several built-in materializers for common data types, which operate automatically without user intervention:

| Materializer | Handled Data Types | Storage Format |
|--------------|---------------------|----------------|
| BuiltInMaterializer | bool, float, int, str, None | .json |
| BytesInMaterializer | bytes | .txt |
| BuiltInContainerMaterializer | dict, list, set, tuple | Directory |
| NumpyMaterializer | np.ndarray | .npy |
| PandasMaterializer | pd.DataFrame, pd.Series | .csv (or .gzip if parquet is installed) |
| PydanticMaterializer | pydantic.BaseModel | .json |
| ServiceMaterializer | zenml.services.service.BaseService | .json |
| StructuredStringMaterializer | zenml.types.CSVString, zenml.types.HTMLString, zenml.types.MarkdownString | .csv / .html / .md |

**Note:** The `CloudpickleMaterializer` can handle any object but is not production-ready due to compatibility issues across Python versions and potential security risks.

#### Integration Materializers
ZenML also supports integration-specific materializers, activated by installing the respective integrations:

| Integration | Materializer | Handled Data Types | Storage Format |
|-------------|--------------|---------------------|----------------|
| bentoml | BentoMaterializer | bentoml.Bento | .bento |
| deepchecks | DeepchecksResultMaterializer | deepchecks.CheckResult, deepchecks.SuiteResult | .json |
| evidently | EvidentlyProfileMaterializer | evidently.Profile | .json |
| great_expectations | GreatExpectationsMaterializer | great_expectations.ExpectationSuite, great_expectations.CheckpointResult | .json |
| huggingface | HFDatasetMaterializer | datasets.Dataset, datasets.DatasetDict | Directory |
| ... | ... | ... | ... |

**Important:** For Docker-based orchestrators, specify the required integration in the `DockerSettings` to ensure materializers are available in the container.

#### Custom Materializers
To use a custom materializer, ZenML detects imported materializers and registers them for the corresponding data types. However, it is recommended to explicitly define which materializer to use for clarity and best practices.

```python
class MyObj:
    ...

class MyMaterializer(BaseMaterializer):
    """Materializer to read data to and from MyObj."""

    ASSOCIATED_TYPES = (MyObj)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    # Read below to learn how to implement this materializer

# You can define it at the decorator level
@step(output_materializers=MyMaterializer)
def my_first_step() -> MyObj:
    return 1

# No need to explicitly specify materializer here:
# it is coupled with Artifact Version generated by
# `my_first_step` already.
def my_second_step(a: MyObj):
    print(a)

# or you can use the `configure()` method of the step. E.g.:
my_first_step.configure(output_materializers=MyMaterializer)
```

To specify multiple outputs, provide a dictionary in the format `{<OUTPUT_NAME>: <MATERIALIZER_CLASS>}` to the decorator or the `.configure(...)` method.

```python
class MyObj1:
    ...

class MyObj2:
    ...

class MyMaterializer1(BaseMaterializer):
    """Materializer to read data to and from MyObj1."""

    ASSOCIATED_TYPES = (MyObj1)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

class MyMaterializer2(BaseMaterializer):
    """Materializer to read data to and from MyObj2."""

    ASSOCIATED_TYPES = (MyObj2)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

# This is where we connect the objects to the materializer
@step(output_materializers={"1": MyMaterializer1, "2": MyMaterializer2})
def my_first_step() -> Tuple[Annotated[MyObj1, "1"], Annotated[MyObj2, "2"]]:
    return 1
```

You can configure which materializer to use for the output of each step in YAML config files, as detailed in the [configuration docs](../../pipeline-development/use-configuration-files/what-can-be-configured.md). Custom materializers can be defined for handling loading and saving outputs of your steps.

```yaml
...
steps:
  <STEP_NAME>:
    ...
    outputs:
      <OUTPUT_NAME>:
        materializer_source: run.MyMaterializer
```

For information on customizing step output names, refer to [this page](../../../user-guide/starter-guide/manage-artifacts.md). 

### Defining a Global Materializer
To configure ZenML to use a custom materializer globally for all pipelines, you can override the default built-in materializers. This is useful for specific data types, such as creating a custom materializer for `pandas.DataFrame` to manage its reading and writing differently. You can achieve this by utilizing ZenML's internal materializer registry to modify its behavior.

```python
# Entrypoint file where we run pipelines (i.e. run.py)

from zenml.materializers.materializer_registry import materializer_registry

# Create a new materializer
class FastPandasMaterializer(BaseMaterializer):
    ...

# Register the FastPandasMaterializer for pandas dataframes objects
materializer_registry.register_and_overwrite_type(key=pd.DataFrame, type_=FastPandasMaterializer)

# Run your pipelines: They will now all use the custom materializer
```

### Developing a Custom Materializer

To implement a custom materializer, you need to understand the base implementation. The abstract class `BaseMaterializer` defines the interface for all materializers.

```python
class BaseMaterializer(metaclass=BaseMaterializerMeta):
    """Base Materializer to realize artifact data."""

    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.BASE
    ASSOCIATED_TYPES = ()

    def __init__(
        self, uri: str, artifact_store: Optional[BaseArtifactStore] = None
    ):
        """Initializes a materializer with the given URI.

        Args:
            uri: The URI where the artifact data will be stored.
            artifact_store: The artifact store used to store this artifact.
        """
        self.uri = uri
        self._artifact_store = artifact_store

    def load(self, data_type: Type[Any]) -> Any:
        """Write logic here to load the data of an artifact.

        Args:
            data_type: The type of data that the artifact should be loaded as.

        Returns:
            The data of the artifact.
        """
        # read from a location inside self.uri
        # 
        # Example:
        # data_path = os.path.join(self.uri, "abc.json")
        # with self.artifact_store.open(filepath, "r") as fid:
        #     return json.load(fid)
        ...

    def save(self, data: Any) -> None:
        """Write logic here to save the data of an artifact.

        Args:
            data: The data of the artifact to save.
        """
        # write `data` into self.uri
        # 
        # Example:
        # data_path = os.path.join(self.uri, "abc.json")
        # with self.artifact_store.open(filepath, "w") as fid:
        #     json.dump(data,fid)
        ...

    def save_visualizations(self, data: Any) -> Dict[str, VisualizationType]:
        """Save visualizations of the given data.

        Args:
            data: The data of the artifact to visualize.

        Returns:
            A dictionary of visualization URIs and their types.
        """
        # Optionally, define some visualizations for your artifact
        #
        # E.g.:
        # visualization_uri = os.path.join(self.uri, "visualization.html")
        # with self.artifact_store.open(visualization_uri, "w") as f:
        #     f.write("<html><body>data</body></html>")

        # visualization_uri_2 = os.path.join(self.uri, "visualization.png")
        # data.save_as_png(visualization_uri_2)

        # return {
        #     visualization_uri: ArtifactVisualizationType.HTML,
        #     visualization_uri_2: ArtifactVisualizationType.IMAGE
        # }
        ...

    def extract_metadata(self, data: Any) -> Dict[str, "MetadataType"]:
        """Extract metadata from the given data.

        This metadata will be tracked and displayed alongside the artifact.

        Args:
            data: The data to extract metadata from.

        Returns:
            A dictionary of metadata.
        """
        # Optionally, extract some metadata from `data` for ZenML to store.
        #
        # Example:
        # return {
        #     "some_attribute_i_want_to_track": self.some_attribute,
        #     "pi": 3.14,
        # }
        ...
```

### Summary of Materializer Documentation

- **Handled Data Types**: Each materializer has an `ASSOCIATED_TYPES` attribute listing the data types it can handle. ZenML uses this to select the appropriate materializer based on the output type of a step (e.g., `pd.DataFrame`).

- **Generated Artifact Type**: The `ASSOCIATED_ARTIFACT_TYPE` attribute defines the `zenml.enums.ArtifactType` for the data, typically `ArtifactType.DATA` or `ArtifactType.MODEL`. If uncertain, use `ArtifactType.DATA`, as it primarily serves as a tag in ZenML visualizations.

- **Artifact Storage Location**: The `uri` attribute indicates the storage location of the artifact in the artifact store, created automatically by ZenML during pipeline execution.

- **Artifact Storage and Retrieval**: The `load()` and `save()` methods manage artifact serialization and deserialization:
  - `load()`: Reads and deserializes data from the artifact store.
  - `save()`: Serializes and saves data to the artifact store.
  Override these methods based on your serialization needs (e.g., using `torch.save()` and `torch.load()` for custom PyTorch classes).

- **Temporary Directory**: Use the `get_temporary_directory(...)` helper method in the materializer class for creating temporary directories, ensuring proper cleanup.

```python
with self.get_temporary_directory(...) as temp_dir:
    ...
```

### Visualization of Artifacts
You can override the `save_visualizations()` method to save visualizations for artifacts in your materializer, which will appear in the dashboard. Supported visualization formats include CSV, HTML, image, and Markdown. To create visualizations:
1. Compute visualizations based on the artifact.
2. Save visualizations to paths in `self.uri`.
3. Return a dictionary mapping visualization paths to types.

For an example, refer to the [NumpyMaterializer](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/numpy_materializer.py) implementation.

### Metadata Extraction
Override the `extract_metadata()` method to track custom metadata for artifacts. Return a dictionary of values, ensuring they are built-in types or special types defined in [zenml.metadata.metadata_types](https://github.com/zenml-io/zenml/blob/main/src/zenml/metadata/metadata_types.py). By default, this method extracts only the artifact's storage size, but you can customize it to track additional properties, as seen in the `NumpyMaterializer`.

To disable artifact visualization or metadata extraction, set `enable_artifact_visualization` or `enable_artifact_metadata` to `False` at the pipeline or step level.

### Skipping Materialization
Refer to the documentation on [skipping materialization](../complex-usecases/unmaterialized-artifacts.md) for more details.

### Custom Artifact Stores
When creating a custom artifact store, the default materializers may not work if `self.artifact_store.open` is incompatible. In such cases, modify the materializer to copy the artifact to a local path before accessing it. For example, the custom [PandasMaterializer](https://github.com/zenml-io/zenml/blob/main/src/zenml/materializers/pandas_materializer.py) implementation demonstrates this approach. Note that copying artifacts may introduce performance bottlenecks.

```python
import os
from typing import Any, ClassVar, Dict, Optional, Tuple, Type, Union

import pandas as pd

from zenml.artifact_stores.base_artifact_store import BaseArtifactStore
from zenml.enums import ArtifactType, VisualizationType
from zenml.logger import get_logger
from zenml.materializers.base_materializer import BaseMaterializer
from zenml.metadata.metadata_types import DType, MetadataType

logger = get_logger(__name__)

PARQUET_FILENAME = "df.parquet.gzip"
COMPRESSION_TYPE = "gzip"

CSV_FILENAME = "df.csv"


class PandasMaterializer(BaseMaterializer):
    """Materializer to read data to and from pandas."""

    ASSOCIATED_TYPES: ClassVar[Tuple[Type[Any], ...]] = (
        pd.DataFrame,
        pd.Series,
    )
    ASSOCIATED_ARTIFACT_TYPE: ClassVar[ArtifactType] = ArtifactType.DATA

    def __init__(
        self, uri: str, artifact_store: Optional[BaseArtifactStore] = None
    ):
        """Define `self.data_path`.

        Args:
            uri: The URI where the artifact data is stored.
            artifact_store: The artifact store where the artifact data is stored.
        """
        super().__init__(uri, artifact_store)
        try:
            import pyarrow  # type: ignore # noqa

            self.pyarrow_exists = True
        except ImportError:
            self.pyarrow_exists = False
            logger.warning(
                "By default, the `PandasMaterializer` stores data as a "
                "`.csv` file. If you want to store data more efficiently, "
                "you can install `pyarrow` by running "
                "'`pip install pyarrow`'. This will allow `PandasMaterializer` "
                "to automatically store the data as a `.parquet` file instead."
            )
        finally:
            self.parquet_path = os.path.join(self.uri, PARQUET_FILENAME)
            self.csv_path = os.path.join(self.uri, CSV_FILENAME)

    def load(self, data_type: Type[Any]) -> Union[pd.DataFrame, pd.Series]:
        """Reads `pd.DataFrame` or `pd.Series` from a `.parquet` or `.csv` file.

        Args:
            data_type: The type of the data to read.

        Raises:
            ImportError: If pyarrow or fastparquet is not installed.

        Returns:
            The pandas dataframe or series.
        """
        if self.artifact_store.exists(self.parquet_path):
            if self.pyarrow_exists:
                with self.artifact_store.open(
                    self.parquet_path, mode="rb"
                ) as f:
                    df = pd.read_parquet(f)
            else:
                raise ImportError(
                    "You have an old version of a `PandasMaterializer` "
                    "data artifact stored in the artifact store "
                    "as a `.parquet` file, which requires `pyarrow` "
                    "for reading, You can install `pyarrow` by running "
                    "'`pip install pyarrow fastparquet`'."
                )
        else:
            with self.artifact_store.open(self.csv_path, mode="rb") as f:
                df = pd.read_csv(f, index_col=0, parse_dates=True)

        # validate the type of the data.
        def is_dataframe_or_series(
            df: Union[pd.DataFrame, pd.Series],
        ) -> Union[pd.DataFrame, pd.Series]:
            """Checks if the data is a `pd.DataFrame` or `pd.Series`.

            Args:
                df: The data to check.

            Returns:
                The data if it is a `pd.DataFrame` or `pd.Series`.
            """
            if issubclass(data_type, pd.Series):
                # Taking the first column if it is a series as the assumption
                # is that there will only be one
                assert len(df.columns) == 1
                df = df[df.columns[0]]
                return df
            else:
                return df

        return is_dataframe_or_series(df)

    def save(self, df: Union[pd.DataFrame, pd.Series]) -> None:
        """Writes a pandas dataframe or series to the specified filename.

        Args:
            df: The pandas dataframe or series to write.
        """
        if isinstance(df, pd.Series):
            df = df.to_frame(name="series")

        if self.pyarrow_exists:
            with self.artifact_store.open(self.parquet_path, mode="wb") as f:
                df.to_parquet(f, compression=COMPRESSION_TYPE)
        else:
            with self.artifact_store.open(self.csv_path, mode="wb") as f:
                df.to_csv(f, index=True)

```

## Code Example

This example demonstrates materialization using a custom class `MyObject` that is passed between two steps in a pipeline.

```python
import logging
from zenml import step, pipeline


class MyObj:
    def __init__(self, name: str):
        self.name = name


@step
def my_first_step() -> MyObj:
    """Step that returns an object of type MyObj."""
    return MyObj("my_object")


@step
def my_second_step(my_obj: MyObj) -> None:
    """Step that logs the input object and returns nothing."""
    logging.info(
        f"The following object was passed to this step: `{my_obj.name}`"
    )


@pipeline
def first_pipeline():
    output_1 = my_first_step()
    my_second_step(output_1)


first_pipeline()
```

Running the process without a custom materializer will trigger a warning: `No materializer is registered for type MyObj, so the default Pickle materializer was used. Pickle is not production ready and should only be used for prototyping as the artifacts cannot be loaded with a different Python version. Please consider implementing a custom materializer for type MyObj.` To eliminate this warning and enhance pipeline robustness, subclass `BaseMaterializer`, include `MyObj` in `ASSOCIATED_TYPES`, and override `load()` and `save()`.

```python
import os
from typing import Type

from zenml.enums import ArtifactType
from zenml.materializers.base_materializer import BaseMaterializer


class MyMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (MyObj,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[MyObj]) -> MyObj:
        """Read from artifact store."""
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'r') as f:
            name = f.read()
        return MyObj(name=name)

    def save(self, my_obj: MyObj) -> None:
        """Write to artifact store."""
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'w') as f:
            f.write(my_obj.name)
```

To utilize the materializer for handling outputs and inputs of custom objects in ZenML, edit your pipeline accordingly. Use the `self.artifact_store` property to ensure compatibility with both local and remote artifact stores, such as S3 buckets.

```python
my_first_step.configure(output_materializers=MyMaterializer)
first_pipeline()
```

The `ASSOCIATED_TYPES` attribute of the materializer allows for automatic detection of input and output types, eliminating the need to explicitly add `.configure(output_materializers=MyMaterializer)` to the step. However, being explicit is still acceptable. The process will function as intended and produce the expected output.

```shell
Creating run for pipeline: `first_pipeline`
Cache enabled for pipeline `first_pipeline`
Using stack `default` to run pipeline `first_pipeline`...
Step `my_first_step` has started.
Step `my_first_step` has finished in 0.081s.
Step `my_second_step` has started.
The following object was passed to this step: `my_object`
Step `my_second_step` has finished in 0.048s.
Pipeline run `first_pipeline-22_Apr_22-10_58_51_135729` has finished in 0.153s.
```

The documentation provides a code example for materializing custom objects. It outlines the necessary steps and key components involved in the process, ensuring that users can effectively implement custom object creation in their applications. Key points include the required libraries, the structure of the custom object, and the methods for instantiation and manipulation. The example serves as a practical guide for developers looking to integrate custom objects into their projects.

```python
import logging
import os
from typing import Type

from zenml import step, pipeline

from zenml.enums import ArtifactType
from zenml.materializers.base_materializer import BaseMaterializer


class MyObj:
    def __init__(self, name: str):
        self.name = name


class MyMaterializer(BaseMaterializer):
    ASSOCIATED_TYPES = (MyObj,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA

    def load(self, data_type: Type[MyObj]) -> MyObj:
        """Read from artifact store."""
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'r') as f:
            name = f.read()
        return MyObj(name=name)

    def save(self, my_obj: MyObj) -> None:
        """Write to artifact store."""
        with self.artifact_store.open(os.path.join(self.uri, 'data.txt'), 'w') as f:
            f.write(my_obj.name)


@step
def my_first_step() -> MyObj:
    """Step that returns an object of type MyObj."""
    return MyObj("my_object")


my_first_step.configure(output_materializers=MyMaterializer)


@step
def my_second_step(my_obj: MyObj) -> None:
    """Step that log the input object and returns nothing."""
    logging.info(
        f"The following object was passed to this step: `{my_obj.name}`"
    )


@pipeline
def first_pipeline():
    output_1 = my_first_step()
    my_second_step(output_1)


if __name__ == "__main__":
    first_pipeline()
```

The provided text contains an image of "ZenML Scarf" but lacks any specific documentation content to summarize. Please provide the relevant documentation text for summarization.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/delete-an-artifact.md

### Delete an Artifact

Artifacts cannot be deleted directly to avoid breaking the ZenML database due to dangling references from pipeline runs. However, you can delete artifacts that are no longer referenced by any pipeline runs.

```shell
zenml artifact prune
```

By default, this method deletes artifacts from the artifact store and the database. You can modify this behavior using the `--only-artifact` and `--only-metadata` flags. If errors occur during pruning due to locally stored artifacts that no longer exist, you can use the `--ignore-errors` flag to continue the process, although warning messages will still be displayed in the terminal.



================================================================================

# docs/book/how-to/data-artifact-management/handle-data-artifacts/return-multiple-outputs-from-a-step.md

The `Annotated` type allows you to return multiple outputs from a step, each with a designated name. This naming facilitates easy retrieval of specific artifacts and enhances the readability of your pipeline's dashboard.

```python
from typing import Annotated, Tuple

import pandas as pd
from zenml import step


@step
def clean_data(
    data: pd.DataFrame,
) -> Tuple[
    Annotated[pd.DataFrame, "x_train"],
    Annotated[pd.DataFrame, "x_test"],
    Annotated[pd.Series, "y_train"],
    Annotated[pd.Series, "y_test"],
]:
    from sklearn.model_selection import train_test_split

    x = data.drop("target", axis=1)
    y = data["target"]

    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

    return x_train, x_test, y_train, y_test
```

The `clean_data` step processes a pandas DataFrame and returns a tuple: `x_train`, `x_test`, `y_train`, and `y_test`, each annotated with the `Annotated` type for easy identification. The step splits the input data into features (`x`) and target (`y`), then utilizes `train_test_split` from scikit-learn to create training and testing sets. The annotated tuple enhances readability on the pipeline's dashboard.



================================================================================

# docs/book/how-to/infrastructure-deployment/README.md

# Infrastructure and Deployment

This section outlines the infrastructure setup and deployment processes in ZenML. It includes essential technical details and key points necessary for effective implementation.



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/implement-a-custom-stack-component.md

### How to Write a Custom Stack Component Flavor

When developing an MLOps platform, custom solutions for infrastructure or tooling are often necessary. ZenML emphasizes composability and reusability, allowing for modular and extendable stack component flavors. This guide explains what a flavor is and how to create custom flavors in ZenML.

#### Understanding Component Flavors

In ZenML, a component type categorizes the functionality of a stack component, with multiple flavors representing specific implementations. For example, the `artifact_store` type can include flavors like `local` and `s3`, each providing distinct implementations.

#### Base Abstractions

Before creating custom flavors, it's essential to understand three core abstractions related to stack components:

1. **StackComponent**: This abstraction defines core functionality. For example, `BaseArtifactStore` inherits from `StackComponent`, establishing the public interface for all artifact stores. Custom flavors must adhere to the standards set by this base class.

```python
from zenml.stack import StackComponent


class BaseArtifactStore(StackComponent):
    """Base class for all ZenML artifact stores."""

    # --- public interface ---

    @abstractmethod
    def open(self, path, mode = "r"):
        """Open a file at the given path."""

    @abstractmethod
    def exists(self, path):
        """Checks if a path exists."""

    ...
```

To implement a custom stack component, refer to the base class definition for the specific component type and consult the documentation on extending stack components. For automatic tracking of metadata during pipeline runs, define additional methods in your implementation class, as detailed in the section on tracking custom stack component metadata. The base `StackComponent` class code can be found [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/stack/stack_component.py#L301).

### Base Abstraction 2: `StackComponentConfig`
`StackComponentConfig` is used to configure a stack component instance separately from its implementation, allowing ZenML to validate configurations during registration or updates without importing heavy dependencies. 

The `config` represents the static configuration defined at registration, while `settings` are dynamic and can be overridden at runtime. For more details on these differences, refer to the runtime configuration documentation. 

Next, we will examine the `BaseArtifactStoreConfig` using the previous base artifact store example.

```python
from zenml.stack import StackComponentConfig


class BaseArtifactStoreConfig(StackComponentConfig):
    """Config class for `BaseArtifactStore`."""

    path: str

    SUPPORTED_SCHEMES: ClassVar[Set[str]]

    ...
```

The `BaseArtifactStoreConfig` requires users to define a `path` variable for each artifact store. It also mandates that all artifact store flavors specify a `SUPPORTED_SCHEMES` class variable, which ZenML uses to validate the user-provided `path`. For further details, refer to the `StackComponentConfig` class [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/stack/stack_component.py#L44). 

### Base Abstraction 3: `Flavor`
The `Flavor` abstraction integrates the implementation of a `StackComponent` with its corresponding `StackComponentConfig` definition, defining the `name` and `type` of the flavor. An example of the `local` artifact store flavor is provided below.

```python
from zenml.enums import StackComponentType
from zenml.stack import Flavor


class LocalArtifactStore(BaseArtifactStore):
    ...


class LocalArtifactStoreConfig(BaseArtifactStoreConfig):
    ...


class LocalArtifactStoreFlavor(Flavor):

    @property
    def name(self) -> str:
        """Returns the name of the flavor."""
        return "local"

    @property
    def type(self) -> StackComponentType:
        """Returns the flavor type."""
        return StackComponentType.ARTIFACT_STORE

    @property
    def config_class(self) -> Type[LocalArtifactStoreConfig]:
        """Config class of this flavor."""
        return LocalArtifactStoreConfig

    @property
    def implementation_class(self) -> Type[LocalArtifactStore]:
        """Implementation class of this flavor."""
        return LocalArtifactStore
```

The base `Flavor` class definition can be found [here](https://github.com/zenml-io/zenml/blob/main/src/zenml/stack/flavor.py#L29). 

To implement a custom stack component flavor, we will reimplement the `S3ArtifactStore` from the `aws` integration. Begin by defining the `SUPPORTED_SCHEMES` class variable from the `BaseArtifactStore`. Additionally, specify configuration values for user authentication with AWS.

```python
from zenml.artifact_stores import BaseArtifactStoreConfig
from zenml.utils.secret_utils import SecretField


class MyS3ArtifactStoreConfig(BaseArtifactStoreConfig):
    """Configuration for the S3 Artifact Store."""

    SUPPORTED_SCHEMES: ClassVar[Set[str]] = {"s3://"}

    key: Optional[str] = SecretField(default=None)
    secret: Optional[str] = SecretField(default=None)
    token: Optional[str] = SecretField(default=None)
    client_kwargs: Optional[Dict[str, Any]] = None
    config_kwargs: Optional[Dict[str, Any]] = None
    s3_additional_kwargs: Optional[Dict[str, Any]] = None
```

You can pass sensitive configuration values as secrets by defining them as type `SecretField` in the configuration class. After defining the configuration, proceed to implement the class that uses the S3 file system to fulfill the abstract methods of `BaseArtifactStore`.

```python
import s3fs

from zenml.artifact_stores import BaseArtifactStore


class MyS3ArtifactStore(BaseArtifactStore):
    """Custom artifact store implementation."""

    _filesystem: Optional[s3fs.S3FileSystem] = None

    @property
    def filesystem(self) -> s3fs.S3FileSystem:
        """Get the underlying S3 file system."""
        if self._filesystem:
            return self._filesystem

        self._filesystem = s3fs.S3FileSystem(
            key=self.config.key,
            secret=self.config.secret,
            token=self.config.token,
            client_kwargs=self.config.client_kwargs,
            config_kwargs=self.config.config_kwargs,
            s3_additional_kwargs=self.config.s3_additional_kwargs,
        )
        return self._filesystem

    def open(self, path, mode: = "r"):
        """Custom logic goes here."""
        return self.filesystem.open(path=path, mode=mode)

    def exists(self, path):
        """Custom logic goes here."""
        return self.filesystem.exists(path=path)
```

The configuration values from the configuration class are accessible in the implementation class via `self.config`. To integrate both classes, define a custom flavor with a globally unique name.

```python
from zenml.artifact_stores import BaseArtifactStoreFlavor


class MyS3ArtifactStoreFlavor(BaseArtifactStoreFlavor):
    """Custom artifact store implementation."""

    @property
    def name(self):
        """The name of the flavor."""
        return 'my_s3_artifact_store'

    @property
    def implementation_class(self):
        """Implementation class for this flavor."""
        from ... import MyS3ArtifactStore

        return MyS3ArtifactStore

    @property
    def config_class(self):
        """Configuration class for this flavor."""
        from ... import MyS3ArtifactStoreConfig

        return MyS3ArtifactStoreConfig
```

To manage a custom stack component flavor in ZenML, ensure that your implementation, config, and flavor classes are defined in separate Python files. Only import the implementation class in the `implementation_class` property of the flavor class to allow ZenML to load and validate the flavor configuration without requiring additional dependencies. You can register your new flavor using the ZenML CLI after defining these classes.

```shell
zenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>
```

To register your flavor class, use dot notation to specify its path. For instance, if your flavor class is `MyS3ArtifactStoreFlavor` located in `flavors/my_flavor.py`, register it accordingly.

```shell
zenml artifact-store flavor register flavors.my_flavor.MyS3ArtifactStoreFlavor
```

The new custom artifact store flavor will appear in the list of available artifact store flavors.

```shell
zenml artifact-store flavor list
```

You have successfully created a custom stack component flavor that can be utilized in your stacks like any other existing flavor.

```shell
zenml artifact-store register <ARTIFACT_STORE_NAME> \
    --flavor=my_s3_artifact_store \
    --path='some-path' \
    ...

zenml stack register <STACK_NAME> \
    --artifact-store <ARTIFACT_STORE_NAME> \
    ...
```

## Tips and Best Practices

- **Initialization**: Execute `zenml init` consistently at the root of your repository to avoid unexpected behavior. If not executed, the current working directory will be used for resolution.
  
- **Configuration**: Use the ZenML CLI to identify required configuration values for specific flavors. You can modify `Config` and `Settings` after registration, and ZenML will apply these changes during pipeline execution. However, breaking changes to config require component updates, which may necessitate deleting and re-registering the component.

- **Testing**: Thoroughly test your flavor before production use to ensure it functions correctly and handles errors.

- **Code Quality**: Maintain clean and well-documented flavor code, adhering to best practices for your programming language and libraries to enhance efficiency and maintainability.

- **Development Reference**: Use existing flavors, particularly those in the [official ZenML integrations](https://github.com/zenml-io/zenml/tree/main/src/zenml/integrations), as a reference when developing new flavors.

## Extending Specific Stack Components

To build a custom stack component flavor, refer to the following resources:

| **Type of Stack Component** | **Description** |
|------------------------------|-----------------|
| [Orchestrator](../../../component-guide/orchestrators/custom.md) | Manages pipeline runs |
| [Artifact Store](../../../component-guide/artifact-stores/custom.md) | Stores pipeline artifacts |
| [Container Registry](../../../component-guide/container-registries/custom.md) | Stores containers |
| [Step Operator](../../../component-guide/step-operators/custom.md) | Executes steps in specific environments |
| [Model Deployer](../../../component-guide/model-deployers/custom.md) | Online model serving platforms |
| [Feature Store](../../../component-guide/feature-stores/custom.md) | Manages data/features |
| [Experiment Tracker](../../../component-guide/experiment-trackers/custom.md) | Tracks ML experiments |
| [Alerter](../../../component-guide/alerters/custom.md) | Sends alerts via specified channels |
| [Annotator](../../../component-guide/annotators/custom.md) | Annotates and labels data |
| [Data Validator](../../../component-guide/data-validators/custom.md) | Validates and monitors data |



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/export-stack-requirements.md

To export the `pip` requirements of your stack, use the command `zenml stack export-requirements <STACK-NAME>`. For installation, it's recommended to save the requirements to a file and then install them from that file.

```bash
zenml stack export-requirements <STACK-NAME> --output-file stack_requirements.txt
pip install -r stack_requirements.txt
```

The provided documentation text includes an image of ZenML Scarf but lacks any accompanying descriptive content. Therefore, there are no technical details or key points to summarize. If there is additional text or context related to the image, please provide that for a more comprehensive summary.



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/README.md

## Managing Stacks & Components

### What is a Stack?
A **stack** in the ZenML framework represents the configuration of infrastructure and tools for executing pipelines. It consists of various components, each responsible for specific tasks, such as:
- **Container Registry**
- **Kubernetes Cluster** (orchestrator)
- **Artifact Store**
- **Experiment Tracker** (e.g., MLflow)

### Organizing Execution Environments
ZenML allows running pipelines across multiple stacks, facilitating testing in different environments. This approach helps:
- Prevent accidental deployment of staging pipelines to production.
- Reduce costs by using less powerful resources in staging.
- Control access by assigning permissions to specific stacks.

### Managing Credentials
Most stack components require credentials for infrastructure interaction. ZenML recommends using **Service Connectors** to manage these credentials securely, minimizing the risk of leaks and simplifying auditing.

#### Recommended Roles
- Limit Service Connector creation to individuals with direct cloud resource access to enhance security and auditing.

#### Recommended Workflow
1. Allow a limited number of users to create Service Connectors.
2. Create a connector for development/staging environments for data scientists.
3. Create a separate connector for production to ensure safe resource usage.

### Deploying and Managing Stacks
Deploying MLOps stacks can be complex due to:
- Specific tool requirements (e.g., Kubernetes for Kubeflow).
- Difficulty in setting reasonable infrastructure defaults.
- Potential issues with standard installations (e.g., custom service accounts needed).
- Ensuring all components have the right permissions to communicate.
- Challenges in cleaning up resources post-experiment.

The documentation provides guidance on provisioning, configuring, and extending stacks in ZenML.

### Key Resources
- [Deploy a Cloud Stack with ZenML](./deploy-a-cloud-stack.md)
- [Register a Cloud Stack](./register-a-cloud-stack.md)
- [Deploy a Cloud Stack with Terraform](./deploy-a-cloud-stack-with-terraform.md)
- [Export and Install Stack Requirements](./export-stack-requirements.md)
- [Reference Secrets in Stack Configuration](./reference-secrets-in-stack-configuration.md)
- [Implement a Custom Stack Component](./implement-a-custom-stack-component.md)



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/deploy-a-cloud-stack.md

### Deploy a Cloud Stack with a Single Click

In ZenML, a **stack** represents your infrastructure configuration. Traditionally, creating a stack involves deploying infrastructure components and defining them in ZenML, which can be complex in remote settings. To simplify this, ZenML offers a feature to **deploy infrastructure on your chosen cloud provider with a single click**.

#### Alternative Options
- For more control, use [Terraform modules](deploy-a-cloud-stack-with-terraform.md) to manage infrastructure as code.
- If infrastructure is already deployed, use [the stack wizard](../../infrastructure-deployment/stack-deployment/register-a-cloud-stack.md) to register your stack.

### Using the 1-Click Deployment Tool
1. Ensure you have a deployed ZenML instance (not local via `zenml login --local`). Instructions for setup can be found [here](../../../getting-started/deploying-zenml/README.md).
2. Access the 1-click deployment tool via the dashboard or CLI.

#### Dashboard Deployment Steps
- Navigate to the stacks page and click "+ New Stack".
- Select "New Infrastructure".
  
**For AWS:**
- Choose `aws`, select a region and stack name.
- Complete configuration and click "Deploy in AWS" to be redirected to AWS Cloud Formation.
- Log in, review, and confirm the configuration to create the stack.

**For GCP:**
- Choose `gcp`, select a region and stack name.
- Complete configuration and click "Deploy in GCP" to start a Cloud Shell session.
- Review the ZenML GitHub repository and check the `Trust repo` box.
- Authenticate with GCP, configure deployment using values from the ZenML dashboard, and run the provided script to deploy resources and register the stack.

**For Azure:**
- Choose `azure`, select a location and stack name.
- Review the resources to be deployed and note the `main.tf` file values.
- Click "Deploy in Azure" to start a Cloud Shell session.
- Paste the `main.tf` content, run `terraform init --upgrade` and `terraform apply` to deploy resources and register the stack.

#### CLI Deployment
To create a remote stack via CLI, use the appropriate command (not specified in the provided text).

### Conclusion
The 1-click deployment feature streamlines the process of setting up a cloud stack in ZenML, significantly reducing complexity and time required for deployment.

```shell
zenml stack deploy -p {aws|gcp|azure}
```

### AWS Deployment
- **Provider**: `aws`
- **Process**: The command initiates a Cloud Formation stack deployment. After confirming, you will be redirected to the AWS Console to deploy the stack, requiring AWS account login and permissions.
- **Resources Provisioned**:
  - S3 bucket (ZenML Artifact Store)
  - ECR container registry (ZenML Container Registry)
  - CloudBuild project (ZenML Image Builder)
  - SageMaker permissions (Orchestrator and Step Operator)
  - IAM user/role with necessary permissions
- **Permissions**: Includes access to S3, ECR, CloudBuild, and SageMaker with specific actions listed.

### GCP Deployment
- **Provider**: `gcp`
- **Process**: The command guides you through deploying a Deployment Manager template. After confirmation, you enter a Cloud Shell session, where you must trust the ZenML GitHub repository and authenticate with GCP.
- **Resources Provisioned**:
  - GCS bucket (ZenML Artifact Store)
  - GCP Artifact Registry (ZenML Container Registry)
  - Vertex AI permissions (Orchestrator and Step Operator)
  - Cloud Builder permissions (Image Builder)
- **Permissions**: Includes roles for GCS, Artifact Registry, Vertex AI, and Cloud Build with specific actions listed.

### Azure Deployment
- **Provider**: `azure`
- **Process**: The command leads you to deploy the ZenML Azure Stack Terraform module. You will use Terraform to create a `main.tf` file and run `terraform init` and `terraform apply`.
- **Resources Provisioned**:
  - Azure Resource Group
  - Azure Storage Account and Blob Storage Container (ZenML Artifact Store)
  - Azure Container Registry (ZenML Container Registry)
  - AzureML Workspace (Orchestrator and Step Operator)
- **Permissions**: Includes permissions for Storage Account, Container Registry, and AzureML Workspace with specific roles listed.

### Summary
With a single command, you can deploy a cloud stack on AWS, GCP, or Azure, enabling you to run pipelines in a remote setting. Each provider's deployment process includes specific resources and permissions tailored to ZenML's requirements.



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/register-a-cloud-stack.md

**Description:** Register a cloud stack using existing infrastructure in ZenML.

In ZenML, a **stack** represents your infrastructure configuration. Typically, creating a stack involves deploying infrastructure components and defining them in ZenML with authentication, which can be complex, especially remotely. To simplify this, ZenML offers a **stack wizard** that lets you browse and register your existing infrastructure as a ZenML cloud stack.

If you lack the necessary infrastructure, you can use the **1-click deployment tool** to build your cloud stack. For more control over resource provisioning, consider using **Terraform modules** for infrastructure management.

### How to Use the Stack Wizard

The stack wizard is accessible via both the CLI and the dashboard.

#### Dashboard Instructions:
1. Navigate to the stacks page and click on "+ New Stack."
2. Select "Use existing Cloud."
3. Choose your cloud provider.
4. Select an authentication method and complete the required fields.

#### AWS Authentication:
If you select AWS as your provider and haven't chosen a connector or declined auto-configuration, you'll need to select an authentication method for your cloud connector. 

This streamlined process allows for efficient registration of cloud stacks using pre-existing infrastructure.

```
                  Available authentication methods for AWS                   
┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice  ┃ Name                           ┃ Required                       ┃
┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]     │ AWS Secret Key                 │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [1]     │ AWS STS Token                  │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ aws_session_token  (AWS        │
│         │                                │ Session Token)                 │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [2]     │ AWS IAM Role                   │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │ role_arn  (AWS IAM Role ARN)   │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [3]     │ AWS Session Token              │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [4]     │ AWS Federation Token           │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
└─────────┴────────────────────────────────┴────────────────────────────────┘
```

### GCP: Authentication Methods

When selecting `gcp` as your cloud provider without a connector or auto-configuration, you must choose an authentication method for your cloud connector. 

#### Available Authentication Methods for GCP:
- [List of methods would be provided here] 

(Note: The specific authentication methods are not included in the provided text.)

```
                  Available authentication methods for GCP                   
┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice  ┃ Name                           ┃ Required                       ┃
┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]     │ GCP User Account               │ user_account_json  (GCP User   │
│         │                                │ Account Credentials JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [1]     │ GCP Service Account            │ service_account_json  (GCP     │
│         │                                │ Service Account Key JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [2]     │ GCP External Account           │ external_account_json  (GCP    │
│         │                                │ External Account JSON          │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [3]     │ GCP Oauth 2.0 Token            │ token  (GCP OAuth 2.0 Token)   │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [4]     │ GCP Service Account            │ service_account_json  (GCP     │
│         │ Impersonation                  │ Service Account Key JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ target_principal  (GCP Service │
│         │                                │ Account Email to impersonate)  │
│         │                                │                                │
└─────────┴────────────────────────────────┴────────────────────────────────┘
```

### Azure: Authentication Methods

When selecting `azure` as your cloud provider without a chosen connector or declined auto-configuration, you will be prompted to select an authentication method for your cloud connector.

**Available Authentication Methods for Azure:** 
- (List of methods would typically follow here, but is not provided in the text.)

```
    Available authentication methods for AZURE                         
┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice ┃ Name                    ┃ Required                           ┃
┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]    │ Azure Service Principal │ client_secret  (Service principal  │
│        │                         │ client secret)                     │
│        │                         │ tenant_id  (Azure Tenant ID)       │
│        │                         │ client_id  (Azure Client ID)       │
│        │                         │                                    │
├────────┼─────────────────────────┼────────────────────────────────────┤
│ [1]    │ Azure Access Token      │ token  (Azure Access Token)        │
│        │                         │                                    │
└────────┴─────────────────────────┴────────────────────────────────────┘
```

ZenML will display available resources from your existing infrastructure to create stack components like an artifact store, orchestrator, and container registry. To register a remote stack via the CLI using the stack wizard, use the specified command.

```shell
zenml stack register <STACK_NAME> -p {aws|gcp|azure}
```

To register the cloud stack, the wizard requires a service connector. You can use an existing connector by providing its ID or name with the command `-sc <SERVICE_CONNECTOR_ID_OR_NAME>` (CLI-Only), or the wizard can create one for you. Note that existing stack components can also be used via CLI, provided they are configured with the same service connector. 

### Define Service Connector
The configuration wizard first checks for cloud provider credentials in the local environment. If found, you can choose to use them or proceed with manual configuration. 

```plaintext
Example prompt for AWS auto-configuration
```

```
AWS cloud service connector has detected connection 
credentials in your environment.
Would you like to use these credentials or create a new 
configuration by providing connection details? [y/n] (y):
```

If you decline auto-configuration, you will see a list of existing service connectors on the server. Choose one or select `0` to create a new connector. 

**AWS: Authentication Methods**  
If you choose `aws` as your cloud provider without selecting a connector or declining auto-configuration, you will be prompted to select an authentication method for your cloud connector.

```
                  Available authentication methods for AWS                   
┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice  ┃ Name                           ┃ Required                       ┃
┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]     │ AWS Secret Key                 │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [1]     │ AWS STS Token                  │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ aws_session_token  (AWS        │
│         │                                │ Session Token)                 │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [2]     │ AWS IAM Role                   │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │ role_arn  (AWS IAM Role ARN)   │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [3]     │ AWS Session Token              │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [4]     │ AWS Federation Token           │ aws_access_key_id  (AWS Access │
│         │                                │ Key ID)                        │
│         │                                │ aws_secret_access_key  (AWS    │
│         │                                │ Secret Access Key)             │
│         │                                │ region  (AWS Region)           │
│         │                                │                                │
└─────────┴────────────────────────────────┴────────────────────────────────┘
```

### GCP: Authentication Methods

When selecting `gcp` as your cloud provider without a connector or auto-configuration, you must choose an authentication method for your cloud connector. 

#### Available Authentication Methods for GCP:
- [List of methods not provided in the text] 

(Note: The specific authentication methods should be included here if available in the original documentation.)

```
                  Available authentication methods for GCP                   
┏━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice  ┃ Name                           ┃ Required                       ┃
┡━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]     │ GCP User Account               │ user_account_json  (GCP User   │
│         │                                │ Account Credentials JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [1]     │ GCP Service Account            │ service_account_json  (GCP     │
│         │                                │ Service Account Key JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [2]     │ GCP External Account           │ external_account_json  (GCP    │
│         │                                │ External Account JSON          │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [3]     │ GCP Oauth 2.0 Token            │ token  (GCP OAuth 2.0 Token)   │
│         │                                │ project_id  (GCP Project ID    │
│         │                                │ where the target resource is   │
│         │                                │ located.)                      │
│         │                                │                                │
├─────────┼────────────────────────────────┼────────────────────────────────┤
│ [4]     │ GCP Service Account            │ service_account_json  (GCP     │
│         │ Impersonation                  │ Service Account Key JSON       │
│         │                                │ optionally base64 encoded.)    │
│         │                                │ target_principal  (GCP Service │
│         │                                │ Account Email to impersonate)  │
│         │                                │                                │
└─────────┴────────────────────────────────┴────────────────────────────────┘
```

### Azure: Authentication Methods

When selecting `azure` as your cloud provider without a connector or auto-configuration, you must choose an authentication method for your cloud connector. 

#### Available Authentication Methods for Azure
- [List of authentication methods would typically follow here] 

(Note: The specific authentication methods are not provided in the excerpt.)

```
    Available authentication methods for AZURE                         
┏━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice ┃ Name                    ┃ Required                           ┃
┡━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]    │ Azure Service Principal │ client_secret  (Service principal  │
│        │                         │ client secret)                     │
│        │                         │ tenant_id  (Azure Tenant ID)       │
│        │                         │ client_id  (Azure Client ID)       │
│        │                         │                                    │
├────────┼─────────────────────────┼────────────────────────────────────┤
│ [1]    │ Azure Access Token      │ token  (Azure Access Token)        │
│        │                         │                                    │
└────────┴─────────────────────────┴────────────────────────────────────┘
```

### Defining Cloud Components

You will define three essential components of your cloud stack:

- **Artifact Store**
- **Orchestrator**
- **Container Registry**

These components are fundamental for a basic cloud stack, with the option to add more later. For each component, you will decide whether to reuse an existing component connected via a defined service connector.

```
                    Available orchestrator
┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice           ┃ Name                                               ┃
┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]              │ Create a new orchestrator                          │
├──────────────────┼────────────────────────────────────────────────────┤
│ [1]              │ existing_orchestrator_1                            │
├──────────────────┼────────────────────────────────────────────────────┤
│ [2]              │ existing_orchestrator_2                            │
└──────────────────┴────────────────────────────────────────────────────┘
```

The command `{% endcode %}` is used to create a new resource from the available service connector resources if an existing one is not selected. The output will include an example command for artifact stores.

```
                        Available GCP storages                            
┏━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ Choice        ┃ Storage                                               ┃
┡━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩
│ [0]           │ gs://***************************                      │
├───────────────┼───────────────────────────────────────────────────────┤
│ [1]           │ gs://***************************                      │
└───────────────┴───────────────────────────────────────────────────────┘
```

ZenML will create and register the selected stack component for you. You have successfully registered a cloud stack and can now run your pipelines in a remote environment.



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/deploy-a-cloud-stack-with-terraform.md

### Deploy a Cloud Stack with Terraform

ZenML offers a collection of [Terraform modules](https://registry.terraform.io/modules/zenml-io/zenml-stack) to facilitate the provisioning of cloud resources and their integration with ZenML Stacks. These modules streamline setup, enabling quick provisioning and configuration for running AI/ML pipelines. Users can leverage these modules for efficient, scalable machine learning infrastructure deployment and as a reference for custom Terraform configurations.

**Important Notes:**
- Terraform requires manual infrastructure management, including installation and state management.
- For a more automated approach, consider using the [1-click stack deployment feature](deploy-a-cloud-stack.md).
- If infrastructure is already deployed, use the [stack wizard to register your stack](../../infrastructure-deployment/stack-deployment/register-a-cloud-stack.md).

### Pre-requisites
- A deployed ZenML server instance accessible from the desired cloud provider (not a local server).
- To set up a ZenML Pro server, run `zenml login --pro` or [register for a free account](https://cloud.zenml.io/signup).
- For self-hosting, refer to the guide on [deploying ZenML](../../../getting-started/deploying-zenml/README.md).
- Create a service account and API key for programmatic access to your ZenML server. More information can be found [here](../../project-setup-and-management/connecting-to-zenml/connect-with-a-service-account.md). The process involves running a CLI command while connected to your ZenML server.

```shell
zenml service-account create <account-name>
```

Sure! Please provide the documentation text you'd like me to summarize.

```shell
$ zenml service-account create terraform-account
Created service account 'terraform-account'.
Successfully created API key `default`.
The API key value is: 'ZENKEY_...'
Please store it safely as it will not be shown again.
To configure a ZenML client to use this API key, run:

zenml login https://842ed6a9-zenml.staging.cloudinfra.zenml.io --api-key

and enter the following API key when prompted:
ZENKEY_...
```

To run Terraform with ZenML, ensure you have the following:

- **Terraform**: Install version 1.9 or higher from [Terraform downloads](https://www.terraform.io/downloads.html).
- **Cloud Provider Authentication**: You must be authenticated with your cloud provider via its CLI or SDK and have the necessary permissions to create resources.

### Using Terraform Stack Deployment Modules

If you're familiar with Terraform and your chosen cloud provider, follow these steps:

1. Set up the ZenML Terraform provider using your ZenML server URL and API key. It is recommended to use environment variables instead of hardcoding these values in your configuration file.

```shell
export ZENML_SERVER_URL="https://your-zenml-server.com"
export ZENML_API_KEY="<your-api-key>"
```

To create a new Terraform configuration, create a file named `main.tf` in a new directory. The file should contain configuration specific to your chosen cloud provider, which can be `aws`, `gcp`, or `azure`.

```hcl
terraform {
    required_providers {
        aws = {
            source  = "hashicorp/aws"
        }
        zenml = {
            source = "zenml-io/zenml"
        }
    }
}

provider "zenml" {
    # server_url = <taken from the ZENML_SERVER_URL environment variable if not set here>
    # api_key = <taken from the ZENML_API_KEY environment variable if not set here>
}

module "zenml_stack" {
  source = "zenml-io/zenml-stack/<cloud-provider>"
  version = "x.y.z"

  # Optional inputs
  zenml_stack_name = "<your-stack-name>"
  orchestrator = "<your-orchestrator-type>" # e.g., "local", "sagemaker", "vertex", "azureml", "skypilot"
}
output "zenml_stack_id" {
  value = module.zenml_stack.zenml_stack_id
}
output "zenml_stack_name" {
  value = module.zenml_stack.zenml_stack_name
}
```

Depending on your cloud provider, there may be additional required or optional inputs. For a complete list of inputs for each module, refer to the [Terraform Registry](https://registry.terraform.io/modules/zenml-io/zenml-stack) documentation. To proceed, run the following commands in the directory containing your Terraform configuration file:

```shell
terraform init
terraform apply
```

**Important Notes on Terraform Usage:**

- The directory containing your Terraform configuration file and where you execute `terraform` commands is crucial, as it stores the state of your infrastructure. Do not delete this directory or the state file unless you are certain you no longer need to manage these resources or have deprovisioned them using `terraform destroy`.

- Terraform will prompt for confirmation before making changes to your cloud infrastructure. Type `yes` to proceed.

- Upon successful provisioning of resources specified in your configuration file, a message will display the ZenML stack ID and name.

```shell
...
Apply complete! Resources: 15 added, 0 changed, 0 destroyed.

Outputs:

zenml_stack_id = "04c65b96-b435-4a39-8484-8cc18f89b991"
zenml_stack_name = "terraform-gcp-588339e64d06"
```

A ZenML stack has been created and registered with your ZenML server, allowing you to start running your pipelines.

```shell
zenml integration install <list-of-required-integrations>
zenml stack set <zenml_stack_id>
```

For detailed information specific to your cloud provider, refer to the following sections. 

### AWS
The [ZenML AWS Terraform module documentation](https://registry.terraform.io/modules/zenml-io/zenml-stack/aws/latest) provides essential details on permissions, inputs, outputs, and resources. 

#### Authentication
To authenticate with AWS, install the [AWS CLI](https://aws.amazon.com/cli/) and run `aws configure` to set up your credentials.

#### Example Terraform Configuration
An example Terraform configuration file for deploying a ZenML stack on AWS is provided in the documentation.

```hcl
terraform {
    required_providers {
        aws = {
            source  = "hashicorp/aws"
        }
        zenml = {
            source = "zenml-io/zenml"
        }
    }
}

provider "zenml" {
    # server_url = <taken from the ZENML_SERVER_URL environment variable if not set here>
    # api_key = <taken from the ZENML_API_KEY environment variable if not set here>
}

provider "aws" {
    region = "eu-central-1"
}

module "zenml_stack" {
  source = "zenml-io/zenml-stack/aws"

  # Optional inputs
  orchestrator = "<your-orchestrator-type>" # e.g., "local", "sagemaker", "skypilot"
  zenml_stack_name = "<your-stack-name>"
}

output "zenml_stack_id" {
  value = module.zenml_stack.zenml_stack_id
}
output "zenml_stack_name" {
  value = module.zenml_stack.zenml_stack_name
}
```

### Stack Components

The Terraform module creates a ZenML stack configuration with the following components:

1. **S3 Artifact Store**: Linked to an S3 bucket via an AWS Service Connector with IAM role credentials.
2. **ECR Container Registry**: Linked to an ECR repository via an AWS Service Connector with IAM role credentials.
3. **Orchestrator** (based on the `orchestrator` input variable):
   - **Local**: If set to `local`, allows running steps locally or on SageMaker.
   - **SageMaker**: Default setting, linked to the AWS account via an AWS Service Connector with IAM role credentials.
   - **SkyPilot**: Linked to the AWS account via an AWS Service Connector with IAM role credentials.
4. **AWS CodeBuild Image Builder**: Linked to the AWS account via an AWS Service Connector with IAM role credentials.
5. **SageMaker Step Operator**: Linked to the AWS account via an AWS Service Connector with IAM role credentials.

To use the ZenML stack, install the required integrations for the local or SageMaker orchestrator.

```shell
zenml integration install aws s3
```

Please provide the documentation text you would like summarized.

```shell
zenml integration install aws s3 skypilot_aws
```

### GCP Terraform Module Summary

The ZenML GCP Terraform module documentation provides essential details regarding permissions, inputs, outputs, and resources. 

#### Authentication
To authenticate with GCP, install the `gcloud` CLI and run either `gcloud init` or `gcloud auth application-default login` to configure your credentials.

#### Example Terraform Configuration
An example Terraform configuration file for deploying a ZenML stack on AWS is included in the full documentation. 

For comprehensive information, refer to the [original documentation](https://registry.terraform.io/modules/zenml-io/zenml-stack/gcp/latest).

```hcl
terraform {
    required_providers {
        google = {
            source  = "hashicorp/google"
        }
        zenml = {
            source = "zenml-io/zenml"
        }
    }
}

provider "zenml" {
    # server_url = <taken from the ZENML_SERVER_URL environment variable if not set here>
    # api_key = <taken from the ZENML_API_KEY environment variable if not set here>
}

provider "google" {
    region  = "europe-west3"
    project = "my-project"
}

module "zenml_stack" {
  source = "zenml-io/zenml-stack/gcp"

  # Optional inputs
  orchestrator = "<your-orchestrator-type>" # e.g., "local", "vertex", "skypilot" or "airflow"
  zenml_stack_name = "<your-stack-name>"
}

output "zenml_stack_id" {
  value = module.zenml_stack.zenml_stack_id
}
output "zenml_stack_name" {
  value = module.zenml_stack.zenml_stack_name
}
```

### Stack Components

The Terraform module creates a ZenML stack configuration with the following components:

1. **GCP Artifact Store**: Linked to a GCS bucket via a GCP Service Connector using GCP service account credentials.
2. **GCP Container Registry**: Linked to a Google Artifact Registry via a GCP Service Connector using GCP service account credentials.
3. **Orchestrator** (based on `orchestrator` input variable):
   - **Local**: If set to `local`, allows selective execution of steps locally and on Vertex AI.
   - **Vertex** (default): Vertex AI Orchestrator linked to the GCP project via a GCP Service Connector.
   - **SkyPilot**: SkyPilot Orchestrator linked to the GCP project via a GCP Service Connector.
   - **Airflow**: Airflow Orchestrator linked to the Cloud Composer environment.
4. **Google Cloud Build Image Builder**: Linked to the GCP project via a GCP Service Connector.
5. **Vertex AI Step Operator**: Linked to the GCP project via a GCP Service Connector.

**Required Integrations**: Install necessary integrations for local and Vertex AI orchestrators.

```shell
zenml integration install gcp
```

Please provide the documentation text you would like summarized.

```shell
zenml integration install gcp skypilot_gcp
```

Please provide the documentation text you would like summarized.

```shell
zenml integration install gcp airflow
```

### Azure ZenML Terraform Module Summary

The ZenML Azure Terraform module documentation provides essential details on permissions, inputs, outputs, and resources. 

#### Authentication
- Install the [Azure CLI](https://learn.microsoft.com/en-us/cli/azure/).
- Run `az login` to set up credentials.

#### Example Terraform Configuration
- An example configuration file for deploying a ZenML stack on Azure is provided in the full documentation. 

For comprehensive details, refer to the [original documentation](https://registry.terraform.io/modules/zenml-io/zenml-stack/azure/latest).

```hcl
terraform {{
    required_providers {{
        azurerm = {{
            source  = "hashicorp/azurerm"
        }}
        azuread = {{
            source  = "hashicorp/azuread"
        }}
        zenml = {{
            source = "zenml-io/zenml"
        }}
    }}
}}

provider "zenml" {
    # server_url = <taken from the ZENML_SERVER_URL environment variable if not set here>
    # api_key = <taken from the ZENML_API_KEY environment variable if not set here>
}

provider "azurerm" {{
    features {{
        resource_group {{
            prevent_deletion_if_contains_resources = false
        }}
    }}
}}

module "zenml_stack" {
  source = "zenml-io/zenml-stack/azure"

  # Optional inputs
  location = "<your-azure-location>"
  orchestrator = "<your-orchestrator-type>" # e.g., "local", "skypilot_azure"
  zenml_stack_name = "<your-stack-name>"
}

output "zenml_stack_id" {
  value = module.zenml_stack.zenml_stack_id
}
output "zenml_stack_name" {
  value = module.zenml_stack.zenml_stack_name
}
```

### Stack Components

The Terraform module creates a ZenML stack configuration with the following components:

1. **Azure Artifact Store**: Linked to an Azure Storage Account and Blob Container via an Azure Service Connector using Azure Service Principal credentials.
2. **ACR Container Registry**: Linked to an Azure Container Registry via an Azure Service Connector using Azure Service Principal credentials.
3. **Orchestrator** (based on the `orchestrator` input variable):
   - **local**: A local Orchestrator for running steps locally or on AzureML.
   - **skypilot** (default): An Azure SkyPilot Orchestrator linked to the Azure subscription via an Azure Service Connector with Azure Service Principal credentials.
   - **azureml**: An AzureML Orchestrator linked to an AzureML Workspace via an Azure Service Connector with Azure Service Principal credentials.
4. **AzureML Step Operator**: Linked to an AzureML Workspace via an Azure Service Connector using Azure Service Principal credentials.

To use the ZenML stack, install the required integrations for the local and AzureML orchestrators.

```shell
zenml integration install azure
```

Please provide the documentation text you would like me to summarize.

```shell
zenml integration install azure skypilot_azure
```

## How to Clean Up Terraform Stack Deployments

To clean up resources provisioned by Terraform, run the `terraform destroy` command in the directory containing your Terraform configuration file. This command will remove all resources provisioned by the Terraform module and delete the registered ZenML stack from your ZenML server.

```shell
terraform destroy
```

The provided text includes an image of "ZenML Scarf" but does not contain any technical information or key points to summarize. Please provide the relevant documentation text for summarization.



================================================================================

# docs/book/how-to/infrastructure-deployment/stack-deployment/reference-secrets-in-stack-configuration.md

### Reference Secrets in Stack Configuration

Some stack components require sensitive information, such as passwords or tokens, for infrastructure connections. To secure this information, use secret references instead of direct values. Reference a secret by specifying the secret name and key in the following format: `{{<SECRET_NAME>.<SECRET_KEY>}}`. 

**Example:**
- Use this syntax for any string attribute in your stack components.

```shell
# Register a secret called `mlflow_secret` with key-value pairs for the
# username and password to authenticate with the MLflow tracking server

# Using central secrets management
zenml secret create mlflow_secret \
    --username=admin \
    --password=abc123
    

# Then reference the username and password in our experiment tracker component
zenml experiment-tracker register mlflow \
    --flavor=mlflow \
    --tracking_username={{mlflow_secret.username}} \
    --tracking_password={{mlflow_secret.password}} \
    ...
```

When using secret references in ZenML stacks, the system validates that all referenced secrets and keys exist before executing a pipeline, preventing late failures due to missing secrets. By default, this validation fetches and reads every secret, which can be time-consuming and may fail due to insufficient permissions. You can control the validation level using the `ZENML_SECRET_VALIDATION_LEVEL` environment variable:

- `NONE`: Disables validation.
- `SECRET_EXISTS`: Validates only the existence of secrets, useful for environments with limited permissions.
- `SECRET_AND_KEY_EXISTS`: (default) Validates both the existence of secrets and their key-value pairs.

For centralized secrets management, you can access secrets directly within your steps using the ZenML `Client` API, allowing you to query APIs without hard-coding access keys.

```python
from zenml import step
from zenml.client import Client


@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    # Fetch the secret from ZenML.
    secret = Client().get_secret( < SECRET_NAME >)

    # `secret.secret_values` will contain a dictionary with all key-value
    # pairs within your secret.
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
    ...
```

## See Also - [Interact with secrets](../../interact-with-secrets.md): This section covers how to create, list, and delete secrets using the ZenML CLI and Python SDK.



================================================================================

# docs/book/how-to/infrastructure-deployment/infrastructure-as-code/terraform-stack-management.md

### Registering Existing Infrastructure with ZenML - A Guide for Terraform Users

#### Manage Your Stacks with Terraform
Terraform is a leading tool for infrastructure as code (IaC) and is widely used for managing existing setups. This guide is intended for advanced users who wish to integrate ZenML with their custom Terraform code, utilizing the [ZenML provider](https://registry.terraform.io/providers/zenml-io/zenml/latest).

#### Two-Phase Approach
When working with ZenML stacks, there are two phases:
1. **Infrastructure Deployment**: Creation of cloud resources, typically managed by platform teams.
2. **ZenML Registration**: Registering these resources as ZenML stack components.

While official modules like [`zenml-stack/aws`](https://registry.terraform.io/modules/zenml-io/zenml-stack/aws/latest), [`zenml-stack/gcp`](https://registry.terraform.io/modules/zenml-io/zenml-stack/gcp/latest), and [`zenml-stack/azure`](https://registry.terraform.io/modules/zenml-io/zenml-stack/azure/latest) handle both phases, this guide focuses on registering existing infrastructure with ZenML. 

#### Phase 1: Infrastructure Deployment
This phase is assumed to be managed through your existing Terraform configurations.

```hcl
# Example of existing GCP infrastructure
resource "google_storage_bucket" "ml_artifacts" {
  name     = "company-ml-artifacts"
  location = "US"
}

resource "google_artifact_registry_repository" "ml_containers" {
  repository_id = "ml-containers"
  format        = "DOCKER"
}
```

## Phase 2: ZenML Registration

### Setup the ZenML Provider
Configure the [ZenML provider](https://registry.terraform.io/providers/zenml-io/zenml/latest) to connect with your ZenML server.

```hcl
terraform {
  required_providers {
    zenml = {
      source = "zenml-io/zenml"
    }
  }
}

provider "zenml" {
  # Configuration options will be loaded from environment variables:
  # ZENML_SERVER_URL
  # ZENML_API_KEY
}
```

To generate an API key, use the command:

```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```

To generate a `ZENML_API_KEY` using service accounts, refer to the documentation [here](../../project-setup-and-management/connecting-to-zenml/connect-with-a-service-account.md). 

### Create Service Connectors
Proper authentication between components is essential for successful registration. ZenML utilizes [service connectors](../auth-management/README.md) for managing this authentication.

```hcl
# First, create a service connector
resource "zenml_service_connector" "gcp_connector" {
  name        = "gcp-${var.environment}-connector"
  type        = "gcp"
  auth_method = "service-account"  
  
  configuration = {
    project_id = var.project_id
    service_account_json = file("service-account.json")
  }
}

# Create a stack component referencing the connector
resource "zenml_stack_component" "artifact_store" {
  name   = "existing-artifact-store"
  type   = "artifact_store"
  flavor = "gcp"
  
  configuration = {
    path = "gs://${google_storage_bucket.ml_artifacts.name}"
  }
  
  connector_id = zenml_service_connector.gcp_connector.id
}
```

### Register the Stack Components

Register various types of components as outlined in the component guide.

```hcl
# Generic component registration pattern
locals {
  component_configs = {
    artifact_store = {
      type = "artifact_store"
      flavor = "gcp"
      configuration = {
        path = "gs://${google_storage_bucket.ml_artifacts.name}"
      }
    }
    container_registry = {
      type = "container_registry"
      flavor = "gcp"
      configuration = {
        uri = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.ml_containers.repository_id}"
      }
    }
    orchestrator = {
      type = "orchestrator"
      flavor = "vertex"
      configuration = {
        project = var.project_id
        region  = var.region
      }
    }
  }
}

# Register multiple components
resource "zenml_stack_component" "components" {
  for_each = local.component_configs
  
  name          = "existing-${each.key}"
  type          = each.value.type
  flavor        = each.value.flavor
  configuration = each.value.configuration
  
  connector_id = zenml_service_connector.env_connector.id
}
```

### Assemble the Stack
Assemble the components into a stack.

```hcl
resource "zenml_stack" "ml_stack" {
  name = "${var.environment}-ml-stack"
  
  components = {
    for k, v in zenml_stack_component.components : k => v.id
  }
}
```

## Practical Walkthrough: Registering Existing GCP Infrastructure

### Prerequisites
- GCS bucket for artifacts
- Artifact Registry repository
- Service account for ML operations
- Vertex AI enabled for orchestration

### Step 1: Variables Configuration
(Additional details on this step would follow here.)

```hcl
# variables.tf
variable "zenml_server_url" {
  description = "URL of the ZenML server"
  type        = string
}

variable "zenml_api_key" {
  description = "API key for ZenML server authentication"
  type        = string
  sensitive   = true
}

variable "project_id" {
  description = "GCP project ID"
  type        = string
}

variable "region" {
  description = "GCP region"
  type        = string
  default     = "us-central1"
}

variable "environment" {
  description = "Environment name (e.g., dev, staging, prod)"
  type        = string
}

variable "gcp_service_account_key" {
  description = "GCP service account key in JSON format"
  type        = string
  sensitive   = true
}
```

### Step 2: Main Configuration

This section outlines the essential steps for configuring the main settings of the system. Key points include:

1. **Accessing Configuration Settings**: Navigate to the configuration menu in the application interface.
   
2. **Setting Parameters**: Adjust parameters such as user permissions, system preferences, and operational modes. Ensure all values are within acceptable ranges.

3. **Saving Changes**: After modifications, click the 'Save' button to apply changes. Confirm that settings are updated successfully.

4. **Testing Configuration**: Conduct tests to verify that the configuration works as intended. Monitor for any errors or unexpected behavior.

5. **Backup Configuration**: Regularly back up configuration settings to prevent data loss. Use the backup feature in the settings menu.

6. **Documentation**: Maintain a record of configuration changes for future reference and troubleshooting.

Ensure all steps are followed to achieve optimal system performance.

```hcl
# main.tf
terraform {
  required_providers {
    zenml = {
      source = "zenml-io/zenml"
    }
    google = {
      source = "hashicorp/google"
    }
  }
}

# Configure providers
provider "zenml" {
  server_url = var.zenml_server_url
  api_key    = var.zenml_api_key
}

provider "google" {
  project = var.project_id
  region  = var.region
}

# Create GCP resources if needed
resource "google_storage_bucket" "artifacts" {
  name     = "${var.project_id}-zenml-artifacts-${var.environment}"
  location = var.region
}

resource "google_artifact_registry_repository" "containers" {
  location      = var.region
  repository_id = "zenml-containers-${var.environment}"
  format        = "DOCKER"
}

# ZenML Service Connector for GCP
resource "zenml_service_connector" "gcp" {
  name        = "gcp-${var.environment}"
  type        = "gcp"
  auth_method = "service-account"

  configuration = {
    project_id = var.project_id
    region     = var.region
    service_account_json = var.gcp_service_account_key
  }

  labels = {
    environment = var.environment
    managed_by  = "terraform"
  }
}

# Artifact Store Component
resource "zenml_stack_component" "artifact_store" {
  name   = "gcs-${var.environment}"
  type   = "artifact_store"
  flavor = "gcp"

  configuration = {
    path = "gs://${google_storage_bucket.artifacts.name}/artifacts"
  }

  connector_id = zenml_service_connector.gcp.id

  labels = {
    environment = var.environment
  }
}

# Container Registry Component
resource "zenml_stack_component" "container_registry" {
  name   = "gcr-${var.environment}"
  type   = "container_registry"
  flavor = "gcp"

  configuration = {
    uri = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.containers.repository_id}"
  }

  connector_id = zenml_service_connector.gcp.id

  labels = {
    environment = var.environment
  }
}

# Vertex AI Orchestrator
resource "zenml_stack_component" "orchestrator" {
  name   = "vertex-${var.environment}"
  type   = "orchestrator"
  flavor = "vertex"

  configuration = {
    location    = var.region
    synchronous = true
  }

  connector_id = zenml_service_connector.gcp.id

  labels = {
    environment = var.environment
  }
}

# Complete Stack
resource "zenml_stack" "gcp_stack" {
  name = "gcp-${var.environment}"

  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator      = zenml_stack_component.orchestrator.id
  }

  labels = {
    environment = var.environment
    managed_by  = "terraform"
  }
}
```

### Step 3: Outputs Configuration

This section outlines the configuration of outputs for the system. Key points include:

- **Output Types**: Specify the types of outputs required (e.g., JSON, XML).
- **Destination Settings**: Define where outputs will be sent (e.g., file path, network address).
- **Format Specifications**: Detail the format requirements for each output type.
- **Error Handling**: Implement error handling mechanisms to manage output failures.
- **Testing Outputs**: Conduct tests to ensure outputs are generated correctly and meet specifications.

Ensure all configurations are validated before deployment.

```hcl
# outputs.tf
output "stack_id" {
  description = "ID of the created ZenML stack"
  value       = zenml_stack.gcp_stack.id
}

output "stack_name" {
  description = "Name of the created ZenML stack"
  value       = zenml_stack.gcp_stack.name
}

output "artifact_store_path" {
  description = "GCS path for artifacts"
  value       = "${google_storage_bucket.artifacts.name}/artifacts"
}

output "container_registry_uri" {
  description = "URI of the container registry"
  value       = "${var.region}-docker.pkg.dev/${var.project_id}/${google_artifact_registry_repository.containers.repository_id}"
}
```

### Step 4: terraform.tfvars Configuration

Create a `terraform.tfvars` file. Ensure this file is excluded from version control.

```hcl
zenml_server_url = "https://your-zenml-server.com"
project_id       = "your-gcp-project-id"
region           = "us-central1"
environment      = "dev"
```

Store sensitive variables in environment variables to enhance security. This practice helps prevent hardcoding sensitive information in code, reducing the risk of exposure. Use a secure method to set and manage these variables, ensuring they are accessible only to authorized applications and users. Regularly review and update these variables to maintain security.

```bash
export TF_VAR_zenml_api_key="your-zenml-api-key"
export TF_VAR_gcp_service_account_key=$(cat path/to/service-account-key.json)
```

### Usage Instructions

1. **Install Required Providers**: Ensure all necessary providers are installed.
2. **Initialize Terraform**: Run the initialization command to set up the working directory and download required plugins.

```bash
terraform init
```

To install the necessary ZenML integrations, follow these steps: 

1. Identify the required integrations based on your project needs.
2. Use the command line to install the integrations via pip, for example: 
   ```
   pip install zenml[<integration_name>]
   ```
3. Verify the installation by checking the ZenML version and available integrations with:
   ```
   zenml version
   zenml integrations
   ```

Ensure that all dependencies are met for the specific integrations you are using.

```bash
zenml integration install gcp
```

**3. Review the Planned Changes:**

- Assess the proposed modifications for feasibility and impact.
- Ensure alignment with project objectives and stakeholder requirements.
- Identify potential risks and mitigation strategies.
- Confirm resource availability and timelines for implementation.
- Document feedback and necessary adjustments for final approval.

```bash
terraform plan
```

To apply the configuration, follow these steps: 

1. Ensure all settings are correctly defined in the configuration file.
2. Use the command-line interface or management console to initiate the application process.
3. Verify that the configuration is successfully applied by checking the system logs or status indicators.
4. If errors occur, troubleshoot by reviewing error messages and adjusting the configuration as necessary. 

Make sure to back up existing configurations before applying new ones.

```bash
terraform apply
```

To set the newly created stack as active, use the appropriate command or method specified in your system's documentation. Ensure that all prerequisites are met before activation.

```bash
zenml stack set $(terraform output -raw stack_name)
```

6. Verify the Configuration: 

Ensure that the system settings and parameters are correctly configured according to the specifications. This includes checking network settings, user permissions, and service statuses to confirm they align with the intended setup. Conduct tests to validate functionality and troubleshoot any discrepancies.

```bash
zenml stack describe
```

This example covers: 
- Setting up GCP infrastructure 
- Creating a service connector with authentication 
- Registering stack components 
- Building a complete ZenML stack 
- Managing variables and configuring outputs 
- Best practices for handling sensitive information 

The approach can be adapted for AWS and Azure by modifying provider configurations and resource types. Key reminders include: 
- Use appropriate IAM roles and permissions 
- Follow security practices for credentials 
- Consider Terraform workspaces for multiple environments 
- Regularly back up Terraform state files 
- Version control Terraform configurations (excluding sensitive files) 

For more information on the ZenML Terraform provider, visit the [ZenML provider](https://registry.terraform.io/providers/zenml-io/zenml/latest).



================================================================================

# docs/book/how-to/infrastructure-deployment/infrastructure-as-code/README.md

# Integrate with Infrastructure as Code

Leverage Infrastructure as Code (IaC) to manage ZenML stacks and components. IaC allows for the management and provisioning of infrastructure through code rather than manual processes. This section covers integration of ZenML with popular IaC tools, including [Terraform](https://www.terraform.io/). 

![Screenshot of ZenML stack on Terraform Registry](../../../.gitbook/assets/terraform_providers_screenshot.png)



================================================================================

# docs/book/how-to/infrastructure-deployment/infrastructure-as-code/best-practices.md

# Best Practices for Using IaC with ZenML

## Architecting ML Infrastructure with ZenML and Terraform

### The Challenge
As a system architect, you need to establish a scalable ML infrastructure that:
- Supports multiple ML teams with varying requirements
- Operates across different environments (dev, staging, prod)
- Adheres to security and compliance standards
- Enables rapid iteration without infrastructure bottlenecks

### The ZenML Approach
ZenML utilizes stack components as abstractions for infrastructure resources. This guide focuses on effectively architecting with Terraform using the ZenML provider.

### Part 1: Foundation - Stack Component Architecture

#### The Problem
Different teams require distinct ML infrastructure configurations while maintaining consistency and reusability.

#### The Solution: Component-Based Architecture
Decompose your infrastructure into reusable modules that correspond to ZenML stack components.

```hcl
# modules/zenml_stack_base/main.tf
terraform {
  required_providers {
    zenml = {
      source = "zenml-io/zenml"
    }
    google = {
      source = "hashicorp/google"
    }
  }
}

resource "random_id" "suffix" {
  # This will generate a string of 12 characters, encoded as base64 which makes
  # it 8 characters long
  byte_length = 6
}

# Create base infrastructure resources, including a shared object storage,
# and container registry. This module should also create resources used to
# authenticate with the cloud provider and authorize access to the resources
# (e.g. user accounts, service accounts, workload identities, roles,
# permissions etc.)
module "base_infrastructure" {
  source = "./modules/base_infra"
  
  environment = var.environment
  project_id  = var.project_id
  region      = var.region
  
  # Generate consistent random naming across resources
  resource_prefix = "zenml-${var.environment}-${random_id.suffix.hex}"
}

# Create a flexible service connector for authentication
resource "zenml_service_connector" "base_connector" {
  name        = "${var.environment}-base-connector"
  type        = "gcp"
  auth_method = "service-account"

  configuration = {
    project_id = var.project_id
    region     = var.region
    service_account_json = module.base_infrastructure.service_account_key
  }

  labels = {
    environment = var.environment
  }
}

# Create base stack components
resource "zenml_stack_component" "artifact_store" {
  name   = "${var.environment}-artifact-store"
  type   = "artifact_store"
  flavor = "gcp"

  configuration = {
    path = "gs://${module.base_infrastructure.artifact_store_bucket}/artifacts"
  }

  connector_id = zenml_service_connector.base_connector.id
}

resource "zenml_stack_component" "container_registry" {
  name   = "${var.environment}-container-registry"
  type   = "container_registry"
  flavor = "gcp"

  configuration = {
    uri = module.base_infrastructure.container_registry_uri
  }

  connector_id = zenml_service_connector.base_connector.id
}

resource "zenml_stack_component" "orchestrator" {
  name   = "${var.environment}-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"

  configuration = {
    location      = var.region
    workload_service_account = "${module.base_infrastructure.service_account_email}"
  }

  connector_id = zenml_service_connector.base_connector.id
}

# Create the base stack
resource "zenml_stack" "base_stack" {
  name = "${var.environment}-base-stack"

  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator       = zenml_stack_component.orchestrator.id
  }

  labels = {
    environment = var.environment
    type        = "base"
  }
}
```

Teams can enhance the base stack by adding custom components or functionalities tailored to their specific needs.

```hcl
# team_configs/training_stack.tf

# Add training-specific components
resource "zenml_stack_component" "training_orchestrator" {
  name   = "${var.environment}-training-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"

  configuration = {
    location      = var.region
    machine_type  = "n1-standard-8"
    gpu_enabled   = true
    synchronous   = true
  }

  connector_id = zenml_service_connector.base_connector.id
}

# Create specialized training stack
resource "zenml_stack" "training_stack" {
  name = "${var.environment}-training-stack"

  components = {
    artifact_store     = zenml_stack_component.artifact_store.id
    container_registry = zenml_stack_component.container_registry.id
    orchestrator       = zenml_stack_component.training_orchestrator.id
  }

  labels = {
    environment = var.environment
    type        = "training"
  }
}
```

## Part 2: Environment Management and Authentication

### The Problem
Different environments (dev, staging, prod) necessitate:
- Varied authentication methods and security levels
- Environment-specific resource configurations
- Isolation to prevent cross-environment impacts
- Consistent management patterns with flexibility

### The Solution: Environment Configuration Pattern with Smart Authentication
Implement a flexible service connector setup that adapts to each environment. For instance, use a service account in development and workload identity in production. Combine environment-specific configurations with suitable authentication methods.

```hcl
locals {
  # Define configurations per environment
  env_config = {
    dev = {
      # Resource configuration
      machine_type = "n1-standard-4"
      gpu_enabled  = false
      
      # Authentication configuration
      auth_method = "service-account"
      auth_configuration = {
        service_account_json = file("dev-sa.json")
      }
    }
    prod = {
      # Resource configuration
      machine_type = "n1-standard-8"
      gpu_enabled  = true
      
      # Authentication configuration
      auth_method = "external-account"
      auth_configuration = {
        external_account_json = file("prod-sa.json")
      }
    }
  }
}

# Create environment-specific connector
resource "zenml_service_connector" "env_connector" {
  name        = "${var.environment}-connector"
  type        = "gcp"
  auth_method = local.env_config[var.environment].auth_method

  dynamic "configuration" {
    for_each = try(local.env_config[var.environment].auth_configuration, {})
    content {
      key   = configuration.key
      value = configuration.value
    }
  }
}

# Create environment-specific orchestrator
resource "zenml_stack_component" "env_orchestrator" {
  name   = "${var.environment}-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"
  
  configuration = {
    location     = var.region
    machine_type = local.env_config[var.environment].machine_type
    gpu_enabled  = local.env_config[var.environment].gpu_enabled
  }
  
  connector_id = zenml_service_connector.env_connector.id
  
  labels = {
    environment = var.environment
  }
}
```

## Part 3: Resource Sharing and Isolation 

### The Problem
ML projects require strict data isolation and security to prevent unauthorized access and ensure compliance with security policies. Isolating resources like artifact stores and orchestrators is crucial to prevent data leakage and maintain project integrity.

### The Solution: Resource Scoping Pattern
Implement resource sharing while ensuring project isolation.

```hcl
locals {
  project_paths = {
    fraud_detection = "projects/fraud_detection/${var.environment}"
    recommendation  = "projects/recommendation/${var.environment}"
  }
}

# Create shared artifact store components with project isolation
resource "zenml_stack_component" "project_artifact_stores" {
  for_each = local.project_paths
  
  name   = "${each.key}-artifact-store"
  type   = "artifact_store"
  flavor = "gcp"
  
  configuration = {
    path = "gs://${var.shared_bucket}/${each.value}"
  }
  
  connector_id = zenml_service_connector.env_connector.id
  
  labels = {
    project     = each.key
    environment = var.environment
  }
}

# The orchestrator is shared across all stacks
resource "zenml_stack_component" "project_orchestrator" {
  name   = "shared-orchestrator"
  type   = "orchestrator"
  flavor = "vertex"
  
  configuration = {
    location = var.region
    project  = var.project_id
  }
  
  connector_id = zenml_service_connector.env_connector.id
  
  labels = {
    environment = var.environment
  }
}

# Create project-specific stacks separated by artifact stores
resource "zenml_stack" "project_stacks" {
  for_each = local.project_paths
  
  name = "${each.key}-stack"
  
  components = {
    artifact_store = zenml_stack_component.project_artifact_stores[each.key].id
    orchestrator   = zenml_stack_component.project_orchestrator.id
  }
  
  labels = {
    project     = each.key
    environment = var.environment
  }
}
```

## Part 4: Advanced Stack Management Practices

1. **Stack Component Versioning**: 
   - Implement version control for stack components to ensure compatibility and stability.
   - Use semantic versioning (MAJOR.MINOR.PATCH) to indicate changes: 
     - MAJOR for incompatible changes,
     - MINOR for backward-compatible functionality,
     - PATCH for backward-compatible bug fixes.
   - Maintain a changelog for tracking updates and changes in components.
   - Regularly review and update dependencies to mitigate security vulnerabilities and improve performance.

```hcl
locals {
  stack_version = "1.2.0"
  common_labels = {
    version     = local.stack_version
    managed_by  = "terraform"
    environment = var.environment
  }
}

resource "zenml_stack" "versioned_stack" {
  name   = "stack-v${local.stack_version}"
  labels = local.common_labels
}
```

**Service Connector Management**

This section outlines the management of service connectors, which facilitate communication between different services. Key points include:

- **Creation**: Service connectors can be created through a user interface or API, allowing for customization based on service requirements.
- **Configuration**: Each connector requires specific configurations, including authentication methods, endpoint URLs, and data formats.
- **Monitoring**: Tools are available for monitoring the performance and health of service connectors, ensuring they function correctly and efficiently.
- **Troubleshooting**: Common issues can be resolved through logs and error messages, with guidelines provided for diagnosing and fixing problems.
- **Updates**: Regular updates to connectors may be necessary to maintain compatibility with service changes or improvements.

Overall, effective service connector management is crucial for seamless service integration and operational efficiency.

```hcl
# Create environment-specific connectors with clear purposes
resource "zenml_service_connector" "env_connector" {
  name        = "${var.environment}-${var.purpose}-connector"
  type        = var.connector_type
  
  # Use workload identity for production
  auth_method = var.environment == "prod" ? "workload-identity" : "service-account"
  
  # Use a specific resource type and resource ID
  resource_type = var.resource_type
  resource_id   = var.resource_id
  
  labels = merge(local.common_labels, {
    purpose = var.purpose
  })
}
```

**Component Configuration Management**

This section outlines the processes and practices for managing the configuration of components within a system. Key aspects include:

- **Version Control**: Implementing version control systems to track changes and maintain history of component configurations.
- **Change Management**: Establishing procedures for proposing, reviewing, and approving changes to configurations to ensure stability and compliance.
- **Documentation**: Maintaining up-to-date documentation for each component, including configuration settings, dependencies, and operational procedures.
- **Monitoring and Auditing**: Regularly monitoring configurations and conducting audits to ensure compliance with standards and to identify discrepancies.
- **Backup and Recovery**: Implementing backup strategies for configurations to facilitate recovery in case of failures or errors.

Effective component configuration management ensures system integrity, reliability, and performance.

```hcl
# Define reusable configurations
locals {
  base_configs = {
    orchestrator = {
      location = var.region
      project  = var.project_id
    }
    artifact_store = {
      path_prefix = "gs://${var.bucket_name}"
    }
  }
  
  # Environment-specific overrides
  env_configs = {
    dev = {
      orchestrator = {
        machine_type = "n1-standard-4"
      }
    }
    prod = {
      orchestrator = {
        machine_type = "n1-standard-8"
      }
    }
  }
}

resource "zenml_stack_component" "configured_component" {
  name   = "${var.environment}-${var.component_type}"
  type   = var.component_type
  
  # Merge configurations
  configuration = merge(
    local.base_configs[var.component_type],
    try(local.env_configs[var.environment][var.component_type], {})
  )
}
```

**4. Stack Organization and Dependencies**

This section outlines the structure of the stack and its interdependencies. It details the hierarchy of components, including the core modules and their relationships. Each module's functionality and the required dependencies for proper operation are specified. Additionally, it highlights the importance of maintaining version compatibility among dependencies to ensure system stability. Proper organization of the stack is crucial for efficient resource management and performance optimization.

```hcl
# Group related components with clear dependency chains
module "ml_stack" {
  source = "./modules/ml_stack"
  
  depends_on = [
    module.base_infrastructure,
    module.security
  ]
  
  components = {
    # Core components
    artifact_store     = module.storage.artifact_store_id
    container_registry = module.container.registry_id
    
    # Optional components based on team needs
    orchestrator       = var.needs_orchestrator ? module.compute.orchestrator_id : null
    experiment_tracker = var.needs_tracking ? module.mlflow.tracker_id : null
  }
  
  labels = merge(local.common_labels, {
    stack_type = "ml-platform"
  })
}
```

**State Management**

State management involves handling the state of an application efficiently. Key concepts include:

- **State Definition**: The current condition or data of an application at a specific time.
- **State Types**: 
  - **Local State**: Managed within a component.
  - **Global State**: Shared across multiple components.
  - **Server State**: Data fetched from an external server.
  - **URL State**: Data from the URL, including query parameters.

- **State Management Libraries**: Tools like Redux, MobX, and Context API help manage state effectively, providing predictable state transitions and easier debugging.

- **Best Practices**:
  - Keep state minimal and relevant.
  - Use derived state to compute values from existing state.
  - Ensure state updates are immutable to prevent unintended side effects.

Effective state management enhances application performance, maintainability, and user experience.

```hcl
terraform {
  backend "gcs" {
    prefix = "terraform/state"
  }
  
  # Separate state files for infrastructure and ZenML
  workspace_prefix = "zenml-"
}

# Use data sources to reference infrastructure state
data "terraform_remote_state" "infrastructure" {
  backend = "gcs"
  
  config = {
    bucket = var.state_bucket
    prefix = "terraform/infrastructure"
  }
}
```

To maintain a clean, scalable, and maintainable infrastructure codebase while adhering to infrastructure-as-code best practices, follow these key points:

- Keep configurations DRY using locals and variables.
- Use consistent naming conventions across resources.
- Document all required configuration fields.
- Consider component dependencies when organizing stacks.
- Separate infrastructure from ZenML registration state.
- Utilize [Terraform workspaces](https://www.terraform.io/docs/language/state/workspaces.html) for different environments.
- Ensure the ML operations team manages the registration state for better control over ZenML stack components and configurations, facilitating improved tracking and auditing of changes.

In conclusion, using ZenML and Terraform for ML infrastructure allows for a flexible, maintainable, and secure environment, with the official ZenML provider streamlining the process while upholding clean infrastructure patterns.



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/service-connectors-guide.md

# Service Connectors Guide Summary

This documentation provides a comprehensive guide for managing Service Connectors to connect ZenML with external resources. Key points include:

- **Getting Started**: Familiarize yourself with [terminology](service-connectors-guide.md#terminology) if you're new to Service Connectors.
- **Service Connector Types**: Review the [Service Connector Types](service-connectors-guide.md#cloud-provider-service-connector-types) section to understand different implementations and their use cases.
- **Registering Service Connectors**: For quick setup, refer to [Registering Service Connectors](service-connectors-guide.md#register-service-connectors).
- **Connecting Stack Components**: If you need to connect a ZenML Stack Component to resources like Kubernetes, Docker, or object storage, the section on [connecting Stack Components to resources](service-connectors-guide.md#connect-stack-components-to-resources) is essential.

Additionally, there is a section on [best security practices](best-security-practices.md) related to authentication methods, aimed at engineers but accessible to a broader audience.

## Terminology

Service Connectors involve specific terminology to clarify concepts and operations. Key terms include:

- **Service Connector Types**: Identify implementations and their capabilities, such as supported resources and authentication methods. This is similar to how Flavors function for Stack Components. For instance, the AWS Service Connector Type supports multiple authentication methods and provides access to AWS resources like S3 and EKS. Use `zenml service-connector list-types` and `zenml service-connector describe-type` CLI commands for exploration.

Extensive documentation is available regarding supported authentication methods and Resource Types.

```sh
zenml service-connector list-types
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to assist!

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS      │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password          │ ✅    │ ✅     ┃
┃                              │               │                       │ token             │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃   Docker Service Connector   │ 🐳 docker     │ 🐳 docker-registry    │ password          │ ✅    │ ✅     ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃   Azure Service Connector    │ 🇦 azure      │ 🇦 azure-generic      │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 blob-container     │ service-principal │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ access-token      │       │        ┃
┃                              │               │ 🐳 docker-registry    │                   │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    AWS Service Connector     │ 🔶 aws        │ 🔶 aws-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 s3-bucket          │ secret-key        │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ sts-token         │       │        ┃
┃                              │               │ 🐳 docker-registry    │ iam-role          │       │        ┃
┃                              │               │                       │ session-token     │       │        ┃
┃                              │               │                       │ federation-token  │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    GCP Service Connector     │ 🔵 gcp        │ 🔵 gcp-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 gcs-bucket         │ user-account      │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ service-account   │       │        ┃
┃                              │               │ 🐳 docker-registry    │ oauth2-token      │       │        ┃
┃                              │               │                       │ impersonation     │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

It appears that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist you!

```sh
zenml service-connector describe-type aws
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                🔶 AWS Service Connector (connector type: aws)                ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 secret-key                                                                
 • 🔒 sts-token                                                                 
 • 🔒 iam-role                                                                  
 • 🔒 session-token                                                             
 • 🔒 federation-token                                                          
                                                                                
Resource types:                                                                 
                                                                                
 • 🔶 aws-generic                                                               
 • 📦 s3-bucket                                                                 
 • 🌀 kubernetes-cluster                                                        
 • 🐳 docker-registry                                                           
                                                                                
Supports auto-configuration: True                                               
                                                                                
Available locally: True                                                         
                                                                                
Available remotely: False                                                       
                                                                                
The ZenML AWS Service Connector facilitates the authentication and access to    
managed AWS services and resources. These encompass a range of resources,       
including S3 buckets, ECR repositories, and EKS clusters. The connector provides
support for various authentication methods, including explicit long-lived AWS   
secret keys, IAM roles, short-lived STS tokens and implicit authentication.     
                                                                                
To ensure heightened security measures, this connector also enables the         
generation of temporary STS security tokens that are scoped down to the minimum 
permissions necessary for accessing the intended resource. Furthermore, it      
includes automatic configuration and detection of credentials locally configured
through the AWS CLI.                                                            
                                                                                
This connector serves as a general means of accessing any AWS service by issuing
pre-authenticated boto3 sessions to clients. Additionally, the connector can    
handle specialized authentication for S3, Docker and Kubernetes Python clients. 
It also allows for the configuration of local Docker and Kubernetes CLIs.       
                                                                                
The AWS Service Connector is part of the AWS ZenML integration. You can either  
install the entire integration or use a pypi extra to install it independently  
of the integration:                                                             
                                                                                
 • pip install "zenml[connectors-aws]" installs only prerequisites for the AWS    
   Service Connector Type                                                       
 • zenml integration install aws installs the entire AWS ZenML integration      
                                                                                
It is not required to install and set up the AWS CLI on your local machine to   
use the AWS Service Connector to link Stack Components to AWS resources and     
services. However, it is recommended to do so if you are looking for a quick    
setup that includes using the auto-configuration Service Connector features.    
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist!

```sh
zenml service-connector describe-type aws --resource-type kubernetes-cluster
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║      🌀 AWS EKS Kubernetes cluster (resource type: kubernetes-cluster)       ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods: implicit, secret-key, sts-token, iam-role,              
session-token, federation-token                                                 
                                                                                
Supports resource instances: True                                               
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 secret-key                                                                
 • 🔒 sts-token                                                                 
 • 🔒 iam-role                                                                  
 • 🔒 session-token                                                             
 • 🔒 federation-token                                                          
                                                                                
Allows users to access an EKS cluster as a standard Kubernetes cluster resource.
When used by Stack Components, they are provided a pre-authenticated            
python-kubernetes client instance.                                              
                                                                                
The configured credentials must have at least the following AWS IAM permissions 
associated with the ARNs of EKS clusters that the connector will be allowed to  
access (e.g. arn:aws:eks:{region}:{account}:cluster/* represents all the EKS    
clusters available in the target AWS region).                                   
                                                                                
 • eks:ListClusters                                                             
 • eks:DescribeCluster                                                          
                                                                                
In addition to the above permissions, if the credentials are not associated with
the same IAM user or role that created the EKS cluster, the IAM principal must  
be manually added to the EKS cluster's aws-auth ConfigMap, otherwise the        
Kubernetes client will not be allowed to access the cluster's resources. This   
makes it more challenging to use the AWS Implicit and AWS Federation Token      
authentication methods for this resource. For more information, see this        
documentation.                                                                  
                                                                                
If set, the resource name must identify an EKS cluster using one of the         
following formats:                                                              
                                                                                
 • EKS cluster name (canonical resource name): {cluster-name}                   
 • EKS cluster ARN: arn:aws:eks:{region}:{account}:cluster/{cluster-name}       
                                                                                
EKS cluster names are region scoped. The connector can only be used to access   
EKS clusters in the AWS region that it is configured to use.                    
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I will be happy to assist!

```sh
zenml service-connector describe-type aws --auth-method secret-key
```

It seems that the text you provided is incomplete and only contains a code title without any actual content or documentation to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                 🔒 AWS Secret Key (auth method: secret-key)                  ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Supports issuing temporary credentials: False                                   
                                                                                
Long-lived AWS credentials consisting of an AWS access key ID and secret access 
key associated with an AWS IAM user or AWS account root user (not recommended). 
                                                                                
This method is preferred during development and testing due to its simplicity   
and ease of use. It is not recommended as a direct authentication method for    
production use cases because the clients have direct access to long-lived       
credentials and are granted the full set of permissions of the IAM user or AWS  
account root user associated with the credentials. For production, it is        
recommended to use the AWS IAM Role, AWS Session Token or AWS Federation Token  
authentication method instead.                                                  
                                                                                
An AWS region is required and the connector may only be used to access AWS      
resources in the specified region.                                              
                                                                                
If you already have the local AWS CLI set up with these credentials, they will  
be automatically picked up when auto-configuration is used.                     
                                                                                
Attributes:                                                                     
                                                                                
 • aws_access_key_id {string, secret, required}: AWS Access Key ID              
 • aws_secret_access_key {string, secret, required}: AWS Secret Access Key      
 • region {string, required}: AWS Region                                        
 • endpoint_url {string, optional}: AWS Endpoint URL                            
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

### Resource Types

Resource Types organize resources into logical classes based on access standards, protocols, or vendors, creating a unified language for Service Connectors and Stack Components. For instance, the `kubernetes-cluster` resource type encompasses all Kubernetes clusters, regardless of whether they are Amazon EKS, Google GKE, Azure AKS, or other deployments, as they share standard libraries and APIs. Similarly, the `docker-registry` resource type includes all container registries that follow the Docker/OCI interface, such as DockerHub, Amazon ECR, and others. Stack Components can use these resource type identifiers to describe their requirements without vendor specificity. The term Resource Type is consistently used in ZenML for resources accessed through Service Connectors. To list Service Connector Types for Kubernetes Clusters, use the `--resource-type` flag in the CLI command.

```sh
zenml service-connector list-types --resource-type kubernetes-cluster
```

It appears that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I'll be happy to assist you!

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS      │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password          │ ✅    │ ✅     ┃
┃                              │               │                       │ token             │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃   Azure Service Connector    │ 🇦 azure      │ 🇦 azure-generic      │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 blob-container     │ service-principal │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ access-token      │       │        ┃
┃                              │               │ 🐳 docker-registry    │                   │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    AWS Service Connector     │ 🔶 aws        │ 🔶 aws-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 s3-bucket          │ secret-key        │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ sts-token         │       │        ┃
┃                              │               │ 🐳 docker-registry    │ iam-role          │       │        ┃
┃                              │               │                       │ session-token     │       │        ┃
┃                              │               │                       │ federation-token  │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    GCP Service Connector     │ 🔵 gcp        │ 🔵 gcp-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 gcs-bucket         │ user-account      │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ service-account   │       │        ┃
┃                              │               │ 🐳 docker-registry    │ oauth2-token      │       │        ┃
┃                              │               │                       │ impersonation     │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

ZenML offers four Service Connector Types for connecting to Kubernetes clusters: one generic implementation for any standard Kubernetes cluster (including on-premise) and three specific to AWS, GCP, and Azure-managed Kubernetes services. To list all registered Service Connector instances for Kubernetes access, use the appropriate command.

```sh
zenml service-connector list --resource_type kubernetes-cluster
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━┓
┃ ACTIVE │ NAME                  │ ID                           │ TYPE          │ RESOURCE TYPES        │ RESOURCE NAME                │ SHARED │ OWNER   │ EXPIRES IN │ LABELS              ┃
┠────────┼───────────────────────┼──────────────────────────────┼───────────────┼───────────────────────┼──────────────────────────────┼────────┼─────────┼────────────┼─────────────────────┨
┃        │ aws-iam-multi-eu      │ e33c9fac-5daa-48b2-87bb-0187 │ 🔶 aws        │ 🔶 aws-generic        │ <multiple>                   │ ➖     │ default │            │ region:eu-central-1 ┃
┃        │                       │ d3782cde                     │               │ 📦 s3-bucket          │                              │        │         │            │                     ┃
┃        │                       │                              │               │ 🌀 kubernetes-cluster │                              │        │         │            │                     ┃
┃        │                       │                              │               │ 🐳 docker-registry    │                              │        │         │            │                     ┃
┠────────┼───────────────────────┼──────────────────────────────┼───────────────┼───────────────────────┼──────────────────────────────┼────────┼─────────┼────────────┼─────────────────────┨
┃        │ aws-iam-multi-us      │ ed528d5a-d6cb-4fc4-bc52-c3d2 │ 🔶 aws        │ 🔶 aws-generic        │ <multiple>                   │ ➖     │ default │            │ region:us-east-1    ┃
┃        │                       │ d01643e5                     │               │ 📦 s3-bucket          │                              │        │         │            │                     ┃
┃        │                       │                              │               │ 🌀 kubernetes-cluster │                              │        │         │            │                     ┃
┃        │                       │                              │               │ 🐳 docker-registry    │                              │        │         │            │                     ┃
┠────────┼───────────────────────┼──────────────────────────────┼───────────────┼───────────────────────┼──────────────────────────────┼────────┼─────────┼────────────┼─────────────────────┨
┃        │ kube-auto             │ da497715-7502-4cdd-81ed-289e │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ A5F8F4142FB12DDCDE9F21F6E9B0 │ ➖     │ default │            │                     ┃
┃        │                       │ 70664597                     │               │                       │ 7A18.gr7.us-east-1.eks.amazo │        │         │            │                     ┃
┃        │                       │                              │               │                       │ naws.com                     │        │         │            │                     ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━┛
```

### Resource Names (Resource IDs)

Resource Names uniquely identify instances of a Resource Type within a Service Connector. For example, an AWS Service Connector can access multiple S3 buckets by their bucket names or `s3://bucket-name` URIs, and multiple EKS clusters by their cluster names. Resource Names simplify the identification of specific resource instances when used alongside the Service Connector name and Resource Type. Examples of Resource Names for S3 buckets, EKS clusters, ECR registries, and Kubernetes clusters can vary based on implementation and resource type.

```sh
zenml service-connector list-resources
```

It seems there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist!

```
The following resources can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME        │ CONNECTOR TYPE │ RESOURCE TYPE         │ RESOURCE NAMES                                                   ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ 8d307b98-f125-4d7a-b5d5-924c07ba04bb │ aws-session-docker    │ 🔶 aws         │ 🐳 docker-registry    │ 715803424590.dkr.ecr.us-east-1.amazonaws.com                     ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ d1e5ecf5-1531-4507-bbf5-be0a114907a5 │ aws-session-s3        │ 🔶 aws         │ 📦 s3-bucket          │ s3://public-flavor-logos                                         ┃
┃                                      │                       │                │                       │ s3://sagemaker-us-east-1-715803424590                            ┃
┃                                      │                       │                │                       │ s3://spark-artifact-store                                        ┃
┃                                      │                       │                │                       │ s3://spark-demo-as                                               ┃
┃                                      │                       │                │                       │ s3://spark-demo-dataset                                          ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ d2341762-28a3-4dfc-98b9-1ae9aaa93228 │ aws-key-docker-eu     │ 🔶 aws         │ 🐳 docker-registry    │ 715803424590.dkr.ecr.eu-central-1.amazonaws.com                  ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ 0658a465-2921-4d6b-a495-2dc078036037 │ aws-key-kube-zenhacks │ 🔶 aws         │ 🌀 kubernetes-cluster │ zenhacks-cluster                                                 ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ 049e7f5e-e14c-42b7-93d4-a273ef414e66 │ eks-eu-central-1      │ 🔶 aws         │ 🌀 kubernetes-cluster │ kubeflowmultitenant                                              ┃
┃                                      │                       │                │                       │ zenbox                                                           ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼──────────────────────────────────────────────────────────────────┨
┃ b551f3ae-1448-4f36-97a2-52ce303f20c9 │ kube-auto             │ 🌀 kubernetes  │ 🌀 kubernetes-cluster │ A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

Each Service Connector Type has specific rules for formatting Resource Names, which are detailed in the corresponding section for each resource type.

```sh
zenml service-connector describe-type aws --resource-type docker-registry
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║        🐳 AWS ECR container registry (resource type: docker-registry)        ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods: implicit, secret-key, sts-token, iam-role,              
session-token, federation-token                                                 
                                                                                
Supports resource instances: False                                              
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 secret-key                                                                
 • 🔒 sts-token                                                                 
 • 🔒 iam-role                                                                  
 • 🔒 session-token                                                             
 • 🔒 federation-token                                                          
                                                                                
Allows users to access one or more ECR repositories as a standard Docker        
registry resource. When used by Stack Components, they are provided a           
pre-authenticated python-docker client instance.                                
                                                                                
The configured credentials must have at least the following AWS IAM permissions 
associated with the ARNs of one or more ECR repositories that the connector will
be allowed to access (e.g. arn:aws:ecr:{region}:{account}:repository/*          
represents all the ECR repositories available in the target AWS region).        
                                                                                
 • ecr:DescribeRegistry                                                         
 • ecr:DescribeRepositories                                                     
 • ecr:ListRepositories                                                         
 • ecr:BatchGetImage                                                            
 • ecr:DescribeImages                                                           
 • ecr:BatchCheckLayerAvailability                                              
 • ecr:GetDownloadUrlForLayer                                                   
 • ecr:InitiateLayerUpload                                                      
 • ecr:UploadLayerPart                                                          
 • ecr:CompleteLayerUpload                                                      
 • ecr:PutImage                                                                 
 • ecr:GetAuthorizationToken                                                    
                                                                                
This resource type is not scoped to a single ECR repository. Instead, a         
connector configured with this resource type will grant access to all the ECR   
repositories that the credentials are allowed to access under the configured AWS
region (i.e. all repositories under the Docker registry URL                     
https://{account-id}.dkr.ecr.{region}.amazonaws.com).                           
                                                                                
The resource name associated with this resource type uniquely identifies an ECR 
registry using one of the following formats (the repository name is ignored,    
only the registry URL/ARN is used):                                             
                                                                                
 • ECR repository URI (canonical resource name):                                
   [https://]{account}.dkr.ecr.{region}.amazonaws.com[/{repository-name}]       
 • ECR repository ARN:                                                          
   arn:aws:ecr:{region}:{account-id}:repository[/{repository-name}]             
                                                                                
ECR repository names are region scoped. The connector can only be used to access
ECR repositories in the AWS region that it is configured to use.                
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

### Service Connectors

The Service Connector in ZenML is used to authenticate and connect to external resources, storing configuration and security credentials. It can be scoped with a Resource Type and Resource Name. 

**Modes of Configuration:**
1. **Multi-Type Service Connector**: Configured to access multiple resource types, applicable for connectors supporting multiple Resource Types (e.g., AWS, GCP, Azure). To create one, do not scope its Resource Type during registration.
   
2. **Multi-Instance Service Connector**: Configured to access multiple resources of the same type, each identified by a Resource Name. Not all connectors support this; for example, Kubernetes and Docker connectors only allow single-instance configurations. To create a multi-instance connector, do not scope its Resource Name during registration.

**Example**: Configuring a multi-type AWS Service Connector to access various AWS resources.

```sh
zenml service-connector register aws-multi-type --type aws --auto-configure
```

It seems that the text you provided is incomplete and only contains a code title without any actual content or documentation to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to help!

```
⠋ Registering service connector 'aws-multi-type'...
Successfully registered service connector `aws-multi-type` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://aws-ia-mwaa-715803424590                ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┃                       │ s3://zenml-public-datasets                   ┃
┃                       │ s3://zenml-public-swagger-spec               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

This documentation provides an example of configuring a multi-instance AWS S3 Service Connector that can access multiple AWS S3 buckets.

```sh
zenml service-connector register aws-s3-multi-instance --type aws --auto-configure --resource-type s3-bucket
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist!

```
⠸ Registering service connector 'aws-s3-multi-instance'...
Successfully registered service connector `aws-s3-multi-instance` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://aws-ia-mwaa-715803424590         ┃
┃               │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┃               │ s3://zenml-public-datasets            ┃
┃               │ s3://zenml-public-swagger-spec        ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

This documentation provides a configuration example for a single-instance AWS S3 Service Connector that accesses a single AWS S3 bucket.

```sh
zenml service-connector register aws-s3-zenfiles --type aws --auto-configure --resource-type s3-bucket --resource-id s3://zenfiles
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like summarized, and I'll be happy to help!

```
⠼ Registering service connector 'aws-s3-zenfiles'...
Successfully registered service connector `aws-s3-zenfiles` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES ┃
┠───────────────┼────────────────┨
┃ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

## Explore Service Connector Types

Service Connector Types serve as templates for instantiating Service Connectors and provide documentation on best security practices for authentication and authorization. ZenML includes several built-in Service Connector Types for connecting to cloud resources from providers like AWS and GCP, as well as on-premise infrastructure. Users can also create custom Service Connector implementations. To view available Connector Types in your ZenML deployment, use the command: `zenml service-connector list-types`.

```sh
zenml service-connector list-types
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS      │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password          │ ✅    │ ✅     ┃
┃                              │               │                       │ token             │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃   Docker Service Connector   │ 🐳 docker     │ 🐳 docker-registry    │ password          │ ✅    │ ✅     ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃   Azure Service Connector    │ 🇦 azure      │ 🇦 azure-generic      │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 blob-container     │ service-principal │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ access-token      │       │        ┃
┃                              │               │ 🐳 docker-registry    │                   │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    AWS Service Connector     │ 🔶 aws        │ 🔶 aws-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 s3-bucket          │ secret-key        │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ sts-token         │       │        ┃
┃                              │               │ 🐳 docker-registry    │ iam-role          │       │        ┃
┃                              │               │                       │ session-token     │       │        ┃
┃                              │               │                       │ federation-token  │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃    GCP Service Connector     │ 🔵 gcp        │ 🔵 gcp-generic        │ implicit          │ ✅    │ ✅     ┃
┃                              │               │ 📦 gcs-bucket         │ user-account      │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ service-account   │       │        ┃
┃                              │               │ 🐳 docker-registry    │ oauth2-token      │       │        ┃
┃                              │               │                       │ impersonation     │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

### Summary of Service Connector Types Documentation

Service Connector Types encompass more than just a name and resource types; understanding their capabilities, supported authentication methods, and requirements is essential before configuration. This information can be accessed via the CLI. Below are examples illustrating details about the `gcp` Service Connector Type.

```sh
zenml service-connector describe-type gcp
```

It seems that you provided a placeholder for code but did not include the actual documentation text to summarize. Please provide the text you would like summarized, and I will assist you accordingly.

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                🔵 GCP Service Connector (connector type: gcp)                ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 user-account                                                              
 • 🔒 service-account                                                           
 • 🔒 oauth2-token                                                              
 • 🔒 impersonation                                                             
                                                                                
Resource types:                                                                 
                                                                                
 • 🔵 gcp-generic                                                               
 • 📦 gcs-bucket                                                                
 • 🌀 kubernetes-cluster                                                        
 • 🐳 docker-registry                                                           
                                                                                
Supports auto-configuration: True                                               
                                                                                
Available locally: True                                                         
                                                                                
Available remotely: True                                                        
                                                                                
The ZenML GCP Service Connector facilitates the authentication and access to    
managed GCP services and resources. These encompass a range of resources,       
including GCS buckets, GCR container repositories and GKE clusters. The         
connector provides support for various authentication methods, including GCP    
user accounts, service accounts, short-lived OAuth 2.0 tokens and implicit      
authentication.                                                                 
                                                                                
To ensure heightened security measures, this connector always issues short-lived
OAuth 2.0 tokens to clients instead of long-lived credentials. Furthermore, it  
includes automatic configuration and detection of  credentials locally          
configured through the GCP CLI.                                                 
                                                                                
This connector serves as a general means of accessing any GCP service by issuing
OAuth 2.0 credential objects to clients. Additionally, the connector can handle 
specialized authentication for GCS, Docker and Kubernetes Python clients. It    
also allows for the configuration of local Docker and Kubernetes CLIs.          
                                                                                
The GCP Service Connector is part of the GCP ZenML integration. You can either  
install the entire integration or use a pypi extra to install it independently  
of the integration:                                                             
                                                                                
 • pip install "zenml[connectors-gcp]" installs only prerequisites for the GCP    
   Service Connector Type                                                       
 • zenml integration install gcp installs the entire GCP ZenML integration      
                                                                                
It is not required to install and set up the GCP CLI on your local machine to   
use the GCP Service Connector to link Stack Components to GCP resources and     
services. However, it is recommended to do so if you are looking for a quick    
setup that includes using the auto-configuration Service Connector features.    
                                                                                
──────────────────────────────────────────────────────────────────────────────────
```

To fetch details about the GCP `kubernetes-cluster` resource type (GKE cluster), use the appropriate API or command-line tools. Ensure you have the necessary permissions and authentication set up. Key details to retrieve include cluster name, location, status, node configuration, and network settings. Use specific commands or API calls to access this information efficiently.

```sh
zenml service-connector describe-type gcp --resource-type kubernetes-cluster
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║      🌀 GCP GKE Kubernetes cluster (resource type: kubernetes-cluster)       ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods: implicit, user-account, service-account, oauth2-token,  
impersonation                                                                   
                                                                                
Supports resource instances: True                                               
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 user-account                                                              
 • 🔒 service-account                                                           
 • 🔒 oauth2-token                                                              
 • 🔒 impersonation                                                             
                                                                                
Allows Stack Components to access a GKE registry as a standard Kubernetes       
cluster resource. When used by Stack Components, they are provided a            
pre-authenticated Python Kubernetes client instance.                            
                                                                                
The configured credentials must have at least the following GCP permissions     
associated with the GKE clusters that it can access:                            
                                                                                
 • container.clusters.list                                                      
 • container.clusters.get                                                       
                                                                                
In addition to the above permissions, the credentials should include permissions
to connect to and use the GKE cluster (i.e. some or all permissions in the      
Kubernetes Engine Developer role).                                              
                                                                                
If set, the resource name must identify an GKE cluster using one of the         
following formats:                                                              
                                                                                
 • GKE cluster name: {cluster-name}                                             
                                                                                
GKE cluster names are project scoped. The connector can only be used to access  
GKE clusters in the GCP project that it is configured to use.                   
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

The documentation outlines the `service-account` authentication method for Google Cloud Platform (GCP). It provides details on how to display information related to this method, emphasizing its role in managing access and permissions for applications and services. Key points include the configuration requirements, usage scenarios, and best practices for implementing service account authentication securely.

```sh
zenml service-connector describe-type gcp --auth-method service-account
```

It seems there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║            🔒 GCP Service Account (auth method: service-account)             ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Supports issuing temporary credentials: False                                   
                                                                                
Use a GCP service account and its credentials to authenticate to GCP services.  
This method requires a GCP service account and a service account key JSON       
created for it.                                                                 
                                                                                
The GCP connector generates temporary OAuth 2.0 tokens from the user account    
credentials and distributes them to clients. The tokens have a limited lifetime 
of 1 hour.                                                                      
                                                                                
A GCP project is required and the connector may only be used to access GCP      
resources in the specified project.                                             
                                                                                
If you already have the GOOGLE_APPLICATION_CREDENTIALS environment variable     
configured to point to a service account key JSON file, it will be automatically
picked up when auto-configuration is used.                                      
                                                                                
Attributes:                                                                     
                                                                                
 • service_account_json {string, secret, required}: GCP Service Account Key JSON
 • project_id {string, required}: GCP Project ID where the target resource is   
   located.                                                                     
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

### Basic Service Connector Types

Service Connector Types, such as the [Kubernetes Service Connector](kubernetes-service-connector.md) and [Docker Service Connector](docker-service-connector.md), manage one resource at a time: a Kubernetes cluster and a Docker container registry, respectively. These are single-instance connectors, making them easy to instantiate and manage. 

Example configurations include:
- **Docker Service Connector**: Grants authenticated access to DockerHub, enabling image push/pull for private repositories.
- **Kubernetes Service Connector**: Authenticates access to an on-premise Kubernetes cluster for managing containerized workloads.

```
$ zenml service-connector list
┏━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME           │ ID                                   │ TYPE          │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼────────────────┼──────────────────────────────────────┼───────────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ dockerhub      │ b485626e-7fee-4525-90da-5b26c72331eb │ 🐳 docker     │ 🐳 docker-registry    │ docker.io     │ ➖     │ default │            │        ┃
┠────────┼────────────────┼──────────────────────────────────────┼───────────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ kube-on-prem   │ 4315e8eb-fcbd-4938-a4d7-a9218ab372a1 │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ 192.168.0.12  │ ➖     │ default │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛

```

### Cloud Provider Service Connector Types

Cloud service providers (AWS, GCP, Azure) implement unified authentication schemes for accessing various resources with a single set of credentials. Authentication methods vary in complexity and suitability for development or production environments:

- **Resource Support**: Service Connectors support multiple resource types (e.g., Kubernetes clusters, Docker registries, object storage) and include a "generic" Resource Type for accessing unsupported resources. For instance, using the `aws-generic` Resource Type provides a pre-authenticated `boto3` Session for AWS services.

- **Authentication Methods**: 
  - Some methods offer direct access to long-lived credentials, suitable for local development.
  - Others distribute temporary API tokens from long-lived credentials, enhancing security for production but requiring more setup.
  - Certain methods allow down-scoping of permissions for temporary tokens to limit access to specific resources.

- **Resource Access Flexibility**:
  - **Multi-type Service Connector**: Accesses any resource type within supported Resource Types.
  - **Multi-instance Service Connector**: Accesses multiple resources of the same type.
  - **Single-instance Service Connector**: Accesses a single resource.

Example configurations from the same GCP Service Connector Type demonstrate varying scopes with identical credentials:
- A multi-type GCP Service Connector for all resources.
- A multi-instance GCS Service Connector for multiple GCS buckets.
- A single-instance GCS Service Connector for one GCS bucket.

```
$ zenml service-connector list
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME                   │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME           │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼────────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼─────────────────────────┼────────┼─────────┼────────────┼────────┨
┃        │ gcp-multi              │ 9d953320-3560-4a78-817c-926a3898064d │ 🔵 gcp │ 🔵 gcp-generic        │ <multiple>              │ ➖     │ default │            │        ┃
┃        │                        │                                      │        │ 📦 gcs-bucket         │                         │        │         │            │        ┃
┃        │                        │                                      │        │ 🌀 kubernetes-cluster │                         │        │         │            │        ┃
┃        │                        │                                      │        │ 🐳 docker-registry    │                         │        │         │            │        ┃
┠────────┼────────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼─────────────────────────┼────────┼─────────┼────────────┼────────┨
┃        │ gcs-multi              │ ff9c0723-7451-46b7-93ef-fcf3efde30fa │ 🔵 gcp │ 📦 gcs-bucket         │ <multiple>              │ ➖     │ default │            │        ┃
┠────────┼────────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼─────────────────────────┼────────┼─────────┼────────────┼────────┨
┃        │ gcs-langchain-slackbot │ cf3953e9-414c-4875-ba00-24c62a0dc0c5 │ 🔵 gcp │ 📦 gcs-bucket         │ gs://langchain-slackbot │ ➖     │ default │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

### Local and Remote Availability

Local and remote availability for Service Connector Types is relevant when using a Service Connector Type without its package prerequisites or implementing a custom Service Connector Type in ZenML. The `LOCAL` and `REMOTE` flags in the `zenml service-connector list-types` output indicate availability in the local environment (where the ZenML client and pipelines run) and remote environment (where the ZenML server runs).

All built-in Service Connector Types are available on the ZenML server by default, but some require additional Python packages for local availability. Refer to the specific Service Connector Type documentation for prerequisites and installation instructions.

Local/remote availability affects the actions that can be performed with a Service Connector:

**Available Actions (Local or Remote):**
- Register, update, and discover Service Connectors (`zenml service-connector register`, `update`, `list`, `describe`).
- Verify configuration and credentials (`zenml service-connector verify`).
- List accessible resources (`zenml service-connector list-resources`).
- Connect a Stack Component to a remote resource.

**Available Actions (Locally Available Only):**
- Auto-configure and discover credentials stored by a local client, CLI, or SDK.
- Use Service Connector-managed configuration and credentials for local clients, CLIs, or SDKs.
- Run pipelines with a Stack Component connected to a remote resource.

Notably, cloud provider Service Connectors do not need to be available client-side to access some resources. For example:
- The GCP Service Connector Type allows access to GKE clusters and GCR registries without needing GCP libraries on the ZenML client.
- The Kubernetes Service Connector Type can access any Kubernetes cluster, regardless of its cloud provider.
- The Docker Service Connector Type can access any Docker registry, regardless of its cloud provider.

### Register Service Connectors

When registering Service Connectors, consider your infrastructure or cloud provider choice and authentication methods. For first-time users, the interactive CLI mode is recommended for configuring Service Connectors.

```
zenml service-connector register -i
```

The Interactive Service Connector registration example outlines the steps for registering a service connector. Key points include:

1. **Prerequisites**: Ensure you have the necessary permissions and access to the service environment.
2. **Registration Process**:
   - Use the provided API endpoint for registration.
   - Include required parameters such as service name, version, and configuration details.
3. **Response Handling**: Upon successful registration, expect a confirmation response with the service ID and status.
4. **Error Management**: Be prepared to handle common errors, such as invalid parameters or authentication failures.

This summary captures the essential steps and considerations for registering an Interactive Service Connector.

```sh
zenml service-connector register -i
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to assist you!

```
Please enter a name for the service connector: gcp-interactive
Please enter a description for the service connector []: Interactive GCP connector example
╔══════════════════════════════════════════════════════════════════════════════╗
║                      Available service connector types                       ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
                                                                                
          🌀 Kubernetes Service Connector (connector type: kubernetes)          
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 password                                                                  
 • 🔒 token                                                                     
                                                                                
Resource types:                                                                 
                                                                                
 • 🌀 kubernetes-cluster                                                        
                                                                                
Supports auto-configuration: True                                               
                                                                                
Available locally: True                                                         
                                                                                
Available remotely: True                                                        
                                                                                
This ZenML Kubernetes service connector facilitates authenticating and          
connecting to a Kubernetes cluster.                                             
                                                                                
The connector can be used to access to any generic Kubernetes cluster by        
providing pre-authenticated Kubernetes python clients to Stack Components that  
are linked to it and also allows configuring the local Kubernetes CLI (i.e.     
kubectl).                                                                       
                                                                                
The Kubernetes Service Connector is part of the Kubernetes ZenML integration.   
You can either install the entire integration or use a pypi extra to install it 
independently of the integration:                                               
                                                                                
 • pip install "zenml[connectors-kubernetes]" installs only prerequisites for the 
   Kubernetes Service Connector Type                                            
 • zenml integration install kubernetes installs the entire Kubernetes ZenML    
   integration                                                                  
                                                                                
A local Kubernetes CLI (i.e. kubectl ) and setting up local kubectl             
configuration contexts is not required to access Kubernetes clusters in your    
Stack Components through the Kubernetes Service Connector.                      
                                                                                
                                                                                
              🐳 Docker Service Connector (connector type: docker)              
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 password                                                                  
                                                                                
Resource types:                                                                 
                                                                                
 • 🐳 docker-registry                                                           
                                                                                
Supports auto-configuration: False                                              
                                                                                
Available locally: True                                                         
                                                                                
Available remotely: True                                                        
                                                                                
The ZenML Docker Service Connector allows authenticating with a Docker or OCI   
container registry and managing Docker clients for the registry.                
                                                                                
This connector provides pre-authenticated python-docker Python clients to Stack 
Components that are linked to it.                                               
                                                                                
No Python packages are required for this Service Connector. All prerequisites   
are included in the base ZenML Python package. Docker needs to be installed on  
environments where container images are built and pushed to the target container
registry.                                                                       

[...]


────────────────────────────────────────────────────────────────────────────────
Please select a service connector type (kubernetes, docker, azure, aws, gcp): gcp
╔══════════════════════════════════════════════════════════════════════════════╗
║                           Available resource types                           ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
                                                                                
              🔵 Generic GCP resource (resource type: gcp-generic)              
                                                                                
Authentication methods: implicit, user-account, service-account, oauth2-token,  
impersonation                                                                   
                                                                                
Supports resource instances: False                                              
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 user-account                                                              
 • 🔒 service-account                                                           
 • 🔒 oauth2-token                                                              
 • 🔒 impersonation                                                             
                                                                                
This resource type allows Stack Components to use the GCP Service Connector to  
connect to any GCP service or resource. When used by Stack Components, they are 
provided a Python google-auth credentials object populated with a GCP OAuth 2.0 
token. This credentials object can then be used to create GCP Python clients for
any particular GCP service.                                                     
                                                                                
This generic GCP resource type is meant to be used with Stack Components that   
are not represented by other, more specific resource type, like GCS buckets,    
Kubernetes clusters or Docker registries. For example, it can be used with the  
Google Cloud Builder Image Builder stack component, or the Vertex AI            
Orchestrator and Step Operator. It should be accompanied by a matching set of   
GCP permissions that allow access to the set of remote resources required by the
client and Stack Component.                                                     
                                                                                
The resource name represents the GCP project that the connector is authorized to
access.                                                                         
                                                                                
                                                                                
                 📦 GCP GCS bucket (resource type: gcs-bucket)                  
                                                                                
Authentication methods: implicit, user-account, service-account, oauth2-token,  
impersonation                                                                   
                                                                                
Supports resource instances: True                                               
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 user-account                                                              
 • 🔒 service-account                                                           
 • 🔒 oauth2-token                                                              
 • 🔒 impersonation                                                             
                                                                                
Allows Stack Components to connect to GCS buckets. When used by Stack           
Components, they are provided a pre-configured GCS Python client instance.      
                                                                                
The configured credentials must have at least the following GCP permissions     
associated with the GCS buckets that it can access:                             
                                                                                
 • storage.buckets.list                                                         
 • storage.buckets.get                                                          
 • storage.objects.create                                                       
 • storage.objects.delete                                                       
 • storage.objects.get                                                          
 • storage.objects.list                                                         
 • storage.objects.update                                                       
                                                                                
For example, the GCP Storage Admin role includes all of the required            
permissions, but it also includes additional permissions that are not required  
by the connector.                                                               
                                                                                
If set, the resource name must identify a GCS bucket using one of the following 
formats:                                                                        
                                                                                
 • GCS bucket URI: gs://{bucket-name}                                           
 • GCS bucket name: {bucket-name}

[...]

────────────────────────────────────────────────────────────────────────────────
Please select a resource type or leave it empty to create a connector that can be used to access any of the supported resource types (gcp-generic, gcs-bucket, kubernetes-cluster, docker-registry). []: gcs-bucket
Would you like to attempt auto-configuration to extract the authentication configuration from your local environment ? [y/N]: y
Service connector auto-configured successfully with the following configuration:
Service connector 'gcp-interactive' of type 'gcp' is 'private'.
    'gcp-interactive' gcp Service     
          Connector Details           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE           ┃
┠──────────────────┼─────────────────┨
┃ NAME             │ gcp-interactive ┃
┠──────────────────┼─────────────────┨
┃ TYPE             │ 🔵 gcp          ┃
┠──────────────────┼─────────────────┨
┃ AUTH METHOD      │ user-account    ┃
┠──────────────────┼─────────────────┨
┃ RESOURCE TYPES   │ 📦 gcs-bucket   ┃
┠──────────────────┼─────────────────┨
┃ RESOURCE NAME    │ <multiple>      ┃
┠──────────────────┼─────────────────┨
┃ SESSION DURATION │ N/A             ┃
┠──────────────────┼─────────────────┨
┃ EXPIRES IN       │ N/A             ┃
┠──────────────────┼─────────────────┨
┃ SHARED           │ ➖              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━┛
          Configuration           
┏━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY          │ VALUE      ┃
┠───────────────────┼────────────┨
┃ project_id        │ zenml-core ┃
┠───────────────────┼────────────┨
┃ user_account_json │ [HIDDEN]   ┃
┗━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┛
No labels are set for this service connector.
The service connector configuration has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                                  ┃
┠───────────────┼─────────────────────────────────────────────────┨
┃ 📦 gcs-bucket │ gs://annotation-gcp-store                       ┃
┃               │ gs://zenml-bucket-sl                            ┃
┃               │ gs://zenml-core.appspot.com                     ┃
┃               │ gs://zenml-core_cloudbuild                      ┃
┃               │ gs://zenml-datasets                             ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
Would you like to continue with the auto-discovered configuration or switch to manual ? (auto, manual) [auto]: 
The following GCP GCS bucket instances are reachable through this connector:
 - gs://annotation-gcp-store
 - gs://zenml-bucket-sl
 - gs://zenml-core.appspot.com
 - gs://zenml-core_cloudbuild
 - gs://zenml-datasets
Please select one or leave it empty to create a connector that can be used to access any of them []: gs://zenml-datasets
Successfully registered service connector `gcp-interactive` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES      ┃
┠───────────────┼─────────────────────┨
┃ 📦 gcs-bucket │ gs://zenml-datasets ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━┛
```

To connect ZenML to resources such as Kubernetes clusters, Docker container registries, or object storage services (e.g., AWS S3, GCS), consider the following:

1. **Resource Type**: Identify the resources you want to connect to.
2. **Service Connector Implementation**: Choose a Service Connector Type, either a cloud provider type (e.g., AWS, GCP) for broader access or a basic type (e.g., Kubernetes, Docker) for specific resources.
3. **Credentials and Authentication**: Determine the authentication method and ensure all prerequisites (service accounts, roles, permissions) are provisioned.

Consider whether you need to connect a single ZenML Stack Component or configure a wide-access Service Connector for multiple resources with a single credential set. If you have a cloud provider CLI configured locally, you can use auto-configuration for quicker setup.

### Auto-configuration
Many Service Connector Types support auto-configuration to extract configuration and credentials from your local environment, provided the relevant CLI or SDK is set up with valid credentials. Examples include:
- AWS: Use `aws configure`
- GCP: Use `gcloud auth application-default login`
- Azure: Use `az login`

For detailed guidance on auto-configuration for specific Service Connector Types, refer to their respective documentation.

```sh
zenml service-connector register kubernetes-auto --type kubernetes --auto-configure
```

It appears that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

```
Successfully registered service connector `kubernetes-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES ┃
┠───────────────────────┼────────────────┨
┃ 🌀 kubernetes-cluster │ 35.185.95.223  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I'll be happy to assist you!

```sh
zenml service-connector register aws-auto --type aws --auto-configure
```

It seems that the documentation text you wanted to summarize is missing. Please provide the text, and I will help you summarize it while retaining all important technical information.

```
⠼ Registering service connector 'aws-auto'...
Successfully registered service connector `aws-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://aws-ia-mwaa-715803424590                ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```sh
zenml service-connector register gcp-auto --type gcp --auto-configure
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will assist you accordingly.

```
Successfully registered service connector `gcp-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://annotation-gcp-store                       ┃
┃                       │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃  🐳 docker-registry   │ gcr.io/zenml-core                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

### Scopes: Multi-type, Multi-instance, and Single-instance

Service Connectors can be registered to access multiple resource types, multiple instances of the same resource type, or a single resource. Basic Service Connector Types like Kubernetes and Docker are single-resource by default, while connectors for managed cloud resources (e.g., AWS, GCP) can adopt all three forms.

#### Example of Registering Service Connectors with Different Scopes
1. **Multi-type AWS Service Connector**: Access to all resources available with the configured credentials.
2. **Multi-instance AWS Service Connector**: Access to multiple S3 buckets.
3. **Single-instance AWS Service Connector**: Access to a single S3 bucket.

```sh
zenml service-connector register aws-multi-type --type aws --auto-configure
```

It seems that the provided text is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```
⠋ Registering service connector 'aws-multi-type'...
Successfully registered service connector `aws-multi-type` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://aws-ia-mwaa-715803424590                ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┃                       │ s3://zenml-public-datasets                   ┃
┃                       │ s3://zenml-public-swagger-spec               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you'd like summarized, and I'll be happy to assist!

```sh
zenml service-connector register aws-s3-multi-instance --type aws --auto-configure --resource-type s3-bucket
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```
⠸ Registering service connector 'aws-s3-multi-instance'...
Successfully registered service connector `aws-s3-multi-instance` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://aws-ia-mwaa-715803424590         ┃
┃               │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┃               │ s3://zenml-public-datasets            ┃
┃               │ s3://zenml-public-swagger-spec        ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no specific documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist you!

```sh
zenml service-connector register aws-s3-zenfiles --type aws --auto-configure --resource-type s3-bucket --resource-id s3://zenfiles
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like me to summarize, and I'll be happy to assist you!

```
⠼ Registering service connector 'aws-s3-zenfiles'...
Successfully registered service connector `aws-s3-zenfiles` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES ┃
┠───────────────┼────────────────┨
┃ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

### Summary of Service Connector Documentation

**Scopes:**
- **Multi-instance Service Connector:** Resource Type scope is fixed during configuration.
- **Single-instance Service Connector:** Resource Name (Resource ID) scope is fixed during configuration.

**Service Connector Verification:**
- **Multi-type Service Connectors:** Verify that credentials authenticate successfully and list accessible resources for each Resource Type.
- **Multi-instance Service Connectors:** Verify credentials for authentication and list accessible resources.
- **Single-instance Service Connectors:** Check that credentials have permission to access the target resource.

Verification can also be performed later on registered Service Connectors and can be scoped to a Resource Type and Resource Name for multi-type and multi-instance connectors.

**Example:** Verification of multi-type, multi-instance, and single-instance Service Connectors can be done post-registration, with a focus on their configured scopes.

```sh
zenml service-connector list
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME                  │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼───────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-multi-type        │ 373a73c2-8295-45d4-a768-45f5a0f744ea │ 🔶 aws │ 🔶 aws-generic        │ <multiple>    │ ➖     │ default │            │        ┃
┃        │                       │                                      │        │ 📦 s3-bucket          │               │        │         │            │        ┃
┃        │                       │                                      │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                       │                                      │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┠────────┼───────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-s3-multi-instance │ fa9325ab-ce01-4404-aec3-61a3af395d48 │ 🔶 aws │ 📦 s3-bucket          │ <multiple>    │ ➖     │ default │            │        ┃
┠────────┼───────────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-s3-zenfiles       │ 19edc05b-92db-49de-bc84-aa9b3fb8261a │ 🔶 aws │ 📦 s3-bucket          │ s3://zenfiles │ ➖     │ default │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

The multi-type Service Connector verification checks if the provided credentials are valid for authenticating to AWS and identifies the accessible resources through the Service Connector.

```sh
zenml service-connector verify aws-multi-type
```

It appears that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to assist!

```
Service connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://aws-ia-mwaa-715803424590                ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

You can limit verification to a specific Resource Type or Resource Name. This allows you to check if credentials are valid and determine authorized access, such as which S3 buckets can be accessed or if they can access a specific Kubernetes cluster in AWS.

```sh
zenml service-connector verify aws-multi-type --resource-type s3-bucket
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist!

```
Service connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://aws-ia-mwaa-715803424590         ┃
┃               │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It appears that you have not provided any documentation text to summarize. Please provide the text you would like me to summarize, and I will be happy to assist you!

```sh
zenml service-connector verify aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster
```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-multi-type' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES   ┃
┠───────────────────────┼──────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┛
```

To verify the multi-instance Service Connector, ensure it displays all accessible resources. Verification can also be scoped to a single resource.

```sh
zenml service-connector verify aws-s3-multi-instance
```

It appears that the documentation text you intended to provide is missing. Please share the text you'd like me to summarize, and I'll be happy to help!

```
Service connector 'aws-s3-multi-instance' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://aws-ia-mwaa-715803424590         ┃
┃               │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It appears that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist you!

```sh
zenml service-connector verify aws-s3-multi-instance --resource-id s3://zenml-demos
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-s3-multi-instance' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES   ┃
┠───────────────┼──────────────────┨
┃ 📦 s3-bucket  │ s3://zenml-demos ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┛
```

Verifying the single-instance Service Connector is straightforward and requires no additional explanation.

```sh
zenml service-connector verify aws-s3-zenfiles
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like me to summarize, and I'll be happy to help!

```
Service connector 'aws-s3-zenfiles' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES ┃
┠───────────────┼────────────────┨
┃ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

## Configure Local Clients

Service Container Types allow configuration of local CLI and SDK utilities (e.g., Docker, Kubernetes CLI `kubectl`) with credentials from a compatible Service Connector. This feature enables direct CLI access to remote services for managing configurations, debugging workloads, or verifying Service Connector credentials.

**Warning:** Most Service Connectors issue temporary credentials (e.g., API tokens) that may expire quickly. You will need to obtain new credentials from the Service Connector after expiration.

### Examples of Local CLI Configuration

The following examples demonstrate how to configure the local Kubernetes `kubectl` CLI with credentials from a Service Connector to access a Kubernetes cluster directly.

```sh
zenml service-connector list-resources --resource-type kubernetes-cluster
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist you!

```
The following 'kubernetes-cluster' resources can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME       │ CONNECTOR TYPE │ RESOURCE TYPE         │ RESOURCE NAMES                                                                      ┃
┠──────────────────────────────────────┼──────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┨
┃ 9d953320-3560-4a78-817c-926a3898064d │ gcp-user-multi       │ 🔵 gcp         │ 🌀 kubernetes-cluster │ zenml-test-cluster                                                                  ┃
┠──────────────────────────────────────┼──────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────┨
┃ 4a550c82-aa64-4a48-9c7f-d5e127d77a44 │ aws-multi-type       │ 🔶 aws         │ 🌀 kubernetes-cluster │ zenhacks-cluster                                                                    ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there was an error in your request, as there is no documentation text provided for summarization. Please provide the text you would like summarized, and I will be happy to assist you!

```sh
zenml service-connector login gcp-user-multi --resource-type kubernetes-cluster --resource-id zenml-test-cluster
```

It seems that you have not provided the documentation text to summarize. Please provide the text you would like me to condense, and I'll be happy to assist!

```
$ zenml service-connector login gcp-user-multi --resource-type kubernetes-cluster --resource-id zenml-test-cluster
⠇ Attempting to configure local client using service connector 'gcp-user-multi'...
Updated local kubeconfig with the cluster details. The current kubectl context was set to 'gke_zenml-core_zenml-test-cluster'.
The 'gcp-user-multi' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.

# Verify that the local kubectl client is now configured to access the remote Kubernetes cluster
$ kubectl cluster-info
Kubernetes control plane is running at https://35.185.95.223
GLBCDefaultBackend is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
KubeDNS is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
```

It seems there was an issue with the text you intended to provide for summarization. Please share the documentation text again, and I'll be happy to summarize it for you.

```sh
zenml service-connector login aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like me to summarize, and I'll be happy to assist you!

```
$ zenml service-connector login aws-multi-type --resource-type kubernetes-cluster --resource-id zenhacks-cluster
⠏ Attempting to configure local client using service connector 'aws-multi-type'...
Updated local kubeconfig with the cluster details. The current kubectl context was set to 'arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster'.
The 'aws-multi-type' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.

# Verify that the local kubectl client is now configured to access the remote Kubernetes cluster
$ kubectl cluster-info
Kubernetes control plane is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com
CoreDNS is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

The local Docker client can achieve the same functionality.

```sh
zenml service-connector verify aws-session-token --resource-type docker-registry
```

It appears that the text you provided is incomplete, as it only contains a code block title without any accompanying content. Please provide the full documentation text that you would like summarized.

```
Service connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME    │ CONNECTOR TYPE │ RESOURCE TYPE      │ RESOURCE NAMES                               ┃
┠──────────────────────────────────────┼───────────────────┼────────────────┼────────────────────┼──────────────────────────────────────────────┨
┃ 3ae3e595-5cbc-446e-be64-e54e854e0e3f │ aws-session-token │ 🔶 aws         │ 🐳 docker-registry │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist you!

```sh
zenml service-connector login aws-session-token --resource-type docker-registry
```

It seems that the text you provided is incomplete, as it only contains a placeholder for code output without any actual content. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

```
$zenml service-connector login aws-session-token --resource-type docker-registry
⠏ Attempting to configure local client using service connector 'aws-session-token'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

The 'aws-session-token' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.

# Verify that the local Docker client is now configured to access the remote Docker container registry
$ docker pull 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server
Using default tag: latest
latest: Pulling from zenml-server
e9995326b091: Pull complete 
f3d7f077cdde: Pull complete 
0db71afa16f3: Pull complete 
6f0b5905c60c: Pull complete 
9d2154d50fd1: Pull complete 
d072bba1f611: Pull complete 
20e776588361: Pull complete 
3ce69736a885: Pull complete 
c9c0554c8e6a: Pull complete 
bacdcd847a66: Pull complete 
482033770844: Pull complete 
Digest: sha256:bf2cc3895e70dfa1ee1cd90bbfa599fa4cd8df837e27184bac1ce1cc239ecd3f
Status: Downloaded newer image for 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest
715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest
```

## Discover Available Resources

As a ZenML user, you may want to know what resources you can access when connecting a Stack Component to an external resource. Instead of manually verifying each registered Service Connector, you can use the `zenml service-connector list-resources` CLI command to directly query available resources, such as:

- Kubernetes clusters accessible through Service Connectors
- Specific S3 buckets and their corresponding Service Connectors

### Resource Discovery Examples

You can retrieve a comprehensive list of all accessible resources through available Service Connectors, including those in an error state. Note that this operation can be resource-intensive and may take time, depending on the number of Service Connectors involved. The output will also detail any errors encountered during the discovery process.

```sh
zenml service-connector list-resources
```

It seems that the text you provided is incomplete. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
Fetching all service connector resources can take a long time, depending on the number of connectors that you have configured. Consider using the '--connector-type', '--resource-type' and '--resource-id' 
options to narrow down the list of resources to fetch.
The following resources can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME        │ CONNECTOR TYPE │ RESOURCE TYPE         │ RESOURCE NAMES                                                                                          ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 099fb152-cfb7-4af5-86a7-7b77c0961b21 │ gcp-multi             │ 🔵 gcp         │ 🔵 gcp-generic        │ zenml-core                                                                                              ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 📦 gcs-bucket         │ gs://annotation-gcp-store                                                                               ┃
┃                                      │                       │                │                       │ gs://zenml-bucket-sl                                                                                    ┃
┃                                      │                       │                │                       │ gs://zenml-core.appspot.com                                                                             ┃
┃                                      │                       │                │                       │ gs://zenml-core_cloudbuild                                                                              ┃
┃                                      │                       │                │                       │ gs://zenml-datasets                                                                                     ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 🌀 kubernetes-cluster │ zenml-test-cluster                                                                                      ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 🐳 docker-registry    │ gcr.io/zenml-core                                                                                       ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 373a73c2-8295-45d4-a768-45f5a0f744ea │ aws-multi-type        │ 🔶 aws         │ 🔶 aws-generic        │ us-east-1                                                                                               ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 📦 s3-bucket          │ s3://aws-ia-mwaa-715803424590                                                                           ┃
┃                                      │                       │                │                       │ s3://zenfiles                                                                                           ┃
┃                                      │                       │                │                       │ s3://zenml-demos                                                                                        ┃
┃                                      │                       │                │                       │ s3://zenml-generative-chat                                                                              ┃
┃                                      │                       │                │                       │ s3://zenml-public-datasets                                                                              ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 🌀 kubernetes-cluster │ zenhacks-cluster                                                                                        ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃                                      │                       │                │ 🐳 docker-registry    │ 715803424590.dkr.ecr.us-east-1.amazonaws.com                                                            ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ fa9325ab-ce01-4404-aec3-61a3af395d48 │ aws-s3-multi-instance │ 🔶 aws         │ 📦 s3-bucket          │ s3://aws-ia-mwaa-715803424590                                                                           ┃
┃                                      │                       │                │                       │ s3://zenfiles                                                                                           ┃
┃                                      │                       │                │                       │ s3://zenml-demos                                                                                        ┃
┃                                      │                       │                │                       │ s3://zenml-generative-chat                                                                              ┃
┃                                      │                       │                │                       │ s3://zenml-public-datasets                                                                              ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 19edc05b-92db-49de-bc84-aa9b3fb8261a │ aws-s3-zenfiles       │ 🔶 aws         │ 📦 s3-bucket          │ s3://zenfiles                                                                                           ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ c732c768-3992-4cbd-8738-d02cd7b6b340 │ kubernetes-auto       │ 🌀 kubernetes  │ 🌀 kubernetes-cluster │ 💥 error: connector 'kubernetes-auto' authorization failure: failed to verify Kubernetes cluster        ┃
┃                                      │                       │                │                       │ access: (401)                                                                                           ┃
┃                                      │                       │                │                       │ Reason: Unauthorized                                                                                    ┃
┃                                      │                       │                │                       │ HTTP response headers: HTTPHeaderDict({'Audit-Id': '20c96e65-3e3e-4e08-bae3-bcb72c527fbf',              ┃
┃                                      │                       │                │                       │ 'Cache-Control': 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 09 Jun 2023     ┃
┃                                      │                       │                │                       │ 18:52:56 GMT', 'Content-Length': '129'})                                                                ┃
┃                                      │                       │                │                       │ HTTP response body:                                                                                     ┃
┃                                      │                       │                │                       │ {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"Unauthorized","reason":" ┃
┃                                      │                       │                │                       │ Unauthorized","code":401}                                                                               ┃
┃                                      │                       │                │                       │                                                                                                         ┃
┃                                      │                       │                │                       │                                                                                                         ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

To enhance search accuracy, scope the search to a specific Resource Type. This approach provides fewer, more precise results, particularly when multiple Service Connectors are configured.

```sh
zenml service-connector list-resources --resource-type kubernetes-cluster
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to help!

```
The following 'kubernetes-cluster' resources can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME  │ CONNECTOR TYPE │ RESOURCE TYPE         │ RESOURCE NAMES                                                                                                ┃
┠──────────────────────────────────────┼─────────────────┼────────────────┼───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 099fb152-cfb7-4af5-86a7-7b77c0961b21 │ gcp-multi       │ 🔵 gcp         │ 🌀 kubernetes-cluster │ zenml-test-cluster                                                                                            ┃
┠──────────────────────────────────────┼─────────────────┼────────────────┼───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 373a73c2-8295-45d4-a768-45f5a0f744ea │ aws-multi-type  │ 🔶 aws         │ 🌀 kubernetes-cluster │ zenhacks-cluster                                                                                              ┃
┠──────────────────────────────────────┼─────────────────┼────────────────┼───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ c732c768-3992-4cbd-8738-d02cd7b6b340 │ kubernetes-auto │ 🌀 kubernetes  │ 🌀 kubernetes-cluster │ 💥 error: connector 'kubernetes-auto' authorization failure: failed to verify Kubernetes cluster access:      ┃
┃                                      │                 │                │                       │ (401)                                                                                                         ┃
┃                                      │                 │                │                       │ Reason: Unauthorized                                                                                          ┃
┃                                      │                 │                │                       │ HTTP response headers: HTTPHeaderDict({'Audit-Id': '72558f83-e050-4fe3-93e5-9f7e66988a4c', 'Cache-Control':   ┃
┃                                      │                 │                │                       │ 'no-cache, private', 'Content-Type': 'application/json', 'Date': 'Fri, 09 Jun 2023 18:59:02 GMT',             ┃
┃                                      │                 │                │                       │ 'Content-Length': '129'})                                                                                     ┃
┃                                      │                 │                │                       │ HTTP response body:                                                                                           ┃
┃                                      │                 │                │                       │ {"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"Unauthorized","reason":"Unauth ┃
┃                                      │                 │                │                       │ orized","code":401}                                                                                           ┃
┃                                      │                 │                │                       │                                                                                                               ┃
┃                                      │                 │                │                       │                                                                                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

You can request a specific resource using its Resource Name if you have it in advance.

```sh
zenml service-connector list-resources --resource-type s3-bucket --resource-id zenfiles
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
The  's3-bucket' resource with name 'zenfiles' can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME        │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 373a73c2-8295-45d4-a768-45f5a0f744ea │ aws-multi-type        │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ fa9325ab-ce01-4404-aec3-61a3af395d48 │ aws-s3-multi-instance │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 19edc05b-92db-49de-bc84-aa9b3fb8261a │ aws-s3-zenfiles       │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

## Connect Stack Components to Resources

Service Connectors enable Stack Components to access external resources and services. For first-time users, it is recommended to use the interactive CLI mode for connecting a Stack Component to a compatible Service Connector.

```
zenml artifact-store connect <component-name> -i
zenml orchestrator connect <component-name> -i
zenml container-registry connect <component-name> -i
```

To connect a Stack Component to an external resource or service, you must first register one or more Service Connectors. If you lack the necessary infrastructure knowledge, seek assistance from a team member. To check which resources/services you are authorized to access with the available Service Connectors, use the resource discovery feature. This check is included in the interactive ZenML CLI command for connecting a Stack Component to a remote resource. Note that not all Stack Components support connections via Service Connectors; this capability is indicated in the Stack Component flavor details.

```
$ zenml artifact-store flavor describe s3
Configuration class: S3ArtifactStoreConfig

Configuration for the S3 Artifact Store.

[...]

This flavor supports connecting to external resources with a Service
Connector. It requires a 's3-bucket' resource. You can get a list of
all available connectors and the compatible resources that they can
access by running:

'zenml service-connector list-resources --resource-type s3-bucket'
If no compatible Service Connectors are yet registered, you can can
register a new one by running:

'zenml service-connector register -i'

```

Stack Components that support Service Connectors have a flavor indicating the compatible Resource Type and optional Service Connector Type. This helps identify available resources and the Service Connectors that can access them. Additionally, ZenML can automatically determine the exact Resource Name based on the attributes configured in the Stack Component during interactive mode.

```sh
zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
zenml service-connector list-resources --resource-type s3-bucket --resource-id s3://zenfiles
zenml artifact-store connect s3-zenfiles --connector aws-multi-type
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist!

```
$ zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
Running with active stack: 'default' (global)
Successfully registered artifact_store `s3-zenfiles`.

$ zenml service-connector list-resources --resource-type s3-bucket --resource-id zenfiles
The  's3-bucket' resource with name 'zenfiles' can be accessed by service connectors that you have configured:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME       │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼──────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 4a550c82-aa64-4a48-9c7f-d5e127d77a44 │ aws-multi-type       │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼──────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 66c0922d-db84-4e2c-9044-c13ce1611613 │ aws-multi-instance   │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼──────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 65c82e59-cba0-4a01-b8f6-d75e8a1d0f55 │ aws-single-instance  │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛

$ zenml artifact-store connect s3-zenfiles --connector aws-multi-type
Running with active stack: 'default' (global)
Successfully connected artifact store `s3-zenfiles` to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼────────────────┼────────────────┼───────────────┼────────────────┨
┃ 4a550c82-aa64-4a48-9c7f-d5e127d77a44 │ aws-multi-type │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

To connect a Stack Component to a remote resource using interactive CLI mode, follow these steps: 

1. Open the CLI.
2. Use the appropriate command to initiate the connection.
3. Follow the prompts to input necessary parameters for the remote resource.

Ensure all required credentials and configurations are provided for a successful connection.

```sh
zenml artifact-store connect s3-zenfiles -i
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I'll be happy to help!

```
The following connectors have compatible resources:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME        │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 373a73c2-8295-45d4-a768-45f5a0f744ea │ aws-multi-type        │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ fa9325ab-ce01-4404-aec3-61a3af395d48 │ aws-s3-multi-instance │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┠──────────────────────────────────────┼───────────────────────┼────────────────┼───────────────┼────────────────┨
┃ 19edc05b-92db-49de-bc84-aa9b3fb8261a │ aws-s3-zenfiles       │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
Please enter the name or ID of the connector you want to use: aws-s3-zenfiles
Successfully connected artifact store `s3-zenfiles` to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME  │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼─────────────────┼────────────────┼───────────────┼────────────────┨
┃ 19edc05b-92db-49de-bc84-aa9b3fb8261a │ aws-s3-zenfiles │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

## End-to-End Examples

For a complete overview of the end-to-end process, from registering Service Connectors to configuring Stacks and running pipelines that access remote resources, refer to the following examples:

- [AWS Service Connector end-to-end examples](aws-service-connector.md)
- [GCP Service Connector end-to-end examples](gcp-service-connector.md)
- [Azure Service Connector end-to-end examples](azure-service-connector.md)



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/best-security-practices.md

### Security Best Practices for Service Connectors

Service Connectors for cloud providers support various authentication methods, but there is no unified standard. This section outlines best practices for selecting authentication methods.

#### Username and Password
- **Avoid using primary account passwords** as authentication credentials. Opt for alternatives like session tokens, API keys, or API tokens whenever possible.
- Passwords should never be shared within teams or used for automated workloads. Cloud platforms typically require exchanging account/password credentials for long-lived credentials instead.

#### Implicit Authentication
- **Key Takeaway**: Implicit authentication provides immediate access to cloud resources without configuration but may limit portability and reproducibility.
- **Security Risk**: This method can grant users access to the same resources as the ZenML Server, so it is disabled by default. To enable, set `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` or adjust the helm chart configuration.

Implicit authentication utilizes locally stored credentials, configuration files, and environment variables. It can automatically discover and use authentication methods based on the environment, including:

- **AWS**: Uses instance metadata service with IAM roles for EC2, ECS, EKS, and Lambda.
- **GCP**: Accesses resources via service accounts attached to GCP workloads.
- **Azure**: Utilizes Azure Managed Identity for access without explicit credentials.

**Caveats**:
- With local ZenML deployments, implicit authentication relies on local configurations, which are not accessible outside the local environment.
- For remote ZenML servers, the server must be in the same cloud as the Service Connector Type. Additional permissions may need to be configured for resource access.

#### Example
- **GCP Implicit Authentication**: Access GCP resources immediately if the ZenML server is deployed in GCP with the appropriate service account permissions.

```sh
zenml service-connector register gcp-implicit --type gcp --auth-method implicit --project_id=zenml-core
```

It appears that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will assist you accordingly.

```text
Successfully registered service connector `gcp-implicit` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://annotation-gcp-store                       ┃
┃                       │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃  🐳 docker-registry   │ gcr.io/zenml-core                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

### Long-lived Credentials (API Keys, Account Keys)

Long-lived credentials, such as API keys and account keys, are essential for authentication, especially in production environments with ZenML. They should be paired with methods for generating short-lived API tokens or impersonating accounts to enhance security.

**Best Practices:**
- Avoid using account passwords directly for cloud API authentication. Instead, utilize processes that exchange credentials for long-lived credentials:
  - AWS: `aws configure`
  - GCP: `gcloud auth application-default login`
  - Azure: `az login`

Original login information is not stored locally; instead, intermediate credentials are generated for API authentication.

**Types of Long-lived Credentials:**
- **User Credentials:** Tied to human users with broad permissions. Not recommended for sharing.
- **Service Credentials:** Used for automated processes, not tied to individual user accounts, and can have restricted permissions, making them safer for broader sharing.

**Recommendations:**
- Use service credentials over user credentials in production to protect user identities and adhere to the least-privilege principle.

**Security Enhancements:**
Long-lived credentials alone can pose security risks if leaked. ZenML Service Connectors provide mechanisms to enhance security:
- Generate temporary credentials from long-lived ones with limited permission scopes.
- Implement authentication schemes that impersonate accounts or assume roles.

### Generating Temporary and Down-scoped Credentials

Authentication methods utilizing long-lived credentials often include mechanisms to minimize credential exposure. 

**Issuing Temporary Credentials:**
- Long-lived credentials are stored securely on the ZenML server, while clients receive temporary API tokens with limited lifetimes. 
- The Service Connector can generate these tokens as needed, supported by various authentication methods in AWS and GCP.

**Example:**
- AWS Service Connector can issue temporary credentials like "Session Token" or "Federation Token" while keeping long-lived credentials secure on the server.

```sh
zenml service-connector describe eks-zenhacks-cluster
```

It seems you intended to provide a specific documentation text for summarization, but it appears to be missing. Please provide the text you'd like summarized, and I'll be happy to assist!

```text
Service connector 'eks-zenhacks-cluster' of type 'aws' with id 'be53166a-b39c-4e39-8e31-84658e50eec4' is owned by user 'default' and is 'private'.
   'eks-zenhacks-cluster' aws Service Connector Details    
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ ID               │ be53166a-b39c-4e39-8e31-84658e50eec4 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ NAME             │ eks-zenhacks-cluster                 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ AUTH METHOD      │ session-token                        ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🌀 kubernetes-cluster                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE NAME    │ zenhacks-cluster                     ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SECRET ID        │ fa42ab38-3c93-4765-a4c6-9ce0b548a86c ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SESSION DURATION │ 43200s                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ OWNER            │ default                              ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SHARED           │ ➖                                   ┃
┠──────────────────┼──────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-16 10:15:26.393769           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-16 10:15:26.393772           ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

The documentation highlights the issuance of temporary credentials to clients, specifically emphasizing the expiration time associated with the Kubernetes API token.

```sh
zenml service-connector describe eks-zenhacks-cluster --client
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

```text
Service connector 'eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client)' of type 'kubernetes' with id 'be53166a-b39c-4e39-8e31-84658e50eec4' is owned by user 'default' and is 'private'.
 'eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client)' kubernetes Service 
                                    Connector Details                                     
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ ID               │ be53166a-b39c-4e39-8e31-84658e50eec4                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ NAME             │ eks-zenhacks-cluster (kubernetes-cluster | zenhacks-cluster client) ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🌀 kubernetes                                                       ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ token                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🌀 kubernetes-cluster                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster         ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h59m57s                                                           ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                             ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-16 10:17:46.931091                                          ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-16 10:17:46.931094                                          ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                           Configuration                                            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE                                                                    ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ server                │ https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ insecure              │ False                                                                    ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ cluster_name          │ arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster              ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ token                 │ [HIDDEN]                                                                 ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ certificate_authority │ [HIDDEN]                                                                 ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

**Issuing Downscoped Credentials**: Some authentication methods allow for generating temporary API tokens with restricted permissions tailored to specific resources. This feature is available for the AWS Service Connector's "Federation Token" and "IAM Role" methods. 

**Example**: An AWS client token issued to an S3 client can only access the designated S3 bucket, despite the originating AWS Service Connector having access to multiple buckets with long-lived credentials.

```sh
zenml service-connector register aws-federation-multi --type aws --auth-method=federation-token --auto-configure 
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

```text
Successfully registered service connector `aws-federation-multi` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://aws-ia-mwaa-715803424590                ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┃                       │ s3://zenml-public-datasets                   ┃
┃                       │ s3://zenml-public-swagger-spec               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The next step is to execute ZenML Python code to demonstrate that the downscoped credentials granted to a client are limited to the specific S3 bucket requested by the client.

```python
from zenml.client import Client

client = Client()

# Get a Service Connector client for a particular S3 bucket
connector_client = client.get_service_connector_client(
    name_id_or_prefix="aws-federation-multi",
    resource_type="s3-bucket",
    resource_id="s3://zenfiles"
)

# Get the S3 boto3 python client pre-configured and pre-authenticated
# from the Service Connector client
s3_client = connector_client.connect()

# Verify access to the chosen S3 bucket using the temporary token that
# was issued to the client.
s3_client.head_bucket(Bucket="zenfiles")

# Try to access another S3 bucket that the original AWS long-lived credentials can access.
# An error will be thrown indicating that the bucket is not accessible.
s3_client.head_bucket(Bucket="zenml-demos")
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I'll be happy to assist you!

```text
>>> from zenml.client import Client
>>> 
>>> client = Client()
Unable to find ZenML repository in your current working directory (/home/stefan/aspyre/src/zenml) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run zenml init.
Running without an active repository root.
>>> 
>>> # Get a Service Connector client for a particular S3 bucket
>>> connector_client = client.get_service_connector_client(
...     name_id_or_prefix="aws-federation-multi",
...     resource_type="s3-bucket",
...     resource_id="s3://zenfiles"
... )
>>> 
>>> # Get the S3 boto3 python client pre-configured and pre-authenticated
>>> # from the Service Connector client
>>> s3_client = connector_client.connect()
>>> 
>>> # Verify access to the chosen S3 bucket using the temporary token that
>>> # was issued to the client.
>>> s3_client.head_bucket(Bucket="zenfiles")
{'ResponseMetadata': {'RequestId': '62YRYW5XJ1VYPCJ0', 'HostId': 'YNBXcGUMSOh90AsTgPW6/Ra89mqzfN/arQq/FMcJzYCK98cFx53+9LLfAKzZaLhwaiJTm+s3mnU=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'YNBXcGUMSOh90AsTgPW6/Ra89mqzfN/arQq/FMcJzYCK98cFx53+9LLfAKzZaLhwaiJTm+s3mnU=', 'x-amz-request-id': '62YRYW5XJ1VYPCJ0', 'date': 'Fri, 16 Jun 2023 11:04:20 GMT', 'x-amz-bucket-region': 'us-east-1', 'x-amz-access-point-alias': 'false', 'content-type': 'application/xml', 'server': 'AmazonS3'}, 'RetryAttempts': 0}}
>>> 
>>> # Try to access another S3 bucket that the original AWS long-lived credentials can access.
>>> # An error will be thrown indicating that the bucket is not accessible.
>>> s3_client.head_bucket(Bucket="zenml-demos")
╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮
│ <stdin>:1 in <module>                                                                            │
│                                                                                                  │
│ /home/stefan/aspyre/src/zenml/.venv/lib/python3.8/site-packages/botocore/client.py:508 in        │
│ _api_call                                                                                        │
│                                                                                                  │
│    505 │   │   │   │   │   f"{py_operation_name}() only accepts keyword arguments."              │
│    506 │   │   │   │   )                                                                         │
│    507 │   │   │   # The "self" in this scope is referring to the BaseClient.                    │
│ ❱  508 │   │   │   return self._make_api_call(operation_name, kwargs)                            │
│    509 │   │                                                                                     │
│    510 │   │   _api_call.__name__ = str(py_operation_name)                                       │
│    511                                                                                           │
│                                                                                                  │
│ /home/stefan/aspyre/src/zenml/.venv/lib/python3.8/site-packages/botocore/client.py:915 in        │
│ _make_api_call                                                                                   │
│                                                                                                  │
│    912 │   │   if http.status_code >= 300:                                                       │
│    913 │   │   │   error_code = parsed_response.get("Error", {}).get("Code")                     │
│    914 │   │   │   error_class = self.exceptions.from_code(error_code)                           │
│ ❱  915 │   │   │   raise error_class(parsed_response, operation_name)                            │
│    916 │   │   else:                                                                             │
│    917 │   │   │   return parsed_response                                                        │
│    918                                                                                           │
╰──────────────────────────────────────────────────────────────────────────────────────────────────╯
ClientError: An error occurred (403) when calling the HeadBucket operation: Forbidden
```

### Impersonating Accounts and Assuming Roles

These authentication methods require advanced setup involving multiple permission-bearing accounts and roles, providing flexibility and control. They are suitable for platform engineers with infrastructure expertise.

These methods allow for configuring long-lived credentials in Service Connectors without exposing them to clients, serving as an alternative to cloud provider authentication methods that lack automatic downscoping of temporary token permissions.

**Process Summary:**
1. Configure a Service Connector with long-lived credentials linked to a primary user or service account (preferably with minimal permissions).
2. Provision secondary access entities in the cloud platform with necessary permissions:
   - One or more IAM roles (to be assumed)
   - One or more service accounts (to be impersonated)
3. Include the target IAM role or service account name in the Service Connector configuration.
4. Upon request, the Service Connector exchanges long-lived credentials for short-lived API tokens with permissions tied to the target IAM role or service account. These temporary credentials are issued to clients while keeping long-lived credentials secure within the ZenML server.

**GCP Account Impersonation Example:**
- Primary service account: `empty-connectors@zenml-core.iam.gserviceaccount.com` (no permissions except "Service Account Token Creator").
- Secondary service account: `zenml-bucket-sl@zenml-core.iam.gserviceaccount.com` (permissions to access `zenml-bucket-sl` GCS bucket).

The `empty-connectors` service account has no permissions to access GCS buckets or other resources. A regular GCP Service Connector is registered using the service account key (long-lived credentials).

```sh
zenml service-connector register gcp-empty-sa --type gcp --auth-method service-account --service_account_json=@empty-connectors@zenml-core.json --project_id=zenml-core
```

It appears that the text you provided is incomplete, as it only includes a code block title without any actual content or documentation to summarize. Please provide the full documentation text for summarization.

```text
Expanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.
Successfully registered service connector `gcp-empty-sa` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                                                               ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                                                                   ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ 💥 error: connector authorization failure: failed to list GCS buckets: 403 GET               ┃
┃                       │ https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint= ┃
┃                       │ false: empty-connectors@zenml-core.iam.gserviceaccount.com does not have                     ┃
┃                       │ storage.buckets.list access to the Google Cloud project. Permission 'storage.buckets.list'   ┃
┃                       │ denied on resource (or it may not exist).                                                    ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ 💥 error: connector authorization failure: Failed to list GKE clusters: 403 Required         ┃
┃                       │ "container.clusters.list" permission(s) for "projects/20219041791". [request_id:             ┃
┃                       │ "0xcb7086235111968a"                                                                         ┃
┃                       │ ]                                                                                            ┃
┠───────────────────────┼──────────────────────────────────────────────────────────────────────────────────────────────┨
┃  🐳 docker-registry   │ gcr.io/zenml-core                                                                            ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

To register a GCP Service Connector using account impersonation for accessing the `zenml-bucket-sl` GCS bucket, follow these steps to verify access to the bucket.

```sh
zenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to assist!

```text
Expanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.
Successfully registered service connector `gcp-impersonate-sa` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES       ┃
┠───────────────┼──────────────────────┨
┃ 📦 gcs-bucket │ gs://zenml-bucket-sl ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┛
```

### Short-lived Credentials

Short-lived credentials are temporary authentication methods configured or generated by the Service Connector. While they provide a way to grant temporary access without exposing long-lived credentials, they are often impractical due to the need for manual updates or replacements when they expire.

Temporary credentials can be generated automatically from long-lived credentials by cloud provider Service Connectors or manually via cloud provider CLIs. This allows for temporary access to resources, ensuring long-lived credentials remain secure.

#### AWS Short-lived Credentials Auto-Configuration Example
An example is provided for using Service Connector auto-configuration to generate a short-lived token from long-lived AWS credentials configured in the local cloud provider CLI.

```sh
AWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist you!

```text
⠸ Registering service connector 'aws-sts-token'...
Successfully registered service connector `aws-sts-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┃                       │ s3://zenml-public-datasets                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector is configured with a short-lived token that expires after a set duration. Verification can be done by inspecting the Service Connector.

```sh
zenml service-connector describe aws-sts-token 
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

```text
Service connector 'aws-sts-token' of type 'aws' with id '63e14350-6719-4255-b3f5-0539c8f7c303' is owned by user 'default' and is 'private'.
                        'aws-sts-token' aws Service Connector Details                         
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ e316bcb3-6659-467b-81e5-5ec25bfd36b0                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-sts-token                                                           ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 971318c9-8db9-4297-967d-80cda070a121                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h58m17s                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 17:58:42.999323                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 17:58:42.999324                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

The Service Connector is temporary and will become unusable in 12 hours.

```sh
zenml service-connector list --name aws-sts-token 
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I will help you condense it while retaining all important technical information.

```text
┏━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME          │ ID                              │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼───────────────┼─────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-sts-token │ e316bcb3-6659-467b-81e5-5ec25bf │ 🔶 aws │ 🔶 aws-generic        │ <multiple>    │ ➖     │ default │ 11h57m12s  │        ┃
┃        │               │ d36b0                           │        │ 📦 s3-bucket          │               │        │         │            │        ┃
┃        │               │                                 │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │               │                                 │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

The documentation includes an image of "ZenML Scarf" with the following attributes: 
- **Alt Text**: ZenML Scarf
- **Referrer Policy**: no-referrer-when-downgrade
- **Image Source**: ![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/gcp-service-connector.md

### GCP Service Connector

The ZenML GCP Service Connector enables authentication and access to GCP resources like GCS buckets, GKE clusters, and GCR container registries. It supports multiple authentication methods, including GCP user accounts, service accounts, short-lived OAuth 2.0 tokens, and implicit authentication. 

Key features include:
- Issuance of short-lived OAuth 2.0 tokens for enhanced security, unless configured otherwise.
- Automatic configuration and detection of locally configured credentials via the GCP CLI.
- General access to any GCP service through OAuth 2.0 credential objects.
- Specialized authentication for GCS, Docker, and Kubernetes Python clients.
- Configuration support for local Docker and Kubernetes CLIs.

```shell
$ zenml service-connector list-types --type gcp
```

```shell
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃         NAME          │ TYPE   │ RESOURCE TYPES        │ AUTH METHODS     │ LOCAL │ REMOTE ┃
┠───────────────────────┼────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃ GCP Service Connector │ 🔵 gcp │ 🔵 gcp-generic        │ implicit         │ ✅    │ ✅     ┃
┃                       │        │ 📦 gcs-bucket         │ user-account     │       │        ┃
┃                       │        │ 🌀 kubernetes-cluster │ service-account  │       │        ┃
┃                       │        │ 🐳 docker-registry    │ external-account │       │        ┃
┃                       │        │                       │ oauth2-token     │       │        ┃
┃                       │        │                       │ impersonation    │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Prerequisites

The GCP Service Connector is part of the GCP ZenML integration. You can install it in two ways:

- `pip install "zenml[connectors-gcp]"` for only the GCP Service Connector prerequisites.
- `zenml integration install gcp` for the entire GCP ZenML integration.

Installing the GCP CLI on your local machine is not required to use the GCP Service Connector for linking Stack Components to GCP resources, but it is recommended for quick setup and auto-configuration features. 

**Note:** Auto-configuration examples require the GCP CLI to be installed and configured with valid credentials. If you prefer not to install the GCP CLI, use the interactive mode of the ZenML CLI to register Service Connectors.

```
zenml service-connector register -i --type gcp
```

## Resource Types

### Generic GCP Resource
This resource type enables Stack Components to connect to any GCP service via the GCP Service Connector, providing a Python google-auth credentials object with a GCP OAuth 2.0 token for creating GCP Python clients. It is intended for Stack Components not covered by specific resource types (e.g., GCS buckets, Kubernetes clusters). It requires appropriate GCP permissions for accessing remote resources.

### GCS Bucket
Allows Stack Components to connect to GCS buckets with a pre-configured GCS Python client. Required GCP permissions include:
- `storage.buckets.list`
- `storage.buckets.get`
- `storage.objects.create`
- `storage.objects.delete`
- `storage.objects.get`
- `storage.objects.list`
- `storage.objects.update`

Resource names must be in the format:
- GCS bucket URI: `gs://{bucket-name}`
- GCS bucket name: `{bucket-name}`

### GKE Kubernetes Cluster
Enables access to a GKE cluster as a standard Kubernetes resource, providing a pre-authenticated Python Kubernetes client. Required GCP permissions include:
- `container.clusters.list`
- `container.clusters.get`

Additionally, permissions to connect to the GKE cluster are needed. Resource names must identify a GKE cluster in the format: `{cluster-name}`.

### GAR Container Registry (including legacy GCR support)
**Important Notice:** Google Container Registry is being replaced by Artifact Registry. Transition to Artifact Registry is recommended before May 15, 2024. Legacy GCR support remains available but will be phased out.

This resource type allows access to Google Artifact Registry, providing a pre-authenticated Python Docker client. Required GCP permissions include:
- `artifactregistry.repositories.createOnPush`
- `artifactregistry.repositories.downloadArtifacts`
- `artifactregistry.repositories.get`
- `artifactregistry.repositories.list`
- `artifactregistry.repositories.readViaVirtualRepository`
- `artifactregistry.repositories.uploadArtifacts`
- `artifactregistry.locations.list`

For legacy GCR, required permissions include:
- `storage.buckets.get`
- `storage.multipartUploads.abort`
- `storage.multipartUploads.create`
- `storage.multipartUploads.list`
- `storage.multipartUploads.listParts`
- `storage.objects.create`
- `storage.objects.delete`
- `storage.objects.list`

Resource names must identify a GAR or GCR registry in specified formats.

## Authentication Methods

### Implicit Authentication
Implicit authentication uses Application Default Credentials (ADC) to access GCP services. This method is disabled by default due to potential security risks. It can be enabled via the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable.

This method automatically discovers credentials from:
- Environment variables (GOOGLE_APPLICATION_CREDENTIALS)
- Local ADC credential files
- A GCP service account attached to the ZenML server resource

While convenient, it may lead to privilege escalation due to inherited permissions. For production use, it is recommended to use Service Account Key or Service Account Impersonation methods for better permission control. A GCP project is required, and the connector can only access resources in the specified project.

```sh
zenml service-connector register gcp-implicit --type gcp --auth-method implicit --auto-configure
```

It seems that the text you provided is incomplete, as it only includes a code title without any actual content or documentation to summarize. Please provide the full documentation text you'd like summarized, and I'll be happy to assist you!

```
Successfully registered service connector `gcp-implicit` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                               ┃
┃                       │ us.gcr.io/zenml-core                            ┃
┃                       │ eu.gcr.io/zenml-core                            ┃
┃                       │ asia.gcr.io/zenml-core                          ┃
┃                       │ asia-docker.pkg.dev/zenml-core/asia.gcr.io      ┃
┃                       │ europe-docker.pkg.dev/zenml-core/eu.gcr.io      ┃
┃                       │ europe-west1-docker.pkg.dev/zenml-core/test     ┃
┃                       │ us-docker.pkg.dev/zenml-core/gcr.io             ┃
┃                       │ us-docker.pkg.dev/zenml-core/us.gcr.io          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector does not store any credentials.

```sh
zenml service-connector describe gcp-implicit
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to assist!

```
Service connector 'gcp-implicit' of type 'gcp' with id '0c49a7fe-5e87-41b9-adbe-3da0a0452e44' is owned by user 'default' and is 'private'.
                         'gcp-implicit' gcp Service Connector Details                          
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                    ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 0c49a7fe-5e87-41b9-adbe-3da0a0452e44                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ gcp-implicit                                                             ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔵 gcp                                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ implicit                                                                 ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔵 gcp-generic, 📦 gcs-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                                          ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                  ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                       ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-19 08:04:51.037955                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-19 08:04:51.037958                                               ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
       Configuration       
┏━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY   │ VALUE      ┃
┠────────────┼────────────┨
┃ project_id │ zenml-core ┃
┗━━━━━━━━━━━━┷━━━━━━━━━━━━┛
```

### GCP User Account

Long-lived GCP credentials consist of a GCP user account and its credentials, generated via the `gcloud auth application-default login` command. The GCP connector generates temporary OAuth 2.0 tokens from these credentials, which have a 1-hour lifetime. This can be disabled by setting `generate_temporary_tokens` to `False`, allowing distribution of user account credentials JSON (not recommended). This method is suitable for development and testing but not for production, as it grants full permissions of the GCP user account. For production, use GCP Service Account or GCP Service Account Impersonation methods. The connector requires a GCP project and can only access resources within that project. If the local GCP CLI is set up with these credentials, they will be automatically detected during auto-configuration. 

<details>
<summary>Example auto-configuration</summary>
Assumes local GCP CLI is configured with GCP user account credentials via `gcloud auth application-default login`.
</details>

```sh
zenml service-connector register gcp-user-account --type gcp --auth-method user-account --auto-configure
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the complete documentation text so I can assist you in summarizing it effectively.

```
Successfully registered service connector `gcp-user-account` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                               ┃
┃                       │ us.gcr.io/zenml-core                            ┃
┃                       │ eu.gcr.io/zenml-core                            ┃
┃                       │ asia.gcr.io/zenml-core                          ┃
┃                       │ asia-docker.pkg.dev/zenml-core/asia.gcr.io      ┃
┃                       │ europe-docker.pkg.dev/zenml-core/eu.gcr.io      ┃
┃                       │ europe-west1-docker.pkg.dev/zenml-core/test     ┃
┃                       │ us-docker.pkg.dev/zenml-core/gcr.io             ┃
┃                       │ us-docker.pkg.dev/zenml-core/us.gcr.io          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The GCP user account credentials were extracted from the local host.

```sh
zenml service-connector describe gcp-user-account
```

It seems that the documentation text you intended to provide is missing. Please provide the text you'd like summarized, and I'll be happy to assist!

```
Service connector 'gcp-user-account' of type 'gcp' with id 'ddbce93f-df14-4861-a8a4-99a80972f3bc' is owned by user 'default' and is 'private'.
                       'gcp-user-account' gcp Service Connector Details                        
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                    ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ ID               │ ddbce93f-df14-4861-a8a4-99a80972f3bc                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ gcp-user-account                                                         ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔵 gcp                                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ user-account                                                             ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔵 gcp-generic, 📦 gcs-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 17692951-614f-404f-a13a-4abb25bfa758                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                  ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                       ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-19 08:09:44.102934                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-19 08:09:44.102936                                               ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
          Configuration           
┏━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY          │ VALUE      ┃
┠───────────────────┼────────────┨
┃ project_id        │ zenml-core ┃
┠───────────────────┼────────────┨
┃ user_account_json │ [HIDDEN]   ┃
┗━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┛
```

### GCP Service Account

Long-lived GCP credentials consist of a GCP service account and its credentials, requiring a service account and a service account key JSON. The GCP connector generates temporary OAuth 2.0 tokens from these credentials, with a default lifetime of 1 hour. This can be disabled by setting `generate_temporary_tokens` to `False`, allowing distribution of the service account credentials JSON (not recommended). A GCP project is necessary, and the connector can only access resources within that project. If the `GOOGLE_APPLICATION_CREDENTIALS` environment variable points to a service account key JSON file, it will be automatically used during auto-configuration.

<details>
<summary>Example configuration</summary>
Assumes a GCP service account is created, granted permissions for GCS buckets in the target project, and a service account key JSON is saved locally as `connectors-devel@zenml-core.json`.
</details>

```sh
zenml service-connector register gcp-service-account --type gcp --auth-method service-account --resource-type gcs-bucket --project_id=zenml-core --service_account_json=@connectors-devel@zenml-core.json
```

It seems that the text you intended to provide is missing. Please share the documentation text you would like me to summarize, and I'll be happy to assist you!

```
Expanding argument value service_account_json to contents of file connectors-devel@zenml-core.json.
Successfully registered service connector `gcp-service-account` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The GCP service connector requires specific configuration and service account credentials to function properly. Ensure that the service account has the necessary permissions for the services being accessed. Properly configure the connector settings to establish a secure connection between GCP services and your application.

```sh
zenml service-connector describe gcp-service-account
```

It seems there was an error in your request as the documentation text you wanted summarized is missing. Please provide the text you would like summarized, and I will assist you accordingly.

```
Service connector 'gcp-service-account' of type 'gcp' with id '4b3d41c9-6a6f-46da-b7ba-8f374c3f49c5' is owned by user 'default' and is 'private'.
    'gcp-service-account' gcp Service Connector Details    
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ ID               │ 4b3d41c9-6a6f-46da-b7ba-8f374c3f49c5 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ NAME             │ gcp-service-account                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ TYPE             │ 🔵 gcp                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ AUTH METHOD      │ service-account                      ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 gcs-bucket                        ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SECRET ID        │ 0d0a42bb-40a4-4f43-af9e-6342eeca3f28 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ OWNER            │ default                              ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SHARED           │ ➖                                   ┃
┠──────────────────┼──────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-19 08:15:48.056937           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-19 08:15:48.056940           ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY             │ VALUE      ┃
┠──────────────────────┼────────────┨
┃ project_id           │ zenml-core ┃
┠──────────────────────┼────────────┨
┃ service_account_json │ [HIDDEN]   ┃
┗━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┛
```

### GCP Service Account Impersonation

This process generates temporary STS credentials by impersonating another GCP service account. The connector requires the email of the target service account and a JSON key for the primary service account, which must have the Service Account Token Creator role to generate tokens for the target account. 

The connector produces temporary OAuth 2.0 tokens upon request, with a configurable lifetime of up to 1 hour. Best practices suggest minimizing permissions for the primary service account and granting necessary permissions to the privilege-bearing service account. 

A GCP project is required, and the connector can only access resources within that project. If the `GOOGLE_APPLICATION_CREDENTIALS` environment variable is set to the primary service account key JSON file, it will be used automatically during configuration.

#### Configuration Example
- **Primary Service Account**: `empty-connectors@zenml-core.iam.gserviceaccount.com` with only the "Service Account Token Creator" role.
- **Secondary Service Account**: `zenml-bucket-sl@zenml-core.iam.gserviceaccount.com` with permissions to access the `zenml-bucket-sl` GCS bucket.

This setup ensures that the primary service account has no permissions to access GCS buckets or other resources.

```sh
zenml service-connector register gcp-empty-sa --type gcp --auth-method service-account --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I will be happy to assist you!

```
Expanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.
Successfully registered service connector `gcp-empty-sa` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                                                                                            ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                                                                                                ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ 💥 error: connector authorization failure: failed to list GCS buckets: 403 GET                                            ┃
┃                       │ https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint=false:                        ┃
┃                       │ empty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud         ┃
┃                       │ project. Permission 'storage.buckets.list' denied on resource (or it may not exist).                                      ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ 💥 error: connector authorization failure: Failed to list GKE clusters: 403 Required "container.clusters.list"            ┃
┃                       │ permission(s) for "projects/20219041791". [request_id: "0x84808facdac08541"                                               ┃
┃                       │ ]                                                                                                                         ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                                                                                                         ┃
┃                       │ us.gcr.io/zenml-core                                                                                                      ┃
┃                       │ eu.gcr.io/zenml-core                                                                                                      ┃
┃                       │ asia.gcr.io/zenml-core                                                                                                    ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

Verifying access to individual resource types will fail.

```sh
zenml service-connector verify gcp-empty-sa --resource-type kubernetes-cluster
```

It seems there was an error in your request as the documentation text to summarize is missing. Please provide the text you'd like me to summarize, and I'll be happy to help!

```
Error: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: Failed to list GKE clusters:
403 Required "container.clusters.list" permission(s) for "projects/20219041791".
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist you!

```sh
zenml service-connector verify gcp-empty-sa --resource-type gcs-bucket
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will help you with that.

```
Error: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: failed to list GCS buckets:
403 GET https://storage.googleapis.com/storage/v1/b?project=zenml-core&projection=noAcl&prettyPrint=false:
empty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.list access to the Google Cloud project.
Permission 'storage.buckets.list' denied on resource (or it may not exist).
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I will be happy to assist you!

```sh
zenml service-connector verify gcp-empty-sa --resource-type gcs-bucket --resource-id zenml-bucket-sl
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
Error: Service connector 'gcp-empty-sa' verification failed: connector authorization failure: failed to fetch GCS bucket
zenml-bucket-sl: 403 GET https://storage.googleapis.com/storage/v1/b/zenml-bucket-sl?projection=noAcl&prettyPrint=false:
empty-connectors@zenml-core.iam.gserviceaccount.com does not have storage.buckets.get access to the Google Cloud Storage bucket.
Permission 'storage.buckets.get' denied on resource (or it may not exist).
```

To register a GCP Service Connector using account impersonation for accessing the `zenml-bucket-sl` GCS bucket, follow these steps to verify access to the bucket.

```sh
zenml service-connector register gcp-impersonate-sa --type gcp --auth-method impersonation --service_account_json=@empty-connectors@zenml-core.json  --project_id=zenml-core --target_principal=zenml-bucket-sl@zenml-core.iam.gserviceaccount.com --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to assist you!

```
Expanding argument value service_account_json to contents of file /home/stefan/aspyre/src/zenml/empty-connectors@zenml-core.json.
Successfully registered service connector `gcp-impersonate-sa` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES       ┃
┠───────────────┼──────────────────────┨
┃ 📦 gcs-bucket │ gs://zenml-bucket-sl ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┛
```

### External Account (GCP Workload Identity)

Use [GCP workload identity federation](https://cloud.google.com/iam/docs/workload-identity-federation) to authenticate GCP services with AWS IAM credentials, Azure Active Directory credentials, or generic OIDC tokens. This method requires a GCP workload identity external account JSON file containing only configuration details, not sensitive credentials. It supports a two-layer authentication scheme that minimizes permissions associated with implicit credentials and grants permissions to the privileged GCP service account.

This authentication method allows workloads on AWS or Azure to automatically use their associated credentials for GCP service authentication. However, it may pose a security risk by granting access to the identity linked with the ZenML server's environment. Therefore, implicit authentication methods are disabled by default and can be enabled by setting the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable or the helm chart `enableImplicitAuthMethods` option to `true`.

By default, the GCP connector generates temporary OAuth 2.0 tokens from external account credentials, valid for 1 hour. This can be disabled by setting `generate_temporary_tokens` to `False`, which will distribute the external account credentials JSON instead (not recommended). A GCP project is required, and the connector can only access resources in the specified project, which must match the one for the external account configuration. If the `GOOGLE_APPLICATION_CREDENTIALS` environment variable points to an external account key JSON file, it will be automatically used during auto-configuration.

#### Example Configuration

Prerequisites include:
- ZenML server deployed in AWS (EKS or other compute environments).
- ZenML server EKS pods associated with an AWS IAM role via an IAM OIDC provider.
- A GCP workload identity pool and AWS provider configured for the GCP project.
- A GCP service account with permissions to access target resources and granted the `roles/iam.workloadIdentityUser` role for the workload identity pool and AWS provider.
- A GCP external account JSON file generated for the GCP service account to configure the GCP connector.

```sh
zenml service-connector register gcp-workload-identity --type gcp \
    --auth-method external-account --project_id=zenml-core \
    --external_account_json=@clientLibraryConfig-aws-zenml.json
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will be happy to assist you!

```
Successfully registered service connector `gcp-workload-identity` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                               ┃
┃                       │ us.gcr.io/zenml-core                            ┃
┃                       │ eu.gcr.io/zenml-core                            ┃
┃                       │ asia.gcr.io/zenml-core                          ┃
┃                       │ asia-docker.pkg.dev/zenml-core/asia.gcr.io      ┃
┃                       │ europe-docker.pkg.dev/zenml-core/eu.gcr.io      ┃
┃                       │ europe-west1-docker.pkg.dev/zenml-core/test     ┃
┃                       │ us-docker.pkg.dev/zenml-core/gcr.io             ┃
┃                       │ us-docker.pkg.dev/zenml-core/us.gcr.io          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector does not store sensitive credentials; it only retains meta-information regarding the external provider and account.

```sh
zenml service-connector describe gcp-workload-identity -x
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to assist!

```
Service connector 'gcp-workload-identity' of type 'gcp' with id '37b6000e-3f7f-483e-b2c5-7a5db44fe66b' is
owned by user 'default'.
                        'gcp-workload-identity' gcp Service Connector Details                        
┏━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY               │ VALUE                                                                    ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ ID                     │ 37b6000e-3f7f-483e-b2c5-7a5db44fe66b                                     ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ NAME                   │ gcp-workload-identity                                                    ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ TYPE                   │ 🔵 gcp                                                                   ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD            │ external-account                                                         ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES         │ 🔵 gcp-generic, 📦 gcs-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME          │ <multiple>                                                               ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID              │ 1ff6557f-7f60-4e63-b73d-650e64f015b5                                     ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION       │ N/A                                                                      ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN             │ N/A                                                                      ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES_SKEW_TOLERANCE │ N/A                                                                      ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ OWNER                  │ default                                                                  ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT             │ 2024-01-30 20:44:14.020514                                               ┃
┠────────────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT             │ 2024-01-30 20:44:14.020516                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                              Configuration                                              
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE                                                                         ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────┨
┃ project_id            │ zenml-core                                                                    ┃
┠───────────────────────┼───────────────────────────────────────────────────────────────────────────────┨
┃ external_account_json │ {                                                                             ┃
┃                       │   "type": "external_account",                                                 ┃
┃                       │   "audience":                                                                 ┃
┃                       │ "//iam.googleapis.com/projects/30267569827/locations/global/workloadIdentityP ┃
┃                       │ ools/mypool/providers/myprovider",                                            ┃
┃                       │   "subject_token_type": "urn:ietf:params:aws:token-type:aws4_request",        ┃
┃                       │   "service_account_impersonation_url":                                        ┃
┃                       │ "https://iamcredentials.googleapis.com/v1/projects/-/serviceAccounts/myrole@  ┃
┃                       │ zenml-core.iam.gserviceaccount.com:generateAccessToken",                      ┃
┃                       │   "token_url": "https://sts.googleapis.com/v1/token",                         ┃
┃                       │   "credential_source": {                                                      ┃
┃                       │     "environment_id": "aws1",                                                 ┃
┃                       │     "region_url":                                                             ┃
┃                       │ "http://169.254.169.254/latest/meta-data/placement/availability-zone",        ┃
┃                       │     "url":                                                                    ┃
┃                       │ "http://169.254.169.254/latest/meta-data/iam/security-credentials",           ┃
┃                       │     "regional_cred_verification_url":                                         ┃
┃                       │ "https://sts.{region}.amazonaws.com?Action=GetCallerIdentity&Version=2011-06- ┃
┃                       │ 15"                                                                           ┃
┃                       │   }                                                                           ┃
┃                       │ }                                                                             ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

### GCP OAuth 2.0 Token

GCP uses temporary OAuth 2.0 tokens configured by the user, requiring regular updates as tokens expire. This method is suitable for short-term access, such as temporary team sharing. Other authentication methods automatically generate and refresh OAuth 2.0 tokens upon request. 

A GCP project is necessary, and the connector can only access resources within that project.

#### Example Auto-Configuration

To fetch OAuth 2.0 tokens from the local GCP CLI, ensure valid credentials are set up by running `gcloud auth application-default login`. Use the `--auth-method oauth2-token` option with the ZenML CLI to enforce OAuth 2.0 token authentication, as it defaults to long-term credentials otherwise.

```sh
zenml service-connector register gcp-oauth2-token --type gcp --auto-configure --auth-method oauth2-token
```

It seems that there is no documentation text provided for summarization. Please share the text you would like summarized, and I'll be happy to assist!

```
Successfully registered service connector `gcp-oauth2-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                               ┃
┃                       │ us.gcr.io/zenml-core                            ┃
┃                       │ eu.gcr.io/zenml-core                            ┃
┃                       │ asia.gcr.io/zenml-core                          ┃
┃                       │ asia-docker.pkg.dev/zenml-core/asia.gcr.io      ┃
┃                       │ europe-docker.pkg.dev/zenml-core/eu.gcr.io      ┃
┃                       │ europe-west1-docker.pkg.dev/zenml-core/test     ┃
┃                       │ us-docker.pkg.dev/zenml-core/gcr.io             ┃
┃                       │ us-docker.pkg.dev/zenml-core/us.gcr.io          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It appears that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist!

```sh
zenml service-connector describe gcp-oauth2-token
```

It appears that the provided text does not contain any specific documentation content to summarize. Please provide the relevant documentation text, and I will be happy to summarize it for you.

```
Service connector 'gcp-oauth2-token' of type 'gcp' with id 'ec4d7d85-c71c-476b-aa76-95bf772c90da' is owned by user 'default' and is 'private'.
                       'gcp-oauth2-token' gcp Service Connector Details                        
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                    ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ ID               │ ec4d7d85-c71c-476b-aa76-95bf772c90da                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ gcp-oauth2-token                                                         ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔵 gcp                                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ oauth2-token                                                             ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔵 gcp-generic, 📦 gcs-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 4694de65-997b-4929-8831-b49d5e067b97                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 59m46s                                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                  ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                       ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-19 09:04:33.557126                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-19 09:04:33.557127                                               ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
       Configuration       
┏━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY   │ VALUE      ┃
┠────────────┼────────────┨
┃ project_id │ zenml-core ┃
┠────────────┼────────────┨
┃ token      │ [HIDDEN]   ┃
┗━━━━━━━━━━━━┷━━━━━━━━━━━━┛
```

The Service Connector is temporary and will expire in 1 hour, becoming unusable.

```sh
zenml service-connector list --name gcp-oauth2-token
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME             │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼──────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ gcp-oauth2-token │ ec4d7d85-c71c-476b-aa76-95bf772c90da │ 🔵 gcp │ 🔵 gcp-generic        │ <multiple>    │ ➖     │ default │ 59m35s     │        ┃
┃        │                  │                                      │        │ 📦 gcs-bucket         │               │        │         │            │        ┃
┃        │                  │                                      │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                  │                                      │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

## Auto-configuration

The GCP Service Connector enables auto-discovery and fetching of credentials and configuration set up via the GCP CLI on your local host. 

### Auto-configuration Example

This example demonstrates how to lift GCP user credentials to access the same GCP resources and services permitted by the local GCP CLI. Ensure the GCP CLI is configured with valid credentials (e.g., by executing `gcloud auth application-default login`). The GCP user account authentication method is automatically detected in this scenario.

```sh
zenml service-connector register gcp-auto --type gcp --auto-configure
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
Successfully registered service connector `gcp-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃    🔵 gcp-generic     │ zenml-core                                      ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃     📦 gcs-bucket     │ gs://zenml-bucket-sl                            ┃
┃                       │ gs://zenml-core.appspot.com                     ┃
┃                       │ gs://zenml-core_cloudbuild                      ┃
┃                       │ gs://zenml-datasets                             ┃
┃                       │ gs://zenml-internal-artifact-store              ┃
┃                       │ gs://zenml-kubeflow-artifact-store              ┃
┃                       │ gs://zenml-project-time-series-bucket           ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster                              ┃
┠───────────────────────┼─────────────────────────────────────────────────┨
┃ 🐳 docker-registry    │ gcr.io/zenml-core                               ┃
┃                       │ us.gcr.io/zenml-core                            ┃
┃                       │ eu.gcr.io/zenml-core                            ┃
┃                       │ asia.gcr.io/zenml-core                          ┃
┃                       │ asia-docker.pkg.dev/zenml-core/asia.gcr.io      ┃
┃                       │ europe-docker.pkg.dev/zenml-core/eu.gcr.io      ┃
┃                       │ europe-west1-docker.pkg.dev/zenml-core/test     ┃
┃                       │ us-docker.pkg.dev/zenml-core/gcr.io             ┃
┃                       │ us-docker.pkg.dev/zenml-core/us.gcr.io          ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist you!

```sh
zenml service-connector describe gcp-auto
```

It appears that the text you provided is incomplete, as it only contains a code title without any actual documentation content. Please provide the full text or additional details you would like summarized, and I'll be happy to assist!

```
Service connector 'gcp-auto' of type 'gcp' with id 'fe16f141-7406-437e-a579-acebe618a293' is owned by user 'default' and is 'private'.
                           'gcp-auto' gcp Service Connector Details                            
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                    ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ ID               │ fe16f141-7406-437e-a579-acebe618a293                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ gcp-auto                                                                 ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔵 gcp                                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ user-account                                                             ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔵 gcp-generic, 📦 gcs-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 5eca8f6e-291f-4958-ae2d-a3e847a1ad8a                                     ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                  ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                       ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-19 09:15:12.882929                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-19 09:15:12.882930                                               ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
          Configuration           
┏━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┓
┃ PROPERTY          │ VALUE      ┃
┠───────────────────┼────────────┨
┃ project_id        │ zenml-core ┃
┠───────────────────┼────────────┨
┃ user_account_json │ [HIDDEN]   ┃
┗━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┛
```

## Local Client Provisioning

The local `gcloud`, Kubernetes `kubectl`, and Docker CLIs can be configured with credentials from a compatible GCP Service Connector. Unlike the GCP CLI, Kubernetes and Docker credentials have a short lifespan and require regular refreshing for security reasons.

**Important Notes:**
- The `gcloud` client can only use credentials from the GCP Service Connector if it is set up with either the GCP user account or service account authentication methods, and the `generate_temporary_tokens` option is enabled.
- Only the `gcloud` local application default credentials will be updated by the GCP Service Connector, allowing libraries and SDKs that use these credentials to access GCP resources.

### Local CLI Configuration Examples
An example of configuring the local Kubernetes CLI to access a GKE cluster via a GCP Service Connector is provided.

```sh
zenml service-connector list --name gcp-user-account
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME             │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼──────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ gcp-user-account │ ddbce93f-df14-4861-a8a4-99a80972f3bc │ 🔵 gcp │ 🔵 gcp-generic        │ <multiple>    │ ➖     │ default │            │        ┃
┃        │                  │                                      │        │ 📦 gcs-bucket         │               │        │         │            │        ┃
┃        │                  │                                      │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                  │                                      │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

The documentation lists all Kubernetes clusters that can be accessed via the GCP Service Connector.

```sh
zenml service-connector verify gcp-user-account --resource-type kubernetes-cluster
```

It seems that the provided text is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text or details you would like summarized, and I will be happy to assist you.

```
Service connector 'gcp-user-account' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES     ┃
┠───────────────────────┼────────────────────┨
┃ 🌀 kubernetes-cluster │ zenml-test-cluster ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┛
```

The `login` CLI command configures the local Kubernetes `kubectl` CLI to access the Kubernetes cluster via the GCP Service Connector.

```sh
zenml service-connector login gcp-user-account --resource-type kubernetes-cluster --resource-id zenml-test-cluster
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
⠴ Attempting to configure local client using service connector 'gcp-user-account'...
Context "gke_zenml-core_zenml-test-cluster" modified.
Updated local kubeconfig with the cluster details. The current kubectl context was set to 'gke_zenml-core_zenml-test-cluster'.
The 'gcp-user-account' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.
```

To verify the configuration of the local Kubernetes `kubectl` CLI, use the following command:

```sh
kubectl cluster-info
```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

```
Kubernetes control plane is running at https://35.185.95.223
GLBCDefaultBackend is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy
KubeDNS is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://35.185.95.223/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
```

A similar process can be applied to GCR (Google Container Registry) container registries.

```sh
zenml service-connector verify gcp-user-account --resource-type docker-registry --resource-id europe-west1-docker.pkg.dev/zenml-core/test
```

It seems that the text you provided is incomplete. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```
Service connector 'gcp-user-account' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃   RESOURCE TYPE    │ RESOURCE NAMES                              ┃
┠────────────────────┼─────────────────────────────────────────────┨
┃ 🐳 docker-registry │ europe-west1-docker.pkg.dev/zenml-core/test ┃
┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It appears that you have not provided any documentation text to summarize. Please provide the text you would like me to condense, and I will be happy to assist you!

```sh
zenml service-connector login gcp-user-account --resource-type docker-registry --resource-id europe-west1-docker.pkg.dev/zenml-core/test
```

It seems that the text you provided is incomplete or missing. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
⠦ Attempting to configure local client using service connector 'gcp-user-account'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

The 'gcp-user-account' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.
```

To verify the configuration of the local Docker container registry client, use the following command:

```sh
docker push europe-west1-docker.pkg.dev/zenml-core/test/zenml
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

```
The push refers to repository [europe-west1-docker.pkg.dev/zenml-core/test/zenml]
d4aef4f5ed86: Pushed 
2d69a4ce1784: Pushed 
204066eca765: Pushed 
2da74ab7b0c1: Pushed 
75c35abda1d1: Layer already exists 
415ff8f0f676: Layer already exists 
c14cb5b1ec91: Layer already exists 
a1d005f5264e: Layer already exists 
3a3fd880aca3: Layer already exists 
149a9c50e18e: Layer already exists 
1f6d3424b922: Layer already exists 
8402c959ae6f: Layer already exists 
419599cb5288: Layer already exists 
8553b91047da: Layer already exists 
connectors: digest: sha256:a4cfb18a5cef5b2201759a42dd9fe8eb2f833b788e9d8a6ebde194765b42fe46 size: 3256
```

You can update the local `gcloud` CLI configuration using credentials from the GCP Service Connector.

```sh
zenml service-connector login gcp-user-account --resource-type gcp-generic
```

It seems that you have not provided the actual documentation text to summarize. Please share the text you'd like summarized, and I'll be happy to help!

```
Updated the local gcloud default application credentials file at '/home/user/.config/gcloud/application_default_credentials.json'
The 'gcp-user-account' GCP Service Connector connector was used to successfully configure the local Generic GCP resource client/SDK.
```

## Stack Components Use

The GCS Artifact Store Stack Component connects to a remote GCS bucket via a GCP Service Connector. The Google Cloud Image Builder, VertexAI Orchestrator, and VertexAI Step Operator can also connect to a target GCP project using this connector. It supports any Orchestrator or Model Deployer that utilizes Kubernetes, allowing GKE workloads to be managed without explicit GCP or Kubernetes configurations in the environment or Stack Component. Additionally, Container Registry Stack Components can connect to Google Artifact Registry or GCR through the GCP Service Connector, enabling image building and publishing without needing explicit GCP credentials.

## End-to-End Examples

### GKE Kubernetes Orchestrator, GCS Artifact Store, and GCR Container Registry with a Multi-Type GCP Service Connector

This example illustrates an end-to-end workflow using a multi-type GCP Service Connector for multiple Stack Components. The ZenML Stack includes:
- A Kubernetes Orchestrator connected to a GKE cluster
- A GCS Artifact Store linked to a GCS bucket
- A GCP Container Registry connected to a Docker Google Artifact Registry
- A local Image Builder

To run a pipeline on this Stack, configure the local GCP CLI with valid user credentials (e.g., `gcloud auth application-default login`) and install ZenML integration prerequisites.

```sh
    zenml integration install -y gcp
    ```

```sh
    gcloud auth application-default login
    ```

It seems that the text you provided is incomplete and only contains a placeholder for code. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

````
```

Credentials have been saved to [/home/stefan/.config/gcloud/application_default_credentials.json] and will be used by libraries requesting Application Default Credentials (ADC). The quota project "zenml-core" has been added to ADC for billing and quota purposes, although some services may still bill the project that owns the resource.

```
```

Ensure that the GCP Service Connector Type is available.

```sh
    zenml service-connector list-types --type gcp
    ```

It seems you have not provided the actual documentation text to summarize. Please share the text you would like me to condense, and I will be happy to assist you!

````
```

### Summary of GCP Service Connector Documentation

- **Name**: GCP Service Connector
- **Type**: gcp
- **Resource Types**:
  - gcp-generic
  - gcs-bucket (user-account)
  - kubernetes-cluster (service-account)
  - docker-registry (oauth2-token)
- **Auth Methods**: Implicit
- **Local Access**: Yes
- **Remote Access**: Yes

```
```

To register a multi-type GCP Service Connector using auto-configuration, follow these steps:

1. **Define Service Connector**: Specify the types of services to be connected in your configuration file.
2. **Auto-Configuration**: Ensure that your application is set up to automatically configure the Service Connector based on the defined services.
3. **Deployment**: Deploy your application to GCP, ensuring that the Service Connector is properly registered and functional.

Make sure to verify the connection and functionality of the services after deployment.

```sh
    zenml service-connector register gcp-demo-multi --type gcp --auto-configure
    ```

It appears that the text you provided is incomplete or missing. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

Service connector `gcp-demo-multi` has been successfully registered with access to the following resources:

- **gcp-generic**: zenml-core
- **gcs-bucket**:
  - gs://zenml-bucket-sl
  - gs://zenml-core.appspot.com
  - gs://zenml-core_cloudbuild
  - gs://zenml-datasets
- **kubernetes-cluster**: zenml-test-cluster
- **docker-registry**:
  - gcr.io/zenml-core
  - us.gcr.io/zenml-core
  - eu.gcr.io/zenml-core
  - asia.gcr.io/zenml-core
  - asia-docker.pkg.dev/zenml-core/asia.gcr.io
  - europe-docker.pkg.dev/zenml-core/eu.gcr.io
  - europe-west1-docker.pkg.dev/zenml-core/test
  - us-docker.pkg.dev/zenml-core/gcr.io
  - us-docker.pkg.dev/zenml-core/us.gcr.io

```
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like me to summarize, and I'll be happy to assist you!

```
**NOTE**: from this point forward, we don't need the local GCP CLI credentials or the local GCP CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the GCP project.
```

Identify accessible GCS buckets, GAR registries, and GKE Kubernetes clusters to configure the Stack Components in the minimal GCP stack, which includes a GCS Artifact Store, a Kubernetes Orchestrator, and a GCP Container Registry.

````
```

The command `sh zenml service-connector list-resources --resource-type gcs-bucket` is used to list all resources of the type Google Cloud Storage (GCS) bucket within the ZenML service connector.

```

```

It seems that the documentation text you provided is incomplete and only includes a code title without any actual content. Please provide the full documentation text you would like summarized, and I will be happy to assist you!

````
```

The following 'gcs-bucket' resources are accessible via configured service connectors:

| CONNECTOR ID                          | CONNECTOR NAME  | CONNECTOR TYPE | RESOURCE TYPE | RESOURCE NAMES                       |
|---------------------------------------|------------------|----------------|---------------|--------------------------------------|
| eeeabc13-9203-463b-aa52-216e629e903c | gcp-demo-multi   | 🔵 gcp         | 📦 gcs-bucket | gs://zenml-bucket-sl                 |
|                                       |                  |                |               | gs://zenml-core.appspot.com          |
|                                       |                  |                |               | gs://zenml-core_cloudbuild           |
|                                       |                  |                |               | gs://zenml-datasets                  |

```
```

It appears that the text you provided is incomplete or contains only a code block delimiter without any actual content to summarize. Please provide the relevant documentation text for summarization.

````
```

The command `sh zenml service-connector list-resources --resource-type kubernetes-cluster` is used to list all resources of the type "kubernetes-cluster" within the ZenML service connector.

```

```

It seems that the text you provided is incomplete and only contains a code title without any additional content or context. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

The following 'kubernetes-cluster' resources are accessible via configured service connectors:

| CONNECTOR ID                             | CONNECTOR NAME  | CONNECTOR TYPE | RESOURCE TYPE       | RESOURCE NAMES       |
|------------------------------------------|------------------|----------------|----------------------|-----------------------|
| eeeabc13-9203-463b-aa52-216e629e903c    | gcp-demo-multi   | 🔵 gcp         | 🌀 kubernetes-cluster | zenml-test-cluster    |

```
```

It seems that the text you provided is incomplete or contains only a code termination tag. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

````
```

The command `sh zenml service-connector list-resources --resource-type docker-registry` is used to list all resources of the type "docker-registry" in the ZenML service connector.

```

```

It seems that the text you provided is incomplete and only contains a code title without any additional content or context. Please provide the full documentation text you would like summarized, and I will be happy to assist you!

````
```

The 'docker-registry' resources accessible by configured service connectors are as follows:

| CONNECTOR ID                           | CONNECTOR NAME | CONNECTOR TYPE | RESOURCE TYPE    | RESOURCE NAMES                                      |
|----------------------------------------|----------------|----------------|------------------|-----------------------------------------------------|
| eeeabc13-9203-463b-aa52-216e629e903c | gcp-demo-multi | 🔵 gcp         | 🐳 docker-registry| gcr.io/zenml-core, us.gcr.io/zenml-core, eu.gcr.io/zenml-core, asia.gcr.io/zenml-core, asia-docker.pkg.dev/zenml-core/asia.gcr.io, europe-docker.pkg.dev/zenml-core/eu.gcr.io, europe-west1-docker.pkg.dev/zenml-core/test, us-docker.pkg.dev/zenml-core/gcr.io, us-docker.pkg.dev/zenml-core/us.gcr.io |

This table summarizes the connector ID, name, type, resource type, and associated resource names.

```
```

To register and connect a GCS Artifact Store Stack Component to a GCS bucket, follow these steps: 

1. **Register the Component**: Use the appropriate command or API to register the GCS Artifact Store component within your stack.
2. **Connect to GCS Bucket**: Specify the GCS bucket details, including the bucket name and any necessary authentication credentials, to establish the connection.

Ensure all configurations are correctly set to facilitate seamless interaction with the GCS bucket.

```sh
    zenml artifact-store register gcs-zenml-bucket-sl --flavor gcp --path=gs://zenml-bucket-sl
    ```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to help!

````
```

The active stack is set to 'default' (global), and the artifact store `gcs-zenml-bucket-sl` has been successfully registered.

```
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I will be happy to assist you.

````
```

To connect to a Google Cloud Storage bucket named `gcs-zenml-bucket-sl` using the `gcp-demo-multi` connector, use the following command:

```bash
sh zenml artifact-store connect gcs-zenml-bucket-sl --connector gcp-demo-multi
```

```

```

It seems that the text you provided is incomplete and only contains a code title without any additional information or context. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

````
```

Running with active stack: 'default' (global). Successfully connected artifact store `gcs-zenml-bucket-sl` to the following resources:

| CONNECTOR ID                           | CONNECTOR NAME | CONNECTOR TYPE | RESOURCE TYPE | RESOURCE NAMES       |
|----------------------------------------|----------------|----------------|---------------|-----------------------|
| eeeabc13-9203-463b-aa52-216e629e903c | gcp-demo-multi | 🔵 gcp         | 📦 gcs-bucket | gs://zenml-bucket-sl  |

```
```

To register and connect a Kubernetes Orchestrator Stack Component to a GKE cluster, follow these steps:

1. Ensure you have the necessary permissions and access to the GKE cluster.
2. Use the appropriate command-line tools or APIs to register the stack component.
3. Configure the connection settings, including authentication and endpoint details.
4. Verify the connection by checking the status of the registered component in the GKE cluster.

Make sure to consult the specific documentation for any additional configuration options or troubleshooting steps.

```sh
    zenml orchestrator register gke-zenml-test-cluster --flavor kubernetes --synchronous=true 
    --kubernetes_namespace=zenml-workloads
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

````
```

The orchestrator `gke-zenml-test-cluster` has been successfully registered while running with the active stack 'default' (global).

```
```

It seems there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

````
```

To connect the ZenML orchestrator to the GKE cluster named "gke-zenml-test-cluster," use the following command:

```
sh zenml orchestrator connect gke-zenml-test-cluster --connector gcp-demo-multi
```

```

```

It seems that the provided text is incomplete. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

````
```

The active stack 'default' is successfully connected to the orchestrator `gke-zenml-test-cluster`. The following resources are linked:

- **Connector ID**: eeeabc13-9203-463b-aa52-216e629e903c
- **Connector Name**: gcp-demo-multi
- **Connector Type**: gcp
- **Resource Type**: kubernetes-cluster
- **Resource Name**: zenml-test-cluster

```
```

To register and connect a GCP Container Registry Stack Component to a GAR registry, follow these steps:

1. **Register the Stack Component**: Use the appropriate command or interface to register the GCP Container Registry Stack Component.
2. **Connect to GAR Registry**: Ensure that the connection to the Google Artifact Registry (GAR) is established, which may involve authentication and permissions setup.
3. **Verify Connection**: Confirm that the Stack Component is successfully connected to the GAR registry.

Ensure all necessary credentials and permissions are in place for a seamless integration.

```sh
    zenml container-registry register gcr-zenml-core --flavor gcp --uri=europe-west1-docker.pkg.dev/zenml-core/test
    ```

It appears that the provided text does not contain any specific documentation content to summarize. Please provide the relevant documentation text, and I will be happy to summarize it for you.

````
```

The active stack is 'default' (global), and the container registry `gcr-zenml-core` has been successfully registered.

```
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I will be happy to assist!

````
```

The command `sh zenml container-registry connect gcr-zenml-core --connector gcp-demo-multi` connects the ZenML framework to the Google Container Registry (GCR) named `gcr-zenml-core` using the connector `gcp-demo-multi`.

```

```

It seems that the text you provided is incomplete and only contains a code title without any additional content or context. Please provide the full documentation text that you would like summarized, and I'll be happy to help!

````
```

Running with active stack: 'default' (global). Successfully connected container registry `gcr-zenml-core` to the following resources:

| CONNECTOR ID                           | CONNECTOR NAME | CONNECTOR TYPE | RESOURCE TYPE    | RESOURCE NAMES                              |
|----------------------------------------|----------------|----------------|------------------|---------------------------------------------|
| eeeabc13-9203-463b-aa52-216e629e903c | gcp-demo-multi | 🔵 gcp         | 🐳 docker-registry| europe-west1-docker.pkg.dev/zenml-core/test|

```
```

Combine all Stack Components into a Stack and set it as active, including a local Image Builder for completeness.

```sh
    zenml image-builder register local --flavor local
    ```

It appears that the provided text does not contain any specific documentation content to summarize. Please provide the relevant documentation text, and I will be happy to assist you in summarizing it while retaining all critical technical information.

````
```

The active stack is 'default' (global), and the image_builder `local` has been successfully registered.

```
```

It seems that the text you provided is incomplete or missing the actual documentation content to summarize. Please provide the relevant documentation text, and I'll be happy to help you summarize it.

````
```

To register a ZenML stack named "gcp-demo," use the following command:

```
sh zenml stack register gcp-demo -a gcs-zenml-bucket-sl -o gke-zenml-test-cluster -c gcr-zenml-core -i local --set
```

This command specifies the following components:
- Artifact Store: `gcs-zenml-bucket-sl`
- Orchestrator: `gke-zenml-test-cluster`
- Container Registry: `gcr-zenml-core`
- Identity: `local`

The `--set` flag is included to apply the configuration immediately.

```

```

It appears that the text you provided is incomplete or missing the actual content to summarize. Please provide the full documentation text for me to summarize effectively.

````
```

The stack 'gcp-demo' has been successfully registered and is now the active global stack.

```
```

To verify that everything functions correctly, execute a basic pipeline. This example will utilize the simplest possible pipelines.

```python
    from zenml import pipeline, step


    @step
    def step_1() -> str:
        """Returns the `world` string."""
        return "world"


    @step(enable_cache=False)
    def step_2(input_one: str, input_two: str) -> None:
        """Combines the two strings at its input and prints them."""
        combined_str = f"{input_one} {input_two}"
        print(combined_str)


    @pipeline
    def my_pipeline():
        output_step_one = step_1()
        step_2(input_one="hello", input_two=output_step_one)


    if __name__ == "__main__":
        my_pipeline()
    ```

To execute the script saved in a `run.py` file, run the file, which will produce the specified command output.

````
```

The command `python run.py` initiates the building of Docker images for the `simple_pipeline`. The image being built is `europe-west1-docker.pkg.dev/zenml-core/test/zenml:simple_pipeline-orchestrator`, which includes integration requirements such as `gcsfs`, `google-cloud-aiplatform>=1.11.0`, `google-cloud-build>=3.11.0`, and others. No `.dockerignore` file is found, so all files in the build context are included.

The Docker build process consists of the following steps:
1. Base image: `FROM zenmldocker/zenml:0.39.1-py3.8`
2. Set working directory: `WORKDIR /app`
3. Copy integration requirements: `COPY .zenml_integration_requirements .`
4. Install requirements: `RUN pip install --default-timeout=60 --no-cache-dir -r .zenml_integration_requirements`
5. Set environment variables: 
   - `ENV ZENML_ENABLE_REPO_INIT_WARNINGS=False`
   - `ENV ZENML_CONFIG_PATH=/app/.zenconfig`
6. Copy all files: `COPY . .`
7. Set permissions: `RUN chmod -R a+rw .`

The Docker image is then pushed to the specified repository. The pipeline `simple_pipeline` is executed on the `gcp-demo` stack with caching disabled. The Kubernetes orchestrator pod starts, followed by the execution of two steps:
- `step_1` completes in 1.357 seconds.
- `step_2` outputs "Hello World!" and finishes in 3.136 seconds.

The orchestration pod completes, and the dashboard URL is provided: `http://34.148.132.191/default/pipelines/cec118d1-d90a-44ec-8bd7-d978f726b7aa/runs`.

```
```

### Summary

This documentation outlines an end-to-end workflow using multiple single-instance GCP Service Connectors within a ZenML Stack. The Stack includes the following components, each linked through its Service Connector:

- **VertexAI Orchestrator**: Connected to the GCP project.
- **GCS Artifact Store**: Linked to a GCS bucket.
- **GCP Container Registry**: Associated with a GCR container registry.
- **Google Cloud Image Builder**: Connected to the GCP project.

The workflow culminates in running a simple pipeline on the configured Stack. To set up, configure the local GCP CLI with valid user credentials (using `gcloud auth application-default login`) and install ZenML integration prerequisites.

```sh
    zenml integration install -y gcp
    ```

```sh
    gcloud auth application-default login
    ```

It seems that the text you provided is incomplete. Please provide the full documentation text you would like summarized, and I'll be happy to help!

````
```

Credentials have been saved to [/home/stefan/.config/gcloud/application_default_credentials.json] and will be used by libraries requesting Application Default Credentials (ADC). The quota project "zenml-core" has been added to ADC for billing and quota purposes, although some services may still bill the project owning the resource.

```
```

Ensure the GCP Service Connector Type is available.

```sh
    zenml service-connector list-types --type gcp
    ```

It seems that the text you provided is incomplete and only contains a code title without any additional content or context. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

### Summary of GCP Service Connector Documentation

- **Name**: GCP Service Connector
- **Type**: gcp
- **Resource Types**:
  - gcp-generic
  - gcs-bucket (user-account)
  - kubernetes-cluster (service-account)
  - docker-registry (oauth2-token)
- **Authentication Methods**: Implicit
- **Local Access**: Yes
- **Remote Access**: Yes

```
```

To register a single-instance GCP Service Connector using auto-configuration, create the following resources for Stack Components: a GCS bucket, a GCR registry, and generic GCP access for the VertexAI orchestrator and GCP Cloud Builder.

```sh
    zenml service-connector register gcs-zenml-bucket-sl --type gcp --resource-type gcs-bucket --resource-id gs://zenml-bucket-sl --auto-configure
    ```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist you!

````
```

Successfully registered the service connector `gcs-zenml-bucket-sl` with access to the GCS bucket resource: 

- **Resource Type:** gcs-bucket 
- **Resource Name:** gs://zenml-bucket-sl

```
```

It appears that the text you provided is incomplete and only contains a code block delimiter. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

To register a service connector for Google Cloud Platform (GCP) with ZenML, use the following command:

```bash
sh zenml service-connector register gcr-zenml-core --type gcp --resource-type docker-registry --auto-configure
```

This command registers a Docker registry service connector named `gcr-zenml-core` and enables automatic configuration.

```

```

It appears that the documentation text you provided is incomplete, as it only includes a code title without any actual content or details. Please provide the full documentation text for summarization.

````
```

The service connector `gcr-zenml-core` has been successfully registered with access to the following Docker registry resources:

- gcr.io/zenml-core
- us.gcr.io/zenml-core
- eu.gcr.io/zenml-core
- asia.gcr.io/zenml-core
- asia-docker.pkg.dev/zenml-core/asia.gcr.io
- europe-docker.pkg.dev/zenml-core/eu.gcr.io
- europe-west1-docker.pkg.dev/zenml-core/test
- us-docker.pkg.dev/zenml-core/gcr.io
- us-docker.pkg.dev/zenml-core/us.gcr.io

```
```

It appears that the text you provided is incomplete or consists only of a code block ending tag. Please provide the full documentation text that you would like summarized, and I'll be happy to assist!

````
```

To register a service connector for Vertex AI in ZenML, use the following command:

```bash
sh zenml service-connector register vertex-ai-zenml-core --type gcp --resource-type gcp-generic --auto-configure
```

This command registers the service connector with GCP as the type and specifies the resource type as GCP generic, enabling automatic configuration.

```

```

It appears that the provided text is incomplete and only contains a code block title without any actual content or documentation details. Please provide the full documentation text for summarization.

````
```

The service connector `vertex-ai-zenml-core` has been successfully registered with access to the resource type `gcp-generic`, specifically the resource named `zenml-core`.

```
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like me to summarize, and I'll be happy to help!

````
```

To register a service connector for Google Cloud Platform (GCP) using ZenML, use the following command:

```bash
sh zenml service-connector register gcp-cloud-builder-zenml-core --type gcp --resource-type gcp-generic --auto-configure
```

This command registers a GCP service connector with automatic configuration.

```

```

It seems that the text you provided is incomplete and only contains a code title without any accompanying content. Please provide the full documentation text for summarization.

````
```

The service connector `gcp-cloud-builder-zenml-core` has been successfully registered with access to the resource type `gcp-generic`, specifically the resource named `zenml-core`.

```
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

````
**NOTE**: from this point forward, we don't need the local GCP CLI credentials or the local GCP CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the GCP project.

In the end, the service connector list should look like this:

```

The command `sh zenml service-connector list` is used to display a list of available service connectors in ZenML. This command provides users with an overview of the connectors that can be utilized within their ZenML projects.

```

```

It seems that the text you provided is incomplete and only contains a placeholder for code output. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

````
```

The documentation presents a table of active resources in a GCP environment, detailing the following key points:

1. **Resource Overview**:
   - **gcs-zenml-bucket-sl**: 
     - ID: 405034fe-5e6e-4d29-ba62-8ae025381d98
     - Type: GCP
     - Resource Type: GCS Bucket
     - Resource Name: gs://zenml-bucket-sl
     - Shared: No
     - Owner: Default

   - **gcr-zenml-core**: 
     - ID: 9fddfaba-6d46-4806-ad96-9dcabef74639
     - Type: GCP
     - Resource Type: Docker Registry
     - Resource Name: gcr.io/zenml-core
     - Shared: No
     - Owner: Default

   - **vertex-ai-zenml-core**: 
     - ID: f97671b9-8c73-412b-bf5e-4b7c48596f5f
     - Type: GCP
     - Resource Type: GCP Generic
     - Resource Name: zenml-core
     - Shared: No
     - Owner: Default

   - **gcp-cloud-builder-zenml-core**: 
     - ID: 648c1016-76e4-4498-8de7-808fd20f057b
     - Type: GCP
     - Resource Type: GCP Generic
     - Resource Name: zenml-core
     - Shared: No
     - Owner: Default

2. **Common Attributes**:
   - All resources are owned by the default user and are not shared. 
   - Expiration details are not specified for any resources. 

This summary encapsulates the essential technical details without redundancy.

```
```

To register and connect a GCS Artifact Store Stack Component to a GCS bucket, follow these steps: 

1. **Register the Component**: Use the appropriate command or API to register the GCS Artifact Store.
2. **Connect to GCS Bucket**: Specify the GCS bucket details in the configuration settings to establish the connection.

Ensure that all necessary permissions and configurations are in place for successful integration.

```sh
    zenml artifact-store register gcs-zenml-bucket-sl --flavor gcp --path=gs://zenml-bucket-sl
    ```

It appears that the text you provided is incomplete and only contains a placeholder for code output. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

````
```

The active stack is set to 'default' (global), and the artifact store `gcs-zenml-bucket-sl` has been successfully registered.

```
```

It appears that the text you provided is incomplete or contains only a code block delimiter without any actual content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

````
```

To connect to a Google Cloud Storage (GCS) bucket using ZenML, use the following command:

```
sh zenml artifact-store connect gcs-zenml-bucket-sl --connector gcs-zenml-bucket-sl
```

This command establishes a connection to the specified GCS bucket for artifact storage.

```

```

It seems that the provided text is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text for summarization.

````
```

The active stack 'default' is successfully connected to the artifact store `gcs-zenml-bucket-sl`. The following resource details are noted:

- **Connector ID**: 405034fe-5e6e-4d29-ba62-8ae025381d98
- **Connector Name**: gcs-zenml-bucket-sl
- **Connector Type**: GCP
- **Resource Type**: GCS Bucket
- **Resource Name**: gs://zenml-bucket-sl

```
```

To register and connect a Google Cloud Image Builder Stack Component to your target GCP project, follow these steps:

1. **Register the Component**: Use the Google Cloud Console or CLI to register the Image Builder Stack Component with your GCP project.
2. **Connect to Project**: Ensure that the component is linked to the correct project by verifying the project ID and permissions.
3. **Configuration**: Configure any necessary settings specific to your project requirements.

Make sure to check for any prerequisites or permissions needed for successful registration and connection.

```sh
    zenml image-builder register gcp-zenml-core --flavor gcp
    ```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

````
```

The image builder `gcp-zenml-core` has been successfully registered while running with the active stack 'default' (repository).

```
```

It seems that the text you provided is incomplete or consists of a code block delimiter without any content to summarize. Please provide the actual documentation text you would like me to summarize, and I'll be happy to help!

````
```

To connect the ZenML image builder to Google Cloud Platform (GCP), use the following command:

```
sh zenml image-builder connect gcp-zenml-core --connector gcp-cloud-builder-zenml-core
``` 

This command links the ZenML image builder with the specified GCP connector.

```

```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text, and I will be happy to help you summarize it while retaining all critical information.

````
```

The active stack 'default' is running successfully with the image builder `gcp-zenml-core`. It is connected to the following resource:

- **Connector ID**: 648c1016-76e4-4498-8de7-808fd20f057b
- **Connector Name**: gcp-cloud-builder-zenml-core
- **Connector Type**: gcp
- **Resource Type**: gcp-generic
- **Resource Name**: zenml-core

```
```

To register and connect a Vertex AI Orchestrator Stack Component to a target GCP project, note that if no workload service account is specified, the default Compute Engine service account will be used. This account must have the Vertex AI Service Agent role granted to avoid pipeline failures. Additional configuration options for the Vertex AI Orchestrator are available [here](../../../component-guide/orchestrators/vertex.md#how-to-use-it).

```sh
    zenml orchestrator register vertex-ai-zenml-core --flavor=vertex --location=europe-west1 --synchronous=true
    ```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the actual documentation text you would like summarized, and I will be happy to assist you!

````
```

The active stack 'default' (repository) is running, and the orchestrator `vertex-ai-zenml-core` has been successfully registered.

```
```

It seems that the text you provided is incomplete or contains only a code block delimiter without any actual content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

````
```

To connect the ZenML orchestrator to Vertex AI, use the following command:

```bash
sh zenml orchestrator connect vertex-ai-zenml-core --connector vertex-ai-zenml-core
```

```

```

It seems that the text you provided is incomplete and only includes a code title without any actual content or details to summarize. Please provide the full documentation text for me to summarize effectively.

````
```

Running with active stack: 'default' (repository). Successfully connected orchestrator `vertex-ai-zenml-core` to resources:

| CONNECTOR ID                           | CONNECTOR NAME          | CONNECTOR TYPE | RESOURCE TYPE   | RESOURCE NAMES |
|----------------------------------------|-------------------------|----------------|------------------|-----------------|
| f97671b9-8c73-412b-bf5e-4b7c48596f5f | vertex-ai-zenml-core   | 🔵 gcp          | 🔵 gcp-generic    | zenml-core      |

```
```

To register and connect a GCP Container Registry Stack Component to a GCR container registry, follow these steps:

1. **Setup GCP Project**: Ensure you have a Google Cloud project with billing enabled.
2. **Enable APIs**: Activate the Container Registry API in your project.
3. **Authenticate**: Use the Google Cloud SDK to authenticate your local environment with `gcloud auth login`.
4. **Create a GCR Repository**: Use the command `gcloud artifacts repositories create [REPOSITORY_NAME] --repository-format=docker --location=[LOCATION]` to create a new container registry.
5. **Tag and Push Images**: Tag your Docker images with the GCR path and push them using `docker push [GCR_PATH]`.

Ensure you have the necessary IAM permissions to access and manage the GCR.

```sh
    zenml container-registry register gcr-zenml-core --flavor gcp --uri=gcr.io/zenml-core 
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

````
```

The active stack 'default' (repository) is running, and the container registry `gcr-zenml-core` has been successfully registered.

```
```

It appears that the text you provided is incomplete or consists only of a code block delimiter without any actual content to summarize. Please provide the relevant documentation text, and I will gladly summarize it for you.

````
```

To connect to the Google Container Registry (GCR) using ZenML, use the following command:

```
sh zenml container-registry connect gcr-zenml-core --connector gcr-zenml-core
``` 

This command establishes a connection to the specified GCR connector.

```

```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

````
```

The active stack 'default' is running, and the container registry `gcr-zenml-core` has been successfully connected to the following resource:

- **Connector ID**: 9fddfaba-6d46-4806-ad96-9dcabef74639
- **Connector Name**: gcr-zenml-core
- **Connector Type**: GCP
- **Resource Type**: Docker Registry
- **Resource Name**: gcr.io/zenml-core

```
```

To combine all Stack Components into a Stack and set it as active, follow these steps: 

1. Integrate all individual Stack Components.
2. Designate the combined Stack as the active one.

Ensure all components are correctly configured before activation.

```sh
    zenml stack register gcp-demo -a gcs-zenml-bucket-sl -o vertex-ai-zenml-core -c gcr-zenml-core -i gcp-zenml-core --set
    ```

It seems that the text you provided is incomplete. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

````
```

The stack 'gcp-demo' has been successfully registered, and the active repository stack is set to 'gcp-demo'.

```
```

To verify functionality, execute a basic pipeline. This example will utilize the simplest pipeline configuration available.

```python
    from zenml import pipeline, step


    @step
    def step_1() -> str:
        """Returns the `world` string."""
        return "world"


    @step(enable_cache=False)
    def step_2(input_one: str, input_two: str) -> None:
        """Combines the two strings at its input and prints them."""
        combined_str = f"{input_one} {input_two}"
        print(combined_str)


    @pipeline
    def my_pipeline():
        output_step_one = step_1()
        step_2(input_one="hello", input_two=output_step_one)


    if __name__ == "__main__":
        my_pipeline()
    ```

To execute the code saved in a `run.py` file, simply run the file, which will produce the specified output.

````
```

The process begins with the command `python run.py`, which builds Docker images for the pipeline `simple_pipeline`. The image `gcr.io/zenml-core/zenml:simple_pipeline-orchestrator` is created, including integration requirements such as `gcsfs`, `google-cloud-aiplatform>=1.11.0`, and others. The build uses Cloud Build and uploads the context to `gs://zenml-bucket-sl/cloud-build-contexts/...`. 

The build logs can be accessed at: [Cloud Build Logs](https://console.cloud.google.com/cloud-build/builds/068e77a1-4e6f-427a-bf94-49c52270af7a?project=20219041791). The Docker image is built successfully, and the pipeline `simple_pipeline` is executed on the stack `gcp-demo`, with caching disabled. An automatic `pipeline_root` is generated: `gs://zenml-bucket-sl/vertex_pipeline_root/simple_pipeline/simple_pipeline_default_6e72f3e1`.

A warning indicates that v1 APIs will not be supported by the v2 compiler. The Vertex workflow definition is written to a specified path, and a one-off vertex job is created and submitted to the Vertex AI Pipelines service using the service account `connectors-vertex-ai-workload@zenml-core.iam.gserviceaccount.com`. 

The PipelineJob is created with the resource name: `projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1`. To access this job in another session, use: 
```python
pipeline_job = aiplatform.PipelineJob.get('projects/20219041791/locations/europe-west1/pipelineJobs/simple-pipeline-default-6e72f3e1')
```
The job can be viewed at: [Pipeline Job](https://console.cloud.google.com/vertex-ai/locations/europe-west1/pipelines/runs/simple-pipeline-default-6e72f3e1?project=20219041791). 

The job's state is monitored until completion, after which the final state is logged. The dashboard URL for the completed run is: [Dashboard](https://34.148.132.191/default/pipelines/17cac6b5-3071-45fa-a2ef-cda4a7965039/runs).

```
```

The documentation includes an image related to ZenML Scarf, identified by the URL: `https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc`.



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/README.md

### Connect Services (AWS, GCP, Azure, K8s, etc.)

Connecting your ZenML deployment to cloud providers and other infrastructure services is crucial for a production-grade MLOps platform. This involves configuring secure access to various resources, such as AWS S3 buckets, Kubernetes clusters, and container registries. 

ZenML simplifies this process by allowing authentication information to be embedded in Stack Components. However, this approach does not scale well and poses usability and security challenges. Proper authentication and authorization setup is essential, especially when services need to interact, such as a Kubernetes container accessing an S3 bucket or cloud services like AWS SageMaker.

There is no universal standard for authentication and authorization, but ZenML offers an abstraction through **ZenML Service Connectors**, which manage this complexity and implement security best practices.

#### Use Case Example

To illustrate the functionality of Service Connectors, consider connecting ZenML to an AWS S3 bucket using the AWS Service Connector. This allows linking an S3 Artifact Store Stack Component to the S3 bucket.

#### Alternatives to Service Connectors

While there are quicker alternatives, such as embedding authentication information directly into Stack Components, this is not recommended due to security concerns. Using Service Connectors is the preferred method for maintaining secure and manageable connections.

```shell
    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --key=AWS_ACCESS_KEY --secret=AWS_SECRET_KEY
    ```

A ZenML secret can store AWS credentials, which can then be referenced in the S3 Artifact Store configuration attributes.

```shell
    zenml secret create aws --aws_access_key_id=AWS_ACCESS_KEY --aws_secret_access_key=AWS_SECRET_KEY
    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --key='{{aws.aws_access_key_id}}' --secret='{{aws.aws_secret_access_key}}'
    ```

To enhance the S3 Artifact Store configuration, reference the secret directly within the configuration settings.

```shell
    zenml secret create aws --aws_access_key_id=AWS_ACCESS_KEY --aws_secret_access_key=AWS_SECRET_KEY
    zenml artifact-store register s3 --flavor s3 --path=s3://BUCKET_NAME --authentication_secret=aws
    ```

The documentation outlines the limitations of using Stack Components for managing credentials in pipelines:

1. **Limited Support**: Not all Stack Components can reference secrets in configuration attributes.
2. **Portability Issues**: Some components, especially those linked to Kubernetes, require credentials to be set up on the pipeline machine, complicating portability.
3. **Cloud SDKs Required**: Certain components necessitate the installation of cloud-specific SDKs and CLIs.
4. **Access to Credentials**: Users need access to cloud credentials, requiring knowledge of the cloud provider platform.
5. **Security Risks**: Long-lived credentials can pose security risks if compromised; rotating them is complex and maintenance-heavy.
6. **Lack of Validation**: Stack Components do not verify the validity or permissions of configured credentials, leading to potential runtime failures.
7. **Redundant Logic**: Duplicating authentication and authorization logic across different Stack Component implementations is poor design.

Service Connectors address these drawbacks by acting as brokers for credential management. They validate credentials on the ZenML server, converting them into short-lived credentials with limited privileges. This allows multiple Stack Components to utilize the same Service Connector for accessing various resources.

To work with Service Connectors, users should first identify the types of resources ZenML can connect to, which can help in planning infrastructure for MLOps platforms or integrating specific Stack Component flavors. A list of available Service Connector types will provide insights into possible configurations.

```sh
zenml service-connector list-types
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS     │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password         │ ✅    │ ✅     ┃
┃                              │               │                       │ token            │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃   Docker Service Connector   │ 🐳 docker     │ 🐳 docker-registry    │ password         │ ✅    │ ✅     ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃    AWS Service Connector     │ 🔶 aws        │ 🔶 aws-generic        │ implicit         │ ✅    │ ✅     ┃
┃                              │               │ 📦 s3-bucket          │ secret-key       │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ sts-token        │       │        ┃
┃                              │               │ 🐳 docker-registry    │ iam-role         │       │        ┃
┃                              │               │                       │ session-token    │       │        ┃
┃                              │               │                       │ federation-token │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃    GCP Service Connector     │ 🔵 gcp        │ 🔵 gcp-generic        │ implicit         │ ✅    │ ✅     ┃
┃                              │               │ 📦 gcs-bucket         │ user-account     │       │        ┃
┃                              │               │ 🌀 kubernetes-cluster │ service-account  │       │        ┃
┃                              │               │ 🐳 docker-registry    │ oauth2-token     │       │        ┃
┃                              │               │                       │ impersonation    │       │        ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃  HyperAI Service Connector   │ 🤖 hyperai    │ 🤖 hyperai-instance   │ rsa-key          │ ✅   │ ✅     ┃
┃                              │               │                       │ dsa-key          │       │        ┃
┃                              │               │                       │ ecdsa-key        │       │        ┃
┃                              │               │                       │ ed25519-key      │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

Service Connector Types are displayed in the dashboard during the configuration of a new Service Connector. For example, when connecting an S3 bucket to an S3 Artifact Store Stack Component, the AWS Service Connector Type is used. 

Before configuring a Service Connector, it's important to understand the capabilities and supported authentication methods of the Service Connector Type. This information can be accessed via the CLI or the dashboard. Examples of the AWS Service Connector Type are provided for reference.

```sh
zenml service-connector describe-type aws
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like me to summarize, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                🔶 AWS Service Connector (connector type: aws)                ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 secret-key                                                                
 • 🔒 sts-token                                                                 
 • 🔒 iam-role                                                                  
 • 🔒 session-token                                                             
 • 🔒 federation-token                                                          
                                                                                
Resource types:                                                                 
                                                                                
 • 🔶 aws-generic                                                               
 • 📦 s3-bucket                                                                 
 • 🌀 kubernetes-cluster                                                        
 • 🐳 docker-registry                                                           
                                                                                
Supports auto-configuration: True                                               
                                                                                
Available locally: True                                                         
                                                                                
Available remotely: True                                                        
                                                                                
The ZenML AWS Service Connector facilitates the authentication and access to    
managed AWS services and resources. These encompass a range of resources,       
including S3 buckets, ECR repositories, and EKS clusters. The connector provides
support for various authentication methods, including explicit long-lived AWS   
secret keys, IAM roles, short-lived STS tokens and implicit authentication.     
                                                                                
To ensure heightened security measures, this connector also enables the         
generation of temporary STS security tokens that are scoped down to the minimum 
permissions necessary for accessing the intended resource. Furthermore, it      
includes automatic configuration and detection of credentials locally configured
through the AWS CLI.                                                            
                                                                                
This connector serves as a general means of accessing any AWS service by issuing
pre-authenticated boto3 sessions to clients. Additionally, the connector can    
handle specialized authentication for S3, Docker and Kubernetes Python clients. 
It also allows for the configuration of local Docker and Kubernetes CLIs.       
                                                                                
The AWS Service Connector is part of the AWS ZenML integration. You can either  
install the entire integration or use a pypi extra to install it independently  
of the integration:                                                             
                                                                                
 • pip install "zenml[connectors-aws]" installs only prerequisites for the AWS    
   Service Connector Type                                                       
 • zenml integration install aws installs the entire AWS ZenML integration      
                                                                                
It is not required to install and set up the AWS CLI on your local machine to   
use the AWS Service Connector to link Stack Components to AWS resources and     
services. However, it is recommended to do so if you are looking for a quick    
setup that includes using the auto-configuration Service Connector features.    
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

The documentation provides a visual representation of the AWS Service Connector Type. It includes details on fetching information about the S3 bucket resource type.

```sh
zenml service-connector describe-type aws --resource-type s3-bucket
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
╔══════════════════════════════════════════════════════════════════════════════╗
║                 📦 AWS S3 bucket (resource type: s3-bucket)                  ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Authentication methods: implicit, secret-key, sts-token, iam-role,              
session-token, federation-token                                                 
                                                                                
Supports resource instances: True                                               
                                                                                
Authentication methods:                                                         
                                                                                
 • 🔒 implicit                                                                  
 • 🔒 secret-key                                                                
 • 🔒 sts-token                                                                 
 • 🔒 iam-role                                                                  
 • 🔒 session-token                                                             
 • 🔒 federation-token                                                          
                                                                                
Allows users to connect to S3 buckets. When used by Stack Components, they are  
provided a pre-configured boto3 S3 client instance.                             
                                                                                
The configured credentials must have at least the following AWS IAM permissions 
associated with the ARNs of S3 buckets that the connector will be allowed to    
access (e.g. arn:aws:s3:::* and arn:aws:s3:::*/* represent all the available S3 
buckets).                                                                       

 • s3:ListBucket
 • s3:GetObject
 • s3:PutObject
 • s3:DeleteObject
 • s3:ListAllMyBuckets
 • s3:GetBucketVersioning
 • s3:ListBucketVersions
 • s3:DeleteObjectVersion

If set, the resource name must identify an S3 bucket using one of the following 
formats:                                                                        
                                                                                
 • S3 bucket URI (canonical resource name): s3://{bucket-name}                  
 • S3 bucket ARN: arn:aws:s3:::{bucket-name}                                    
 • S3 bucket name: {bucket-name}                                                
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

The documentation provides details on the AWS Session Token authentication method, illustrated with an image of the AWS Service Connector Type.

```sh
zenml service-connector describe-type aws --auth-method session-token
```

It appears that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will be happy to assist you.

```
╔══════════════════════════════════════════════════════════════════════════════╗
║              🔒 AWS Session Token (auth method: session-token)               ║
╚══════════════════════════════════════════════════════════════════════════════╝
                                                                                
Supports issuing temporary credentials: True                                    
                                                                                
Generates temporary session STS tokens for IAM users. The connector needs to be 
configured with an AWS secret key associated with an IAM user or AWS account    
root user (not recommended). The connector will generate temporary STS tokens   
upon request by calling the GetSessionToken STS API.                            
                                                                                
These STS tokens have an expiration period longer that those issued through the 
AWS IAM Role authentication method and are more suitable for long-running       
processes that cannot automatically re-generate credentials upon expiration.    
                                                                                
An AWS region is required and the connector may only be used to access AWS      
resources in the specified region.                                              
                                                                                
The default expiration period for generated STS tokens is 12 hours with a       
minimum of 15 minutes and a maximum of 36 hours. Temporary credentials obtained 
by using the AWS account root user credentials (not recommended) have a maximum 
duration of 1 hour.                                                             
                                                                                
As a precaution, when long-lived credentials (i.e. AWS Secret Keys) are detected
on your environment by the Service Connector during auto-configuration, this    
authentication method is automatically chosen instead of the AWS Secret Key     
authentication method alternative.                                              
                                                                                
Generated STS tokens inherit the full set of permissions of the IAM user or AWS 
account root user that is calling the GetSessionToken API. Depending on your    
security needs, this may not be suitable for production use, as it can lead to  
accidental privilege escalation. Instead, it is recommended to use the AWS      
Federation Token or AWS IAM Role authentication methods to restrict the         
permissions of the generated STS tokens.                                        
                                                                                
For more information on session tokens and the GetSessionToken AWS API, see: the
official AWS documentation on the subject.                                      
                                                                                
Attributes:                                                                     
                                                                                
 • aws_access_key_id {string, secret, required}: AWS Access Key ID              
 • aws_secret_access_key {string, secret, required}: AWS Secret Access Key      
 • region {string, required}: AWS Region                                        
 • endpoint_url {string, optional}: AWS Endpoint URL                            
                                                                                
────────────────────────────────────────────────────────────────────────────────
```

Not all Stack Components can be linked to a Service Connector; this is specified in each component's flavor description. The example provided uses the S3 Artifact Store, which does support this functionality.

```sh
$ zenml artifact-store flavor describe s3
Configuration class: S3ArtifactStoreConfig

[...]

This flavor supports connecting to external resources with a Service Connector. It requires a 's3-bucket' resource. You can get a list of all available connectors and the compatible resources that they can 
access by running:

'zenml service-connector list-resources --resource-type s3-bucket'
If no compatible Service Connectors are yet registered, you can register a new one by running:

'zenml service-connector register -i'
```

The second step is to _register a Service Connector_, allowing ZenML to authenticate and access remote resources. This process is best performed by someone with infrastructure knowledge, but most Service Connectors have defaults and auto-detection features that simplify the task. In this example, we register an AWS Service Connector using AWS credentials automatically obtained from your local host, enabling ZenML to access the same resources available through the AWS CLI. This assumes the AWS CLI is installed and configured on your machine (e.g., by running `aws configure`).

```sh
zenml service-connector register aws-s3 --type aws --auto-configure --resource-type s3-bucket
```

It seems that the provided text does not contain any content to summarize. Please provide the documentation text you would like summarized, and I'll be happy to assist!

```
⠼ Registering service connector 'aws-s3'...
Successfully registered service connector `aws-s3` with access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://aws-ia-mwaa-715803424590         ┃
┃               │ s3://zenbytes-bucket                  ┃
┃               │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┃               │ s3://zenml-public-datasets            ┃
┃               │ s3://zenml-public-swagger-spec        ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The CLI validates and displays all accessible S3 buckets using auto-discovered credentials. To register Service Connectors interactively, use the `-i` command line argument and follow the guide.

```
zenml service-connector register -i
```

During auto-configuration, the Service Connector automatically detects and configures settings. This process streamlines setup by identifying necessary parameters and establishing connections without manual input.

```sh
zenml service-connector describe aws-s3
```

It seems there was an issue with the text you intended to provide for summarization. Please share the documentation text again, and I'll be happy to summarize it for you!

```
Service connector 'aws-s3' of type 'aws' with id '96a92154-4ec7-4722-bc18-21eeeadb8a4f' is owned by user 'default' and is 'private'.
          'aws-s3' aws Service Connector Details           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ ID               │ 96a92154-4ec7-4722-bc18-21eeeadb8a4f ┃
┠──────────────────┼──────────────────────────────────────┨
┃ NAME             │ aws-s3                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ AUTH METHOD      │ session-token                        ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                         ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SECRET ID        │ a8c6d0ff-456a-4b25-8557-f0d7e3c12c5f ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SESSION DURATION │ 43200s                               ┃
┠──────────────────┼──────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ OWNER            │ default                              ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SHARED           │ ➖                                   ┃
┠──────────────────┼──────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-15 18:45:17.822337           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-15 18:45:17.822341           ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

The AWS Service Connector securely retrieves the AWS Secret Key from the local machine and stores it in the Secrets Store. It enforces a security best practice by keeping the AWS Secret Key hidden on the ZenML Server, ensuring clients do not access it directly. Instead, the connector generates short-lived security tokens for client access to AWS resources and manages token renewal. This process is indicated by the `session-token` authentication method and session duration attributes. To verify this, one can request ZenML to display the configuration for a Service Connector client, requiring the selection of an S3 bucket for temporary credential generation.

```sh
zenml service-connector describe aws-s3 --resource-id s3://zenfiles
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

```
Service connector 'aws-s3 (s3-bucket | s3://zenfiles client)' of type 'aws' with id '96a92154-4ec7-4722-bc18-21eeeadb8a4f' is owned by user 'default' and is 'private'.
    'aws-s3 (s3-bucket | s3://zenfiles client)' aws Service     
                       Connector Details                        
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                     ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ ID               │ 96a92154-4ec7-4722-bc18-21eeeadb8a4f      ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ NAME             │ aws-s3 (s3-bucket | s3://zenfiles client) ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                    ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                 ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                              ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://zenfiles                             ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ SECRET ID        │                                           ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                       ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h59m56s                                 ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ OWNER            │ default                                   ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ SHARED           │ ➖                                        ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-15 18:56:33.880081                ┃
┠──────────────────┼───────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-15 18:56:33.880082                ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

The configuration involves a temporary AWS STS token that expires in 12 hours, with the AWS Secret Key hidden from the client side. The next step is to configure and connect Stack Components to a remote resource using the previously registered Service Connector. This process is straightforward; for example, you can specify that an S3 Artifact Store should use the `s3://my-bucket` S3 bucket without needing to understand the authentication mechanisms or resource provenance. An example follows, demonstrating the creation of an S3 Artifact store linked to the specified S3 bucket.

```sh
zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
zenml artifact-store connect s3-zenfiles --connector aws-s3
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
$ zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
Successfully registered artifact_store `s3-zenfiles`.

$ zenml artifact-store connect s3-zenfiles --connector aws-s3
Successfully connected artifact store `s3-zenfiles` to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃             CONNECTOR ID             │ CONNECTOR NAME │ CONNECTOR TYPE │ RESOURCE TYPE │ RESOURCE NAMES ┃
┠──────────────────────────────────────┼────────────────┼────────────────┼───────────────┼────────────────┨
┃ 96a92154-4ec7-4722-bc18-21eeeadb8a4f │ aws-s3         │ 🔶 aws         │ 📦 s3-bucket  │ s3://zenfiles  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

The ZenML CLI offers an interactive method to connect a stack component to an external resource. Use the `-i` command line argument to access the interactive guide.

```
zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
zenml artifact-store connect s3-zenfiles -i
```

The S3 Artifact Store Stack Component is now connected to the infrastructure and ready for use in a stack to run a pipeline.

```sh
zenml stack register s3-zenfiles -o default -a s3-zenfiles --set
```

A simple pipeline consists of a series of stages that process data sequentially. Each stage performs a specific function, transforming the input data into output for the next stage. Key components include:

1. **Input Stage**: Receives raw data.
2. **Processing Stages**: Perform operations such as filtering, transformation, or aggregation.
3. **Output Stage**: Produces the final result or stores the processed data.

This structure allows for efficient data handling and modular design, facilitating easier updates and maintenance.

```python
from zenml import step, pipeline

@step
def simple_step_one() -> str:
    """Simple step one."""
    return "Hello World!"


@step
def simple_step_two(msg: str) -> None:
    """Simple step two."""
    print(msg)


@pipeline
def simple_pipeline() -> None:
    """Define single step pipeline."""
    message = simple_step_one()
    simple_step_two(msg=message)


if __name__ == "__main__":
    simple_pipeline()
```

To execute the script, save the code as `run.py` and run it using the appropriate command.

```sh
python run.py
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

```
Running pipeline simple_pipeline on stack s3-zenfiles (caching enabled)
Step simple_step_one has started.
Step simple_step_one has finished in 1.065s.
Step simple_step_two has started.
Hello World!
Step simple_step_two has finished in 5.681s.
Pipeline run simple_pipeline-2023_06_15-19_29_42_159831 has finished in 12.522s.
Dashboard URL: http://127.0.0.1:8237/default/pipelines/8267b0bc-9cbd-42ac-9b56-4d18275bdbb4/runs
```

This documentation provides a brief overview of using Service Connectors to integrate ZenML Stack Components with various infrastructures. ZenML includes built-in Service Connectors for AWS, GCP, and Azure, supporting multiple authentication methods and security best practices. 

Key resources include:

- **[Complete Guide to Service Connectors](./service-connectors-guide.md)**: Comprehensive information on utilizing Service Connectors.
- **[Security Best Practices](./best-security-practices.md)**: Guidelines for authentication methods used by Service Connectors.
- **[Docker Service Connector](./docker-service-connector.md)**: Connect ZenML to a Docker container registry.
- **[Kubernetes Service Connector](./kubernetes-service-connector.md)**: Connect ZenML to a Kubernetes cluster.
- **[AWS Service Connector](./aws-service-connector.md)**: Connect ZenML to AWS resources.
- **[GCP Service Connector](./gcp-service-connector.md)**: Connect ZenML to GCP resources.
- **[Azure Service Connector](./azure-service-connector.md)**: Connect ZenML to Azure resources.

![ZenML Scarf](https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc)



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/kubernetes-service-connector.md

### Kubernetes Service Connector

The ZenML Kubernetes service connector enables authentication and connection to Kubernetes clusters. It provides pre-authenticated Kubernetes Python clients to Stack Components and allows configuration of the local Kubernetes CLI (`kubectl`).

#### Prerequisites

- The Kubernetes Service Connector is part of the Kubernetes ZenML integration. 
- To install only the Kubernetes Service Connector, use:  
  `pip install "zenml[connectors-kubernetes]"`
- To install the entire Kubernetes ZenML integration, use:  
  `zenml integration install kubernetes`
- A local Kubernetes CLI (`kubectl`) and its configuration are not required to access Kubernetes clusters through the connector.

```shell
$ zenml service-connector list-types --type kubernetes
```

```
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃             NAME             │ TYPE          │ RESOURCE TYPES        │ AUTH METHODS │ LOCAL │ REMOTE ┃
┠──────────────────────────────┼───────────────┼───────────────────────┼──────────────┼───────┼────────┨
┃ Kubernetes Service Connector │ 🌀 kubernetes │ 🌀 kubernetes-cluster │ password     │ ✅    │ ✅     ┃
┃                              │               │                       │ token        │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Resource Types
The Kubernetes Service Connector supports authentication and access for generic Kubernetes clusters, identified by the `kubernetes-cluster` Resource Type. The resource name is a user-friendly cluster name set during registration.

## Authentication Methods
Two authentication methods are available:
1. Username and password (not recommended for production).
2. Authentication token (with or without client certificates). For local K3D clusters, an empty token can be used.

**Warning:** The Service Connector does not generate short-lived credentials; configured credentials are directly distributed to clients for authentication to the Kubernetes API. It is advisable to use API tokens with client certificates when possible.

## Auto-configuration
The Service Connector can fetch credentials from the local Kubernetes CLI (`kubectl`) during registration, using the current Kubernetes context. An example includes accessing a GKE cluster.

```sh
zenml service-connector register kube-auto --type kubernetes --auto-configure
```

It seems that you have not provided the documentation text to summarize. Please provide the text you'd like me to condense, and I'll be happy to help!

```text
Successfully registered service connector `kube-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES ┃
┠───────────────────────┼────────────────┨
┃ 🌀 kubernetes-cluster │ 35.185.95.223  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

```sh
zenml service-connector describe kube-auto 
```

It seems you've provided a placeholder for code output without any actual content to summarize. Please provide the specific documentation text or content you'd like summarized, and I'll be happy to assist!

```text
Service connector 'kube-auto' of type 'kubernetes' with id '4315e8eb-fcbd-4938-a4d7-a9218ab372a1' is owned by user 'default' and is 'private'.
     'kube-auto' kubernetes Service Connector Details      
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ ID               │ 4315e8eb-fcbd-4938-a4d7-a9218ab372a1 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ NAME             │ kube-auto                            ┃
┠──────────────────┼──────────────────────────────────────┨
┃ TYPE             │ 🌀 kubernetes                        ┃
┠──────────────────┼──────────────────────────────────────┨
┃ AUTH METHOD      │ token                                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🌀 kubernetes-cluster                ┃
┠──────────────────┼──────────────────────────────────────┨
┃ RESOURCE NAME    │ 35.175.95.223                        ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SECRET ID        │ a833e86d-b845-4584-9656-4b041335e299 ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                  ┃
┠──────────────────┼──────────────────────────────────────┨
┃ OWNER            │ default                              ┃
┠──────────────────┼──────────────────────────────────────┨
┃ SHARED           │ ➖                                   ┃
┠──────────────────┼──────────────────────────────────────┨
┃ CREATED_AT       │ 2023-05-16 21:45:33.224740           ┃
┠──────────────────┼──────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-05-16 21:45:33.224743           ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                  Configuration                  
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE                 ┃
┠───────────────────────┼───────────────────────┨
┃ server                │ https://35.175.95.223 ┃
┠───────────────────────┼───────────────────────┨
┃ insecure              │ False                 ┃
┠───────────────────────┼───────────────────────┨
┃ cluster_name          │ 35.175.95.223         ┃
┠───────────────────────┼───────────────────────┨
┃ token                 │ [HIDDEN]              ┃
┠───────────────────────┼───────────────────────┨
┃ certificate_authority │ [HIDDEN]              ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┛
```

Credentials auto-discovered via the Kubernetes Service Connector may have a limited lifetime, particularly with third-party authentication providers like GCP or AWS. Using short-lived credentials can result in connectivity issues and errors in your pipeline.

## Local Client Provisioning
The Service Connector enables the configuration of the local Kubernetes client (`kubectl`) with credentials.

```sh
zenml service-connector login kube-auto 
```

It seems that the documentation text you intended to provide is missing. Please provide the text you'd like summarized, and I'll be happy to assist you!

```text
⠦ Attempting to configure local client using service connector 'kube-auto'...
Cluster "35.185.95.223" set.
⠇ Attempting to configure local client using service connector 'kube-auto'...
⠏ Attempting to configure local client using service connector 'kube-auto'...
Updated local kubeconfig with the cluster details. The current kubectl context was set to '35.185.95.223'.
The 'kube-auto' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.
```

## Stack Components

The Kubernetes Service Connector enables the management of Kubernetes container workloads in Orchestrator and Model Deployer stack components without requiring explicit configuration of `kubectl` contexts and credentials.



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/aws-service-connector.md

### AWS Service Connector

The ZenML AWS Service Connector enables authentication and access to AWS resources such as S3 buckets, ECR container repositories, and EKS clusters. It supports various authentication methods, including long-lived AWS secret keys, IAM roles, short-lived STS tokens, and implicit authentication. 

Key features include:
- Generation of temporary STS security tokens with minimized permissions for resource access.
- Automatic detection of locally configured AWS CLI credentials.
- Issuance of pre-authenticated boto3 sessions for general AWS service access.
- Specialized authentication support for S3, Docker, and Kubernetes Python clients.
- Configuration capabilities for local Docker and Kubernetes CLIs.

```shell
$ zenml service-connector list-types --type aws
```

```shell
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃         NAME          │ TYPE   │ RESOURCE TYPES        │ AUTH METHODS     │ LOCAL │ REMOTE ┃
┠───────────────────────┼────────┼───────────────────────┼──────────────────┼───────┼────────┨
┃ AWS Service Connector │ 🔶 aws │ 🔶 aws-generic        │ implicit         │ ✅    │ ✅     ┃
┃                       │        │ 📦 s3-bucket          │ secret-key       │       │        ┃
┃                       │        │ 🌀 kubernetes-cluster │ sts-token        │       │        ┃
┃                       │        │ 🐳 docker-registry    │ iam-role         │       │        ┃
┃                       │        │                       │ session-token    │       │        ┃
┃                       │        │                       │ federation-token │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

The AWS Service Connector for ZenML cannot function if Multi-Factor Authentication (MFA) is enabled on the AWS CLI role. MFA generates temporary credentials that are incompatible with the connector, which requires long-lived credentials. To use the connector, set the `AWS_PROFILE` environment variable to a profile without MFA before executing ZenML CLI commands.

### Prerequisites
- The AWS Service Connector is part of the AWS ZenML integration. You can install it in two ways:
  - `pip install "zenml[connectors-aws]"` for the AWS Service Connector only.
  - `zenml integration install aws` for the complete AWS ZenML integration.
  
While installing the AWS CLI is not mandatory for linking Stack Components to AWS resources, it is recommended for quick setup and auto-configuration features. If you prefer not to install the AWS CLI, use the interactive mode of the ZenML CLI to register Service Connectors.

```
zenml service-connector register -i --type aws
```

## Resource Types

### Generic AWS Resource
- Connects to any AWS service/resource via AWS Service Connector.
- Provides a pre-configured Python boto3 session with AWS credentials.
- Used for Stack Components not covered by specific resource types (e.g., S3, EKS).
- Requires matching AWS permissions for remote resource access.
- Resource name indicates the AWS region for access.

### S3 Bucket
- Connects to S3 buckets with a pre-configured boto3 S3 client.
- Requires specific AWS IAM permissions for S3 bucket access:
  - `s3:ListBucket`
  - `s3:GetObject`
  - `s3:PutObject`
  - `s3:DeleteObject`
  - `s3:ListAllMyBuckets`
  - `s3:GetBucketVersioning`
  - `s3:ListBucketVersions`
  - `s3:DeleteObjectVersion`
- Resource name formats:
  - S3 bucket URI: `s3://{bucket-name}`
  - S3 bucket ARN: `arn:aws:s3:::{bucket-name}`
  - S3 bucket name: `{bucket-name}`

### EKS Kubernetes Cluster
- Accesses EKS clusters as standard Kubernetes resources.
- Provides a pre-authenticated Python Kubernetes client.
- Requires specific AWS IAM permissions for EKS cluster access:
  - `eks:ListClusters`
  - `eks:DescribeCluster`
- Resource name formats:
  - EKS cluster name: `{cluster-name}`
  - EKS cluster ARN: `arn:aws:eks:{region}:{account-id}:cluster/{cluster-name}`
- IAM principal must be added to the EKS cluster's `aws-auth` ConfigMap if not using the same IAM user/role that created the cluster.

### ECR Container Registry
- Accesses ECR repositories as a Docker registry resource.
- Provides a pre-authenticated Python Docker client.
- Requires specific AWS IAM permissions for ECR repository access:
  - `ecr:DescribeRegistry`
  - `ecr:DescribeRepositories`
  - `ecr:ListRepositories`
  - `ecr:BatchGetImage`
  - `ecr:DescribeImages`
  - `ecr:BatchCheckLayerAvailability`
  - `ecr:GetDownloadUrlForLayer`
  - `ecr:InitiateLayerUpload`
  - `ecr:UploadLayerPart`
  - `ecr:CompleteLayerUpload`
  - `ecr:PutImage`
  - `ecr:GetAuthorizationToken`
- Resource name formats:
  - ECR repository URI: `[https://]{account}.dkr.ecr.{region}.amazonaws.com[/{repository-name}]`
  - ECR repository ARN: `arn:aws:ecr:{region}:{account-id}:repository[/{repository-name}]`

## Authentication Methods

### Implicit Authentication
- Uses environment variables, local configuration files, or IAM roles.
- Disabled by default; requires enabling via `ZENML_ENABLE_IMPLICIT_AUTH_METHODS`.
- Automatically discovers credentials from:
  - Environment variables (e.g., AWS_ACCESS_KEY_ID)
  - Local AWS CLI configuration files
  - IAM roles attached to AWS resources
- Can be less secure; recommended to configure IAM roles to limit permissions.
- EKS cluster's `aws-auth` ConfigMap may need manual configuration for access.
- Requires AWS region specification for resource access.

### Example Configuration
- Assumes local AWS CLI has a `connectors` profile configured with credentials.

```sh
AWS_PROFILE=connectors zenml service-connector register aws-implicit --type aws --auth-method implicit --region=us-east-1
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
⠸ Registering service connector 'aws-implicit'...
Successfully registered service connector `aws-implicit` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┃                       │ s3://zenml-public-datasets                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector does not store any credentials.

```sh
zenml service-connector describe aws-implicit 
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```
Service connector 'aws-implicit' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.
                         'aws-implicit' aws Service Connector Details                         
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 9a810521-ef41-4e45-bb48-8569c5943dc6                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-implicit                                                            ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ implicit                                                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                                         ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 18:08:37.969928                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 18:08:37.969930                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
     Configuration      
┏━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY │ VALUE     ┃
┠──────────┼───────────┨
┃ region   │ us-east-1 ┃
┗━━━━━━━━━━┷━━━━━━━━━━━┛
```

To verify access to resources, ensure the `AWS_PROFILE` environment variable points to the same AWS CLI profile used during registration. Note that using a different profile may yield different results, making this method unsuitable for reproducible outcomes.

```sh
AWS_PROFILE=connectors zenml service-connector verify aws-implicit --resource-type s3-bucket
```

It seems that you have not provided the documentation text to summarize. Please share the text you would like me to condense, and I'll be happy to help!

```
⠸ Verifying service connector 'aws-implicit'...
Service connector 'aws-implicit' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                        ┃
┠───────────────┼───────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://zenfiles                         ┃
┃               │ s3://zenml-demos                      ┃
┃               │ s3://zenml-generative-chat            ┃
┃               │ s3://zenml-public-datasets            ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```sh
zenml service-connector verify aws-implicit --resource-type s3-bucket
```

It seems that you've provided a placeholder for code output but no actual documentation text to summarize. Please provide the specific documentation text you would like summarized, and I'll be happy to assist!

```
⠸ Verifying service connector 'aws-implicit'...
Service connector 'aws-implicit' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ RESOURCE TYPE │ RESOURCE NAMES                                 ┃
┠───────────────┼────────────────────────────────────────────────┨
┃ 📦 s3-bucket  │ s3://sagemaker-studio-907999144431-m11qlsdyqr8 ┃
┃               │ s3://sagemaker-studio-d8a14tvjsmb              ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

Clients receive either temporary STS tokens or long-lived credentials based on the environment, making this method unsuitable for production use.

```sh
AWS_PROFILE=zenml zenml service-connector describe aws-implicit --resource-type s3-bucket --resource-id zenfiles --client
```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

```
INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials
Service connector 'aws-implicit (s3-bucket | s3://zenfiles client)' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.
    'aws-implicit (s3-bucket | s3://zenfiles client)' aws Service     
                          Connector Details                           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                           ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ ID               │ 9a810521-ef41-4e45-bb48-8569c5943dc6            ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ NAME             │ aws-implicit (s3-bucket | s3://zenfiles client) ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                          ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                       ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                                    ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://zenfiles                                   ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SECRET ID        │                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                             ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 59m57s                                          ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ OWNER            │ default                                         ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                              ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 18:13:34.146659                      ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 18:13:34.146664                      ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```sh
zenml service-connector describe aws-implicit --resource-type s3-bucket --resource-id s3://sagemaker-studio-d8a14tvjsmb --client
```

It seems that the text you provided is incomplete, as it only contains a code title without any accompanying documentation or content to summarize. Please provide the full documentation text, and I'll be happy to help summarize it for you.

```
INFO:botocore.credentials:Found credentials in shared credentials file: ~/.aws/credentials
Service connector 'aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client)' of type 'aws' with id 'e3853748-34a0-4d78-8006-00422ad32884' is owned by user 'default' and is 'private'.
    'aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client)' aws Service     
                                    Connector Details                                     
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ ID               │ 9a810521-ef41-4e45-bb48-8569c5943dc6                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-implicit (s3-bucket | s3://sagemaker-studio-d8a14tvjsmb client) ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ secret-key                                                          ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                                                        ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://sagemaker-studio-d8a14tvjsmb                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                             ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 18:12:42.066053                                          ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 18:12:42.066055                                          ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

### AWS Secret Key

Long-lived AWS credentials consist of an AWS access key ID and secret access key linked to an AWS IAM user or root user (not recommended). This method is suitable for development and testing due to its simplicity but is not advised for production as it grants clients direct access to credentials and full permissions of the associated IAM user or root user.

For production, use AWS IAM Role, AWS Session Token, or AWS Federation Token for authentication. An AWS region is required, and the connector can only access resources in that region. If the local AWS CLI is configured with these credentials, they will be automatically detected during auto-configuration.

#### Example Auto-Configuration
To force the ZenML CLI to use Secret Key authentication, pass the `--auth-method secret-key` option, as it defaults to using AWS Session Token authentication otherwise.

```sh
AWS_PROFILE=connectors zenml service-connector register aws-secret-key --type aws --auth-method secret-key --auto-configure
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like summarized, and I'll be happy to assist you!

```
⠸ Registering service connector 'aws-secret-key'...
Successfully registered service connector `aws-secret-key` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The AWS Secret Key was extracted from the local host.

```sh
zenml service-connector describe aws-secret-key
```

It seems that the text you provided is incomplete, as it only contains a placeholder for code output without any actual content or context. Please provide the complete documentation text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-secret-key' of type 'aws' with id 'a1b07c5a-13af-4571-8e63-57a809c85790' is owned by user 'default' and is 'private'.
                        'aws-secret-key' aws Service Connector Details                        
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 37c97fa0-fa47-4d55-9970-e2aa6e1b50cf                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-secret-key                                                          ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ secret-key                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ b889efe1-0e23-4e2d-afc3-bdd785ee2d80                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:23:39.982950                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:23:39.982952                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

### AWS STS Token Uses

Temporary STS tokens can be user-configured or auto-configured from a local environment. A key limitation is that users must regularly generate new tokens and update the connector configuration as tokens expire. This method is suitable for short-term access, such as temporary team sharing. 

In contrast, using authentication methods like IAM roles, Session Tokens, or Federation Tokens allows for automatic generation and refreshing of STS tokens upon request. Note that an AWS region is required, and the connector can only access resources within that specified region.

#### Example Auto-Configuration

To fetch STS tokens from the local AWS CLI, ensure it is configured with valid credentials. For instance, if the `connectors` AWS CLI profile uses an IAM user Secret Key, the ZenML CLI must be instructed to use STS token authentication by passing the `--auth-method sts-token` option; otherwise, it defaults to session token authentication.

```sh
AWS_PROFILE=connectors zenml service-connector register aws-sts-token --type aws --auto-configure --auth-method sts-token
```

It seems that the text you provided is incomplete and only contains a placeholder for code output. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```
⠸ Registering service connector 'aws-sts-token'...
Successfully registered service connector `aws-sts-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector is configured with an STS token.

```sh
zenml service-connector describe aws-sts-token
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

```
Service connector 'aws-sts-token' of type 'aws' with id '63e14350-6719-4255-b3f5-0539c8f7c303' is owned by user 'default' and is 'private'.
                        'aws-sts-token' aws Service Connector Details                         
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ a05ef4ef-92cb-46b2-8a3a-a48535adccaf                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-sts-token                                                           ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ bffd79c7-6d76-483b-9001-e9dda4e865ae                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h58m24s                                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:25:40.278681                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:25:40.278684                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

The Service Connector is temporary and will become unusable in 12 hours.

```sh
zenml service-connector list --name aws-sts-token
```

It appears that the provided text does not contain any actual documentation content to summarize. Please provide the relevant documentation text you would like summarized, and I will be happy to assist you.

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME          │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼───────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-sts-token │ a05ef4ef-92cb-46b2-8a3a-a48535adccaf │ 🔶 aws │ 🔶 aws-generic        │ <multiple>    │ ➖     │ default │ 11h57m51s  │        ┃
┃        │               │                                      │        │ 📦 s3-bucket          │               │        │         │            │        ┃
┃        │               │                                      │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │               │                                      │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

### AWS IAM Role and Temporary STS Credentials

AWS IAM roles generate temporary STS credentials by assuming a role, requiring explicit credential configuration. For ZenML servers running in AWS, using implicit authentication with a configured IAM role is recommended for security benefits. 

**Configuration Requirements:**
- The connector must be set up with the IAM role to assume, along with an AWS secret key or STS token from another IAM role.
- The IAM user or role must have permission to assume the target IAM role.

**Token Generation:**
- The connector generates temporary STS tokens by calling the AssumeRole STS API.
- Best practices suggest minimizing permissions for the primary IAM user/role and granting them to the privilege-bearing IAM role instead.

**Region and Policies:**
- An AWS region is required; the connector can only access resources in that region.
- Optional IAM session policies can further restrict permissions of generated STS tokens, which default to the minimum permissions necessary for the target resource.

**Token Expiration:**
- Default expiration for STS tokens is 1 hour (minimum 15 minutes, up to the IAM role's maximum duration, which can be set to 12 hours).
- For longer-lived tokens, consider configuring the IAM role for a higher maximum expiration or using AWS Federation Token or Session Token methods.

For further details on IAM roles and the AssumeRole API, refer to the [official AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#api_assumerole). For differences between this method and AWS Federation Token authentication, see [this AWS documentation page](https://aws.amazon.com/blogs/security/understanding-the-api-options-for-securely-delegating-access-to-your-aws-account/).

<details>
<summary>Example auto-configuration</summary>
Assumes the local AWS CLI has a `zenml` profile configured with an AWS Secret Key and an IAM role to be assumed.
</details>

```sh
AWS_PROFILE=zenml zenml service-connector register aws-iam-role --type aws --auto-configure
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
⠸ Registering service connector 'aws-iam-role'...
Successfully registered service connector `aws-iam-role` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector configuration includes an IAM role and long-lived credentials.

```sh
zenml service-connector describe aws-iam-role
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-iam-role' of type 'aws' with id '8e499202-57fd-478e-9d2f-323d76d8d211' is owned by user 'default' and is 'private'.
                         'aws-iam-role' aws Service Connector Details                         
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 2b99de14-6241-4194-9608-b9d478e1bcfc                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-iam-role                                                            ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ iam-role                                                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 87795fdd-b70e-4895-b0dd-8bca5fd4d10e                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ 3600s                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:28:31.679843                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:28:31.679848                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                          Configuration                                           
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE                                                                  ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ region                │ us-east-1                                                              ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ role_arn              │ arn:aws:iam::715803424590:role/OrganizationAccountRestrictedAccessRole ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ aws_access_key_id     │ [HIDDEN]                                                               ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ aws_secret_access_key │ [HIDDEN]                                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

Clients receive temporary STS tokens instead of the configured AWS Secret Key in the connector. Key points to note include the authentication method, expiration time, and credentials.

```sh
zenml service-connector describe aws-iam-role --resource-type s3-bucket --resource-id zenfiles --client
```

It seems that the text you provided is incomplete. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```
Service connector 'aws-iam-role (s3-bucket | s3://zenfiles client)' of type 'aws' with id '8e499202-57fd-478e-9d2f-323d76d8d211' is owned by user 'default' and is 'private'.
    'aws-iam-role (s3-bucket | s3://zenfiles client)' aws Service     
                          Connector Details                           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                           ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ ID               │ 2b99de14-6241-4194-9608-b9d478e1bcfc            ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ NAME             │ aws-iam-role (s3-bucket | s3://zenfiles client) ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                          ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                       ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                                    ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://zenfiles                                   ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SECRET ID        │                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                             ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 59m56s                                          ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ OWNER            │ default                                         ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                              ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:30:51.462445                      ┃
┠──────────────────┼─────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:30:51.462449                      ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

### AWS Session Token Overview

AWS Session Tokens generate temporary STS tokens for IAM users. The connector requires an AWS secret key linked to an IAM user or AWS account root user (the latter is not recommended). It calls the GetSessionToken STS API to generate these tokens, which have a longer expiration period than those from AWS IAM Role authentication, making them suitable for long-running processes.

Key Points:
- **Expiration**: Default is 12 hours; minimum is 15 minutes, maximum is 36 hours. Tokens from root user credentials last up to 1 hour.
- **Region Specific**: The connector can only access resources in the specified AWS region.
- **Permissions**: STS tokens inherit the full permissions of the calling IAM user or root user, which may lead to privilege escalation. For enhanced security, use AWS Federation Token or AWS IAM Role authentication to restrict permissions.
- **Auto-Configuration**: If long-lived credentials (AWS Secret Keys) are detected, the connector defaults to this authentication method.

For detailed information on session tokens and the GetSessionToken API, refer to the [official AWS documentation](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp_request.html#api_getsessiontoken).

```sh
AWS_PROFILE=connectors zenml service-connector register aws-session-token --type aws --auth-method session-token --auto-configure
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like summarized, and I'll be happy to assist!

```
⠸ Registering service connector 'aws-session-token'...
Successfully registered service connector `aws-session-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector configuration indicates that long-lived credentials were removed from the local environment and the AWS Session Token authentication method was set up.

```sh
zenml service-connector describe aws-session-token
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text or additional details, and I will be happy to help summarize it for you.

```
Service connector 'aws-session-token' of type 'aws' with id '3ae3e595-5cbc-446e-be64-e54e854e0e3f' is owned by user 'default' and is 'private'.
                      'aws-session-token' aws Service Connector Details                       
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ c0f8e857-47f9-418b-a60f-c3b03023da54                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-session-token                                                       ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ session-token                                                           ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 16f35107-87ef-4a86-bbae-caa4a918fc15                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ 43200s                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:31:54.971869                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:31:54.971871                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

Clients receive temporary STS tokens instead of the configured AWS Secret Key in the connector. Important details include the authentication method, expiration time, and credentials.

```sh
zenml service-connector describe aws-session-token --resource-type s3-bucket --resource-id zenfiles --client
```

It seems that the text you provided is incomplete and only includes a code title without any accompanying content. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

```
Service connector 'aws-session-token (s3-bucket | s3://zenfiles client)' of type 'aws' with id '3ae3e595-5cbc-446e-be64-e54e854e0e3f' is owned by user 'default' and is 'private'.
    'aws-session-token (s3-bucket | s3://zenfiles client)' aws Service     
                             Connector Details                             
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ ID               │ c0f8e857-47f9-418b-a60f-c3b03023da54                 ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ NAME             │ aws-session-token (s3-bucket | s3://zenfiles client) ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                               ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                            ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                                         ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://zenfiles                                        ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                      ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                  ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h59m56s                                            ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ OWNER            │ default                                              ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                   ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:35:24.090861                           ┃
┠──────────────────┼──────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:35:24.090863                           ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

### AWS Federation Token Overview

AWS Federation Token generates temporary STS tokens for federated users by impersonating another user. The connector requires an AWS secret key linked to an IAM user (not root user) with permission to call the GetFederationToken STS API (`sts:GetFederationToken` on `*` resource). 

Key Points:
- **Temporary STS Tokens**: Generated upon request via the GetFederationToken API, suitable for long-running processes due to longer expiration periods compared to AWS IAM Role tokens.
- **Region Requirement**: The connector is restricted to the specified AWS region.
- **IAM Session Policies**: Optional policies can be configured to limit permissions of STS tokens. If not specified, default policies restrict permissions to the minimum required for the target resource.
- **Warning**: For the generic AWS resource type, a session policy must be specified; otherwise, STS tokens will lack permissions.
- **Expiration**: Default is 12 hours (min 15 mins, max 36 hours). Tokens from root user credentials have a max duration of 1 hour.
- **EKS Access**: The EKS cluster's `aws-auth` ConfigMap may need manual configuration for federated user authentication.

For further details on user federation tokens, session policies, and the GetFederationToken API, refer to the official AWS documentation. For differences between this method and AWS IAM Role authentication, consult the relevant AWS documentation page. 

#### Example Auto-Configuration
Assumes the local AWS CLI has a `connectors` profile configured with an AWS Secret Key.

```sh
AWS_PROFILE=connectors zenml service-connector register aws-federation-token --type aws --auth-method federation-token --auto-configure
```

It appears that you have not provided the documentation text to summarize. Please provide the text you would like me to condense, and I'll be happy to assist you!

```
⠸ Registering service connector 'aws-federation-token'...
Successfully registered service connector `aws-federation-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector configuration indicates that long-lived credentials have been retrieved from the local AWS CLI configuration.

```sh
zenml service-connector describe aws-federation-token
```

It appears that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-federation-token' of type 'aws' with id '868b17d4-b950-4d89-a6c4-12e520e66610' is owned by user 'default' and is 'private'.
                     'aws-federation-token' aws Service Connector Details                     
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ e28c403e-8503-4cce-9226-8a7cd7934763                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-federation-token                                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ federation-token                                                        ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 958b840d-2a27-4f6b-808b-c94830babd99                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ 43200s                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:36:28.619751                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:36:28.619753                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

Clients receive temporary STS tokens instead of the configured AWS Secret Key in the connector. Important details include the authentication method, expiration time, and credentials.

```sh
zenml service-connector describe aws-federation-token --resource-type s3-bucket --resource-id zenfiles --client
```

It appears that you intended to provide a specific documentation text for summarization, but the text is missing. Please provide the documentation content you would like summarized, and I'll be happy to assist!

```
Service connector 'aws-federation-token (s3-bucket | s3://zenfiles client)' of type 'aws' with id '868b17d4-b950-4d89-a6c4-12e520e66610' is owned by user 'default' and is 'private'.
    'aws-federation-token (s3-bucket | s3://zenfiles client)' aws Service     
                              Connector Details                               
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ ID               │ e28c403e-8503-4cce-9226-8a7cd7934763                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ NAME             │ aws-federation-token (s3-bucket | s3://zenfiles client) ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ sts-token                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 📦 s3-bucket                                            ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ s3://zenfiles                                           ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                         ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 11h59m56s                                               ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:38:29.406986                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:38:29.406991                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
            Configuration            
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE     ┃
┠───────────────────────┼───────────┨
┃ region                │ us-east-1 ┃
┠───────────────────────┼───────────┨
┃ aws_access_key_id     │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_secret_access_key │ [HIDDEN]  ┃
┠───────────────────────┼───────────┨
┃ aws_session_token     │ [HIDDEN]  ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┛
```

## Auto-configuration

The AWS Service Connector enables auto-discovery and fetching of credentials and configurations set up by the AWS CLI during registration. The default AWS CLI profile is utilized unless the AWS_PROFILE environment variable specifies a different profile.

### Auto-configuration Example

An example demonstrates the lifting of AWS credentials to access the same AWS resources and services permitted by the local AWS CLI. In this scenario, the IAM role authentication method was automatically detected.

```sh
AWS_PROFILE=zenml zenml service-connector register aws-auto --type aws --auto-configure
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will be happy to assist you!

```
⠹ Registering service connector 'aws-auto'...
Successfully registered service connector `aws-auto` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                               ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃    🔶 aws-generic     │ us-east-1                                    ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃     📦 s3-bucket      │ s3://zenbytes-bucket                         ┃
┃                       │ s3://zenfiles                                ┃
┃                       │ s3://zenml-demos                             ┃
┃                       │ s3://zenml-generative-chat                   ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster                             ┃
┠───────────────────────┼──────────────────────────────────────────────┨
┃  🐳 docker-registry   │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector configuration demonstrates the automatic retrieval of credentials from the local AWS CLI configuration.

```sh
zenml service-connector describe aws-auto
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist you!

```
Service connector 'aws-auto' of type 'aws' with id '9f3139fd-4726-421a-bc07-312d83f0c89e' is owned by user 'default' and is 'private'.
                           'aws-auto' aws Service Connector Details                           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 9cdc926e-55d7-49f0-838e-db5ac34bb7dc                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ aws-auto                                                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🔶 aws                                                                  ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ iam-role                                                                ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🔶 aws-generic, 📦 s3-bucket, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ a137151e-1778-4f50-b64b-7cf6c1f715f5                                    ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ 3600s                                                                   ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                     ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                 ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                      ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-19 19:39:11.958426                                              ┃
┠──────────────────┼─────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-19 19:39:11.958428                                              ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                                          Configuration                                           
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY              │ VALUE                                                                  ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ region                │ us-east-1                                                              ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ role_arn              │ arn:aws:iam::715803424590:role/OrganizationAccountRestrictedAccessRole ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ aws_access_key_id     │ [HIDDEN]                                                               ┃
┠───────────────────────┼────────────────────────────────────────────────────────────────────────┨
┃ aws_secret_access_key │ [HIDDEN]                                                               ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

## Local Client Provisioning

The local AWS CLI, Kubernetes `kubectl`, and Docker CLI can be configured with credentials from a compatible AWS Service Connector. Unlike AWS CLI configurations, Kubernetes and Docker credentials have a short lifespan and require regular refreshing for security reasons.

### Important Note
Configuring the local AWS CLI with Service Connector credentials creates a configuration profile named after the first eight digits of the Service Connector UUID. For example, a Service Connector with UUID `9f3139fd-4726-421a-bc07-312d83f0c89e` will create a profile named `zenml-9f3139fd`.

### Example
An example of configuring the local Kubernetes CLI to access an EKS cluster via an AWS Service Connector is provided in the documentation.

```sh
zenml service-connector list --name aws-session-token
```

It seems that the text you provided is incomplete, as it only includes a code block title without any actual content or documentation to summarize. Please provide the full documentation text, and I will be happy to summarize it for you.

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME              │ ID                                   │ TYPE   │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼───────────────────┼──────────────────────────────────────┼────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ aws-session-token │ c0f8e857-47f9-418b-a60f-c3b03023da54 │ 🔶 aws │ 🔶 aws-generic        │ <multiple>    │ ➖     │ default │            │        ┃
┃        │                   │                                      │        │ 📦 s3-bucket          │               │        │         │            │        ┃
┃        │                   │                                      │        │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                   │                                      │        │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

The AWS Service Connector checks the Kubernetes clusters it can access.

```sh
zenml service-connector verify aws-session-token --resource-type kubernetes-cluster
```

It seems that you have not provided the actual documentation text to summarize. Please share the text you would like me to summarize, and I'll be happy to assist you!

```
Service connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES   ┃
┠───────────────────────┼──────────────────┨
┃ 🌀 kubernetes-cluster │ zenhacks-cluster ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━┛
```

Running the `login` CLI command configures the local `kubectl` CLI for accessing the Kubernetes cluster.

```sh
zenml service-connector login aws-session-token --resource-type kubernetes-cluster --resource-id zenhacks-cluster
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```
⠇ Attempting to configure local client using service connector 'aws-session-token'...
Cluster "arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster" set.
Context "arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster" modified.
Updated local kubeconfig with the cluster details. The current kubectl context was set to 'arn:aws:eks:us-east-1:715803424590:cluster/zenhacks-cluster'.
The 'aws-session-token' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.
```

To verify that the local `kubectl` CLI is properly configured, use the following command:

```sh
kubectl cluster-info
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist!

```
Kubernetes control plane is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com
CoreDNS is running at https://A5F8F4142FB12DDCDE9F21F6E9B07A18.gr7.us-east-1.eks.amazonaws.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
```

The process for ECR container registries is similar to other container registry operations.

```sh
zenml service-connector verify aws-session-token --resource-type docker-registry
```

It appears that the provided text does not contain any specific documentation content to summarize. Please provide the relevant documentation text for summarization.

```
Service connector 'aws-session-token' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃   RESOURCE TYPE    │ RESOURCE NAMES                               ┃
┠────────────────────┼──────────────────────────────────────────────┨
┃ 🐳 docker-registry │ 715803424590.dkr.ecr.us-east-1.amazonaws.com ┃
┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist you!

```sh
zenml service-connector login aws-session-token --resource-type docker-registry 
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
⠏ Attempting to configure local client using service connector 'aws-session-token'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

The 'aws-session-token' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.
```

To verify that the local Docker client is properly configured, use the following command:

```sh
docker pull 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

```
Using default tag: latest
latest: Pulling from zenml-server
e9995326b091: Pull complete 
f3d7f077cdde: Pull complete 
0db71afa16f3: Pull complete 
6f0b5905c60c: Pull complete 
9d2154d50fd1: Pull complete 
d072bba1f611: Pull complete 
20e776588361: Pull complete 
3ce69736a885: Pull complete 
c9c0554c8e6a: Pull complete 
bacdcd847a66: Pull complete 
482033770844: Pull complete 
Digest: sha256:bf2cc3895e70dfa1ee1cd90bbfa599fa4cd8df837e27184bac1ce1cc239ecd3f
Status: Downloaded newer image for 715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest
715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml-server:latest
```

You can update the local AWS CLI configuration using credentials obtained from the AWS Service Connector.

```sh
zenml service-connector login aws-session-token --resource-type aws-generic
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you would like me to summarize, and I'll be happy to assist!

```
Configured local AWS SDK profile 'zenml-c0f8e857'.
The 'aws-session-token' AWS Service Connector connector was used to successfully configure the local Generic AWS resource client/SDK.
```

A new profile is created in the local AWS CLI configuration to store credentials for accessing AWS resources and services.

```sh
aws --profile zenml-c0f8e857 s3 ls
```

## Stack Components Overview

The **S3 Artifact Store Stack Component** connects to a remote AWS S3 bucket via an **AWS Service Connector**. This connector is compatible with any **Orchestrator** or **Model Deployer** stack component that utilizes Kubernetes clusters, enabling management of EKS Kubernetes workloads without needing explicit AWS or Kubernetes `kubectl` configurations in the environment or Stack Component.

Similarly, **Container Registry Stack Components** can connect to an **ECR Container Registry** through the AWS Service Connector, allowing for the building and publishing of container images to ECR without requiring explicit AWS credentials.

## End-to-End Example

### EKS Kubernetes Orchestrator, S3 Artifact Store, and ECR Container Registry

This example illustrates an end-to-end workflow using a single multi-type AWS Service Connector to access multiple resources for various Stack Components. The complete ZenML Stack includes:

- **Kubernetes Orchestrator** connected to an EKS cluster
- **S3 Artifact Store** linked to an S3 bucket
- **ECR Container Registry** connected to an ECR container registry
- A local **Image Builder**

Finally, a simple pipeline is executed on the resulting Stack.

1. Configure the local AWS CLI with valid IAM user credentials (using `aws configure`) and install ZenML integration prerequisites.

```sh
    zenml integration install -y aws s3
    ```

```sh
    aws configure --profile connectors
    ```

It appears that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will be happy to assist you!

````
```

AWS Access Key ID: AKIAIOSFODNN7EXAMPLE  
AWS Secret Access Key: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY  
Default region name: us-east-1  
Default output format: json

```
```

Ensure the AWS Service Connector Type is available.

```sh
    zenml service-connector list-types --type aws
    ```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

````
```

### Summary of AWS Service Connector Documentation

- **Name**: AWS Service Connector
- **Type**: aws
- **Resource Types**: 
  - aws-generic
  - s3-bucket
  - kubernetes-cluster
  - docker-registry
- **Authentication Methods**: 
  - Implicit
  - Secret Key
  - STS Token
  - IAM Role
  - Session Token
  - Federation Token
- **Local Access**: Yes (✅)
- **Remote Access**: Yes (✅)

```
```

To register a multi-type AWS Service Connector using auto-configuration, follow these steps:

1. **Define Connector**: Specify the service connector in your configuration file, detailing the types of services it will connect to.
2. **Auto-Configuration**: Ensure that auto-configuration is enabled in your application settings to facilitate automatic registration of the connector.
3. **Dependencies**: Include necessary dependencies in your project to support AWS services.
4. **Environment Variables**: Set up required environment variables for AWS credentials and region.
5. **Testing**: Verify the registration by testing the connection to the specified AWS services.

This process streamlines the integration of multiple AWS services within your application.

```sh
    AWS_PROFILE=connectors zenml service-connector register aws-demo-multi --type aws --auto-configure
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I will assist you with it.

````
```

Successfully registered service connector `aws-demo-multi` with access to the following resources:

- **Resource Type: aws-generic**
  - Region: us-east-1

- **Resource Type: s3-bucket**
  - Buckets: 
    - s3://zenfiles
    - s3://zenml-demos
    - s3://zenml-generative-chat

- **Resource Type: kubernetes-cluster**
  - Cluster: zenhacks-cluster

- **Resource Type: docker-registry**
  - Registry: 715803424590.dkr.ecr.us-east-1.amazonaws.com

```
```

It seems that the text you provided is incomplete or contains only a code block delimiter (`{% endcode %}`). Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

```
**NOTE**: from this point forward, we don't need the local AWS CLI credentials or the local AWS CLI at all. The steps that follow can be run on any machine regardless of whether it has been configured and authorized to access the AWS platform or not.
```

Identify accessible S3 buckets, ECR registries, and EKS Kubernetes clusters to configure the Stack Components in the minimal AWS stack, including an S3 Artifact Store, a Kubernetes Orchestrator, and an ECR Container Registry.

````
```

The command `sh zenml service-connector list-resources --resource-type s3-bucket` is used to list all resources of the type S3 bucket in the ZenML service connector.

```

```

It appears that the text you provided is incomplete and only contains a code title without any additional information. Please provide the full documentation text for summarization, and I will be happy to assist you.

````
```

The following 's3-bucket' resources are accessible via configured service connectors:

| CONNECTOR ID                           | CONNECTOR NAME    | CONNECTOR TYPE | RESOURCE TYPE | RESOURCE NAMES                     |
|----------------------------------------|-------------------|----------------|---------------|------------------------------------|
| bf073e06-28ce-4a4a-8100-32e7cb99dced | aws-demo-multi    | 🔶 aws         | 📦 s3-bucket  | s3://zenfiles                      |
|                                        |                   |                |               | s3://zenml-demos                   |
|                                        |                   |                |               | s3://zenml-generative-chat         |

```
```

It appears that the text you provided is incomplete or consists of a code block delimiter without any actual content to summarize. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

The command `sh zenml service-connector list-resources --resource-type kubernetes-cluster` is used to list all resources of the type "kubernetes-cluster" within the ZenML service connector.

```

```

It seems that the text you provided is incomplete and only contains a code title without any additional information or context. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

````
```

The following 'kubernetes-cluster' resources are accessible via configured service connectors:

| CONNECTOR ID                           | CONNECTOR NAME  | CONNECTOR TYPE | RESOURCE TYPE       | RESOURCE NAMES   |
|----------------------------------------|------------------|----------------|---------------------|-------------------|
| bf073e06-28ce-4a4a-8100-32e7cb99dced | aws-demo-multi   | 🔶 aws         | 🌀 kubernetes-cluster| zenhacks-cluster   |

```
```

It seems there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

````
```

The command `sh zenml service-connector list-resources --resource-type docker-registry` is used to list all resources of the type "docker-registry" within the ZenML service connector.

```

```

It appears that the provided text is incomplete and only includes a code title without any actual content or context. Please provide the full documentation text that you would like summarized.

````
```

The following 'docker-registry' resources are accessible via configured service connectors:

| CONNECTOR ID                           | CONNECTOR NAME | CONNECTOR TYPE | RESOURCE TYPE   | RESOURCE NAMES                          |
|----------------------------------------|----------------|----------------|------------------|-----------------------------------------|
| bf073e06-28ce-4a4a-8100-32e7cb99dced | aws-demo-multi | 🔶 aws         | 🐳 docker-registry | 715803424590.dkr.ecr.us-east-1.amazonaws.com |

```
```

To register and connect an S3 Artifact Store Stack Component to an S3 bucket, follow these steps:

1. **Create an S3 Bucket**: Ensure you have an S3 bucket set up in your AWS account.
2. **Register the Component**: Use the appropriate command or API to register the S3 Artifact Store Stack Component.
3. **Connect to the S3 Bucket**: Provide the necessary credentials and configuration settings to link the component to your S3 bucket.

Make sure to verify permissions and access settings to ensure proper connectivity and functionality.

```sh
    zenml artifact-store register s3-zenfiles --flavor s3 --path=s3://zenfiles
    ```

It seems that the provided text is incomplete, as it only includes a code title without any actual content or documentation to summarize. Please provide the relevant documentation text, and I'll be happy to help summarize it.

````
```

The active stack 'default' is running, and the artifact store `s3-zenfiles` has been successfully registered.

```
```

It seems that the text you provided is incomplete or possibly a placeholder. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

````
```

To connect an S3 artifact store named "s3-zenfiles" using the "aws-demo-multi" connector in ZenML, use the following command:

```bash
sh zenml artifact-store connect s3-zenfiles --connector aws-demo-multi
```

```

```

It appears that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text or additional details for me to assist you effectively.

````
```

Running with active stack: 'default' (repository). Successfully connected artifact store `s3-zenfiles` to resources:

| CONNECTOR ID                           | CONNECTOR NAME  | CONNECTOR TYPE | RESOURCE TYPE | RESOURCE NAMES  |
|----------------------------------------|------------------|----------------|----------------|------------------|
| bf073e06-28ce-4a4a-8100-32e7cb99dced  | aws-demo-multi   | 🔶 aws         | 📦 s3-bucket   | s3://zenfiles     |

```
```

To register and connect a Kubernetes Orchestrator Stack Component to an EKS cluster, follow these steps:

1. **Prerequisites**: Ensure you have access to the EKS cluster and necessary permissions.
2. **Install CLI Tools**: Use the AWS CLI and kubectl for interaction with EKS.
3. **Configure AWS CLI**: Set up your AWS credentials and region.
4. **Connect to EKS**: Use `aws eks update-kubeconfig --name <cluster_name>` to configure kubectl to connect to your EKS cluster.
5. **Register Component**: Deploy the Kubernetes Orchestrator Stack Component using the appropriate YAML configuration file.
6. **Verify Connection**: Check the status of the deployed component with `kubectl get pods` to ensure it is running correctly.

Ensure all commands are executed in the correct context and that the necessary IAM roles are assigned for access.

```sh
    zenml orchestrator register eks-zenml-zenhacks --flavor kubernetes --synchronous=true --kubernetes_namespace=zenml-workloads
    ```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text or details that you would like summarized, and I will be happy to assist you.

````
```

The orchestrator `eks-zenml-zenhacks` has been successfully registered while running with the active stack 'default' (repository).

```
```

It seems that the text you provided is incomplete or consists of a code block marker without any actual content to summarize. Please provide the relevant documentation text, and I'll be happy to summarize it for you.

````
```

To connect the ZenML orchestrator to the EKS cluster named "eks-zenml-zenhacks," use the following command:

```
sh zenml orchestrator connect eks-zenml-zenhacks --connector aws-demo-multi
```

```

```

It seems that the provided text does not contain any specific documentation content to summarize. Please provide the actual documentation text you'd like summarized, and I'll be happy to assist!

````
```

The active stack 'default' (repository) is successfully connected to the orchestrator `eks-zenml-zenhacks`. The following resource connection details are provided:

- **Connector ID**: bf073e06-28ce-4a4a-8100-32e7cb99dced
- **Connector Name**: aws-demo-multi
- **Connector Type**: aws
- **Resource Type**: kubernetes-cluster
- **Resource Name**: zenhacks-cluster

```
```

To register and connect an EC GCP Container Registry Stack Component to an ECR container registry, follow these steps:

1. **Register the Stack Component**: Use the appropriate command or interface to register the EC GCP Container Registry Stack Component.
  
2. **Connect to ECR**: Configure the connection settings to link the EC GCP Container Registry with the ECR container registry.

Ensure all necessary credentials and permissions are in place for successful integration.

```sh
    zenml container-registry register ecr-us-east-1 --flavor aws --uri=715803424590.dkr.ecr.us-east-1.amazonaws.com
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist!

````
```

The active stack 'default' (repository) is running, and the container registry `ecr-us-east-1` has been successfully registered.

```
```

It seems that the text you provided is incomplete or consists only of a code block delimiter. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

````
```

To connect to an Amazon ECR container registry using ZenML, use the following command:

```bash
sh zenml container-registry connect ecr-us-east-1 --connector aws-demo-multi
```

This command specifies the ECR region (`ecr-us-east-1`) and the connector (`aws-demo-multi`).

```

```

It seems that the documentation text you provided is incomplete, as it only includes a code title without any actual content or details. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

````
```

The system is running with the active stack 'default' and has successfully connected the container registry `ecr-us-east-1` to the following resource:

- **Connector ID**: bf073e06-28ce-4a4a-8100-32e7cb99dced
- **Connector Name**: aws-demo-multi
- **Connector Type**: AWS
- **Resource Type**: Docker Registry
- **Resource Name**: 715803424590.dkr.ecr.us-east-1.amazonaws.com

```
```

Combine all Stack Components into a Stack and set it as active, including a local Image Builder for completeness.

```sh
    zenml image-builder register local --flavor local
    ```

It appears that you provided a placeholder for code output but did not include the actual documentation text to summarize. Please provide the specific documentation content you would like summarized, and I will be happy to assist!

````
```

The active stack is 'default' (global), and the image_builder `local` has been successfully registered.

```
```

It seems there was an error in your request, as there is no documentation text provided to summarize. Please provide the text you would like me to summarize, and I'll be happy to help!

````
```

The command `sh zenml stack register aws-demo` registers a ZenML stack with the following parameters: 

- **Artifact Store**: `s3-zenfiles`
- **Orchestrator**: `eks-zenml-zenhacks`
- **Container Registry**: `ecr-us-east-1`
- **Identity**: `local`

The `--set` flag is included to apply the specified configurations.

```

```

It seems that the text you provided is incomplete and only includes a code block title without any actual content or documentation to summarize. Please provide the full documentation text for me to summarize effectively.

````
```

Connected to ZenML server at 'https://stefan.develaws.zenml.io'. Stack 'aws-demo' registered successfully. Active repository stack is set to 'aws-demo'.

```
```

To verify functionality, execute a basic pipeline. This example utilizes the simplest pipeline configuration available.

```python
    from zenml import pipeline, step


    @step
    def step_1() -> str:
        """Returns the `world` string."""
        return "world"


    @step(enable_cache=False)
    def step_2(input_one: str, input_two: str) -> None:
        """Combines the two strings at its input and prints them."""
        combined_str = f"{input_one} {input_two}"
        print(combined_str)


    @pipeline
    def my_pipeline():
        output_step_one = step_1()
        step_2(input_one="hello", input_two=output_step_one)


    if __name__ == "__main__":
        my_pipeline()
    ```

To execute the code, save it in a `run.py` file and run the file. The output will be displayed as shown in the example command output.

````
```

The command `python run.py` initiates the building of a Docker image for the `simple_pipeline`. Key steps include:

1. **Image Building**: The image `715803424590.dkr.ecr.us-east-1.amazonaws.com/zenml:simple_pipeline-orchestrator` is built with user-defined requirements (`boto3==1.26.76`) and integration requirements (`boto3`, `kubernetes==18.20.0`, `s3fs>2022.3.0,<=2023.4.0`, `sagemaker==2.117.0`).
2. **Dockerfile Steps**:
   - Base image: `zenmldocker/zenml:0.39.1-py3.8`
   - Set working directory to `/app`
   - Copy user and integration requirements files
   - Install requirements using pip
   - Set environment variables: `ZENML_ENABLE_REPO_INIT_WARNINGS=False`, `ZENML_CONFIG_PATH=/app/.zenconfig`
   - Copy all files and set permissions.

3. **Repository Requirement**: A repository must be created in Amazon ECR before pushing the image. ZenML attempts to push the image but detects no existing repositories.

4. **Pipeline Execution**: The `simple_pipeline` runs on the `aws-demo` stack with caching disabled. 
   - Kubernetes orchestrator pod starts and runs steps sequentially:
     - Step 1 completes in 0.390s.
     - Step 2 outputs "Hello World!" and finishes in 2.364s.
   - The orchestration pod completes successfully.

5. **Dashboard Access**: The run can be monitored at the provided dashboard URL: `https://stefan.develaws.zenml.io/default/pipelines/be5adfe9-45af-4709-a8eb-9522c01640ce/runs`.

```
```

The provided text includes a closing tag for a code block (`{% endcode %}`) and a figure element displaying an image from a specified URL, with an associated alt text ("ZenML Scarf"). There is also an empty figcaption.



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/azure-service-connector.md

**Azure Service Connector Overview**

The ZenML Azure Service Connector enables authentication and access to various Azure resources, including Blob storage, ACR repositories, and AKS clusters. It supports automatic configuration and credential detection via the Azure CLI. The connector facilitates access to any Azure service by issuing credentials to clients and provides specialized authentication for Azure Blob storage, Docker, and Kubernetes Python clients. It also allows for the configuration of local Docker and Kubernetes CLIs.

```shell
$ zenml service-connector list-types --type azure
```

```shell
┏━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃          NAME           │ TYPE     │ RESOURCE TYPES        │ AUTH METHODS      │ LOCAL │ REMOTE ┃
┠─────────────────────────┼──────────┼───────────────────────┼───────────────────┼───────┼────────┨
┃ Azure Service Connector │ 🇦 azure │ 🇦 azure-generic      │ implicit          │ ✅    │ ✅     ┃
┃                         │          │ 📦 blob-container     │ service-principal │       │        ┃
┃                         │          │ 🌀 kubernetes-cluster │ access-token      │       │        ┃
┃                         │          │ 🐳 docker-registry    │                   │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Prerequisites
The Azure Service Connector is part of the Azure ZenML integration. You can install it in two ways:
- `pip install "zenml[connectors-azure]"` for the Azure Service Connector only.
- `zenml integration install azure` for the entire Azure ZenML integration.

Installing the Azure CLI is not mandatory but recommended for quick setup and auto-configuration features. Note that auto-configuration is limited to temporary access tokens, which do not support Azure blob storage resources. For full functionality, configure an Azure service principal.

## Resource Types

### Generic Azure Resource
This resource type allows Stack Components to connect to any Azure service using generic azure-identity credentials. It requires appropriate Azure permissions for the resources accessed.

### Azure Blob Storage Container
Connects to Azure Blob containers using a pre-configured Azure Blob Storage client. Required permissions include:
- Read and write access to blobs (e.g., `Storage Blob Data Contributor` role).
- Listing storage accounts and containers (e.g., `Reader and Data Access` role).

Resource names can be specified as:
- Blob container URI: `{az|abfs}://{container-name}`
- Blob container name: `{container-name}`

The only authentication method for Azure blob storage is the service principal.

### AKS Kubernetes Cluster
Allows access to an AKS cluster using a pre-authenticated python-kubernetes client. Required permissions include:
- Listing AKS clusters and fetching credentials (e.g., `Azure Kubernetes Service Cluster Admin Role`).

Resource names can be specified as:
- Resource group scoped: `[{resource-group}/]{cluster-name}`
- AKS cluster name: `{cluster-name}`

### ACR Container Registry
Enables access to ACR registries via a pre-authenticated python-docker client. Required permissions include:
- Pull and push images (e.g., `AcrPull` and `AcrPush` roles).
- Listing registries (e.g., `Contributor` role).

Resource names can be specified as:
- ACR registry URI: `[https://]{registry-name}.azurecr.io`
- ACR registry name: `{registry-name}`

If using an authentication method other than the Azure service principal, the admin account must be enabled for the registry.

## Authentication Methods

### Implicit Authentication
Implicit authentication can be done using environment variables, local configuration files, workload, or managed identities. This method is disabled by default due to potential security risks and must be enabled via the `ZENML_ENABLE_IMPLICIT_AUTH_METHODS` environment variable.

This method automatically discovers credentials from:
- Environment variables
- Workload identity (for AKS with Managed Identity)
- Managed identity (for Azure-hosted applications)
- Azure CLI (if signed in via `az login`)

The permissions of the discovered credentials can lead to privilege escalation, so using Azure service principal authentication is recommended for production environments.

```sh
zenml service-connector register azure-implicit --type azure --auth-method implicit --auto-configure
```

It seems that the text you provided is incomplete or missing. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you!

```
⠙ Registering service connector 'azure-implicit'...
Successfully registered service connector `azure-implicit` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃   🇦 azure-generic    │ ZenML Subscription                            ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃   📦 blob-container   │ az://demo-zenmlartifactstore                  ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ demo-zenml-demos/demo-zenml-terraform-cluster ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃  🐳 docker-registry   │ demozenmlcontainerregistry.azurecr.io         ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector does not store any credentials.

```sh
zenml service-connector describe azure-implicit
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the complete documentation text you would like summarized, and I will be happy to assist you.

```
Service connector 'azure-implicit' of type 'azure' with id 'ad645002-0cd4-4d4f-ae20-499ce888a00a' is owned by user 'default' and is 'private'.
                          'azure-implicit' azure Service Connector Details                           
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                          ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ ID               │ ad645002-0cd4-4d4f-ae20-499ce888a00a                                           ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ azure-implicit                                                                 ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🇦  azure                                                                       ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ implicit                                                                       ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🇦  azure-generic, 📦 blob-container, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │                                                                                ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                        ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                             ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-05 09:47:42.415949                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-05 09:47:42.415954                                                     ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

### Azure Service Principal

Azure service principal credentials consist of an Azure client ID and client secret, used for authenticating clients to Azure services. To use this authentication method, an Azure service principal must be created, and a client secret generated.

#### Example Configuration

Assuming an Azure service principal is configured with a client secret and has access permissions to an Azure blob storage container, an AKS Kubernetes cluster, and an ACR container registry, the service principal's client ID, tenant ID, and client secret are utilized to configure the Azure Service Connector.

```sh
zenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=a79f3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234d491e --client_secret=AzureSuperSecret
```

It seems that the text you intended to provide for summarization is missing. Please provide the documentation text you'd like summarized, and I'll be happy to assist!

```
⠙ Registering service connector 'azure-service-principal'...
Successfully registered service connector `azure-service-principal` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃   🇦 azure-generic    │ ZenML Subscription                            ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃   📦 blob-container   │ az://demo-zenmlartifactstore                  ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ demo-zenml-demos/demo-zenml-terraform-cluster ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃  🐳 docker-registry   │ demozenmlcontainerregistry.azurecr.io         ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The Service Connector is configured using service principal credentials.

```sh
zenml service-connector describe azure-service-principal
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                          ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 273d2812-2643-4446-82e6-6098b8ccdaa4                                           ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ azure-service-principal                                                        ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🇦  azure                                                                       ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ service-principal                                                              ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🇦  azure-generic, 📦 blob-container, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ 50d9f230-c4ea-400e-b2d7-6b52ba2a6f90                                           ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ N/A                                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                        ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                             ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-20 19:16:26.802374                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-20 19:16:26.802378                                                     ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
                     Configuration                      
┏━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY      │ VALUE                                ┃
┠───────────────┼──────────────────────────────────────┨
┃ tenant_id     │ a79ff333-8f45-4a74-a42e-68871c17b7fb ┃
┠───────────────┼──────────────────────────────────────┨
┃ client_id     │ 8926254a-8c3f-430a-a2fd-bdab234d491e ┃
┠───────────────┼──────────────────────────────────────┨
┃ client_secret │ [HIDDEN]                             ┃
┗━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

### Azure Access Token Uses

Azure access tokens can be configured by the user or auto-configured from a local environment. Users must regularly generate new tokens and update the connector configuration as API tokens expire. This method is suitable for short-term access, such as temporary team sharing. 

During auto-configuration, if the local Azure CLI is set up with credentials, the connector generates an access token from these credentials and stores it in the connector configuration. 

**Important Note:** Azure access tokens are scoped to specific resources. The token generated during auto-configuration is scoped to the Azure Management API and does not work with Azure blob storage resources. For blob storage, use the Azure service principal authentication method instead.

**Example Auto-Configuration:** Fetching Azure session tokens from the local Azure CLI requires valid credentials, which can be set up by running `az login`.

```sh
zenml service-connector register azure-session-token --type azure --auto-configure
```

It seems that the documentation text you intended to provide is missing. Please provide the text you'd like me to summarize, and I'll be happy to assist!

```
⠙ Registering service connector 'azure-session-token'...
connector authorization failure: the 'access-token' authentication method is not supported for blob storage resources
Successfully registered service connector `azure-session-token` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                                                                                                  ┃
┠───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃   🇦 azure-generic    │ ZenML Subscription                                                                                                              ┃
┠───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃   📦 blob-container   │ 💥 error: connector authorization failure: the 'access-token' authentication method is not supported for blob storage resources ┃
┠───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ demo-zenml-demos/demo-zenml-terraform-cluster                                                                                   ┃
┠───────────────────────┼─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┨
┃  🐳 docker-registry   │ demozenmlcontainerregistry.azurecr.io                                                                                           ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

```sh
zenml service-connector describe azure-session-token 
```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```
Service connector 'azure-session-token' of type 'azure' with id '94d64103-9902-4aa5-8ce4-877061af89af' is owned by user 'default' and is 'private'.
                        'azure-session-token' azure Service Connector Details                        
┏━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃ PROPERTY         │ VALUE                                                                          ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ ID               │ 94d64103-9902-4aa5-8ce4-877061af89af                                           ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ NAME             │ azure-session-token                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ TYPE             │ 🇦 azure                                                                       ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ AUTH METHOD      │ access-token                                                                   ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE TYPES   │ 🇦 azure-generic, 📦 blob-container, 🌀 kubernetes-cluster, 🐳 docker-registry ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ RESOURCE NAME    │ <multiple>                                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SECRET ID        │ b34f2e95-ae16-43b6-8ab6-f0ee33dbcbd8                                           ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SESSION DURATION │ N/A                                                                            ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ EXPIRES IN       │ 42m25s                                                                         ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ OWNER            │ default                                                                        ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ SHARED           │ ➖                                                                             ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ CREATED_AT       │ 2023-06-05 10:03:32.646351                                                     ┃
┠──────────────────┼────────────────────────────────────────────────────────────────────────────────┨
┃ UPDATED_AT       │ 2023-06-05 10:03:32.646352                                                     ┃
┗━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
     Configuration     
┏━━━━━━━━━━┯━━━━━━━━━━┓
┃ PROPERTY │ VALUE    ┃
┠──────────┼──────────┨
┃ token    │ [HIDDEN] ┃
┗━━━━━━━━━━┷━━━━━━━━━━┛
```

The Service Connector is temporary and will expire in approximately 1 hour, becoming unusable.

```sh
zenml service-connector list --name azure-session-token 
```

It appears that the documentation text you intended to provide is missing. Please provide the text you'd like summarized, and I'll be happy to assist!

```
Could not import GCP service connector: No module named 'google.api_core'.
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME                │ ID                                   │ TYPE     │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼─────────────────────┼──────────────────────────────────────┼──────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ azure-session-token │ 94d64103-9902-4aa5-8ce4-877061af89af │ 🇦 azure │ 🇦 azure-generic      │ <multiple>    │ ➖     │ default │ 40m58s     │        ┃
┃        │                     │                                      │          │ 📦 blob-container     │               │        │         │            │        ┃
┃        │                     │                                      │          │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                     │                                      │          │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

## Auto-configuration
The Azure Service Connector enables auto-discovery and credential fetching, as well as configuration setup via the Azure CLI on your local host. 

**Limitations:**
1. Only temporary Azure access tokens are supported, making it unsuitable for long-term authentication.
2. It does not support authentication for Azure Blob Storage. For this, use the Azure service principal authentication method.

Refer to the section on Azure access tokens for an example of auto-configuration.

## Local Client Provisioning
The local Azure CLI, Kubernetes `kubectl`, and Docker CLI can be configured with credentials from a compatible Azure Service Connector. 

**Note:** The Azure local CLI can only use credentials from the Azure Service Connector if configured with the service principal authentication method.

### Local CLI Configuration Examples
An example of configuring the local Kubernetes CLI to access an AKS cluster via an Azure Service Connector is provided.

```sh
zenml service-connector list --name azure-service-principal
```

It seems that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I'll be happy to assist!

```
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━┓
┃ ACTIVE │ NAME                    │ ID                                   │ TYPE     │ RESOURCE TYPES        │ RESOURCE NAME │ SHARED │ OWNER   │ EXPIRES IN │ LABELS ┃
┠────────┼─────────────────────────┼──────────────────────────────────────┼──────────┼───────────────────────┼───────────────┼────────┼─────────┼────────────┼────────┨
┃        │ azure-service-principal │ 3df920bc-120c-488a-b7fc-0e79bc8b021a │ 🇦 azure │ 🇦 azure-generic      │ <multiple>    │ ➖     │ default │            │        ┃
┃        │                         │                                      │          │ 📦 blob-container     │               │        │         │            │        ┃
┃        │                         │                                      │          │ 🌀 kubernetes-cluster │               │        │         │            │        ┃
┃        │                         │                                      │          │ 🐳 docker-registry    │               │        │         │            │        ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━┛
```

The `verify` CLI command lists all Kubernetes clusters accessible via the Azure Service Connector.

```sh
zenml service-connector verify azure-service-principal --resource-type kubernetes-cluster
```

It appears that the documentation text you intended to provide is missing. Please provide the text you would like summarized, and I will assist you accordingly.

```
⠙ Verifying service connector 'azure-service-principal'...
Service connector 'azure-service-principal' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃     RESOURCE TYPE     │ RESOURCE NAMES                                ┃
┠───────────────────────┼───────────────────────────────────────────────┨
┃ 🌀 kubernetes-cluster │ demo-zenml-demos/demo-zenml-terraform-cluster ┃
┗━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

The login CLI command configures the local Kubernetes CLI to access a Kubernetes cluster via an Azure Service Connector.

```sh
zenml service-connector login azure-service-principal --resource-type kubernetes-cluster --resource-id demo-zenml-demos/demo-zenml-terraform-cluster
```

It seems that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text that you would like summarized, and I'll be happy to assist you.

```
⠙ Attempting to configure local client using service connector 'azure-service-principal'...
Updated local kubeconfig with the cluster details. The current kubectl context was set to 'demo-zenml-terraform-cluster'.
The 'azure-service-principal' Kubernetes Service Connector connector was used to successfully configure the local Kubernetes cluster client/SDK.
```

The local Kubernetes CLI can now be utilized to interact with the Kubernetes cluster.

```sh
kubectl cluster-info
```

It appears that the text you provided is incomplete or missing. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

```
Kubernetes control plane is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443
CoreDNS is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://demo-43c5776f7.hcp.westeurope.azmk8s.io:443/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy
```

ACR container registries can undergo a similar process.

```sh
zenml service-connector verify azure-service-principal --resource-type docker-registry
```

It seems that the text you provided is incomplete, as it only includes a code title without any actual content or documentation to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

```
⠦ Verifying service connector 'azure-service-principal'...
Service connector 'azure-service-principal' is correctly configured with valid credentials and has access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓
┃   RESOURCE TYPE    │ RESOURCE NAMES                        ┃
┠────────────────────┼───────────────────────────────────────┨
┃ 🐳 docker-registry │ demozenmlcontainerregistry.azurecr.io ┃
┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┛
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like summarized, and I'll be happy to assist you!

```sh
zenml service-connector login azure-service-principal --resource-type docker-registry --resource-id demozenmlcontainerregistry.azurecr.io
```

It seems that the documentation text you intended to provide is missing. Please share the text you'd like summarized, and I'll be happy to help!

```
⠹ Attempting to configure local client using service connector 'azure-service-principal'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

The 'azure-service-principal' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.
```

The local Docker CLI can now interact with the container registry.

```sh
docker push demozenmlcontainerregistry.azurecr.io/zenml:example_pipeline
```

It seems you provided a placeholder for a code block but did not include the actual documentation text to summarize. Please provide the text you would like me to summarize, and I'll be happy to assist!

```
The push refers to repository [demozenmlcontainerregistry.azurecr.io/zenml]
d4aef4f5ed86: Pushed 
2d69a4ce1784: Pushed 
204066eca765: Pushed 
2da74ab7b0c1: Pushed 
75c35abda1d1: Layer already exists 
415ff8f0f676: Layer already exists 
c14cb5b1ec91: Layer already exists 
a1d005f5264e: Layer already exists 
3a3fd880aca3: Layer already exists 
149a9c50e18e: Layer already exists 
1f6d3424b922: Layer already exists 
8402c959ae6f: Layer already exists 
419599cb5288: Layer already exists 
8553b91047da: Layer already exists 
connectors: digest: sha256:a4cfb18a5cef5b2201759a42dd9fe8eb2f833b788e9d8a6ebde194765b42fe46 size: 3256
```

You can update the local Azure CLI configuration using credentials from the Azure Service Connector.

```sh
zenml service-connector login azure-service-principal --resource-type azure-generic
```

It seems that the documentation text you intended to provide is missing. Please provide the text you'd like summarized, and I'll be happy to assist you!

```
Updated the local Azure CLI configuration with the connector's service principal credentials.
The 'azure-service-principal' Azure Service Connector connector was used to successfully configure the local Generic Azure resource client/SDK.
```

## Stack Components Use

The Azure Artifact Store Stack Component connects to a remote Azure blob storage container via an Azure Service Connector. This connector is compatible with any Orchestrator or Model Deployer stack component that utilizes Kubernetes clusters, enabling management of AKS Kubernetes workloads without the need for explicit Azure or Kubernetes `kubectl` configurations in the target environment or the Stack Component. Additionally, Container Registry Stack Components can connect to an ACR Container Registry through the Azure Service Connector, allowing for the building and publishing of container images to private ACR registries without requiring explicit Azure credentials.

## End-to-End Examples

### AKS Kubernetes Orchestrator, Azure Blob Storage Artifact Store, and ACR Container Registry with a Multi-Type Azure Service Connector

This example demonstrates an end-to-end workflow using a single multi-type Azure Service Connector to access multiple resources across various Stack Components. The complete ZenML Stack includes:
- A Kubernetes Orchestrator connected to an AKS Kubernetes cluster
- An Azure Blob Storage Artifact Store connected to an Azure blob storage container
- An Azure Container Registry connected to an ACR container registry
- A local Image Builder

The final step involves running a simple pipeline on the configured Stack, which requires a remote ZenML Server accessible from Azure. 

1. Configure an Azure service principal with a client secret, granting permissions for the Azure blob storage container, AKS Kubernetes cluster, and ACR container registry. Ensure the Azure ZenML integration is installed.

```sh
    zenml integration install -y azure
    ```

Ensure that the Azure Service Connector Type is accessible.

```sh
    zenml service-connector list-types --type azure
    ```

It seems that the text you provided is incomplete, as it only includes a code title without any accompanying content. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

````
```

### Summary of Azure Service Connector Documentation

- **Name**: Azure Service Connector
- **Type**: Azure
- **Resource Types**: 
  - Azure Generic
  - Blob Container
  - Kubernetes Cluster
  - Docker Registry
- **Authentication Methods**: 
  - Implicit
  - Service Principal
  - Access Token
- **Local Access**: Yes
- **Remote Access**: Yes

```
```

To register a multi-type Azure Service Connector, use the Azure service principal credentials established in the first step. Be aware of the resources that the connector can access.

```sh
    zenml service-connector register azure-service-principal --type azure --auth-method service-principal --tenant_id=a79ff3633-8f45-4a74-a42e-68871c17b7fb --client_id=8926254a-8c3f-430a-a2fd-bdab234fd491e --client_secret=AzureSuperSecret
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

````
```

Successfully registered the service connector `azure-service-principal` with access to the following resources:

- **Resource Type**: azure-generic  
  **Resource Name**: ZenML Subscription

- **Resource Type**: blob-container  
  **Resource Name**: az://demo-zenmlartifactstore

- **Resource Type**: kubernetes-cluster  
  **Resource Name**: demo-zenml-demos/demo-zenml-terraform-cluster

- **Resource Type**: docker-registry  
  **Resource Name**: demozenmlcontainerregistry.azurecr.io

```
```

To register and connect an Azure Blob Storage Artifact Store Stack Component to an Azure blob container, follow these steps:

1. **Register the Artifact Store**: Use the appropriate command or API to register the Azure Blob Storage as an artifact store within your stack.
2. **Configure Connection**: Provide the necessary credentials and configuration details to connect to the Azure blob container.
3. **Verify Connection**: Ensure that the connection is established successfully by testing access to the blob container.

Make sure to have the required permissions and access rights for the Azure resources involved.

```sh
    zenml artifact-store register azure-demo --flavor azure --path=az://demo-zenmlartifactstore
    ```

It seems that the text you provided is incomplete or missing. Please provide the full documentation text you would like summarized, and I'll be happy to assist you!

````
```

Artifact store `azure-demo` has been successfully registered.

```
```

It appears that the text you provided is incomplete or consists only of a code block delimiter (`{% endcode %}`). Please provide the full documentation text you would like summarized, and I will be happy to assist you.

````
```

To connect to an Azure demo artifact store using ZenML, use the following command:

```bash
sh zenml artifact-store connect azure-demo --connector azure-service-principal
```

This command establishes a connection to the Azure artifact store using the Azure Service Principal as the authentication method.

```

```

It appears that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text or additional details so I can assist you effectively.

````
```

The artifact store `azure-demo` is successfully connected to the following resource:

- **Connector ID**: f2316191-d20b-4348-a68b-f5e347862196
- **Connector Name**: azure-service-principal
- **Connector Type**: Azure
- **Resource Type**: Blob Container
- **Resource Name**: az://demo-zenmlartifactstore

```
```

To register and connect a Kubernetes Orchestrator Stack Component to an AKS cluster, follow these steps:

1. **Prerequisites**: Ensure you have access to an AKS cluster and necessary permissions.
2. **Register Component**: Use the appropriate CLI or API command to register the Kubernetes Orchestrator Stack Component.
3. **Connect to AKS**: Execute the connection command, specifying the AKS cluster details.
4. **Verify Connection**: Check the status to confirm that the component is successfully connected to the AKS cluster.

Ensure all configurations are correct to facilitate smooth integration.

```sh
    zenml orchestrator register aks-demo-cluster --flavor kubernetes --synchronous=true --kubernetes_namespace=zenml-workloads
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to help!

````
```

The orchestrator `aks-demo-cluster` has been successfully registered.

```
```

It seems that the text you provided is incomplete and only contains a code block ending tag. Please provide the full documentation text you would like summarized, and I will be happy to assist you!

````
```

To connect the ZenML orchestrator to the AKS demo cluster using an Azure service principal, use the following command:

```bash
sh zenml orchestrator connect aks-demo-cluster --connector azure-service-principal
```

```

```

It appears that the provided text is incomplete and only contains a code title without any accompanying content. Please provide the full documentation text for summarization.

````
```

The orchestrator `aks-demo-cluster` has been successfully connected to the following resource:

- **Connector ID**: f2316191-d20b-4348-a68b-f5e347862196
- **Connector Name**: azure-service-principal
- **Connector Type**: Azure
- **Resource Type**: Kubernetes Cluster
- **Resource Names**: demo-zenml-demos/demo-zenml-terraform-cluster

```
```

To register and connect an Azure Container Registry (ACR) Stack Component to an ACR container registry, follow these steps:

1. **Create an ACR**: Use the Azure portal or CLI to create an Azure Container Registry.
2. **Register the Stack Component**: Use the appropriate command or API to register your Stack Component with the ACR.
3. **Connect the Component**: Ensure the Stack Component is configured to authenticate and connect to the ACR using the necessary credentials.

Make sure to follow Azure's best practices for security and access management during this process.

```sh
    zenml container-registry register acr-demo-registry --flavor azure --uri=demozenmlcontainerregistry.azurecr.io
    ```

It seems that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

````
```

Container registry `acr-demo-registry` has been successfully registered.

```
```

It seems that the text you provided is incomplete or contains only a code block delimiter without any actual content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to help!

````
```

To connect to the Azure Container Registry using ZenML, use the following command:

```bash
sh zenml container-registry connect acr-demo-registry --connector azure-service-principal
```

This command establishes a connection to the specified Azure Container Registry (`acr-demo-registry`) using the Azure Service Principal as the authentication method.

```

```

It appears that the text you provided is incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

````
```

The container registry `acr-demo-registry` has been successfully connected to the following resource:

- **Connector ID**: f2316191-d20b-4348-a68b-f5e347862196
- **Connector Name**: azure-service-principal
- **Connector Type**: Azure
- **Resource Type**: Docker Registry
- **Resource Name**: demozenmlcontainerregistry.azurecr.io

```
```

Combine all Stack Components into a Stack and set it as active, including a local Image Builder for completeness.

```sh
    zenml image-builder register local --flavor local
    ```

It appears that the text you provided is incomplete, as it only contains a code title without any accompanying documentation or content to summarize. Please provide the full text or documentation that you would like summarized, and I will be happy to assist you.

````
```

The active stack is 'default' (global), and the image_builder `local` has been successfully registered.

```
```

It seems that there is no documentation text provided for summarization. Please provide the text you would like me to summarize, and I'll be happy to assist!

````
```

The command `sh zenml stack register gcp-demo -a azure-demo -o aks-demo-cluster -c acr-demo-registry -i local --set` registers a new ZenML stack named `gcp-demo`. It specifies the following components: 

- **Artifact Store**: `azure-demo`
- **Orchestrator**: `aks-demo-cluster`
- **Container Registry**: `acr-demo-registry`
- **Identity**: `local`

The `--set` flag indicates that the stack should be configured with these settings.

```

```

It seems that the provided text is incomplete and only includes a code title without any actual content or details to summarize. Please provide the full documentation text for me to summarize effectively.

````
```

The stack 'gcp-demo' has been successfully registered, and the active repository stack is now set to 'gcp-demo'.

```
```

To verify the setup, execute a basic pipeline using the simplest configuration available.

```python
    from zenml import pipeline, step


    @step
    def step_1() -> str:
        """Returns the `world` string."""
        return "world"


    @step(enable_cache=False)
    def step_2(input_one: str, input_two: str) -> None:
        """Combines the two strings at its input and prints them."""
        combined_str = f"{input_one} {input_two}"
        print(combined_str)


    @pipeline
    def my_pipeline():
        output_step_one = step_1()
        step_2(input_one="hello", input_two=output_step_one)


    if __name__ == "__main__":
        my_pipeline()
    ```

To execute the code, save it in a `run.py` file and run the file. The output will be displayed as shown in the example command output.

````
```

The process begins by executing the command `$ python run.py` to build Docker images for the pipeline `simple_pipeline`. The image `demozenmlcontainerregistry.azurecr.io/zenml:simple_pipeline-orchestrator` is created, including integration requirements such as:

- adlfs==2021.10.0
- azure-identity==1.10.0
- azure-keyvault-keys
- azure-keyvault-secrets
- azure-mgmt-containerservice>=20.0.0
- azureml-core==1.48.0
- kubernetes==18.20.0

No `.dockerignore` file is found, so all files in the build context are included. The Docker build process consists of the following steps:

1. Base image: `FROM zenmldocker/zenml:0.40.0-py3.8`
2. Set working directory: `WORKDIR /app`
3. Copy user requirements: `COPY .zenml_user_requirements .`
4. Install user requirements: `RUN pip install --default-timeout=60 --no-cache-dir -r .zenml_user_requirements`
5. Copy integration requirements: `COPY .zenml_integration_requirements .`
6. Install integration requirements: `RUN pip install --default-timeout=60 --no-cache-dir -r .zenml_integration_requirements`
7. Set environment variables: 
   - `ENV ZENML_ENABLE_REPO_INIT_WARNINGS=False`
   - `ENV ZENML_CONFIG_PATH=/app/.zenconfig`
8. Copy all files: `COPY . .`
9. Change permissions: `RUN chmod -R a+rw .`

The Docker image is then pushed to the registry, and the build process is completed. The pipeline `simple_pipeline` is executed on the `gcp-demo` stack with caching disabled. 

The Kubernetes orchestrator pod starts, followed by the execution of two steps:
- `simple_step_one` completes in 0.396 seconds.
- `simple_step_two` completes in 3.203 seconds.

Both steps successfully retrieve tokens using `ClientSecretCredential`. The orchestration pod finishes, and the dashboard URL for the pipeline run is provided: [Dashboard URL](https://zenml.stefan.20.23.46.143.nip.io/default/pipelines/98c41e2a-1ab0-4ec9-8375-6ea1ab473686/runs).

```
```

The documentation includes an image related to ZenML Scarf, referenced by the URL "https://static.scarf.sh/a.png?x-pxid=f0b4f458-0a54-4fcd-aa95-d5ee424815bc".



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/docker-service-connector.md

**Docker Service Connector**  
The ZenML Docker Service Connector enables authentication with Docker or OCI container registries and manages Docker clients for these registries. It provides pre-authenticated python-docker clients to Stack Components linked to the connector.

```shell
zenml service-connector list-types --type docker
```

```shell
┏━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃           NAME           │ TYPE      │ RESOURCE TYPES     │ AUTH METHODS │ LOCAL │ REMOTE ┃
┠──────────────────────────┼───────────┼────────────────────┼──────────────┼───────┼────────┨
┃ Docker Service Connector │ 🐳 docker │ 🐳 docker-registry │ password     │ ✅    │ ✅     ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Prerequisites
No additional Python packages are required for the Service Connector; all prerequisites are included in the ZenML package. Docker must be installed in environments where container images are built and pushed to the target registry.

## Resource Types
The Docker Service Connector supports authentication to Docker/OCI container registries, identified by the `docker-registry` Resource Type. The resource name can be in the following formats (repository name is optional):
- DockerHub: `docker.io` or `https://index.docker.io/v1/<repository-name>`
- Generic OCI registry URI: `https://host:port/<repository-name>`

## Authentication Methods
Authentication to Docker/OCI registries can be done using a username and password or an access token. It is recommended to use API tokens instead of passwords when available, such as for DockerHub.

```sh
zenml service-connector register dockerhub --type docker -in
```

It seems that you've included a placeholder for code but not the actual documentation text to summarize. Please provide the specific documentation text you'd like summarized, and I'll be happy to help!

```text
Please enter a name for the service connector [dockerhub]: 
Please enter a description for the service connector []: 
Please select a service connector type (docker) [docker]: 
Only one resource type is available for this connector (docker-registry).
Only one authentication method is available for this connector (password). Would you like to use it? [Y/n]: 
Please enter the configuration for the Docker username and password/token authentication method.
[username] Username {string, secret, required}: 
[password] Password {string, secret, required}: 
[registry] Registry server URL. Omit to use DockerHub. {string, optional}: 
Successfully registered service connector `dockerhub` with access to the following resources:
┏━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┓
┃   RESOURCE TYPE    │ RESOURCE NAMES ┃
┠────────────────────┼────────────────┨
┃ 🐳 docker-registry │ docker.io      ┃
┗━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┛
```

**Service Connector Limitations:**
- Does not support generating short-lived credentials from configured username/password or token credentials. Credentials are directly distributed to clients for authentication with the target Docker/OCI registry.

**Auto-configuration:**
- Does not support auto-discovery and extraction of authentication credentials from local Docker clients. Feedback can be provided via [Slack](https://zenml.io/slack) or by creating an issue on [GitHub](https://github.com/zenml-io/zenml/issues).

**Local Client Provisioning:**
- Allows configuration of the local Docker client with credentials.

```sh
zenml service-connector login dockerhub
```

It appears that the documentation text you intended to provide is missing. Please share the text you would like summarized, and I'll be happy to assist you!

```text
Attempting to configure local client using service connector 'dockerhub'...
WARNING! Your password will be stored unencrypted in /home/stefan/.docker/config.json.
Configure a credential helper to remove this warning. See
https://docs.docker.com/engine/reference/commandline/login/#credentials-store

The 'dockerhub' Docker Service Connector connector was used to successfully configure the local Docker/OCI container registry client/SDK.
```

## Stack Components Use

The Docker Service Connector enables all Container Registry stack components to authenticate with remote Docker/OCI container registries, allowing for the building and publishing of container images without needing to configure Docker credentials in the target environment or Stack Component.

**Warning:** ZenML currently does not support automatic configuration of Docker credentials in container runtimes like Kubernetes (e.g., via imagePullSecrets) for pulling images from private registries. This feature will be included in a future release.



================================================================================

# docs/book/how-to/infrastructure-deployment/auth-management/hyperai-service-connector.md

**HyperAI Service Connector Overview**  
The ZenML HyperAI Service Connector enables authentication with HyperAI instances for deploying pipeline runs. It offers pre-authenticated Paramiko SSH clients to associated Stack Components.

```shell
$ zenml service-connector list-types --type hyperai
```

```shell
┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━┯━━━━━━━┯━━━━━━━━┓
┃           NAME            │ TYPE       │ RESOURCE TYPES     │ AUTH METHODS │ LOCAL │ REMOTE ┃
┠───────────────────────────┼────────────┼────────────────────┼──────────────┼───────┼────────┨
┃ HyperAI Service Connector │ 🤖 hyperai │ 🤖 hyperai-instance │ rsa-key      │ ✅    │ ✅     ┃
┃                           │            │                    │ dsa-key      │       │        ┃
┃                           │            │                    │ ecdsa-key    │       │        ┃
┃                           │            │                    │ ed25519-key  │       │        ┃
┗━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━┷━━━━━━━┷━━━━━━━━┛
```

## Prerequisites
To use the HyperAI Service Connector, install the HyperAI integration with:
* `zenml integration install hyperai`

## Resource Types
The connector supports HyperAI instances.

## Authentication Methods
ZenML establishes an SSH connection to the HyperAI instance for stack components like the HyperAI Orchestrator. Supported authentication methods include:
1. RSA key
2. DSA (DSS) key
3. ECDSA key
4. ED25519 key

**Warning:** SSH private keys are distributed to all clients running pipelines, granting unrestricted access to HyperAI instances.

When configuring the Service Connector, provide at least one `hostname` and `username`. Optionally, include an `ssh_passphrase`. You can:
1. Create separate connectors for each HyperAI instance with different SSH keys.
2. Use a single SSH key for multiple instances, selecting the instance when creating the orchestrator component.

## Auto-configuration
This Service Connector does not support auto-discovery of authentication credentials. Feedback on this feature is welcome via [Slack](https://zenml.io/slack) or [GitHub](https://github.com/zenml-io/zenml/issues).

## Stack Components Use
The HyperAI Service Connector is utilized by the HyperAI Orchestrator to deploy pipeline runs to HyperAI instances.



================================================================================

# docs/book/how-to/handle-data-artifacts/visualize-artifacts.md

### Configuring ZenML for Data Visualizations

ZenML automatically saves visualizations of various data types, viewable in the ZenML dashboard or Jupyter notebooks using the `artifact.visualize()` method. Supported visualization types include:

- **HTML:** Embedded HTML visualizations (e.g., data validation reports)
- **Image:** Visualizations of image data (e.g., Pillow images, numeric numpy arrays)
- **CSV:** Tables (e.g., pandas DataFrame `.describe()` output)
- **Markdown:** Markdown strings or pages

#### Accessing Visualizations

To display visualizations on the dashboard, the ZenML server must access the artifact store where visualizations are stored. Users must configure a service connector to grant this access. For example, see the [AWS S3 artifact store documentation](../../component-guide/artifact-stores/s3.md).

**Note:** With the default/local artifact store in a deployed ZenML, the server cannot access local files, preventing visualizations from displaying. Use a service connector with a remote artifact store to view visualizations.

#### Artifact Store Configuration

If visualizations from a pipeline run are missing, check that the ZenML server has the necessary dependencies and permissions for the artifact store. Refer to the [custom artifact store documentation](../../component-guide/artifact-stores/custom.md#enabling-artifact-visualizations-with-custom-artifact-stores) for details.

#### Creating Custom Visualizations

Custom visualizations can be added in two ways:

1. **Using Existing Data:** If handling HTML, Markdown, or CSV data in a step, cast them to a special class to visualize.
2. **Type-Specific Logic:** Define visualization logic for specific data types by building a custom materializer or create a custom return type class with a corresponding materializer.

##### Visualization via Special Return Types

To visualize existing HTML, Markdown, or CSV data as strings, cast and return them from your step using:

- `zenml.types.HTMLString` for HTML strings (e.g., `"<h1>Header</h1>Some text"`)
- `zenml.types.MarkdownString` for Markdown strings (e.g., `"# Header\nSome text"`)
- `zenml.types.CSVString` for CSV strings (e.g., `"a,b,c\n1,2,3"`)

This setup allows seamless integration of visualizations into the ZenML dashboard.

```python
from zenml.types import CSVString

@step
def my_step() -> CSVString:
    some_csv = "a,b,c\n1,2,3"
    return CSVString(some_csv)
```

### Visualization in ZenML Dashboard

To create visualizations in the ZenML dashboard, you can utilize the following methods:

1. **Materializers**: Override the `save_visualizations()` method in the materializer to automatically extract visualizations for all artifacts of a specific data type. For detailed instructions, refer to the [materializer documentation](handle-custom-data-types.md#optional-how-to-visualize-the-artifact).

2. **Custom Return Type and Materializer**: To visualize any data in the ZenML dashboard, follow these steps:
   - Create a **custom class** to hold the visualization data.
   - Build a custom **materializer** for this class, implementing visualization logic in the `save_visualizations()` method.
   - Return the custom class from any ZenML steps.

#### Example: Facets Data Skew Visualization
For an example, see the [Facets Integration](https://sdkdocs.zenml.io/latest/integration_code_docs/integrations-facets), which visualizes data skew between multiple Pandas DataFrames. The custom class used is [FacetsComparison](https://sdkdocs.zenml.io/0.42.0/integration_code_docs/integrations-facets/#zenml.integrations.facets.models.FacetsComparison). 

![Facets Visualization](../../.gitbook/assets/facets-visualization.png)

```python
class FacetsComparison(BaseModel):
    datasets: List[Dict[str, Union[str, pd.DataFrame]]]
```

**2. Materializer** The [FacetsMaterializer](https://sdkdocs.zenml.io/0.42.0/integration_code_docs/integrations-facets/#zenml.integrations.facets.materializers.facets_materializer.FacetsMaterializer) is a custom materializer designed to manage a specific class and includes the associated visualization logic.

```python
class FacetsMaterializer(BaseMaterializer):

    ASSOCIATED_TYPES = (FacetsComparison,)
    ASSOCIATED_ARTIFACT_TYPE = ArtifactType.DATA_ANALYSIS

    def save_visualizations(
        self, data: FacetsComparison
    ) -> Dict[str, VisualizationType]:
        html = ...  # Create a visualization for the custom type 
        visualization_path = os.path.join(self.uri, VISUALIZATION_FILENAME)
        with fileio.open(visualization_path, "w") as f:
            f.write(html)
        return {visualization_path: VisualizationType.HTML}
```

**3. Step** The `facets` integration consists of three steps to create `FacetsComparison`s for various input sets. For example, the `facets_visualization_step` takes two DataFrames as inputs to construct a `FacetsComparison` object.

```python
@step
def facets_visualization_step(
    reference: pd.DataFrame, comparison: pd.DataFrame
) -> FacetsComparison:  # Return the custom type from your step
    return FacetsComparison(
        datasets=[
            {"name": "reference", "table": reference},
            {"name": "comparison", "table": comparison},
        ]
    )
```

When you add the `facets_visualization_step` to your pipeline, the following occurs:

1. A `FacetsComparison` is created and returned.
2. Upon completion, ZenML locates the `FacetsMaterializer` and invokes the `save_visualizations()` method, which generates and saves the visualization as an HTML file in the artifact store.
3. The visualization HTML file can be accessed from the dashboard by clicking on the artifact in the run DAG.

To disable artifact visualization, set `enable_artifact_visualization` at the pipeline or step level.

```python
@step(enable_artifact_visualization=False)
def my_step():
    ...

@pipeline(enable_artifact_visualization=False)
def my_pipeline():
    ...
```

The provided text contains an image link related to "ZenML Scarf" but lacks any technical information or key points to summarize. Please provide additional content for a more comprehensive summary.



================================================================================

# docs/book/how-to/popular-integrations/gcp-guide.md

# Set Up a Minimal GCP Stack

This guide provides steps to set up a minimal production stack on Google Cloud Platform (GCP) using a service account with scoped permissions for ZenML authentication.

### Quick Links
- For a full GCP ZenML cloud stack, use the [in-browser stack deployment wizard](../../infrastructure-deployment/stack-deployment/deploy-a-cloud-stack.md), the [stack registration wizard](../../infrastructure-deployment/stack-deployment/register-a-cloud-stack.md), or the [ZenML GCP Terraform module](../../infrastructure-deployment/stack-deployment/deploy-a-cloud-stack-with-terraform.md).

### Important Note
This guide focuses on GCP, but contributions for other cloud providers are welcome. Interested contributors can create a [pull request on GitHub](https://github.com/zenml-io/zenml/blob/main/CONTRIBUTING.md).

### Step 1: Choose a GCP Project
In the Google Cloud console, select or [create a Google Cloud project](https://cloud.google.com/resource-manager/docs/creating-managing-projects). Ensure a billing account is attached to enable API usage. CLI instructions are available if preferred.

```bash
gcloud projects create <PROJECT_ID> --billing-project=<BILLING_PROJECT>
```

### Summary

{% hint style="info" %} If you don't plan to keep the resources created in this procedure, create a new project. You can delete the project later to remove all associated resources. {% endhint %}

### Steps:

1. **Enable GCloud APIs**: Enable the following APIs in your GCP project:
   - Cloud Functions API (for vertex orchestrator)
   - Cloud Run Admin API (for vertex orchestrator)
   - Cloud Build API (for container registry)
   - Artifact Registry API (for container registry)
   - Cloud Logging API (generally needed)

2. **Create a Dedicated Service Account**: Assign the following roles to the service account:
   - AI Platform Service Agent
   - Storage Object Admin  
   These roles provide full CRUD permissions on storage objects and compute permissions within VertexAI.

3. **Create a JSON Key for the Service Account**: Generate a JSON key file for the service account, which will allow it to assume its identity. You will need the file path in the next step.

```bash
export JSON_KEY_FILE_PATH=<JSON_KEY_FILE_PATH>
```

### Create a Service Connector within ZenML

The service connector enables authentication for ZenML and its components with Google Cloud Platform (GCP). 

{% tabs %}
{% tab title="CLI" %}

```bash
zenml integration install gcp \
&& zenml service-connector register gcp_connector \
--type gcp \
--auth-method service-account \
--service_account_json=@${JSON_KEY_FILE_PATH} \
--project_id=<GCP_PROJECT_ID>
```

### 6) Create Stack Components

#### Artifact Store
Before using the ZenML CLI, create a GCS bucket in GCP if you don't have one. After that, you can create the ZenML stack component using the CLI.

```bash
export ARTIFACT_STORE_NAME=gcp_artifact_store

# Register the GCS artifact-store and reference the target GCS bucket
zenml artifact-store register ${ARTIFACT_STORE_NAME} --flavor gcp \
    --path=gs://<YOUR_BUCKET_NAME>

# Connect the GCS artifact-store to the target bucket via a GCP Service Connector
zenml artifact-store connect ${ARTIFACT_STORE_NAME} -i
```

### Orchestrator Overview

This guide utilizes Vertex AI as the orchestrator for running pipelines. Vertex AI is a serverless service ideal for rapid prototyping of MLOps stacks. The orchestrator can be replaced later with a solution that better fits specific use cases and budget requirements.

For more information on configuring artifact stores, refer to our [documentation](../../component-guide/artifact-stores/gcp.md).

```bash
export ORCHESTRATOR_NAME=gcp_vertex_orchestrator

# Register the GCS artifact-store and reference the target GCS bucket
zenml orchestrator register ${ORCHESTRATOR_NAME} --flavor=vertex 
  --project=<PROJECT_NAME> --location=europe-west2

# Connect the GCS orchestrator to the target gcp project via a GCP Service Connector
zenml orchestrator connect ${ORCHESTRATOR_NAME} -i
```

For detailed information on orchestrators and their configuration, refer to our [documentation](../../component-guide/orchestrators/vertex.md). 

### Container Registry 
#### CLI 


```bash
export CONTAINER_REGISTRY_NAME=gcp_container_registry

zenml container-registry register ${CONTAINER_REGISTRY_NAME} --flavor=gcp --uri=<GCR-URI>

# Connect the GCS orchestrator to the target gcp project via a GCP Service Connector
zenml container-registry connect ${CONTAINER_REGISTRY_NAME} -i
```

For detailed information on container registries and their configuration, refer to our [documentation](../../component-guide/container-registries/container-registries.md). 

### 7) Create Stack 
{% tabs %}
{% tab title="CLI" %}

```bash
export STACK_NAME=gcp_stack

zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} \
    -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

You now have a fully functional GCP stack ready for use. You can run a pipeline on it to test its functionality. If you no longer need the created resources, delete the project. Additionally, you can add other stack components as needed.

```bash
gcloud project delete <PROJECT_ID_OR_NUMBER>
```

## Best Practices for Using a GCP Stack with ZenML

When utilizing a GCP stack in ZenML, follow these best practices to optimize workflow, enhance security, and improve cost-efficiency:

### Use IAM and Least Privilege Principle
- Adhere to the principle of least privilege by granting only the minimum necessary permissions for ZenML pipelines.
- Regularly review and audit IAM roles for appropriateness and security.

### Leverage GCP Resource Labeling
- Implement a consistent labeling strategy for GCP resources, such as GCS buckets.

```shell
gcloud storage buckets update gs://your-bucket-name --update-labels=project=zenml,environment=production
```

This command adds two labels to the bucket: "project" with value "zenml" and "environment" with value "production." Multiple labels can be added or updated by separating them with commas. To remove a label, set its value to null.

```shell
gcloud storage buckets update gs://your-bucket-name --update-labels=label-to-remove=null
```

Labels assist in billing, cost allocation tracking, and cleanup efforts. To view the labels on a bucket:

```shell
gcloud storage buckets describe gs://your-bucket-name --format="default(labels)"
```

This section displays all labels on the specified bucket. 

### Implement Cost Management Strategies
Utilize Google Cloud's [Cost Management tools](https://cloud.google.com/docs/costs-usage) to monitor and manage spending. To set up a budget alert:
1. Navigate to Google Cloud Console.
2. Go to Billing > Budgets & Alerts.
3. Click "Create Budget."
4. Set your budget amount, scope (project, product, etc.), and alert thresholds.

You can also create a budget using the `gcloud` CLI.

```shell
gcloud billing budgets create --billing-account=BILLING_ACCOUNT_ID --display-name="ZenML Monthly Budget" --budget-amount=1000 --threshold-rule=percent=90
```

To track expenses for ZenML projects, set up cost allocation labels in the Google Cloud Billing Console. 

### Backup Strategy
Implement a robust backup strategy by regularly backing up critical data and configurations. For Google Cloud Storage (GCS), enable versioning and consider cross-region replication for disaster recovery. 

To enable versioning on a GCS bucket:

```shell
gsutil versioning set on gs://your-bucket-name
```

To set up cross-region replication, follow these steps:

1. **Enable Versioning**: Ensure that versioning is enabled on the source bucket.
2. **Create a Destination Bucket**: Set up a destination bucket in the target region.
3. **Configure IAM Policies**: Grant necessary permissions to allow replication from the source to the destination bucket.
4. **Set Up Replication Configuration**: In the source bucket, configure the replication settings, specifying the destination bucket and any required filters.
5. **Review and Confirm**: Verify the configuration and confirm that replication is active.

Ensure that all prerequisites, such as permissions and versioning, are met for successful replication.

```shell
gsutil rewrite -r gs://source-bucket gs://destination-bucket
```

Implement best practices and examples to enhance the security, efficiency, and cost-effectiveness of your GCP stack for ZenML projects. Regularly review and update your practices to align with project evolution and new GCP features.



================================================================================

# docs/book/how-to/popular-integrations/azure-guide.md

# Quick Guide to Set Up Azure for ZenML Pipelines

This guide provides steps to set up a minimal production stack on Azure for running ZenML pipelines.

## Prerequisites
- Active Azure account
- ZenML installed
- ZenML Azure integration installed using `zenml integration install azure`

## Steps

### 1. Set Up Credentials
- Create a service principal via Azure App Registrations:
  1. Go to App Registrations in the Azure portal.
  2. Click `+ New registration`, name it, and register.
- Note the Application ID and Tenant ID.
- Create a client secret under `Certificates & secrets` and save the secret value.

### 2. Create Resource Group and AzureML Instance
- Create a resource group:
  1. Navigate to `Resource Groups` in the Azure portal and click `+ Create`.
- Create an AzureML workspace:
  1. Go to your new resource group's overview page and click `+ Create`.
  2. Select `Azure Machine Learning` from the marketplace.
- Optionally, create a container registry.

### 3. Create Role Assignments
- In your resource group, go to `Access control (IAM)` and click `+ Add` for a new role assignment.
- Assign the following roles:
  - `AzureML Compute Operator`
  - `AzureML Data Scientist`
  - `AzureML Registry User`
- Search for your registered app by its ID and assign the roles.

### 4. Create a Service Connector
- With the setup complete, create a ZenML Azure Service Connector.

For shortcuts on deploying and registering a full Azure ZenML cloud stack, refer to the in-browser stack deployment wizard, stack registration wizard, or the ZenML Azure Terraform module.

```bash
zenml service-connector register azure_connector --type azure \
  --auth-method service-principal \
  --client_secret=<CLIENT_SECRET> \
  --tenant_id=<TENANT_ID> \
  --client_id=<APPLICATION_ID>
```

To run workflows on Azure using ZenML, you need to create an artifact store, orchestrator, and container registry. 

### Artifact Store (Azure Blob Storage)
Use the storage account linked to your AzureML workspace for the artifact store. First, create a container in the blob storage by accessing your storage account. After creating the container, register your artifact store using its path and connect it to your service connector.

```bash 
zenml artifact-store register azure_artifact_store -f azure \
  --path=<PATH_TO_YOUR_CONTAINER> \ 
  --connector azure_connector
```

For Azure Blob Storage artifact stores, refer to the [documentation](../../component-guide/artifact-stores/azure.md). 

### Orchestrator (AzureML)
No additional setup is required for the orchestrator. Use the following command to register it and connect to your service connector:

```bash
zenml orchestrator register azure_orchestrator -f azureml \
    --subscription_id=<YOUR_AZUREML_SUBSCRIPTION_ID> \
    --resource_group=<NAME_OF_YOUR_RESOURCE_GROUP> \
    --workspace=<NAME_OF_YOUR_AZUREML_WORKSPACE> \ 
    --connector azure_connector
```

### Container Registry (Azure Container Registry)

You can register and connect your Azure Container Registry using the specified command. For detailed information on the AzureML orchestrator, refer to the [documentation](../../component-guide/orchestrators/azureml.md).

```bash
zenml container-registry register azure_container_registry -f azure \
  --uri=<URI_TO_YOUR_AZURE_CONTAINER_REGISTRY> \ 
  --connector azure_connector
```

For detailed information on Azure container registries, refer to the [documentation](../../component-guide/container-registries/azure.md). 

## 6. Create a Stack
You can now create an Azure ZenML stack using the registered components.

```shell
zenml stack register azure_stack \
    -o azure_orchestrator \
    -a azure_artifact_store \
    -c azure_container_registry \
    --set
```

## 7. Completion

You now have a fully operational Azure stack. Test it by running a ZenML pipeline.

```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from Azure!"

@pipeline
def azure_pipeline():
    hello_world()

if __name__ == "__main__":
    azure_pipeline()
```

Save the code as `run.py` and execute it. The pipeline utilizes Azure Blob Storage for artifact storage, AzureML for orchestration, and an Azure container registry.

```shell
python run.py
```

With your Azure stack set up using ZenML, consider the following next steps: 

- Review ZenML's [production guide](../../user-guide/production-guide/README.md) for best practices in deploying and managing production-ready pipelines.
- Explore ZenML's [integrations](../../component-guide/README.md) with other machine learning tools and frameworks.
- Join the [ZenML community](https://zenml.io/slack) for support and networking with other users.



================================================================================

# docs/book/how-to/popular-integrations/skypilot.md

### Skypilot with ZenML

The ZenML SkyPilot VM Orchestrator enables provisioning and management of VMs across supported cloud providers (AWS, GCP, Azure, Lambda Labs) for ML pipelines, offering cost savings and high GPU availability.

#### Prerequisites
To use the SkyPilot VM Orchestrator, ensure you have:
- ZenML SkyPilot integration for your cloud provider installed (`zenml integration install <PROVIDER> skypilot_<PROVIDER>`)
- Docker installed and running
- A remote artifact store and container registry in your ZenML stack
- A remote ZenML deployment
- Permissions to provision VMs on your cloud provider
- A service connector configured for authentication (not required for Lambda Labs)

#### Configuration Steps
For AWS, GCP, and Azure:
1. Install the SkyPilot integration and provider-specific connectors.
2. Register a service connector with necessary credentials.
3. Register the orchestrator and link it to the service connector.
4. Register and activate a stack with the new orchestrator.

```bash
zenml service-connector register <PROVIDER>-skypilot-vm -t <PROVIDER> --auto-configure
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_<PROVIDER>  
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <PROVIDER>-skypilot-vm
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

**Lambda Labs Integration Steps:**

1. Install the SkyPilot Lambda integration.
2. Register a secret using your Lambda Labs API key.
3. Register the orchestrator with the API key secret.
4. Register and activate a stack with the new orchestrator.

```bash
zenml secret create lambda_api_key --scope user --api_key=<KEY>
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor vm_lambda --api_key={{lambda_api_key.api_key}}
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

## Running a Pipeline
After configuration, execute any ZenML pipeline using the SkyPilot VM Orchestrator. Each step operates in a Docker container on a provisioned VM.

## Additional Configuration
Further configure the orchestrator with cloud-specific `Settings` objects.

```python
from zenml.integrations.skypilot_<PROVIDER>.flavors.skypilot_orchestrator_<PROVIDER>_vm_flavor import Skypilot<PROVIDER>OrchestratorSettings

skypilot_settings = Skypilot<PROVIDER>OrchestratorSettings(
   cpus="2",
   memory="16", 
   accelerators="V100:2",
   use_spot=True,
   region=<REGION>,
   ...  
)

@pipeline(
   settings={
       "orchestrator": skypilot_settings
   }
)
```

You can specify VM size, spot usage, region, and configure resources for each step.

```python
high_resource_settings = Skypilot<PROVIDER>OrchestratorSettings(...)

@step(settings={"orchestrator": high_resource_settings})  
def resource_intensive_step():
   ...
```

For advanced options, refer to the [full SkyPilot VM Orchestrator documentation](../../component-guide/orchestrators/skypilot-vm.md).



================================================================================

# docs/book/how-to/popular-integrations/mlflow.md

### MLflow Experiment Tracker with ZenML

The ZenML MLflow Experiment Tracker integration allows for logging and visualizing pipeline step information using MLflow without additional coding.

#### Prerequisites
- Install the ZenML MLflow integration: `zenml integration install mlflow -y`
- An MLflow deployment: either local or remote with proxied artifact storage.

#### Configuring the Experiment Tracker
There are two deployment scenarios:
1. **Local**: Uses a local artifact store, suitable for local ZenML runs, requiring no extra configuration.

```bash
zenml experiment-tracker register mlflow_experiment_tracker --flavor=mlflow
zenml stack register custom_stack -e mlflow_experiment_tracker ... --set
```

**Remote with Proxied Artifact Storage (Scenario 5)**: This setup is compatible with any stack components and requires authentication configuration. For remote access, configure authentication using either Basic authentication (not recommended for production) or ZenML secrets (recommended). To utilize ZenML secrets:

```bash
zenml secret create mlflow_secret \
   --username=<USERNAME> \
   --password=<PASSWORD>
   
zenml experiment-tracker register mlflow \
   --flavor=mlflow \
   --tracking_username={{mlflow_secret.username}} \
   --tracking_password={{mlflow_secret.password}} \
   ...
```

## Using the Experiment Tracker

To log information with MLflow in a pipeline step:
1. Enable the experiment tracker with the `@step` decorator.
2. Utilize MLflow's logging or auto-logging features as normal.

```python
import mlflow

@step(experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>")
def train_step(...):
   mlflow.tensorflow.autolog()
   
   mlflow.log_param(...)
   mlflow.log_metric(...)
   mlflow.log_artifact(...)
   
   ...
```

## Viewing Results
To access the MLflow experiment for a ZenML run, locate the corresponding URL.

```python
last_run = client.get_pipeline("<PIPELINE_NAME>").last_run
trainer_step = last_run.get_step("<STEP_NAME>")
tracking_url = trainer_step.run_metadata["experiment_tracker_url"].value
```

This section provides a link to your deployed MLflow instance UI or the local MLflow experiment file. You can configure the experiment tracker using `MLFlowExperimentTrackerSettings`.

```python
from zenml.integrations.mlflow.flavors.mlflow_experiment_tracker_flavor import MLFlowExperimentTrackerSettings

mlflow_settings = MLFlowExperimentTrackerSettings(
   nested=True,
   tags={"key": "value"}  
)

@step(
   experiment_tracker="<MLFLOW_TRACKER_STACK_COMPONENT_NAME>",
   settings={
       "experiment_tracker": mlflow_settings
   }  
)
```

For advanced options, refer to the [full MLflow Experiment Tracker documentation](../../component-guide/experiment-trackers/mlflow.md).



================================================================================

# docs/book/how-to/popular-integrations/README.md

# Popular Integrations

ZenML integrates seamlessly with popular tools in the data science and machine learning ecosystem. This guide provides instructions on how to connect ZenML with these tools.



================================================================================

# docs/book/how-to/popular-integrations/kubernetes.md

### Summary: Deploying ZenML Pipelines on Kubernetes

The ZenML Kubernetes Orchestrator enables running ML pipelines on a Kubernetes cluster without needing to write Kubernetes code, serving as a lightweight alternative to orchestrators like Airflow or Kubeflow. 

#### Prerequisites:
- Install ZenML `kubernetes` integration: `zenml integration install kubernetes`
- Docker installed and running
- `kubectl` installed
- Remote artifact store and container registry in your ZenML stack
- Deployed Kubernetes cluster
- Configured `kubectl` context (optional)

#### Deployment:
To deploy the orchestrator, a Kubernetes cluster is necessary. Various deployment methods exist across cloud providers or custom infrastructure; refer to the [cloud guide](../../user-guide/cloud-guide/cloud-guide.md) for options.

#### Configuration:
The orchestrator can be configured in two ways:
1. Using a [Service Connector](../../infrastructure-deployment/auth-management/service-connectors-guide.md) for connecting to the remote cluster (recommended for cloud-managed clusters, no local `kubectl` context required).

```bash
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubernetes
zenml service-connector list-resources --resource-type kubernetes-cluster -e
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

To configure `kubectl` for a remote cluster, set up a context that points to the cluster. Additionally, update the orchestrator configuration to include the `kubernetes_context`.

```bash
zenml orchestrator register <ORCHESTRATOR_NAME> \
    --flavor=kubernetes \
    --kubernetes_context=<KUBERNETES_CONTEXT>

zenml stack register <STACK_NAME> -o <ORCHESTRATOR_NAME> ... --set
```

## Running a Pipeline

Once configured, you can execute any ZenML pipeline using the Kubernetes Orchestrator.

```bash
python your_pipeline.py
```

This documentation outlines the creation of a Kubernetes pod for each step in your pipeline, with interaction possible via `kubectl` commands. For advanced configuration options and further details, consult the [full Kubernetes Orchestrator documentation](../../component-guide/orchestrators/kubernetes.md).



================================================================================

# docs/book/how-to/popular-integrations/aws-guide.md

### AWS Stack Setup for ZenML Pipelines

This guide provides steps to set up a minimal production stack on AWS for running ZenML pipelines.

#### Prerequisites
- An active AWS account with permissions for S3, SageMaker, ECR, and ECS.
- ZenML installed.
- AWS CLI installed and configured with your credentials. Follow [these instructions](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html).

#### Steps

1. **Choose AWS Region**: 
   - In the AWS console, select the region for your ZenML stack resources (e.g., `us-east-1`, `eu-west-2`).

2. **Create IAM Role**: 
   - Obtain your AWS account ID by running the appropriate command.

For a quicker setup, consider using the [in-browser stack deployment wizard](../../infrastructure-deployment/stack-deployment/deploy-a-cloud-stack.md), the [stack registration wizard](../../infrastructure-deployment/stack-deployment/register-a-cloud-stack.md), or the [ZenML AWS Terraform module](../../infrastructure-deployment/stack-deployment/deploy-a-cloud-stack-with-terraform.md).

```shell
aws sts get-caller-identity --query Account --output text
```

This process outputs your AWS account ID, which is essential for the next steps. Note that this refers to the root account ID used for AWS console login. Next, create a file named `assume-role-policy.json` with the specified content.

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "AWS": "arn:aws:iam::<YOUR_ACCOUNT_ID>:root",
        "Service": "sagemaker.amazonaws.com"
      },
      "Action": "sts:AssumeRole"
    }
  ]
}
```

Replace `<YOUR_ACCOUNT_ID>` with your actual AWS account ID. Create a new IAM role for ZenML to access AWS resources, using `zenml-role` as the role name (you can choose a different name if desired). Use the following command to create the role:

```shell
aws iam create-role --role-name zenml-role --assume-role-policy-document file://assume-role-policy.json
```

Take note of the terminal output, particularly the Role ARN. 

1. Attach the following policies to the role for AWS service access:
   - `AmazonS3FullAccess`
   - `AmazonEC2ContainerRegistryFullAccess`
   - `AmazonSageMakerFullAccess`

```shell
aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
aws iam attach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess
```

To begin, install the AWS and S3 ZenML integrations if you haven't done so already.

```shell
zenml integration install aws s3 -y
```

## 2) Create a Service Connector within ZenML

To create an AWS Service Connector in ZenML, follow these steps to enable authentication for ZenML and its components using an IAM role. 

{% tabs %} 
{% tab title="CLI" %}

```shell
zenml service-connector register aws_connector \
  --type aws \
  --auth-method iam-role \
  --role_arn=<ROLE_ARN> \
  --region=<YOUR_REGION> \
  --aws_access_key_id=<YOUR_ACCESS_KEY_ID> \
  --aws_secret_access_key=<YOUR_SECRET_ACCESS_KEY>
```

Replace `<ROLE_ARN>` with your IAM role ARN, `<YOUR_REGION>` with the appropriate region, and use your AWS access key ID and secret access key. 

## 3) Create Stack Components

### Artifact Store (S3)
An artifact store is essential for storing and versioning data in your pipelines. 

1. Create an AWS S3 bucket before using the ZenML CLI. If you already have a bucket, you can skip this step. Ensure the bucket name is unique, as it may require multiple attempts to find an available name.

```shell
aws s3api create-bucket --bucket your-bucket-name
```

To create the ZenML stack component, first register an S3 Artifact Store using the connector.

```shell
zenml artifact-store register cloud_artifact_store -f s3 --path=s3://bucket-name --connector aws_connector
```

### Orchestrator (SageMaker Pipelines) Summary

An orchestrator serves as the compute backend for running pipelines in ZenML. 

1. **SageMaker Domain Creation**: 
   - Before using the ZenML CLI, create a SageMaker domain on AWS (if not already created). 
   - The domain is a management unit for SageMaker users and resources, providing a single sign-on experience and enabling the management of resources like notebooks, training jobs, and endpoints.
   - Configuration settings include domain name, user profiles, and security settings, with each user having an isolated workspace featuring JupyterLab, compute resources, and persistent storage.

2. **SageMaker Pipelines**: 
   - The SageMaker orchestrator in ZenML requires a SageMaker domain to utilize the SageMaker Pipelines service, which facilitates the definition, execution, and management of machine learning workflows.
   - Creating a SageMaker domain establishes the environment and permissions necessary for the orchestrator to interact with SageMaker resources.

3. **Registering the Orchestrator**: 
   - To register a SageMaker Pipelines orchestrator stack component, you need the IAM role ARN (execution role) noted earlier.

For more details, refer to the [documentation](../../../component-guide/artifact-stores/s3.md).

```shell
zenml orchestrator register sagemaker-orchestrator --flavor=sagemaker --region=<YOUR_REGION> --execution_role=<ROLE_ARN>
```

**Note**: The SageMaker orchestrator operates using AWS configuration and does not need a service connector for authentication, relying instead on AWS CLI configurations or environment variables. More details are available [here](../../../component-guide/orchestrators/sagemaker.md). 

### Container Registry (ECR)
A [container registry](../../../component-guide/container-registries/container-registries.md) stores Docker images for your pipelines. To start, create a repository in ECR unless you already have one.

```shell
aws ecr create-repository --repository-name zenml --region <YOUR_REGION>
```

To create a ZenML stack component, first register an ECR container registry stack component.

```shell
zenml container-registry register ecr-registry --flavor=aws --uri=<ACCOUNT_ID>.dkr.ecr.<YOUR_REGION>.amazonaws.com --connector aws-connector
```

To create a stack using the CLI, refer to the detailed instructions provided in the documentation linked above.

```shell
export STACK_NAME=aws_stack

zenml stack register ${STACK_NAME} -o ${ORCHESTRATOR_NAME} \
    -a ${ARTIFACT_STORE_NAME} -c ${CONTAINER_REGISTRY_NAME} --set
```

You can add additional components to your AWS stack as needed. Once you combine the three main stack components, your AWS stack is complete and ready for use. You can test it by running a pipeline. To do this, define a ZenML pipeline.

```python
from zenml import pipeline, step

@step
def hello_world() -> str:
    return "Hello from SageMaker!"

@pipeline
def aws_sagemaker_pipeline():
    hello_world()

if __name__ == "__main__":
    aws_sagemaker_pipeline()
```

Save the code as `run.py` and execute it. The pipeline utilizes AWS S3 for artifact storage, Amazon SageMaker Pipelines for orchestration, and Amazon ECR for container registry.

```shell
python run.py
```

### Summary of Documentation

**Running a Pipeline on a Remote Stack with a Code Repository**  
Refer to the [production guide](../../../user-guide/production-guide/production-guide.md) for detailed information.

**Cleanup Warning**  
Ensure resources are no longer needed before deletion, as the following instructions are DESTRUCTIVE.

**Action Required**  
Delete any unused AWS resources to prevent additional charges.

```shell
# delete the S3 bucket
aws s3 rm s3://your-bucket-name --recursive
aws s3api delete-bucket --bucket your-bucket-name

# delete the SageMaker domain
aws sagemaker delete-domain --domain-id <DOMAIN_ID>

# delete the ECR repository
aws ecr delete-repository --repository-name zenml-repository --force

# detach policies from the IAM role
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryFullAccess
aws iam detach-role-policy --role-name zenml-role --policy-arn arn:aws:iam::aws:policy/AmazonSageMakerFullAccess

# delete the IAM role
aws iam delete-role --role-name zenml-role
```

Ensure commands are executed in the same AWS region where resources were created. Running the cleanup commands will delete the S3 bucket, SageMaker domain, ECR repository, and IAM role, preventing unnecessary charges. Confirm that these resources are no longer needed before deletion.

### Conclusion
This guide outlined the setup of an AWS stack with ZenML for scalable machine learning pipelines. Key steps included:
1. Setting up credentials and the local environment with an IAM role.
2. Creating a ZenML service connector for AWS authentication.
3. Configuring stack components: S3 for artifact storage, SageMaker Pipelines for orchestration, and ECR for container management.
4. Registering stack components and creating a ZenML stack.

Benefits of this setup include:
- **Scalability**: Handle large-scale workloads with AWS services.
- **Reproducibility**: Maintain versioned artifacts and containerized environments.
- **Collaboration**: Centralized stack for team resource sharing.
- **Flexibility**: Customize stack components as needed.

Next steps:
- Explore ZenML's [production guide](../../user-guide/production-guide/README.md) for best practices.
- Investigate ZenML's [integrations](../../component-guide/README.md) with other tools.
- Join the [ZenML community](https://zenml.io/slack) for support and networking.

### Best Practices for Using an AWS Stack with ZenML
- **Use IAM Roles and Least Privilege Principle**: Grant only necessary permissions and regularly audit IAM roles for security.
- **Leverage AWS Resource Tagging**: Implement a consistent tagging strategy for all AWS resources used in your pipelines.

```shell
aws s3api put-bucket-tagging --bucket your-bucket-name --tagging 'TagSet=[{Key=Project,Value=ZenML},{Key=Environment,Value=Production}]'
```

Use tags for billing and cost allocation tracking, as well as cleanup efforts. 

### Implement Cost Management Strategies
Utilize [AWS Cost Explorer](https://aws.amazon.com/aws-cost-management/aws-cost-explorer/) and [AWS Budgets](https://aws.amazon.com/aws-cost-management/aws-budgets/) to monitor and manage spending. 

To create a cost budget:
1. Create a JSON file (e.g., `budget-config.json`) defining the budget.

```json
{
  "BudgetLimit": {
    "Amount": "100",
    "Unit": "USD"
  },
  "BudgetName": "ZenML Monthly Budget",
  "BudgetType": "COST",
  "CostFilters": {
    "TagKeyValue": [
      "user:Project$ZenML"
    ]
  },
  "CostTypes": {
    "IncludeTax": true,
    "IncludeSubscription": true,
    "UseBlended": false
  },
  "TimeUnit": "MONTHLY"
}
```

**2. Create the Cost Budget:**

- Define the overall project scope and objectives.
- Identify all cost components, including labor, materials, equipment, and overhead.
- Estimate costs for each component using historical data, expert judgment, or market research.
- Compile estimates into a comprehensive budget document.
- Include contingency funds to address potential risks and uncertainties.
- Review and adjust the budget based on stakeholder feedback and project requirements.
- Ensure the budget aligns with project timelines and deliverables. 
- Monitor and update the budget regularly throughout the project lifecycle.

```shell
aws budgets create-budget --account-id your-account-id --budget file://budget-config.json
```

To track expenses for your ZenML projects, set up cost allocation tags. These tags help categorize and monitor spending effectively.

```shell
aws ce create-cost-category-definition --name ZenML-Projects --rules-version 1 --rules file://rules.json
```

### Use Warm Pools for SageMaker Pipelines

Warm Pools in SageMaker can significantly reduce pipeline step startup times, enhancing development efficiency. This feature maintains compute instances in a "warm" state for quick job initiation. To enable Warm Pools, utilize the `SagemakerOrchestratorSettings` class.

```python
sagemaker_orchestrator_settings = SagemakerOrchestratorSettings(
    keep_alive_period_in_seconds = 300, # 5 minutes, default value
)
```

This configuration keeps instances warm for 5 minutes post-job completion, facilitating faster startup for subsequent jobs, which is advantageous for iterative development and frequent pipelines. 

### Implement a Robust Backup Strategy
- Regularly back up critical data and configurations.
- For S3, enable versioning and consider cross-region replication for disaster recovery.

By adhering to these best practices and examples, you can enhance the security, efficiency, and cost-effectiveness of your AWS stack for ZenML projects. Regularly review and update your practices as projects evolve and AWS introduces new features.



================================================================================

# docs/book/how-to/popular-integrations/kubeflow.md

**Kubeflow Orchestrator Overview**

The ZenML Kubeflow Orchestrator enables running ML pipelines on Kubeflow Pipelines without the need for Kubeflow code.

**Prerequisites:**
- Install ZenML `kubeflow` integration: `zenml integration install kubeflow`
- Docker must be installed and running
- `kubectl` installation is optional
- A Kubernetes cluster with Kubeflow Pipelines installed (refer to the deployment guide for your cloud provider)
- A remote artifact store and container registry in your ZenML stack
- A remote ZenML server deployed in the cloud
- Name of your Kubernetes context pointing to the remote cluster (optional)

**Configuration:**
- Configure the orchestrator using a Service Connector for connection to the remote cluster (recommended for cloud-managed clusters), eliminating the need for local `kubectl` context.

```bash
zenml orchestrator register <ORCHESTRATOR_NAME> --flavor kubeflow
zenml service-connector list-resources --resource-type kubernetes-cluster -e  
zenml orchestrator connect <ORCHESTRATOR_NAME> --connector <CONNECTOR_NAME>
zenml stack update -o <ORCHESTRATOR_NAME>
```

To configure `kubectl` for a remote cluster, set up a context that points to the cluster. Additionally, specify the `kubernetes_context` in the orchestrator configuration.

```bash  
zenml orchestrator register <ORCHESTRATOR_NAME> \
    --flavor=kubeflow \
    --kubernetes_context=<KUBERNETES_CONTEXT>
    
zenml stack update -o <ORCHESTRATOR_NAME>
```

## Running a Pipeline
Once configured, you can execute any ZenML pipeline using the Kubeflow Orchestrator.

```python
python your_pipeline.py
```

This documentation outlines the creation of a Kubernetes pod for each step in a pipeline, with the ability to view pipeline runs in the Kubeflow UI. Additional configuration options are available through `KubeflowOrchestratorSettings`.

```python
from zenml.integrations.kubeflow.flavors.kubeflow_orchestrator_flavor import KubeflowOrchestratorSettings

kubeflow_settings = KubeflowOrchestratorSettings(
   client_args={},  
   user_namespace="my_namespace",
   pod_settings={
       "affinity": {...},
       "tolerations": [...]
   }
)

@pipeline(
   settings={
       "orchestrator": kubeflow_settings
   }
)
```

This documentation allows for the specification of client arguments, user namespace, pod affinity, and tolerations. For multi-tenant Kubeflow deployments, use the `kubeflow_hostname` ending in `/pipeline` when registering the orchestrator.

```bash
zenml orchestrator register <NAME> \
   --flavor=kubeflow \
   --kubeflow_hostname=<KUBEFLOW_HOSTNAME> # e.g. https://mykubeflow.example.com/pipeline
```

To configure the orchestrator settings, provide the following credentials: namespace, username, and password.

```python
kubeflow_settings = KubeflowOrchestratorSettings(
   client_username="admin",
   client_password="abc123", 
   user_namespace="namespace_name"
)

@pipeline(
   settings={
       "orchestrator": kubeflow_settings
   }
)
```

For advanced options and details, refer to the [full Kubeflow Orchestrator documentation](../../component-guide/orchestrators/kubeflow.md).



================================================================================

# docs/book/how-to/project-setup-and-management/interact-with-secrets.md

# Interact with Secrets

## What is a ZenML Secret?
ZenML secrets are collections of **key-value pairs** securely stored in the ZenML secrets store. Each secret has a **name** for easy retrieval and reference in pipelines and stacks.

## How to Create a Secret
To create a secret with the name `<SECRET_NAME>` and a key-value pair, use the following CLI command:

```shell
zenml secret create <SECRET_NAME> \
    --<KEY_1>=<VALUE_1> \
    --<KEY_2>=<VALUE_2>

# Another option is to use the '--values' option and provide key-value pairs in either JSON or YAML format.
zenml secret create <SECRET_NAME> \
    --values='{"key1":"value2","key2":"value2"}'
```

You can create the secret interactively by using the `--interactive/-i` parameter, which prompts you for the secret keys and values.

```shell
zenml secret create <SECRET_NAME> -i
```

For large secret values or those with special characters, use the `@` syntax in ZenML to specify that the value should be read from a file.

```bash
zenml secret create <SECRET_NAME> \
   --key=@path/to/file.txt \
   ...
   
# Alternatively, you can utilize the '--values' option by specifying a file path containing key-value pairs in either JSON or YAML format.
zenml secret create <SECRET_NAME> \
    --values=@path/to/file.txt
```

The CLI provides commands for listing, updating, and deleting secrets. A comprehensive guide on managing secrets via the CLI is available [here](https://sdkdocs.zenml.io/latest/cli/#zenml.cli--secrets-management). To ensure all referenced secrets in your stack exist, you can use a specific CLI command to interactively register missing secrets.

```shell
zenml stack register-secrets [<STACK_NAME>]
```

The ZenML client API provides a programmatic interface for creating various components within the framework.

```python
from zenml.client import Client

client = Client()
client.create_secret(
    name="my_secret",
    values={
        "username": "admin",
        "password": "abc123"
    }
)
```

The Client methods for secrets management include:

- `get_secret`: Fetch a secret by name or ID.
- `update_secret`: Update an existing secret.
- `list_secrets`: Query the secrets store with filtering and sorting options.
- `delete_secret`: Remove a secret.

For the complete Client API reference, visit [here](https://sdkdocs.zenml.io/latest/core_code_docs/core-client/).

### Set Scope for Secrets
ZenML secrets can be scoped to individual users, ensuring that secrets are only accessible to the specified user. By default, all created secrets are scoped to the active user. To create a user-scoped secret, use the `--scope` argument in the CLI command.

```shell
zenml secret create <SECRET_NAME> \
    --scope user \
    --<KEY_1>=<VALUE_1> \
    --<KEY_2>=<VALUE_2>
```

Scopes function as individual namespaces, allowing ZenML to reference secrets by name scoped to the active user. 

### Accessing Registered Secrets
To configure stack components that require sensitive information (e.g., passwords or tokens), use secret references instead of direct values. This is done by specifying the secret name and key in the following syntax: `{{<SECRET_NAME>.<SECRET_KEY>}}`. 

For example, this can be applied in CLI commands.

```shell
# Register a secret called `mlflow_secret` with key-value pairs for the
# username and password to authenticate with the MLflow tracking server

# Using central secrets management
zenml secret create mlflow_secret \
    --username=admin \
    --password=abc123
    

# Then reference the username and password in our experiment tracker component
zenml experiment-tracker register mlflow \
    --flavor=mlflow \
    --tracking_username={{mlflow_secret.username}} \
    --tracking_password={{mlflow_secret.password}} \
    ...
```

When using secret references in ZenML, the framework validates the existence of all referenced secrets and keys in your stack components before executing a pipeline. This early validation prevents pipeline failures due to missing secrets. By default, ZenML fetches and reads all secrets, which can be time-consuming and may fail if permissions are insufficient. You can control the validation level using the `ZENML_SECRET_VALIDATION_LEVEL` environment variable:

- `NONE`: Disables validation.
- `SECRET_EXISTS`: Validates only the existence of secrets, useful for environments with limited permissions.
- `SECRET_AND_KEY_EXISTS`: (default) Validates both the existence of secrets and the specified key-value pairs.

If using centralized secrets management, you can access secrets directly within your steps via the ZenML `Client` API, allowing for secure API queries without hard-coding access keys.

```python
from zenml import step
from zenml.client import Client


@step
def secret_loader() -> None:
    """Load the example secret from the server."""
    # Fetch the secret from ZenML.
    secret = Client().get_secret( < SECRET_NAME >)

    # `secret.secret_values` will contain a dictionary with all key-value
    # pairs within your secret.
    authenticate_to_some_api(
        username=secret.secret_values["username"],
        password=secret.secret_values["password"],
    )
    ...
```

The provided text contains an image link related to "ZenML Scarf" but lacks any technical information or key points to summarize. Please provide additional content for a meaningful summary.



================================================================================

# docs/book/how-to/project-setup-and-management/README.md

# Project Setup and Management

This section details the setup and management of ZenML projects, covering essential processes and best practices.



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/stacks-pipelines-models.md

# Organizing Stacks, Pipelines, Models, and Artifacts in ZenML

This guide outlines the organization of stacks, pipelines, models, and artifacts in ZenML, which are essential for structuring your ML project effectively.

## Key Concepts

- **Stacks**: Configuration of tools and infrastructure for running pipelines, consisting of components like orchestrators and artifact stores. Stacks enable consistent environments across local, staging, and production settings.

- **Pipelines**: Sequences of tasks in your ML workflow, automating processes and providing visibility. It's advisable to separate pipelines for different tasks (e.g., training vs. inference) for better modularity and management.

- **Models**: Collections of related pipelines, artifacts, and metadata, serving as a "project" that connects various components. Models facilitate data transfer between pipelines.

- **Artifacts**: Outputs from pipeline steps that can be tracked and reused. Proper naming and logging of metadata enhance traceability and organization.

## Stack Management

- A single stack can support multiple pipelines, reducing configuration overhead and promoting reproducibility. 

## Organizing Pipelines, Models, and Artifacts

- **Pipelines**: Structure your pipelines to encompass the entire ML workflow, separating tasks for easier management and collaboration.

- **Models**: Use models to group related artifacts and pipelines, aiding in data transfer and version control.

- **Artifacts**: Track outputs from pipelines, ensuring clear history and traceability. Artifacts can be associated with models for better organization.

## Example Workflow

1. Team members create separate pipelines for feature engineering, training, and inference.
2. They use a shared stack for local testing, enabling quick iterations.
3. Models are used to connect training outputs with inference inputs, ensuring consistency.
4. The Model Control Plane helps manage model versions and promotes the best-performing models to production.

## Guidelines for Organization

- **Models**: One model per use-case; group related components.
- **Stacks**: Maintain separate stacks for different environments; share production stacks for consistency.
- **Naming and Organization**: Use consistent naming conventions, tags for filtering, and document configurations and dependencies.

Following these guidelines will help maintain a scalable and organized MLOps workflow as your project evolves.



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/README.md

It seems that the text you provided is incomplete or missing. Please provide the documentation text you would like summarized, and I will be happy to assist you!



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/shared-components-for-teams.md

# Shared Libraries and Logic for Teams

Teams often need to collaborate on projects and share versioned logic for cross-cutting functionality. Sharing code libraries enhances incremental improvements, robustness, and standardization. This guide focuses on two key aspects of sharing code using ZenML: 

1. **What Can Be Shared**
2. **How to Distribute Shared Components**

## What Can Be Shared

ZenML allows sharing several types of custom components:

### Custom Flavors
Custom flavors are integrations not included with ZenML. To implement and share a custom flavor:
1. Create it in a shared repository.
2. Implement the custom stack component as per the [ZenML documentation](../../infrastructure-deployment/stack-deployment/implement-a-custom-stack-component.md#implementing-a-custom-stack-component-flavor).
3. Register the component using the ZenML CLI, such as for a custom artifact store flavor.

```bash
zenml artifact-store flavor register <path.to.MyS3ArtifactStoreFlavor>
```

### Custom Steps and Materializers
- **Custom Steps**: Can be created and shared via a separate repository, allowing team members to reference them like Python modules.
- **Custom Materializers**: Commonly shared components. To implement:
  1. Create in a shared repository.
  2. Follow the [ZenML documentation](https://docs.zenml.io/how-to/data-artifact-management/handle-data-artifacts/handle-custom-data-types).
  3. Team members can import and use them in projects.

### Distributing Shared Components
#### Shared Private Wheels
- **Definition**: A method for internal distribution of Python code without public access.
- **Benefits**:
  - Easy installation with pip.
  - Simplified version and dependency management.
  - Can be hosted on internal PyPI servers.
  - Integrated like standard Python packages.

#### Setup Steps:
1. Create a private PyPI server or use services like [AWS CodeArtifact](https://aws.amazon.com/codeartifact/).
2. Build your code into wheel format ([packaging guide](https://packaging.python.org/en/latest/tutorials/packaging-projects/)).
3. Upload the wheel to your private PyPI server.
4. Configure pip to include the private server.
5. Install packages using pip as with public packages.

### Using Shared Libraries with `DockerSettings`
- **Docker Integration**: ZenML generates a `Dockerfile` at runtime for pipelines with remote orchestrators.
- **Library Inclusion**: Specify shared libraries using the `DockerSettings` class, either by listing requirements.

```python
import os
from zenml.config import DockerSettings
from zenml import pipeline

docker_settings = DockerSettings(
    requirements=["my-simple-package==0.1.0"],
    environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ.get('PYPI_TOKEN', '')}@my-private-pypi-server.com/{os.environ.get('PYPI_USERNAME', '')}/"}
)

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

You can utilize a requirements file for managing dependencies.

```python
docker_settings = DockerSettings(requirements="/path/to/requirements.txt")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

The `requirements.txt` file should specify the private index URL as follows:

```
--extra-index-url https://YOURTOKEN@my-private-pypi-server.com/YOURUSERNAME/
my-simple-package==0.1.0
```

For guidance on using private PyPI repositories, refer to our [documentation on how to use a private PyPI repository](../customize-docker-builds/how-to-use-a-private-pypi-repository.md).

## Best Practices
- **Version Control**: Utilize systems like Git for effective collaboration and access to the latest code versions.
- **Access Controls**: Implement authentication and user permission management for private PyPI servers to secure proprietary code.
- **Documentation**: Maintain comprehensive documentation covering installation, API references, usage examples, and guidelines for shared components.
- **Library Updates**: Regularly update shared libraries with bug fixes and enhancements, and communicate these changes to the team.
- **Continuous Integration**: Set up CI to ensure the quality and compatibility of shared libraries by automatically running tests on code changes.

These practices enhance collaboration, maintain consistency, and accelerate development within the ZenML framework.



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/access-management.md

# Access Management and Roles in ZenML

Effective access management is essential for security and efficiency in ZenML projects. This guide outlines user roles and access management strategies.

## Typical Roles in an ML Project
- **Data Scientists**: Develop and run pipelines.
- **MLOps Platform Engineers**: Manage infrastructure and stack components.
- **Project Owners**: Oversee ZenML deployment and user access.

Roles may vary, but responsibilities can be adapted to fit your project.

> **Note**: Create roles in ZenML Pro with specific permissions and assign them to Users or Teams. [Sign up for a free trial](https://cloud.zenml.io/).

## Service Connectors
Service connectors integrate external cloud services with ZenML, managing credentials and configurations. Only MLOps Platform Engineers should create and manage these connectors, while Data Scientists can use them to create stack components without accessing sensitive credentials.

### Example Permissions:
- **Data Scientist**: Can use connectors but cannot create, update, or delete them.
- **MLOps Platform Engineer**: Can create, update, delete connectors, and read secret values.

> **Note**: RBAC features are available in ZenML Pro. Learn more about roles [here](../../../getting-started/zenml-pro/roles.md).

## Server Upgrade Responsibilities
Project Owners decide on server upgrades after consulting teams. MLOps Platform Engineers typically handle the upgrade process, ensuring data backup and no service disruption.

> **Note**: Consider using separate servers for different teams to ease upgrade pressures. ZenML Pro supports [multi-tenancy](../../../getting-started/zenml-pro/tenants.md).

## Pipeline Migration and Maintenance
Data Scientists own pipeline code, while Platform Engineers ensure compatibility with new ZenML versions. Both should review release notes and migration guides during upgrades.

## Best Practices for Access Management
- **Regular Audits**: Periodically review user access and permissions.
- **Role-Based Access Control (RBAC)**: Streamline permission management.
- **Least Privilege**: Grant minimal necessary permissions.
- **Documentation**: Maintain clear records of roles and access policies.

> **Note**: RBAC and permission assignment are exclusive to ZenML Pro users. 

By adhering to these practices, you can maintain a secure and collaborative ZenML environment.



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/project-templates/create-your-own-template.md

### How to Create Your Own ZenML Template

Creating a ZenML template standardizes and shares ML workflows across projects or teams. ZenML utilizes [Copier](https://copier.readthedocs.io/en/stable/) for managing project templates. Follow these steps to create your own template:

1. **Create a Repository:** Set up a new repository to store your template's code and configuration files.
2. **Define Workflows:** Implement your ML workflows as ZenML steps and pipelines. You can modify existing templates, such as the [starter template](https://github.com/zenml-io/template-starter).
3. **Create `copier.yml`:** This file defines the template's parameters and default values. Refer to the [Copier documentation](https://copier.readthedocs.io/en/stable/creating/) for details.
4. **Test Your Template:** Use the `copier` command-line tool to generate a new project from your template and verify its functionality.

```bash
copier copy https://github.com/your-username/your-template.git your-project
```

To use your template with ZenML, replace `https://github.com/your-username/your-template.git` with your template repository URL and `your-project` with your desired project name. Then, run the `zenml init` command to initialize your project.

```bash
zenml init --template https://github.com/your-username/your-template.git
```

Replace `https://github.com/your-username/your-template.git` with your template repository URL. To use a specific version, utilize the `--template-tag` option to specify the desired git tag.

```bash
zenml init --template https://github.com/your-username/your-template.git --template-tag v1.0.0
```

To set up your ZenML project template, replace `v1.0.0` with your desired git tag version. This allows for quick initialization of new ML projects. Ensure your template is updated with the latest best practices. The documentation's [Production Guide](../../../../user-guide/production-guide/README.md) is based on the `E2E Batch` project template. It is recommended to install the `e2e_batch` template using the `--template-with-defaults` flag for a better understanding of the guide in your local environment.

```bash
mkdir e2e_batch
cd e2e_batch
zenml init --template e2e_batch --template-with-defaults
```

The provided text contains an image of "ZenML Scarf" but lacks any technical information or key points to summarize. Please provide additional context or details for a more comprehensive summary.



================================================================================

# docs/book/how-to/project-setup-and-management/collaborate-with-team/project-templates/README.md

### ZenML Project Templates Overview

ZenML project templates provide a quick way to understand the ZenML framework and start building ML pipelines. They include a collection of steps, pipelines, and a simple CLI.

#### Available Project Templates

| Project Template [Short name] | Tags | Description |
|-------------------------------|------|-------------|
| [Starter template](https://github.com/zenml-io/template-starter) [starter] | basic, scikit-learn | Essential ML components for starting with ZenML, including parameterized steps, a model training pipeline, and a flexible configuration using scikit-learn. |
| [E2E Training with Batch Predictions](https://github.com/zenml-io/template-e2e-batch) [e2e_batch] | etl, hp-tuning, model-promotion, drift-detection, batch-prediction, scikit-learn | A comprehensive template with two pipelines covering data loading, preprocessing, hyperparameter tuning, model training, evaluation, promotion, drift detection, and batch inference. |
| [NLP Training Pipeline](https://github.com/zenml-io/template-nlp) [nlp] | nlp, hp-tuning, model-promotion, training, pytorch, gradio, huggingface | A straightforward NLP pipeline for tokenization, training, hyperparameter tuning, evaluation, and deployment of BERT or GPT-2 models, with local testing using Gradio. |

#### Collaboration Opportunity
ZenML invites users to share personal projects as templates to enhance the platform. Interested individuals can join the [ZenML Slack](https://zenml.io/slack/) for collaboration.

#### Getting Started
To use the templates, ensure ZenML and its `templates` extras are installed.

```bash
pip install zenml[templates]
```

{% hint style="warning" %} Note that these templates differ from 'Run Templates' used for triggering a pipeline via the dashboard or Python SDK. More information on 'Run Templates' can be found <a href="https://docs.zenml.io/how-to/trigger-pipelines">here</a>. {% endhint %} To generate a project from an existing template, use the `--template` flag with the `zenml init` command.

```bash
zenml init --template <short_name_of_template>
# example: zenml init --template e2e_batch
```

To use default values for the ZenML project template, add `--template-with-defaults` to the command. This will suppress input prompts.

```bash
zenml init --template <short_name_of_template> --template-with-defaults
# example: zenml init --template e2e_batch --template-with-defaults
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and referrer policy. The image source is a URL that includes a unique identifier.



================================================================================

# docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/connect-your-git-repository.md

### Summary

**Tracking Code with Git Repositories in ZenML**

Connecting your Git repository to ZenML allows for efficient code tracking and reduces unnecessary Docker builds. Supported platforms include [GitHub](https://github.com/) and [GitLab](https://gitlab.com/). 

Using a code repository enables ZenML to monitor the code version for pipeline runs and can expedite Docker image building by avoiding rebuilds for source code changes. 

**Registering a Code Repository**

To use a code repository, install the relevant ZenML integration based on the available implementations.

```
zenml integration install <INTEGRATION_NAME>
```

Code repositories can be registered using the Command Line Interface (CLI).

```shell
zenml code-repository register <NAME> --type=<TYPE> [--CODE_REPOSITORY_OPTIONS]
```

ZenML offers built-in implementations for code repositories on GitHub and GitLab, with the option to develop a custom implementation. 

### GitHub Integration
To use GitHub as a code repository for ZenML pipelines, register it by providing:
- GitHub instance URL
- Repository owner
- Repository name
- GitHub Personal Access Token (PAT) with repository access

Ensure to install the necessary integration before registration. For more details, refer to the sections on [`GitHubCodeRepository`](connect-your-git-repository.md#github) and [`GitLabCodeRepository`](connect-your-git-repository.md#gitlab).

```sh
zenml integration install github
```

To register a GitHub code repository, execute the following CLI command:

```shell
zenml code-repository register <NAME> --type=github \
--url=<GITHUB_URL> --owner=<OWNER> --repository=<REPOSITORY> \
--token=<GITHUB_TOKEN>
```

To register a GitHub code repository, provide the following details: 

- `<REPOSITORY>`: Name of the repository
- `<OWNER>`: Owner of the repository
- `<NAME>`: Repository name
- `<GITHUB_TOKEN>`: Your GitHub Personal Access Token
- `<GITHUB_URL>`: GitHub instance URL (default: `https://github.com`, set for GitHub Enterprise)

ZenML will detect tracked source files and store the commit hash for each pipeline run.

### How to Get a GitHub Token:
1. Go to GitHub account settings and click on [Developer settings](https://github.com/settings/tokens?type=beta).
2. Select "Personal access tokens" and click "Generate new token".
3. Name and describe your token.
4. Select the specific repository and grant `contents` read-only access.
5. Click "Generate token" and securely copy the token.

### GitLab Integration:
ZenML supports GitLab as a code repository. To register, provide the GitLab project URL, project group, project name, and a GitLab Personal Access Token (PAT) with project access. Install the corresponding integration before registration.

```sh
zenml integration install gitlab
```

To register a GitLab code repository, execute the following CLI command:

```shell
zenml code-repository register <NAME> --type=gitlab \
--url=<GITLAB_URL> --group=<GROUP> --project=<PROJECT> \
--token=<GITLAB_TOKEN>
```

To register a GitLab code repository in ZenML, use the following parameters: `<NAME>` (repository name), `<GROUP>` (project group), `<PROJECT>` (project name), `<GITLAB_TOKEN>` (GitLab Personal Access Token), and `<GITLAB_URL>` (GitLab instance URL, defaulting to `https://gitlab.com`). For self-hosted instances, specify the URL. After registration, ZenML will track your source files and store the commit hash for each pipeline run.

### How to Obtain a GitLab Token
1. Navigate to your GitLab account settings and select [Access Tokens](https://gitlab.com/-/profile/personal_access_tokens).
2. Name the token and choose necessary scopes (e.g., `read_repository`, `read_user`, `read_api`).
3. Click "Create personal access token" and securely copy the token.

### Developing a Custom Code Repository
For other code storage platforms, implement and register a custom code repository by subclassing and implementing the abstract methods of the `zenml.code_repositories.BaseCodeRepository` class.

```python
class BaseCodeRepository(ABC):
    """Base class for code repositories."""

    @abstractmethod
    def login(self) -> None:
        """Logs into the code repository."""

    @abstractmethod
    def download_files(
            self, commit: str, directory: str, repo_sub_directory: Optional[str]
    ) -> None:
        """Downloads files from the code repository to a local directory.

        Args:
            commit: The commit hash to download files from.
            directory: The directory to download files to.
            repo_sub_directory: The subdirectory in the repository to
                download files from.
        """

    @abstractmethod
    def get_local_context(
            self, path: str
    ) -> Optional["LocalRepositoryContext"]:
        """Gets a local repository context from a path.

        Args:
            path: The path to the local repository.

        Returns:
            The local repository context object.
        """
```

To register your implementation, follow these steps:

```shell
# The `CODE_REPOSITORY_OPTIONS` are key-value pairs that your implementation will receive
# as configuration in its __init__ method. This will usually include stuff like the username
# and other credentials necessary to authenticate with the code repository platform.
zenml code-repository register <NAME> --type=custom --source=my_module.MyRepositoryClass \
    [--CODE_REPOSITORY_OPTIONS]
```

The provided documentation includes an image related to ZenML Scarf, but lacks specific technical details or key points. For a comprehensive summary, additional context or text is needed to extract and condense the important information.



================================================================================

# docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/README.md

# Setting up a Well-Architected ZenML Project

This guide outlines best practices for structuring ZenML projects to enhance scalability, maintainability, and team collaboration.

## Importance of a Well-Architected Project
A well-architected ZenML project is essential for successful MLOps, providing a foundation for efficient development, deployment, and maintenance of ML models.

## Key Components

### Repository Structure
- Organize folders for pipelines, steps, and configurations.
- Maintain clear separation of concerns and consistent naming conventions.

### Version Control and Collaboration
- Integrate with version control systems like Git for:
  - Faster pipeline builds.
  - Easy change tracking and team collaboration.

### Stacks, Pipelines, Models, and Artifacts
- **Stacks**: Infrastructure and tool configurations.
- **Models**: ML models and metadata.
- **Pipelines**: Encapsulated ML workflows.
- **Artifacts**: Data and model output tracking.

### Access Management and Roles
- Define roles (e.g., data scientists, MLOps engineers).
- Set up service connectors and manage authorizations.
- Use ZenML Pro Teams for role assignment.

### Shared Components and Libraries
- Promote code reuse with:
  - Custom flavors, steps, and materializers.
  - Shared private wheels.
  - Authentication handling for libraries.

### Project Templates
- Utilize pre-made or custom templates to ensure consistency in projects.

### Migration and Maintenance
- Develop strategies for migrating legacy code and upgrading ZenML servers.

## Getting Started
Explore the guides in this section for detailed information on project setup and management. Regularly review and refine your project to meet evolving team needs, leveraging ZenML's features for a robust MLOps environment.



================================================================================

# docs/book/how-to/project-setup-and-management/setting-up-a-project-repository/set-up-repository.md

**Recommended Repository Structure and Best Practices for ZenML Projects**

While the structure of your ZenML project is flexible, the core team suggests the following recommended project layout:

1. **Directory Organization**: Organize your files logically to enhance readability and maintainability.
2. **Naming Conventions**: Use clear and consistent naming for files and directories.
3. **Documentation**: Include README files and comments to explain project components and usage.
4. **Version Control**: Utilize Git for version control to track changes and collaborate effectively.
5. **Environment Management**: Use virtual environments to manage dependencies and avoid conflicts.

Following these practices can improve project organization and collaboration.

```markdown
.
├── .dockerignore
├── Dockerfile
├── steps
│   ├── loader_step
│   │   ├── .dockerignore (optional)
│   │   ├── Dockerfile (optional)
│   │   ├── loader_step.py
│   │   └── requirements.txt (optional)
│   └── training_step
│       └── ...
├── pipelines
│   ├── training_pipeline
│   │   ├── .dockerignore (optional)
│   │   ├── config.yaml (optional)
│   │   ├── Dockerfile (optional)
│   │   ├── training_pipeline.py
│   │   └── requirements.txt (optional)
│   └── deployment_pipeline
│       └── ...
├── notebooks
│   └── *.ipynb
├── requirements.txt
├── .zen
└── run.py
```

ZenML project templates follow a basic structure with `steps` and `pipelines` folders for project definitions. For simpler projects, steps can be placed directly in the `steps` folder without subfolders. It is advisable to register your repository as a code repository to track code versions used in pipeline runs, which can also speed up Docker image builds by avoiding unnecessary rebuilds when source code changes. 

Steps should be organized in separate Python files to maintain distinct utils, dependencies, and Dockerfiles. ZenML automatically logs the output of the root Python logging handler into the artifact store during step execution. Use the `logging` module to ensure logs are visible in the ZenML dashboard.

```python
# Use ZenML handler
from zenml.logger import get_logger

logger = get_logger(__name__)
...

@step
def training_data_loader():
    # This will show up in the dashboard
    logger.info("My logs")
```

### Pipelines
- Store pipelines in separate Python files to manage utils, dependencies, and Dockerfiles independently.
- Separate pipeline execution from definition to prevent automatic execution upon import.
- **Warning**: Avoid naming pipelines or instances "pipeline" to prevent overwriting the imported `pipeline` and decorator, which can cause failures.
- **Info**: Unique pipeline names are crucial; using the same name for different pipelines can lead to a mixed history of runs.

### .dockerignore
- Exclude unnecessary files (e.g., data, virtual environments, git repos) in the `.dockerignore` to speed up Docker image creation and reduce sizes.

### Dockerfile (optional)
- ZenML uses the official [zenml Docker image](https://hub.docker.com/r/zenmldocker/zenml) by default. You can create a custom `Dockerfile` to override this behavior.

### Notebooks
- Organize all notebooks in a designated location.

### .zen
- Run `zenml init` at the project root to define the project scope, known as the "source's root," which resolves import paths and stores configurations. This is particularly important for Jupyter notebooks.
- **Warning**: Ensure all import paths are relative to the source's root.

### run.py
- Place pipeline runners in the repository root to ensure all imports resolve correctly. If no `.zen` is defined, this also establishes the implicit source's root.



================================================================================

# docs/book/how-to/customize-docker-builds/how-to-use-a-private-pypi-repository.md

### How to Use a Private PyPI Repository

For packages requiring authentication, follow these steps:

1. Store credentials securely using environment variables.
2. Configure `pip` or `poetry` to utilize these credentials during package installation.
3. Optionally, use custom Docker images with the necessary authentication setup.

Example for setting up authentication with environment variables is available in the documentation.

```python
import os

from my_simple_package import important_function
from zenml.config import DockerSettings
from zenml import step, pipeline

docker_settings = DockerSettings(
    requirements=["my-simple-package==0.1.0"],
    environment={'PIP_EXTRA_INDEX_URL': f"https://{os.environ.get('PYPI_TOKEN', '')}@my-private-pypi-server.com/{os.environ.get('PYPI_USERNAME', '')}/"}
)

@step
def my_step():
    return important_function()

@pipeline(settings={"docker": docker_settings})
def my_pipeline():
    my_step()

if __name__ == "__main__":
    my_pipeline()
```

**Important Note on Credential Handling:** Always use secure methods to manage and distribute authentication information within your team.



================================================================================

# docs/book/how-to/customize-docker-builds/README.md

# Customize Docker Builds

ZenML runs pipeline steps sequentially in the active Python environment locally. For remote orchestrators or step operators, ZenML builds Docker images to execute pipelines in an isolated environment. This section covers how to manage the dockerization process.



================================================================================

# docs/book/how-to/customize-docker-builds/docker-settings-on-a-step.md

You can customize Docker settings at the step level in a pipeline. By default, all steps use the Docker image defined at the pipeline level. If specific steps require different Docker images, you can achieve this by adding the [DockerSettings](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings) to the step decorator.

```python
from zenml import step
from zenml.config import DockerSettings

@step(
  settings={
    "docker": DockerSettings(
      parent_image="pytorch/pytorch:1.12.1-cuda11.3-cudnn8-runtime"
    )
  }
)
def training(...):
	...
```

This can also be accomplished in the configuration file.

```yaml
steps:
  training:
    settings:
      docker:
        parent_image: pytorch/pytorch:2.2.0-cuda11.8-cudnn8-runtime
        required_integrations:
          - gcp
          - github
        requirements:
          - zenml  # Make sure to include ZenML for other parent images
          - numpy
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and referrer policy. The image source is provided via a URL.



================================================================================

# docs/book/how-to/customize-docker-builds/specify-pip-dependencies-and-apt-packages.md

# Specify pip Dependencies and Apt Packages

**Warning**: Specifying pip and apt dependencies is applicable only for remote pipelines and is ignored in local pipelines.

When a pipeline runs with a remote orchestrator, a Dockerfile is dynamically generated at runtime to build the Docker image using the image builder component of your stack. You can import `DockerSettings` with `from zenml.config import DockerSettings`. 

ZenML automatically installs all packages required by your active stack, but you can specify additional packages in several ways, including installing all packages from your local Python environment using `pip` or `poetry`.

```python
# or use "poetry_export"
docker_settings = DockerSettings(replicate_local_python_environment="pip_freeze")


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

A custom command can be specified to output a list of requirements in the format of a requirements file as detailed in the [requirements file format documentation](https://pip.pypa.io/en/stable/reference/requirements-file-format/).

```python
from zenml.config import DockerSettings

docker_settings = DockerSettings(replicate_local_python_environment=[
    "poetry",
    "export",
    "--extras=train",
    "--format=requirements.txt"
])


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To specify a list of requirements in code, follow these key points:

1. **Define Requirements Clearly**: Use clear and concise language to articulate each requirement.
2. **Use a Structured Format**: Organize requirements in a structured format such as lists, tables, or bullet points for better readability.
3. **Prioritize Requirements**: Indicate the priority of each requirement (e.g., high, medium, low).
4. **Include Acceptance Criteria**: Define criteria for how each requirement will be validated or accepted.
5. **Version Control**: Keep track of changes to requirements using version control systems.
6. **Stakeholder Review**: Ensure requirements are reviewed and approved by relevant stakeholders.
7. **Maintain Traceability**: Link requirements to corresponding design and implementation artifacts for traceability.

By adhering to these guidelines, you can create a comprehensive and effective list of requirements in code.

```python
    docker_settings = DockerSettings(requirements=["torch==1.12.0", "torchvision"])

    @pipeline(settings={"docker": docker_settings})
    def my_pipeline(...):
        ...
    ```

To specify a requirements file, create a text file named `requirements.txt` that lists all the dependencies needed for your project. Each line should contain the package name and optionally its version, following the format `package==version`. You can also include comments by starting a line with `#`. To install the packages listed in the requirements file, use the command `pip install -r requirements.txt`. This approach ensures consistent environment setup across different systems.

```python
    docker_settings = DockerSettings(requirements="/path/to/requirements.txt")

    @pipeline(settings={"docker": docker_settings})
    def my_pipeline(...):
        ...
    ```

Specify the list of ZenML integrations utilized in your pipeline by referring to the [ZenML integrations documentation](../../component-guide/README.md).

```python
from zenml.integrations.constants import PYTORCH, EVIDENTLY

docker_settings = DockerSettings(required_integrations=[PYTORCH, EVIDENTLY])


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To specify a list of apt packages, use the following code format:

```bash
apt install package1 package2 package3
```

Replace `package1`, `package2`, and `package3` with the desired package names. Ensure you have the necessary permissions to install packages, typically requiring root or sudo access.

```python
    docker_settings = DockerSettings(apt_packages=["git"])

    @pipeline(settings={"docker": docker_settings})
    def my_pipeline(...):
        ...
    ```

To prevent ZenML from automatically installing the requirements of your stack, you can configure the settings in your ZenML environment. This allows you to manage dependencies manually, ensuring that only the necessary packages are installed according to your specifications.

```python
    docker_settings = DockerSettings(install_stack_requirements=False)

      @pipeline(settings={"docker": docker_settings})
      def my_pipeline(...):
          ...
    ```

ZenML enables the specification of custom Docker settings for pipeline steps that have conflicting requirements or require large dependencies not needed for other steps.

```python
docker_settings = DockerSettings(requirements=["tensorflow"])


@step(settings={"docker": docker_settings})
def my_training_step(...):
    ...
```

You can combine methods for installing requirements, ensuring no overlap with Docker settings. ZenML installs requirements in this order (each step optional): 

1. Packages in your local Python environment.
2. Packages required by the stack (unless `install_stack_requirements=False`).
3. Packages from `required_integrations`.
4. Packages from the `requirements` attribute.

Additional arguments for the installer can be specified for Python package installation.

```python
# This will result in a `pip install --timeout=1000 ...` call when installing packages in the
# Docker image
docker_settings = DockerSettings(python_package_installer_args={"timeout": 1000})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To use [`uv`](https://github.com/astral-sh/uv) for faster resolving and installation of Python packages, follow the provided instructions.

```python
docker_settings = DockerSettings(python_package_installer="uv")

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

`uv` is a newer project and may not be as stable as `pip`, potentially causing installation errors. If issues arise, revert to `pip` as a solution. For detailed documentation on using `uv` with PyTorch, visit the Astral Docs website [here](https://docs.astral.sh/uv/guides/integration/pytorch/), which includes important tips and details.



================================================================================

# docs/book/how-to/customize-docker-builds/how-to-reuse-builds.md

### Reusing Builds in ZenML

This guide explains how to reuse builds to enhance pipeline efficiency.

#### What is a Build?
A pipeline build encapsulates a pipeline and its associated stack, including Docker images, stack requirements, integrations, and optionally, the pipeline code.

#### Reusing Builds
When a pipeline runs, ZenML checks for an existing build with the same pipeline and stack. If found, it reuses that build; if not, a new build is created.

#### Listing Builds
You can list all builds for a pipeline using the CLI.

```bash
zenml pipeline builds list --pipeline_id='startswith:ab53ca'
```

You can manually create a build using the CLI.

```bash
zenml pipeline build --stack vertex-stack my_module.my_pipeline_instance
```

You can specify the configuration file and stack for the build, with the source being a path to a pipeline instance. ZenML automatically finds existing builds that match your pipeline and stack, but you can force the use of a specific build by passing the build ID to the `build` parameter. Note that reusing a Docker build will execute the code in the Docker image, not your local code. To ensure local changes are included, disconnect your code from the build by registering a code repository or using the artifact store to upload your code.

Using the artifact store is the default behavior if no code repository is detected and the `allow_download_from_artifact_store` flag is not set to `False` in your `DockerSettings`. Connecting a git repository speeds up Docker builds by allowing ZenML to build images without your source files and download them inside the container, facilitating faster iterations and reuse of images built by colleagues. ZenML automatically identifies and reuses the appropriate build ID when a clean repository state and connected git repository are present.

To fully utilize a registered code repository, ensure the relevant integrations are installed for your ZenML setup. For example, if a team member has registered a GitHub repository, you must install the GitHub integration to use it effectively.

```sh
zenml integration install github
```

### Detecting Local Code Repository Checkouts
ZenML checks if the files used in a pipeline are tracked in registered code repositories by:
1. Computing the [source root](./which-files-are-built-into-the-image.md).
2. Verifying if this source root is part of a local checkout of any registered repository.

### Tracking Code Versions for Pipeline Runs
If a local code repository checkout is detected during a pipeline run, ZenML stores a reference to the current commit. This reference is only recorded if the local checkout is clean (no untracked or uncommitted files), ensuring the pipeline runs with the exact code from the specified commit.

### Tips and Best Practices
- File downloads require a clean local checkout and that the latest commit is pushed to the remote repository; otherwise, downloads within the Docker container will fail.
- For options to disable or enforce file downloading, refer to [this docs page](./docker-settings-on-a-pipeline.md).



================================================================================

# docs/book/how-to/customize-docker-builds/which-files-are-built-into-the-image.md

ZenML determines the root directory of your source files based on the following criteria:

1. If `zenml init` has been executed in the current or a parent directory, that directory is used as the repository root.
2. If not, the parent directory of the executing Python file is considered the source root.

You can manage how files in this root directory are handled using the following attributes in the [DockerSettings](https://sdkdocs.zenml.io/latest/core_code_docs/core-config/#zenml.config.docker_settings.DockerSettings):

- `allow_download_from_code_repository`: If `True`, files in a registered code repository with no local changes will be downloaded from the repository instead of being included in the image.
- `allow_download_from_artifact_store`: If the previous option is `False`, and no suitable code repository exists, setting this to `True` will archive and upload your code to the artifact store.
- `allow_including_files_in_images`: If both previous options are `False`, enabling this will include your files in the Docker image, necessitating a new image build for any code changes.

**Warning**: Setting all attributes to `False` is not recommended, as it may lead to unintended behavior. You will be responsible for ensuring correct file paths in the Docker images used for pipeline execution.

### File Management

- **Excluding Files**: Use a `.gitignore` file to exclude files when downloading from a code repository.
- **Including Files**: To exclude files when including them in the image, use a `.dockerignore` file, either by placing it in the source root or by specifying a different `.dockerignore` file.

```python
    docker_settings = DockerSettings(build_config={"dockerignore": "/path/to/.dockerignore"})

    @pipeline(settings={"docker": docker_settings})
    def my_pipeline(...):
        ...
    ```

The documentation includes an image of the ZenML Scarf with the following attributes: it has an alternative text "ZenML Scarf" and utilizes a specific referrer policy ("no-referrer-when-downgrade"). The image source is a URL linking to a static image hosted on Scarf.



================================================================================

# docs/book/how-to/customize-docker-builds/use-a-prebuilt-image.md

### Skip Building a Docker Image for ZenML Pipeline Execution

ZenML typically builds a Docker image with a base ZenML image and project dependencies when running a pipeline on a remote Stack. If no code repository is registered and `allow_download_from_artifact_store` is not set to `True`, the pipeline code is also added to the image. This process can be time-consuming due to the need to pull base layers and push the final image to a container registry, which may slow down pipeline execution.

To optimize time and costs, you can use a prebuilt image instead of building a new one for each pipeline run. However, note that this means updates to your code or dependencies will not be reflected unless included in the prebuilt image.

#### How to Use This Feature

Utilize the `DockerSettings` class in ZenML to specify a parent image for your pipeline runs. Set the `parent_image` attribute to your desired image and `skip_build` to `True` to bypass the image-building process.

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

{% hint style="warning" %} Ensure the image is pushed to a registry accessible by the orchestrator or other components without ZenML's involvement. {% endhint %} 

## Parent Image Requirements
When using a pre-built image with ZenML, the image specified in the `parent_image` attribute of the `DockerSettings` class must include all necessary dependencies for your pipeline. If you do not have a registered code repository and `allow_download_from_artifact_store` is set to `False`, the image should also contain any required code files.

{% hint style="info" %} If you specify a parent image without skipping the build, ZenML will build on top of it rather than the base ZenML image. {% endhint %} 

{% hint style="info" %} If using an image built by ZenML in a previous run for the same stack, it can be used directly without concerns about its contents. {% endhint %} 

### Stack Requirements
A ZenML Stack consists of various components, each with specific requirements. Ensure your image meets these requirements. You can obtain a list of stack requirements to guide your image creation.

```python
from zenml.client import Client

stack_name = <YOUR_STACK>
# set your stack as active if it isn't already
Client().set_active_stack(stack_name)

# get the requirements for the active stack
active_stack = Client().active_stack
stack_requirements = active_stack.requirements()
```

### Integration Requirements

For all integrations in your pipeline, ensure that their dependencies are also installed. You can obtain a list of these dependencies as follows:

```python
from zenml.integrations.registry import integration_registry
from zenml.integrations.constants import HUGGINGFACE, PYTORCH

# define a list of all required integrations
required_integrations = [PYTORCH, HUGGINGFACE]

# Generate requirements for all required integrations
integration_requirements = set(
    itertools.chain.from_iterable(
        integration_registry.select_integration_requirements(
            integration_name=integration,
            target_os=OperatingSystemType.LINUX,
        )
        for integration in required_integrations
    )
)
```

### Project-Specific Requirements

To install project dependencies, include a line in your `Dockerfile` that references a file containing all requirements.

```Dockerfile
RUN pip install <ANY_ARGS> -r FILE
```

### Any System Packages
Include any necessary `apt` packages for your application in the `Dockerfile`.

```Dockerfile
RUN apt-get update && apt-get install -y --no-install-recommends YOUR_APT_PACKAGES
```

### Your Project Code Files

Ensure your pipeline and step code files are accessible in your execution environment:

- If you have a registered [code repository](../../user-guide/production-guide/connect-code-repository.md), ZenML will automatically download your code files to the image.
- If you lack a code repository and `allow_download_from_artifact_store` is set to `True` (default), ZenML will upload your code to the artifact store for the image.
- If both options are disabled, you must manually include your code files in the image, which is not recommended. Refer to the [which files are built into the image](./which-files-are-built-into-the-image.md) page for guidance on what to include.

Ensure your code is located in the `/app` directory, which should be set as the active working directory. Additionally, Python, `pip`, and `zenml` must be installed in your image.



================================================================================

# docs/book/how-to/customize-docker-builds/docker-settings-on-a-pipeline.md

### Summary: Using Docker Images to Run Your Pipeline

When running a pipeline with a remote orchestrator, a Dockerfile is dynamically generated at runtime to build the Docker image using the image builder component. The Dockerfile includes the following steps:

1. **Base Image**: Starts from a parent image with ZenML installed, defaulting to the official ZenML image for the active Python environment. For custom base images, refer to the guide on using a custom parent image.

2. **Install Dependencies**: Automatically detects and installs required pip dependencies based on the integrations used in your stack. For additional requirements, consult the guide on including custom dependencies.

3. **Copy Source Files**: Source files must be available in the Docker container for ZenML to execute step code. More information on customizing source file handling can be found in the relevant section.

4. **Environment Variables**: Sets user-defined environment variables.

ZenML automates this process for basic use cases, but customization options are available. For a comprehensive list of configuration options, refer to the DockerSettings object in the SDKDocs.

### Configuring Pipeline Settings

To customize Docker builds for your pipelines and steps, use the DockerSettings class, which can be imported as needed.

```python
from zenml.config import DockerSettings
```

Settings can be supplied in various ways. Configuring them on a pipeline applies the settings universally to all steps within that pipeline.

```python
from zenml.config import DockerSettings
docker_settings = DockerSettings()

# Either add it to the decorator
@pipeline(settings={"docker": docker_settings})
def my_pipeline() -> None:
    my_step()

# Or configure the pipelines options
my_pipeline = my_pipeline.with_options(
    settings={"docker": docker_settings}
)
```

Configuring Docker images at each step provides fine-grained control and allows for the creation of specialized images tailored to different pipeline steps.

```python
docker_settings = DockerSettings()

# Either add it to the decorator
@step(settings={"docker": docker_settings})
def my_step() -> None:
    pass

# Or configure the step options
my_step = my_step.with_options(
    settings={"docker": docker_settings}
)
```

To use a YAML configuration file, refer to the guidelines provided in the linked documentation.

```yaml
settings:
    docker:
        ...

steps:
  step_name:
    settings:
        docker:
            ...
```

For details on the hierarchy and precedence of configuration settings, refer to [this page](../pipeline-development/use-configuration-files/configuration-hierarchy.md). 

### Specifying Docker Build Options
To specify build options for the default local image builder, these options are passed to the build method of the [image builder](../pipeline-development/configure-python-environments/README.md#image-builder-environment) and subsequently to the [`docker build` command](https://docker-py.readthedocs.io/en/stable/images.html#docker.models.images.ImageCollection.build).

```python
docker_settings = DockerSettings(build_config={"build_options": {...}})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

For MacOS users with ARM architecture, local Docker caching is ineffective unless the target platform of the image is explicitly specified.

```python
docker_settings = DockerSettings(build_config={"build_options": {"platform": "linux/amd64"}})

@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

### Using a Custom Parent Image

ZenML uses the official ZenML image by default for executing pipelines. To gain more control over the environment, you can specify a custom pre-built parent image or provide a Dockerfile for ZenML to build one. 

**Requirements:** The custom image must have Python, pip, and ZenML installed. For a reference, you can view ZenML's Dockerfile [here](https://github.com/zenml-io/zenml/blob/main/docker/base.Dockerfile).

#### Using a Pre-Built Parent Image

To utilize a static parent image with pre-installed dependencies, specify it in the Docker settings for your pipeline.

```python
docker_settings = DockerSettings(parent_image="my_registry.io/image_name:tag")


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

To run your steps using this image without additional code or installations, bypass Docker builds by adjusting the Docker settings accordingly.

```python
docker_settings = DockerSettings(
    parent_image="my_registry.io/image_name:tag",
    skip_build=True
)


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

{% hint style="warning" %} This advanced feature may lead to unintended behavior in your pipelines. Ensure your code files are included in the specified image. Read more about this feature [here](./use-a-prebuilt-image.md) before proceeding. {% endhint %}



================================================================================

# docs/book/how-to/customize-docker-builds/use-your-own-docker-files.md

# Using Custom Docker Files in ZenML

ZenML allows you to specify a custom Dockerfile, build context directory, and build options for dynamic parent image creation during pipeline execution. 

### Build Process:
- **No Dockerfile Specified**: If requirements, environment variables, or file copying necessitate an image build, ZenML will create one. If not, the existing `parent_image` is used.
- **Dockerfile Specified**: ZenML builds an image from the specified Dockerfile. If further requirements necessitate an additional image, it will be built; otherwise, the initial image is used for the pipeline.

### Installation Order for Requirements:
1. Packages from the local Python environment.
2. Packages from the `requirements` attribute.
3. Packages from `required_integrations` and stack requirements.

*Note: The intermediate image may also be used directly for executing pipeline steps, depending on Docker settings.*

```python
docker_settings = DockerSettings(
    dockerfile="/path/to/dockerfile",
    build_context_root="/path/to/build/context",
    parent_image_build_config={
        "build_options": ...
        "dockerignore": ...
    }
)


@pipeline(settings={"docker": docker_settings})
def my_pipeline(...):
    ...
```

The documentation includes an image of the "ZenML Scarf" with a specified alt text and a referrer policy of "no-referrer-when-downgrade." The image source URL is provided for reference.



================================================================================

# docs/book/how-to/customize-docker-builds/define-where-an-image-is-built.md

### Image Builder Definition

ZenML executes pipeline steps sequentially in the active Python environment locally. For remote orchestrators or step operators, it builds Docker images to run pipelines in an isolated environment. By default, execution environments are created locally using the local Docker client, which requires Docker installation and permissions.

ZenML provides image builders, a specialized stack component, to build and push Docker images in a different image builder environment. If no image builder is configured, ZenML defaults to the local image builder, ensuring consistency across builds. The image builder environment aligns with the client environment.

Users do not need to interact directly with image builders in their code. The active ZenML stack automatically uses the configured image builder for any component that requires container image building.



================================================================================

# docs/book/how-to/manage-zenml-server/README.md

# Manage your ZenML Server

This section provides best practices for upgrading your ZenML server, tips for using it in production, and troubleshooting guidance. It includes recommended upgrade steps and migration guides for transitioning between specific versions.



================================================================================

# docs/book/how-to/manage-zenml-server/upgrade-zenml-server.md

### Upgrade ZenML Server

Upgrading your ZenML server varies based on your deployment method. Follow these best practices before upgrading: consult the [best practices for upgrading ZenML](./best-practices-upgrading-zenml.md) guide. It's recommended to upgrade promptly after a new version release to benefit from improvements and fixes.

#### Docker Upgrade Instructions
1. **Delete the existing ZenML container.**
2. **Run the new version of the `zenml-server` image.**

**Important:** Ensure your data is persisted (on persistent storage or an external MySQL instance) before proceeding. Consider performing a backup prior to the upgrade.

```bash
  # find your container ID
  docker ps
  ```

```bash
  # stop the container
  docker stop <CONTAINER_ID>

  # remove the container
  docker rm <CONTAINER_ID>
  ```

To deploy a specific version of the `zenml-server` image, select the desired version from the available options [here](https://hub.docker.com/r/zenmldocker/zenml-server/tags).

```bash
  docker run -it -d -p 8080:8080 --name <CONTAINER_NAME> zenmldocker/zenml-server:<VERSION>
  ```

To upgrade your ZenML server Helm release, follow these steps:

1. Pull the latest version of the Helm chart from the ZenML GitHub repository or select a specific version.

```bash
# If you haven't cloned the ZenML repository yet
git clone https://github.com/zenml-io/zenml.git
# Optional: checkout an explicit release tag
# git checkout 0.21.1
git pull
# Switch to the directory that hosts the helm chart
cd src/zenml/zen_server/deploy/helm/
```

To reuse the `custom-values.yaml` file from a previous installation or upgrade, simply use that file. If it's unavailable, extract the values from the ZenML Helm deployment with the provided command.

```bash
  helm -n <namespace> get values zenml-server > custom-values.yaml
  ```

To upgrade the release, use your modified values file while ensuring you are in the directory containing the Helm chart.

```bash
  helm -n <namespace> upgrade zenml-server . -f custom-values.yaml
  ```

- **Container Image Tag**: Avoid changing the container image tag in the Helm chart to custom values, as each version is tested with the default tag. If necessary, you can modify the `zenml.image.tag` in your `custom-values.yaml` to a specific ZenML version (e.g., `0.32.0`).

- **Downgrading**: Downgrading the server to an older version is unsupported and may cause unexpected behavior.

- **Python Client Version**: Ensure the Python client version matches the server version for compatibility.



================================================================================

# docs/book/how-to/manage-zenml-server/using-zenml-server-in-prod.md

### Best Practices for Using ZenML Server in Production

Setting up a ZenML server for testing is straightforward, but transitioning to production requires adherence to best practices. This guide provides essential tips for configuring a production-ready ZenML server.

**Note:** Users of ZenML Pro do not need to worry about these practices, as they are managed automatically. Sign up for a free trial [here](https://cloud.zenml.io).

#### Autoscaling Replicas
In production, larger and longer-running pipelines can strain server resources. Implementing autoscaling for your ZenML server is advisable to prevent interruptions and maintain Dashboard performance during high traffic.

**Deployment Options for Autoscaling:**

- **Kubernetes with Helm:** Use the official [ZenML Helm chart](https://artifacthub.io/packages/helm/zenml/zenml) and enable autoscaling by setting the `autoscaling.enabled` flag.

```yaml
autoscaling:
  enabled: true
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
```

This documentation outlines how to create a horizontal pod autoscaler for the ZenML server, allowing scaling of replicas between 1 and 10 based on CPU utilization.

**ECS (AWS)**: 
- ECS is a container orchestration service for running ZenML server.
- Steps to enable autoscaling:
  1. Access the ECS console and select your ZenML server service.
  2. Click "Update Service."
  3. In the "Service auto scaling - optional" section, enable autoscaling.
  4. Set the minimum and maximum number of tasks and the scaling metric.

**Cloud Run (GCP)**:
- Cloud Run automatically scales instances based on incoming requests or CPU utilization.
- For production, set a minimum of 1 instance to maintain "warm" instances.
- Steps to configure autoscaling:
  1. Go to the Cloud Run console and select your ZenML server service.
  2. Click "Edit & Deploy new Revision."
  3. In the "Revision auto-scaling" section, set the minimum and maximum instances.

**Docker Compose**:
- Docker Compose does not support autoscaling natively, but you can scale your service using the `scale` flag to specify the number of replicas.

```bash
docker compose up --scale zenml-server=N
```

To scale your ZenML server, you can increase the number of replicas to N. Additionally, to enhance performance, consider increasing the thread pool size by adjusting the `zenml.threadPoolSize` in the ZenML Helm chart values, assuming your hardware supports it.

```yaml
zenml:
  threadPoolSize: 100
```

By default, the `ZENML_SERVER_THREAD_POOL_SIZE` is set to 40. If using a different deployment option, adjust this environment variable accordingly. Additionally, modify `zenml.database.poolSize` and `zenml.database.maxOverflow` to prevent ZenML server workers from blocking on database connections; their sum should be at least equal to the thread pool size. If managing your own database, ensure these values are correctly set.

### Scaling the Backing Database
When scaling ZenML server instances, also scale the backing database to avoid bottlenecks. Start with a single database instance and monitor its performance. Key metrics to monitor include:
- **CPU Utilization**: Consistent usage above 50% may indicate the need for scaling.
- **Freeable Memory**: If it drops below 100-200 MB, consider scaling.

### Setting Up Ingress/Load Balancer
For secure and reliable exposure of your ZenML server in production, set up an ingress/load balancer. If using the official ZenML Helm chart, enable ingress by setting the `zenml.ingress.enabled` flag.

```yaml
zenml:
  ingress:
    enabled: true
    className: "nginx"
    annotations:
      # nginx.ingress.kubernetes.io/ssl-redirect: "true"
      # nginx.ingress.kubernetes.io/rewrite-target: /$1
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      # cert-manager.io/cluster-issuer: "letsencrypt"
```

This documentation outlines how to set up load balancing and monitoring for your ZenML service across various platforms.

### Load Balancing Options:
1. **NGINX Ingress**: Creates a LoadBalancer for your ZenML service on any cloud provider.
2. **ECS**: Use Application Load Balancers to route traffic to your ZenML server tasks. Refer to the [AWS documentation](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-load-balancing.html) for setup instructions.
3. **Cloud Run**: Utilize Cloud Load Balancing to route traffic. Follow the [GCP documentation](https://cloud.google.com/load-balancing/docs/https/setting-up-https-serverless) for guidance.
4. **Docker Compose**: Set up an NGINX server as a reverse proxy for your ZenML server. See this [blog](https://www.docker.com/blog/how-to-use-the-official-nginx-docker-image/) for details.

### Monitoring:
Monitoring is essential for maintaining service performance and early issue detection. The tools vary based on your deployment method:
- **Kubernetes with Helm**: Deploy Prometheus and Grafana using the `kube-prometheus-stack` [Helm chart](https://artifacthub.io/packages/helm/prometheus-community/kube-prometheus-stack). After deployment, access Grafana by port-forwarding or through an ingress. Use specific queries to monitor your ZenML server.

```
sum by(namespace) (rate(container_cpu_usage_seconds_total{namespace=~"zenml.*"}[5m]))
```

This documentation outlines monitoring and backup strategies for ZenML servers across different platforms.

### Monitoring CPU Utilization
- **Kubernetes**: Use a query to monitor CPU utilization of server pods in namespaces starting with `zenml`. 
- **ECS**: Utilize the [CloudWatch integration](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/cloudwatch-metrics.html) to view metrics like CPU and Memory utilization in the ECS console.
- **Cloud Run**: Use the [Cloud Monitoring integration](https://cloud.google.com/run/docs/monitoring) to access metrics such as Container CPU and memory utilization in the Cloud Run console.

### Backups
To protect critical data (pipeline runs, stack configurations), implement a backup strategy:
- Set up automated backups with a retention period (e.g., 30 days).
- Periodically export data to external storage (e.g., S3, GCS).
- Perform manual backups before server upgrades.



================================================================================

# docs/book/how-to/manage-zenml-server/troubleshoot-your-deployed-server.md

# Troubleshooting Tips for ZenML Deployment

This document outlines common issues encountered during ZenML deployment and their solutions.

## Viewing Logs

Analyzing logs is essential for debugging. The method for viewing logs depends on whether you are using Kubernetes or Docker.

### Kubernetes

To view logs of the ZenML server in a Kubernetes deployment, check all pods running your ZenML deployment.

```bash
kubectl -n <KUBERNETES_NAMESPACE> get pods
```

To retrieve logs for all pods when they aren't running, use the following command.

```bash
kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml
```

The error may originate from either the `zenml-db-init` container, which connects to the MySQL database, or the `zenml` container, which runs the server code. If the `get pods` command indicates the pod is in the `Init` state, use `zenml-db-init` as the container name; otherwise, use `zenml`.

```bash
kubectl -n <KUBERNETES_NAMESPACE> logs -l app.kubernetes.io/name=zenml -c <CONTAINER_NAME>
```

To view the logs of the ZenML server in Docker, use the command associated with your deployment method. If you deployed using `zenml login --local --docker`, you can check the logs accordingly. Additionally, the `--tail` flag can limit the number of displayed lines, and the `--follow` flag allows real-time log monitoring.

```shell
  zenml logs -f
  ```

To check the logs of a manually deployed Docker ZenML server using the `docker run` command, use the following command:

```shell
  docker logs zenml -f
  ```

To check the logs of a manually deployed Docker ZenML server using the `docker compose` command, use the following command:

```shell
  docker compose -p zenml logs -f
  ```

## Fixing Database Connection Problems

When using a MySQL database, connection issues may arise. Check the logs from the `zenml-db-init` container for insights. Common issues include:

- **Access Denied Error**: `ERROR 1045 (28000): Access denied for user <USER> using password YES` indicates incorrect username or password. Verify that these credentials are correctly set for your deployment method.
  
- **Connection Error**: `ERROR 2003 (HY000): Can't connect to MySQL server on <HOST> (<IP>)` suggests an incorrect host. Ensure the host is correctly configured for your deployment method.

You can test the connection and credentials using a specific command from your machine.

```bash
mysql -h <HOST> -u <USER> -p
```

If using Kubernetes, utilize the `kubectl port-forward` command to connect the MySQL port to your local machine. 

## Fixing Database Initialization Problems
If you encounter `Revision not found` errors in your `zenml-db-init` logs after migrating to an older ZenML version, drop the existing database and create a new one with the same name. Log in to your MySQL instance to proceed.

```bash
  mysql -h <HOST> -u <NAME> -p
  ```

To drop the database for the server, execute the appropriate command in your database management system. Ensure that you have the necessary permissions and that you have backed up any important data, as this action is irreversible and will permanently delete all database contents.

```sql
  drop database <NAME>;
  ```

Create a database using the same name as the existing one.

```sql
  create database <NAME>;
  ```

To reinitialize the database, restart the Kubernetes pods or the Docker container running your server.



================================================================================

# docs/book/how-to/manage-zenml-server/best-practices-upgrading-zenml.md

### Best Practices for Upgrading ZenML

Upgrading ZenML is generally smooth, but following best practices can help ensure success. 

#### Upgrading Your Server

1. **Data Backups**:
   - **Database Backup**: Create a backup of your MySQL database before upgrading for rollback purposes.
   - **Automated Backups**: Set up daily automated backups using managed services like AWS RDS, Google Cloud SQL, or Azure Database for MySQL.

2. **Upgrade Strategies**:
   - **Staged Upgrade**: Use two ZenML server instances (old and new) to migrate services gradually.
   - **Team Coordination**: Coordinate upgrade timing among multiple teams to minimize disruption.
   - **Separate ZenML Servers**: For teams needing different upgrade schedules, use dedicated ZenML server instances. ZenML Pro supports multi-tenancy for this purpose.

3. **Minimizing Downtime**:
   - **Upgrade Timing**: Schedule upgrades during low-activity periods.
   - **Avoid Mid-Pipeline Upgrades**: Be cautious of upgrades that may interrupt long-running pipelines.

#### Upgrading Your Code

1. **Testing and Compatibility**:
   - **Local Testing**: Test locally after upgrading (`pip install zenml --upgrade`) and run old pipelines to check compatibility.
   - **End-to-End Testing**: Develop simple end-to-end tests to ensure the new version works with your pipeline code. Refer to ZenML's [extensive test suite](https://github.com/zenml-io/zenml/tree/main/tests) for examples.
   - **Artifact Compatibility**: Be cautious with pickle-based materializers, as they may be sensitive to changes in Python versions or libraries. Consider using version-agnostic methods for critical artifacts and test loading older artifacts with the new version using their IDs.

```python
from zenml.client import Client

artifact = Client().get_artifact_version('YOUR_ARTIFACT_ID')
loaded_artifact = artifact.load()
```

### Dependency Management

- **Python Version**: Ensure compatibility between your Python version and the ZenML version you are upgrading to. Refer to the [installation guide](../../getting-started/installation.md) for supported Python versions.
  
- **External Dependencies**: Check for potential incompatibilities with external dependencies from integrations, especially if older versions are no longer supported. Relevant details can be found in the [release notes](https://github.com/zenml-io/zenml/releases).

### Handling API Changes

- **Changelog Review**: Review the [changelog](https://github.com/zenml-io/zenml/releases) for new syntax, instructions, or breaking changes, as ZenML aims for backward compatibility but may introduce breaking changes (e.g., [Pydantic 2 upgrade](https://github.com/zenml-io/zenml/releases/tag/0.60.0)).
  
- **Migration Scripts**: Utilize available [migration scripts](migration-guide/migration-guide.md) for database schema changes.

By following these guidelines, you can minimize risks and ensure a smoother upgrade process for your ZenML server, adapting them to your specific environment as needed.



================================================================================

# docs/book/how-to/manage-zenml-server/connecting-to-zenml/connect-in-with-your-user-interactive.md

# Connect with Your User (Interactive)

Authenticate clients with the ZenML Server using the ZenML CLI or web-based login. Execute the authentication with the following command:

```bash
zenml login https://...
```

This command initiates a validation process for your connecting device in the browser. You can choose to mark the device as trusted or not. If you select "Trust this device," a 30-day authentication token will be issued; otherwise, a 24-hour token will be provided. To view all permitted devices, use the following command:

```bash
zenml authorized-device list
```

The command provided enables detailed inspection of a specific device.

```bash
zenml authorized-device describe <DEVICE_ID>  
```

To enhance security, use the `zenml device lock` command with the device ID to invalidate a token, adding an extra layer of control over your devices.

```
zenml authorized-device lock <DEVICE_ID>  
```

### Summary of ZenML Device Management Steps

1. Use `zenml login <URL>` to initiate a device flow and connect to a ZenML server.
2. Decide whether to trust the device when prompted.
3. List permitted devices with `zenml devices list`.
4. Invalidate a token using `zenml device lock ...`.

### Important Notice
Using the ZenML CLI ensures secure interaction with ZenML tenants. Always use trusted devices to maintain security and privacy. Regularly manage device trust levels, and lock any device if trust needs to be revoked, as each token can access sensitive data and infrastructure.



================================================================================

# docs/book/how-to/manage-zenml-server/connecting-to-zenml/README.md

# Connect to a Server

Once ZenML is deployed, there are multiple methods to connect to it. For detailed deployment instructions, refer to the [production guide](../../../user-guide/production-guide/deploying-zenml.md).



================================================================================

# docs/book/how-to/manage-zenml-server/connecting-to-zenml/connect-with-a-service-account.md

# Connect with a Service Account

To authenticate to a ZenML server in non-interactive environments (e.g., CI/CD workloads, serverless functions), configure a service account and use an API key for authentication.

```bash
zenml service-account create <SERVICE_ACCOUNT_NAME>
```

This command creates a service account and an API key, which is displayed in the command output and cannot be retrieved later. The API key can be used to connect your ZenML client to the server via the CLI.

```bash
# This command will prompt you to enter the API key
zenml login https://... --api-key
```

To set up your ZenML client, configure the `ZENML_STORE_URL` and `ZENML_STORE_API_KEY` environment variables. This is especially beneficial for automated CI/CD environments such as GitHub Actions, GitLab CI, or when using containerized setups like Docker or Kubernetes.

```bash
export ZENML_STORE_URL=https://...
export ZENML_STORE_API_KEY=<API_KEY>
```

You can start interacting with your server immediately without running `zenml login` after setting the required environment variables. To view all created service accounts and their API keys, use the specified commands.

```bash
zenml service-account list
zenml service-account api-key <SERVICE_ACCOUNT_NAME> list
```

You can use the following command to inspect a specific service account and its associated API key.

```bash
zenml service-account describe <SERVICE_ACCOUNT_NAME>
zenml service-account api-key <SERVICE_ACCOUNT_NAME> describe <API_KEY_NAME>
```

API keys do not expire, but for enhanced security, it's recommended to regularly rotate them to prevent unauthorized access to your ZenML server. This can be done using the ZenML CLI.

```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME>
```

Running the command creates a new API key and invalidates the old one, with the new key displayed in the output and not retrievable later. Use the new API key to connect your ZenML client to the server. You can configure a retention period for the old API key using the `--retain` flag, which is useful for ensuring workloads transition to the new key. For example, to rotate an API key and retain the old one for 60 minutes, run the specified command.

```bash
zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate <API_KEY_NAME> \
      --retain 60
```

To enhance security, deactivate a service account or API key using the appropriate command.

```
zenml service-account update <SERVICE_ACCOUNT_NAME> --active false
zenml service-account api-key <SERVICE_ACCOUNT_NAME> update <API_KEY_NAME> \
      --active false
```

Deactivating a service account or API key immediately prevents authentication for all associated workloads. Key steps include:

1. Create a service account and API key: `zenml service-account create`
2. Connect ZenML client to the server: `zenml login <url> --api-key`
3. List configured service accounts: `zenml service-account list`
4. List API keys for a service account: `zenml service-account api-key <SERVICE_ACCOUNT_NAME> list`
5. Rotate API keys regularly: `zenml service-account api-key <SERVICE_ACCOUNT_NAME> rotate`
6. Deactivate service accounts or API keys: `zenml service-account update` or `zenml service-account api-key <SERVICE_ACCOUNT_NAME> update`

**Important:** Regularly rotate API keys and deactivate/delete unused service accounts and API keys to protect data and infrastructure.



================================================================================

# docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-sixty.md

### ZenML Migration Guide: Upgrading from 0.58.2 to 0.60.0 (Pydantic 2 Edition)

**Overview**: ZenML now utilizes Pydantic v2, introducing critical updates that may lead to unexpected behavior due to stricter validation. Users may encounter new validation errors; please report any issues on [GitHub](https://github.com/zenml-io/zenml) or [Slack](https://zenml.io/slack-invite).

#### Key Dependency Changes:
- **SQLModel**: Upgraded from `0.0.8` to `0.0.18` for compatibility with Pydantic v2, necessitating an upgrade of SQLAlchemy from v1 to v2. Refer to [SQLAlchemy migration guide](https://docs.sqlalchemy.org/en/20/changelog/migration_20.html) for details.
  
#### Pydantic v2 Features:
- Enhanced performance due to Rust-based core logic.
- New features in model design, configuration, validation, and serialization. For more information, see the [Pydantic migration guide](https://docs.pydantic.dev/2.7/migration/).

#### Integration Changes:
- **Airflow**: Removed dependencies due to Airflow's continued use of SQLAlchemy v1. Users must run Airflow in a separate environment. Updated documentation is available [here](../../../component-guide/orchestrators/airflow.md).
  
- **AWS**: Upgraded SageMaker to version `2.172.0` to support `protobuf` 4, resolving compatibility issues.
  
- **Evidently**: Updated integration to versions `0.4.16` to `0.4.22` for Pydantic v2 compatibility.
  
- **Feast**: Removed an extra Redis dependency for compatibility with Pydantic v2.
  
- **GCP & Kubeflow**: Upgraded `kfp` dependency to v2, eliminating Pydantic v1 requirements. Functional changes may occur; refer to the [kfp migration guide](https://www.kubeflow.org/docs/components/pipelines/v2/migration/).
  
- **Great Expectations**: Updated dependency to `great-expectations>=0.17.15,<1.0` for Pydantic v2 support.
  
- **MLflow**: Compatible with both Pydantic v1 and v2, but may downgrade Pydantic to v1 due to known issues. Users may encounter deprecation warnings.
  
- **Label Studio**: Updated to support Pydantic v2 in its 1.0 release.
  
- **Skypilot**: Integration remains mostly unchanged, but `skypilot[azure]` is deactivated due to incompatibility with `azurecli`. Users should remain on the previous ZenML version until resolved.
  
- **TensorFlow**: Requires `tensorflow>=2.12.0` due to dependency changes. Issues may arise with TensorFlow 2.12.0 on Python 3.8; consider using a higher Python version.
  
- **Tekton**: Updated to use `kfp` v2, aligning with Pydantic v2 compatibility.

#### Important Note:
Upgrading to ZenML 0.60.0 may lead to dependency issues, particularly with integrations not supporting Pydantic v2. It is recommended to set up a fresh Python environment for the upgrade.



================================================================================

# docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-thirty.md

### Migration Guide: ZenML 0.20.0-0.23.0 to 0.30.0-0.39.1

**Warning:** Migrating to `0.30.0` involves non-reversible database changes, making downgrading to `<=0.23.0` impossible. If using an older version, follow the [0.20.0 Migration Guide](migration-zero-twenty.md) first to avoid database migration issues.

**Key Changes:**
- ZenML 0.30.0 removes the `ml-pipelines-sdk` dependency.
- Pipeline runs and artifacts are now stored natively in the ZenML database.
- Database migration occurs automatically upon executing any `zenml ...` CLI command after installation of the new version.

```bash
pip install zenml==0.30.0
zenml version  # 0.30.0
```

The provided documentation text includes an image related to ZenML Scarf but does not contain any specific technical information or key points to summarize. Please provide additional text or details for summarization.



================================================================================

# docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-twenty.md

### Migration Guide: ZenML 0.13.2 to 0.20.0

**Last Updated: 2023-07-24**

ZenML 0.20.0 introduces significant architectural changes that are not backward compatible. This guide provides instructions for migrating existing ZenML stacks and pipelines with minimal disruption.

**Important Notes:**
- Migration to ZenML 0.20.0 requires updating your ZenML stacks and potentially modifying your pipeline code. Follow the instructions carefully for a smooth transition.
- If issues arise post-update, revert to version 0.13.2 using `pip install zenml==0.13.2`.

**Key Changes:**
1. **Metadata Store:** ZenML now manages its own Metadata Store, eliminating the need for separate remote Metadata Stores. Users must transition to a ZenML server deployment if using remote stores.
2. **ZenML Dashboard:** A new dashboard is available for all deployments.
3. **Profiles Removal:** ZenML Profiles have been replaced by ZenML Projects. Existing profiles must be manually migrated.
4. **Decoupled Configuration:** Stack Component configuration is now separate from implementation, requiring updates for custom components.
5. **Collaborative Features:** The updated ZenML server allows sharing of stacks and components among users.

**Metadata Store Transition:**
- ZenML now operates as a server accessible via REST API and includes a visual dashboard. Commands for managing the server include:
  - `zenml connect`, `disconnect`, `down`, `up`, `logs`, `status` for server management.
  - `zenml pipeline list`, `runs`, `delete` for pipeline management.

**Migration Steps:**
- If using the default `sqlite` Metadata Store, no action is needed; ZenML will switch to its local database automatically.
- For `kubeflow` Metadata Store (local), no action is needed; it will also switch automatically.
- For remote `kubeflow` or `mysql` Metadata Stores, deploy a ZenML Server close to the service.
- If using a `kubernetes` Metadata Store, deploy a ZenML Server in the same Kubernetes cluster and manage the database service yourself.

**Performance Considerations:**
- Local ZenML Servers cannot track remote pipelines unless configured for cloud access. Remote servers tracking local pipelines may experience latency issues.

**Migrating Pipeline Runs:**
- Use the `zenml pipeline runs migrate` command (available in versions 0.21.0, 0.21.1, 0.22.0) to transfer existing run data.
- Backup metadata stores before upgrading ZenML.
- Choose a deployment model and connect your client to the ZenML server.
- Execute the migration command, specifying the path to the old metadata store for SQLite.

This guide ensures that users can effectively transition to ZenML 0.20.0 while maintaining their existing workflows.

```bash
zenml pipeline runs migrate PATH/TO/LOCAL/STORE/metadata.db
```

To migrate another store, set `--database_type=mysql` and provide the MySQL host, username, password, and database.

```bash
zenml pipeline runs migrate DATABASE_NAME \
  --database_type=mysql \
  --mysql_host=URL/TO/MYSQL \
  --mysql_username=MYSQL_USERNAME \
  --mysql_password=MYSQL_PASSWORD
```

### 💾 The New Way (CLI Command Cheat Sheet)

- **Deploy the server:** `zenml deploy --aws` (use with caution; it provisions AWS infrastructure)
- **Spin up a local ZenML Server:** `zenml up`
- **Connect to a pre-existing server:** `zenml connect` (provide URL or use `--config` with a YAML file)
- **List deployed server details:** `zenml status`

### ZenML Dashboard
The ZenML Dashboard is included in the ZenML Python package and can be launched directly from Python. Source code is available in the [ZenML Dashboard repository](https://github.com/zenml-io/zenml-dashboard). To launch locally, run `zenml up` and follow the instructions.

```bash
$ zenml up
Deploying a local ZenML server with name 'local'.
Connecting ZenML to the 'local' local ZenML server (http://127.0.0.1:8237).
Updated the global store configuration.
Connected ZenML to the 'local' local ZenML server (http://127.0.0.1:8237).
The local ZenML dashboard is available at 'http://127.0.0.1:8237'. You can
connect to it using the 'default' username and an empty password.
```

The ZenML Dashboard is accessible at `http://localhost:8237` by default. For alternative deployment options, refer to the [ZenML deployment documentation](../../user-guide/getting-started/deploying-zenml/deploying-zenml.md) or the [starter guide](../../user-guide/starter-guide/pipelines/pipelines.md).

### Removal of Profiles and Local YAML Database
In ZenML 0.20.0, the previous local YAML database and Profiles have been deprecated. All Stacks, Stack Components, Pipelines, and Pipeline Runs are now stored in a single SQL database and organized into Projects instead of Profiles. 

**Warning:** Updating to ZenML 0.20.0 will result in the loss of all configured Stacks and Stack Components. To retain them, you must [manually migrate](migration-zero-twenty.md#-how-to-migrate-your-profiles) after the update.

### Migration Steps
1. Update ZenML to 0.20.0, which invalidates existing Profiles.
2. Choose a ZenML deployment model for your projects. For local or remote server setups, connect your client using `zenml connect`.
3. Use `zenml profile list` and `zenml profile migrate` CLI commands to import Stacks and Stack Components into the new deployment. You can use a naming prefix or different Projects for multiple Profiles.

**Warning:** The ZenML Dashboard currently only displays information from the `default` Project. Migrated Stacks and Stack Components in different Projects will not be visible until a future release.

After migration, you can delete the old YAML files.

```bash
$ zenml profile list
ZenML profiles have been deprecated and removed in this version of ZenML. All
stacks, stack components, flavors etc. are now stored and managed globally,
either in a local database or on a remote ZenML server (see the `zenml up` and
`zenml connect` commands). As an alternative to profiles, you can use projects
as a scoping mechanism for stacks, stack components and other ZenML objects.

The information stored in legacy profiles is not automatically migrated. You can
do so manually by using the `zenml profile list` and `zenml profile migrate` commands.
Found profile with 1 stacks, 3 components and 0 flavors at: /home/stefan/.config/zenml/profiles/default
Found profile with 3 stacks, 6 components and 0 flavors at: /home/stefan/.config/zenml/profiles/zenprojects
Found profile with 3 stacks, 7 components and 0 flavors at: /home/stefan/.config/zenml/profiles/zenbytes

$ zenml profile migrate /home/stefan/.config/zenml/profiles/default
No component flavors to migrate from /home/stefan/.config/zenml/profiles/default/stacks.yaml...
Migrating stack components from /home/stefan/.config/zenml/profiles/default/stacks.yaml...
Created artifact_store 'cloud_artifact_store' with flavor 's3'.
Created container_registry 'cloud_registry' with flavor 'aws'.
Created container_registry 'local_registry' with flavor 'default'.
Created model_deployer 'eks_seldon' with flavor 'seldon'.
Created orchestrator 'cloud_orchestrator' with flavor 'kubeflow'.
Created orchestrator 'kubeflow_orchestrator' with flavor 'kubeflow'.
Created secrets_manager 'aws_secret_manager' with flavor 'aws'.
Migrating stacks from /home/stefan/.config/zenml/profiles/v/stacks.yaml...
Created stack 'cloud_kubeflow_stack'.
Created stack 'local_kubeflow_stack'.

$ zenml stack list
Using the default local database.
Running with active project: 'default' (global)
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┓
┃ ACTIVE │ STACK NAME           │ STACK ID                             │ SHARED │ OWNER   │ CONTAINER_REGISTRY │ ARTIFACT_STORE       │ ORCHESTRATOR          │ MODEL_DEPLOYER │ SECRETS_MANAGER    ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼────────────────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┨
┃        │ local_kubeflow_stack │ 067cc6ee-b4da-410d-b7ed-06da4c983145 │        │ default │ local_registry     │ default              │ kubeflow_orchestrator │                │                    ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼────────────────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┨
┃        │ cloud_kubeflow_stack │ 054f5efb-9e80-48c0-852e-5114b1165d8b │        │ default │ cloud_registry     │ cloud_artifact_store │ cloud_orchestrator    │ eks_seldon     │ aws_secret_manager ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼────────────────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┨
┃   👉   │ default              │ fe913bb5-e631-4d4e-8c1b-936518190ebb │        │ default │                    │ default              │ default               │                │                    ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┛
```

To migrate a profile into the `default` project with a name prefix, follow these steps:

1. Identify the profile to be migrated.
2. Use the migration command with the specified name prefix.
3. Ensure that all dependencies and configurations are updated accordingly.
4. Verify the migration by checking the profile's functionality in the `default` project.

This process ensures that the profile is correctly integrated while maintaining its unique identity through the name prefix.

```bash
$ zenml profile migrate /home/stefan/.config/zenml/profiles/zenbytes --prefix zenbytes_
No component flavors to migrate from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...
Migrating stack components from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...
Created artifact_store 'zenbytes_s3_store' with flavor 's3'.
Created container_registry 'zenbytes_ecr_registry' with flavor 'default'.
Created experiment_tracker 'zenbytes_mlflow_tracker' with flavor 'mlflow'.
Created experiment_tracker 'zenbytes_mlflow_tracker_local' with flavor 'mlflow'.
Created model_deployer 'zenbytes_eks_seldon' with flavor 'seldon'.
Created model_deployer 'zenbytes_mlflow' with flavor 'mlflow'.
Created orchestrator 'zenbytes_eks_orchestrator' with flavor 'kubeflow'.
Created secrets_manager 'zenbytes_aws_secret_manager' with flavor 'aws'.
Migrating stacks from /home/stefan/.config/zenml/profiles/zenbytes/stacks.yaml...
Created stack 'zenbytes_aws_kubeflow_stack'.
Created stack 'zenbytes_local_with_mlflow'.

$ zenml stack list
Using the default local database.
Running with active project: 'default' (global)
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┓
┃ ACTIVE │ STACK NAME           │ STACK ID             │ SHARED │ OWNER   │ ORCHESTRATOR          │ ARTIFACT_STORE    │ CONTAINER_REGISTRY   │ SECRETS_MANAGER       │ MODEL_DEPLOYER      │ EXPERIMENT_TRACKER   ┃
┠────────┼──────────────────────┼──────────────────────┼────────┼─────────┼───────────────────────┼───────────────────┼──────────────────────┼───────────────────────┼─────────────────────┼──────────────────────┨
┃        │ zenbytes_aws_kubeflo │ 9fe90f0b-2a79-47d9-8 │        │ default │ zenbytes_eks_orchestr │ zenbytes_s3_store │ zenbytes_ecr_registr │ zenbytes_aws_secret_m │ zenbytes_eks_seldon │                      ┃
┃        │ w_stack              │ f80-04e45ff02cdb     │        │         │ ator                  │                   │ y                    │ manager                │                     │                      ┃
┠────────┼──────────────────────┼──────────────────────┼────────┼─────────┼───────────────────────┼───────────────────┼──────────────────────┼───────────────────────┼─────────────────────┼──────────────────────┨
┃   👉   │ default              │ 7a587e0c-30fd-402f-a │        │ default │ default               │ default           │                      │                       │                     │                      ┃
┃        │                      │ 3a8-03651fe1458f     │        │         │                       │                   │                      │                       │                     │                      ┃
┠────────┼──────────────────────┼──────────────────────┼────────┼─────────┼───────────────────────┼───────────────────┼──────────────────────┼───────────────────────┼─────────────────────┼──────────────────────┨
┃        │ zenbytes_local_with_ │ c2acd029-8eed-4b6e-a │        │ default │ default               │ default           │                      │                       │ zenbytes_mlflow     │ zenbytes_mlflow_trac ┃
┃        │ mlflow               │ d19-91c419ce91d4     │        │         │                       │                   │                      │                       │                     │ ker                  ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┛
```

To migrate a profile into a new project, follow these steps:

1. **Export Profile**: Use the export feature in the current project to save the profile as a file.
2. **Create New Project**: Set up a new project in the desired environment.
3. **Import Profile**: Utilize the import function in the new project to load the previously exported profile file.
4. **Verify Configuration**: Check the imported settings to ensure they match the original profile.
5. **Test Functionality**: Run tests to confirm that the profile operates correctly within the new project context.

Ensure all dependencies and configurations are compatible with the new project environment.

```bash
$ zenml profile migrate /home/stefan/.config/zenml/profiles/zenprojects --project zenprojects
Unable to find ZenML repository in your current working directory (/home/stefan/aspyre/src/zenml) or any parent directories. If you want to use an existing repository which is in a different location, set the environment variable 'ZENML_REPOSITORY_PATH'. If you want to create a new repository, run zenml init.
Running without an active repository root.
Creating project zenprojects
Creating default stack for user 'default' in project zenprojects...
No component flavors to migrate from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...
Migrating stack components from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...
Created artifact_store 'cloud_artifact_store' with flavor 's3'.
Created container_registry 'cloud_registry' with flavor 'aws'.
Created container_registry 'local_registry' with flavor 'default'.
Created model_deployer 'eks_seldon' with flavor 'seldon'.
Created orchestrator 'cloud_orchestrator' with flavor 'kubeflow'.
Created orchestrator 'kubeflow_orchestrator' with flavor 'kubeflow'.
Created secrets_manager 'aws_secret_manager' with flavor 'aws'.
Migrating stacks from /home/stefan/.config/zenml/profiles/zenprojects/stacks.yaml...
Created stack 'cloud_kubeflow_stack'.
Created stack 'local_kubeflow_stack'.

$ zenml project set zenprojects
Currently the concept of `project` is not supported within the Dashboard. The Project functionality will be completed in the coming weeks. For the time being it is recommended to stay within the `default` 
project.
Using the default local database.
Running with active project: 'default' (global)
Set active project 'zenprojects'.

$ zenml stack list
Using the default local database.
Running with active project: 'zenprojects' (global)
The current global active stack is not part of the active project. Resetting the active stack to default.
You are running with a non-default project 'zenprojects'. Any stacks, components, pipelines and pipeline runs produced in this project will currently not be accessible through the dashboard. However, this will be possible in the near future.
┏━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━┯━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┯━━━━━━━━━━━━━━━━━━━━┓
┃ ACTIVE │ STACK NAME           │ STACK ID                             │ SHARED │ OWNER   │ ARTIFACT_STORE       │ ORCHESTRATOR          │ MODEL_DEPLOYER │ CONTAINER_REGISTRY │ SECRETS_MANAGER    ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┼────────────────────┨
┃   👉   │ default              │ 3ea77330-0c75-49c8-b046-4e971f45903a │        │ default │ default              │ default               │                │                    │                    ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┼────────────────────┨
┃        │ cloud_kubeflow_stack │ b94df4d2-5b65-4201-945a-61436c9c5384 │        │ default │ cloud_artifact_store │ cloud_orchestrator    │ eks_seldon     │ cloud_registry     │ aws_secret_manager ┃
┠────────┼──────────────────────┼──────────────────────────────────────┼────────┼─────────┼──────────────────────┼───────────────────────┼────────────────┼────────────────────┼────────────────────┨
┃        │ local_kubeflow_stack │ 8d9343ac-d405-43bd-ab9c-85637e479efe │        │ default │ default              │ kubeflow_orchestrator │                │ local_registry     │                    ┃
┗━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━┷━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┷━━━━━━━━━━━━━━━━━━━━┛
```

The `zenml profile migrate` CLI command includes flags for overwriting existing components or stacks and ignoring errors. 

### Decoupling Stack Component Configuration
Stack components can now be registered without required integrations. Existing stack component definitions are split into three classes: 
- **Implementation Class**: Defines the logic.
- **Config Class**: Defines attributes and validates inputs.
- **Flavor Class**: Links implementation and config classes.

If using only default stack component flavors, existing stack configurations remain unaffected. Custom implementations must be updated to the new format. See the documentation on writing custom stack component flavors for guidance.

### Shared ZenML Stacks and Components
The 0.20.0 release enhances collaboration by allowing users to share stacks and components via the ZenML server. When connected to the server, entities like Stacks, Stack Components, and Pipelines are scoped to a Project and owned by the user. Users can share objects during creation or afterward. Shared and private stacks can be identified by name, ID, or partial ID in the CLI. 

Local stack components should not be shared on a central ZenML Server, while non-local components require sharing through a deployed ZenML Server. More details are available in the new starter guide.

### Other Changes
- **Repository Renamed to Client**: The `Repository` class is now `Client`. Backwards compatibility is maintained, but future releases will remove `Repository`. Migrate by renaming references in your code.
  
- **BaseStepConfig Renamed to BaseParameters**: The `BaseStepConfig` class is now `BaseParameters`. This change is part of a broader configuration overhaul. Migrate by renaming references in your code.

### Configuration Rework
Pipeline configuration has been restructured. Previously, configurations were scattered across various methods and decorators. The new `BaseSettings` class centralizes runtime configuration for pipeline runs. Configurations can now be defined in decorators and through a `.configure(...)` method, as well as in a YAML file. 

The `enable_xxx` decorators are deprecated. Migrate by removing these decorators and passing configurations directly to steps. 

For a comprehensive overview of configuration changes, refer to the new documentation section on settings.

```python
@step(
    experiment_tracker="mlflow_stack_comp_name",  # name of registered component
    settings={  # settings of registered component
        "experiment_tracker.mlflow": {  # this is `category`.`flavor`, so another example is `step_operator.spark`
            "experiment_name": "name",
            "nested": False
        }
    }
)
```

**Deprecation Notices:**

1. **`pipeline.with_config(...)`**: 
   - **Migration**: Use `pipeline.run(config_path=...)` instead.

2. **`step.with_return_materializer(...)`**: 
   - **Migration**: Remove the `with_return_materializer` method and pass the necessary parameters directly to the step.

```python
@step(
  output_materializers=materializer_or_dict_of_materializers_mapped_to_outputs
)
```

**`DockerConfiguration` has been renamed to `DockerSettings`.** 

**Migration Steps**: 
1. Rename `DockerConfiguration` to `DockerSettings`.
2. Update the decorator to use `docker_settings` instead of `docker_configuration`.

```python
from zenml.config import DockerSettings

@step(settings={"docker": DockerSettings(...)})
def my_step() -> None:
  ...
```

With this change, all stack components (e.g., Orchestrators and Step Operators) that accepted a `docker_parent_image` in Stack Configuration must now use the `DockerSettings` object. For more details, refer to the [user guide](../../user-guide/starter-guide/production-fundamentals/containerization.md). Additionally, **`ResourceConfiguration` is now renamed to `ResourceSettings`**. 

**Migration Steps**: Rename `ResourceConfiguration` to `ResourceSettings` and pass it using the `resource_settings` parameter instead of directly in the decorator.

```python
from zenml.config import ResourceSettings

@step(settings={"resources": ResourceSettings(...)})
def my_step() -> None:
  ...
```

**Deprecation of `requirements` and `required_integrations` Parameters**: Users can no longer pass `requirements` and `required_integrations` directly in the `@pipeline` decorator. Instead, these should now be specified through `DockerSettings`. 

**Migration**: Remove the parameters from the decorator and use `DockerSettings` for configuration.

```python
from zenml.config import DockerSettings

@step(settings={"docker": DockerSettings(requirements=[...], requirements_integrations=[...])})
def my_step() -> None:
  ...
```

### Summary of Documentation

**New Pipeline Intermediate Representation**  
ZenML now utilizes an intermediate representation called `PipelineDeployment` to consolidate configurations and additional information for running pipelines. All orchestrators and step operators will now reference this representation instead of the previous `BaseStep` and `BasePipeline` classes.

**Migration Guidance**  
For users with custom orchestrators or step operators, adjustments should be made according to the new base abstractions provided in the documentation.

**Unique Pipeline Identification**  
Once executed, a pipeline is represented by a `PipelineSpec`, preventing further edits. Users can manage this by:
- Creating `unlisted` runs not explicitly associated with a pipeline.
- Deleting and recreating pipelines.
- Assigning unique names to pipelines for each run.

**Post-Execution Workflow Changes**  
The `get_pipelines` and `get_pipeline` methods have been relocated from the `Repository` (now `Client`) class to the post-execution module. Users must adapt to this new structure for accessing pipeline information.

```python
from zenml.post_execution import get_pipelines, get_pipeline
```

New methods `get_run` and `get_unlisted_runs` have been introduced for retrieving runs, replacing the previous `Repository.get_pipelines` and `Repository.get_pipeline_run` methods. For migration guidance, refer to the [new docs for post-execution](../../user-guide/starter-guide/pipelines/fetching-pipelines.md).

### Future Changes
- The secrets manager stack component may be removed from the stack.
- The ZenML `StepContext` may be deprecated.

### Reporting Bugs
For any issues or bugs, contact the ZenML core team via the [Slack community](https://zenml.io/slack) or submit a [GitHub Issue](https://github.com/zenml-io/zenml/issues/new/choose). Feature requests can be added to the [public feature voting board](https://zenml.io/discussion), and users are encouraged to upvote existing features.



================================================================================

# docs/book/how-to/manage-zenml-server/migration-guide/migration-zero-forty.md

### Migration Guide: ZenML 0.39.1 to 0.41.0

ZenML versions 0.40.0 and 0.41.0 introduced a new syntax for defining steps and pipelines. This guide provides code samples for upgrading to the new syntax. 

**Important Note:** While the old syntax is still supported, it is deprecated and will be removed in future releases. 

#### Overview
{% tabs %}
{% tab title="Old Syntax" %}


```python
from typing import Optional

from zenml.steps import BaseParameters, Output, StepContext, step
from zenml.pipelines import pipeline

# Define a Step
class MyStepParameters(BaseParameters):
    param_1: int
    param_2: Optional[float] = None

@step
def my_step(
    params: MyStepParameters, context: StepContext,
) -> Output(int_output=int, str_output=str):
    result = int(params.param_1 * (params.param_2 or 1))
    result_uri = context.get_output_artifact_uri()
    return result, result_uri

# Run the Step separately
my_step.entrypoint()

# Define a Pipeline
@pipeline
def my_pipeline(my_step):
    my_step()

step_instance = my_step(params=MyStepParameters(param_1=17))
pipeline_instance = my_pipeline(my_step=step_instance)

# Configure and run the Pipeline
pipeline_instance.configure(enable_cache=False)
schedule = Schedule(...)
pipeline_instance.run(schedule=schedule)

# Fetch the Pipeline Run
last_run = pipeline_instance.get_runs()[0]
int_output = last_run.get_step["my_step"].outputs["int_output"].read()
```

The provided text appears to be a fragment from a documentation that includes a tab titled "New Syntax." However, there is no additional content or context provided to summarize. Please provide the complete documentation text for an accurate summary.

```python
from typing import Annotated, Optional, Tuple

from zenml import get_step_context, pipeline, step
from zenml.client import Client

# Define a Step
@step
def my_step(
    param_1: int, param_2: Optional[float] = None
) -> Tuple[Annotated[int, "int_output"], Annotated[str, "str_output"]]:
    result = int(param_1 * (param_2 or 1))
    result_uri = get_step_context().get_output_artifact_uri()
    return result, result_uri

# Run the Step separately
my_step()

# Define a Pipeline
@pipeline
def my_pipeline():
    my_step(param_1=17)

# Configure and run the Pipeline
my_pipeline = my_pipeline.with_options(enable_cache=False, schedule=schedule)
my_pipeline()

# Fetch the Pipeline Run
last_run = my_pipeline.last_run
int_output = last_run.steps["my_step"].outputs["int_output"].load()
```

The documentation outlines the process of defining steps, contrasting old syntax with new syntax. It emphasizes the importance of updating to the new syntax for improved functionality and clarity. Key points include:

- **Old Syntax**: Details on the previous method of defining steps, including specific examples and limitations.
- **New Syntax**: Introduction of the updated format, highlighting enhancements and best practices.
- **Migration Guidance**: Instructions for transitioning from old to new syntax, ensuring compatibility and efficiency.

Overall, the documentation serves as a guide for users to adapt to the new syntax while retaining essential technical information.

```python
from zenml.steps import step, BaseParameters
from zenml.pipelines import pipeline

# Old: Subclass `BaseParameters` to define parameters for a step
class MyStepParameters(BaseParameters):
    param_1: int
    param_2: Optional[float] = None

@step
def my_step(params: MyStepParameters) -> None:
    ...

@pipeline
def my_pipeline(my_step):
    my_step()

step_instance = my_step(params=MyStepParameters(param_1=17))
pipeline_instance = my_pipeline(my_step=step_instance)
```

It seems that the text you provided is incomplete and only contains a tab marker without any actual content. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

```python
# New: Directly define the parameters as arguments of your step function.
# In case you still want to group your parameters in a separate class,
# you can subclass `pydantic.BaseModel` and use that as an argument of your
# step function
from zenml import pipeline, step

@step
def my_step(param_1: int, param_2: Optional[float] = None) -> None:
    ...

@pipeline
def my_pipeline():
    my_step(param_1=17)
```

The documentation discusses how to parameterize steps in pipelines. For detailed guidance, refer to the provided link. It also covers the method for calling a step outside of a pipeline, with a section dedicated to the old syntax.

```python
from zenml.steps import step

@step
def my_step() -> None:
    ...

my_step.entrypoint()  # Old: Call `step.entrypoint(...)`
```

The provided text appears to be a fragment of documentation with a tab structure, specifically titled "New Syntax." However, without additional content or context, I cannot summarize the technical information or key points. Please provide the complete text or additional details for an accurate summary.

```python
from zenml import step

@step
def my_step() -> None:
    ...

my_step()  # New: Call the step directly `step(...)`
```

The documentation discusses defining pipelines, highlighting the use of an "Old Syntax." Specific details regarding the syntax and its application are provided within the context of pipeline creation. Further information on the new syntax and additional features may follow in subsequent sections.

```python
from zenml.pipelines import pipeline

@pipeline
def my_pipeline(my_step):  # Old: steps are arguments of the pipeline function
    my_step()
```

It appears that the provided text is incomplete and only contains a tab title without any content. Please provide the full documentation text that you would like summarized, and I will be happy to assist you.

```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline():
    my_step()  # New: The pipeline function calls the step directly
```

## Configuring Pipelines

### Old Syntax
- Details on the old syntax for configuring pipelines are provided here. 

(Note: The provided text is incomplete, and further details on the old syntax are needed for a more comprehensive summary.)

```python
from zenml.pipelines import pipeline
from zenml.steps import step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline(my_step):
    my_step()

# Old: Create an instance of the pipeline and then call `pipeline_instance.configure(...)`
pipeline_instance = my_pipeline(my_step=my_step())
pipeline_instance.configure(enable_cache=False)
```

It seems that the text you provided is incomplete and only contains a tab indicator without any actual content. Please provide the full documentation text that you would like summarized, and I'll be happy to help!

```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline():
    my_step()

# New: Call the `with_options(...)` method on the pipeline
my_pipeline = my_pipeline.with_options(enable_cache=False)
```

The documentation provides guidance on running pipelines, detailing two syntax options: Old Syntax and New Syntax. Key points include:

- **Old Syntax**: Instructions and examples for executing pipelines using the previous syntax format.
- **New Syntax**: Updated methods and best practices for running pipelines, emphasizing improvements and enhancements over the old syntax.

Ensure to follow the specific syntax guidelines for optimal pipeline execution.

```python
from zenml.pipelines import pipeline
from zenml.steps import step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline(my_step):
    my_step()

# Old: Create an instance of the pipeline and then call `pipeline_instance.run(...)`
pipeline_instance = my_pipeline(my_step=my_step())
pipeline_instance.run(...)
```

The provided text appears to be a fragment of documentation related to a "New Syntax" but does not contain any specific content to summarize. Please provide the complete text or additional details for an accurate summary.

```python
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline():
    my_step()

my_pipeline()  # New: Call the pipeline
```

The documentation discusses scheduling pipelines, highlighting two syntax options: Old Syntax and New Syntax. It provides details on how to implement scheduling effectively, ensuring that users can choose the appropriate method based on their requirements. Key points include the configuration settings, execution intervals, and any prerequisites necessary for successful pipeline scheduling. Users are encouraged to transition to the New Syntax for improved functionality and support.

```python
from zenml.pipelines import pipeline, Schedule
from zenml.steps import step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline(my_step):
    my_step()

# Old: Create an instance of the pipeline and then call `pipeline_instance.run(schedule=...)`
schedule = Schedule(...)
pipeline_instance = my_pipeline(my_step=my_step())
pipeline_instance.run(schedule=schedule)
```

The provided text appears to be incomplete and does not contain any specific documentation content to summarize. Please provide the full documentation text for summarization.

```python
from zenml.pipelines import Schedule
from zenml import pipeline, step

@step
def my_step() -> None:
    ...

@pipeline
def my_pipeline():
    my_step()

# New: Set the schedule using the `pipeline.with_options(...)` method and then run it
schedule = Schedule(...)
my_pipeline = my_pipeline.with_options(schedule=schedule)
my_pipeline()
```

For detailed instructions on scheduling pipelines, refer to [this page](../../pipeline-development/build-pipelines/schedule-a-pipeline.md). 

### Fetching Pipelines After Execution

#### Old Syntax


```python
pipeline: PipelineView = zenml.post_execution.get_pipeline("first_pipeline")

last_run: PipelineRunView = pipeline.runs[0]
# OR: last_run = my_pipeline.get_runs()[0]

model_trainer_step: StepView = last_run.get_step("model_trainer")

model: ArtifactView = model_trainer_step.output
loaded_model = model.read()
```

It appears that the text you provided is incomplete, as it only contains a tab title without any accompanying content. Please provide the full documentation text you would like summarized, and I'll be happy to assist!

```python
pipeline: PipelineResponseModel = zenml.client.Client().get_pipeline("first_pipeline")
# OR: pipeline = pipeline_instance.model

last_run: PipelineRunResponseModel = pipeline.last_run  
# OR: last_run = pipeline.runs[0] 
# OR: last_run = pipeline.get_runs(custom_filters)[0] 
# OR: last_run = pipeline.last_successful_run

model_trainer_step: StepRunResponseModel = last_run.steps["model_trainer"]

model: ArtifactResponseModel = model_trainer_step.output
loaded_model = model.load()
```

The documentation provides guidance on programmatically fetching information about previous pipeline runs. For more details, refer to the specified page. It also discusses controlling the step execution order, with a section dedicated to the "Old Syntax."

```python
from zenml.pipelines import pipeline

@pipeline
def my_pipeline(step_1, step_2, step_3):
    step_1()
    step_2()
    step_3()
    step_3.after(step_1)  # Old: Use the `step.after(...)` method
    step_3.after(step_2)
```

It seems that the provided text is incomplete and only contains a tab indicator without any actual content. Please provide the full documentation text you would like summarized, and I will be happy to assist you.

```python
from zenml import pipeline

@pipeline
def my_pipeline():
    step_1()
    step_2()
    step_3(after=["step_1", "step_2"])  # New: Pass the `after` argument when calling a step
```

The documentation provides guidance on controlling the execution order of steps in pipeline development. For detailed instructions, refer to the linked page on controlling the step execution order. Additionally, it introduces the concept of defining steps that produce multiple outputs. The section includes a comparison of the old syntax for defining these steps.

```python
# Old: Use the `Output` class
from zenml.steps import step, Output

@step
def my_step() -> Output(int_output=int, str_output=str):
    ...
```

The provided text appears to be a fragment of documentation that includes a tab titled "New Syntax." However, there is no content available to summarize. Please provide the complete documentation text for an accurate summary.

```python
# New: Use a `Tuple` annotation and optionally assign custom output names
from typing_extensions import Annotated
from typing import Tuple
from zenml import step

# Default output names `output_0`, `output_1`
@step
def my_step() -> Tuple[int, str]:
    ...

# Custom output names
@step
def my_step() -> Tuple[
    Annotated[int, "int_output"],
    Annotated[str, "str_output"],
]:
    ...
```

The documentation provides guidance on annotating step outputs in pipeline development. For detailed instructions, refer to the specified page on step output typing and annotation. Additionally, it mentions accessing run information within steps, with a section dedicated to the old syntax.

```python
from zenml.steps import StepContext, step
from zenml.environment import Environment

@step
def my_step(context: StepContext) -> Any:  # Old: `StepContext` class defined as arg
    env = Environment().step_environment
    output_uri = context.get_output_artifact_uri()
    step_name = env.step_name  # Old: Run info accessible via `StepEnvironment`
    ...
```

The provided text appears to be incomplete and does not contain any specific content to summarize. Please provide the full documentation text for me to summarize effectively.

```python
from zenml import get_step_context, step

@step
def my_step() -> Any:  # New: StepContext is no longer an argument of the step
    context = get_step_context()
    output_uri = context.get_output_artifact_uri()
    step_name = context.step_name  # New: StepContext now has ALL run/step info
    ...
```

For detailed instructions on fetching run information within your steps, refer to the page on using `get_step_context()`.



================================================================================

# docs/book/how-to/manage-zenml-server/migration-guide/migration-guide.md

# ZenML Migration Guide

Migrations are required for ZenML releases with breaking changes, specifically for minor version increments (e.g., `0.X` to `0.Y`). Major version increments introduce significant changes, detailed in separate migration guides.

## Release Type Examples
- **No Breaking Changes:** `0.40.2` to `0.40.3` (no migration needed)
- **Minor Breaking Changes:** `0.40.3` to `0.41.0` (migration required)
- **Major Breaking Changes:** `0.39.1` to `0.40.0` (significant shifts in usage)

## Major Migration Guides
Follow these guides sequentially for major version migrations:
- [0.13.2 → 0.20.0](migration-zero-twenty.md)
- [0.23.0 → 0.30.0](migration-zero-thirty.md)
- [0.39.1 → 0.41.0](migration-zero-forty.md)
- [0.58.2 → 0.60.0](migration-zero-sixty.md)

## Release Notes
For minor breaking changes (e.g., `0.40.3` to `0.41.0`), refer to the official [ZenML Release Notes](https://github.com/zenml-io/zenml/releases) for details on changes.



================================================================================
